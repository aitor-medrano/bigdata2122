{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales. Bloque Arquitecturas Big Data \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Arquitecturas Big Data Lunes 5 Nov 1p + 2o 2.- Cloud Computing Lunes 12 Nov 1p + 2o Bloque Ingesta de Datos \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- ETL xxx xxx Bloque Big Data Aplicado \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- xxx xxx xxx","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"index.html#bloque-arquitecturas-big-data","text":"","title":"Bloque Arquitecturas Big Data"},{"location":"index.html#resultados-de-aprendizaje","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Arquitecturas Big Data Lunes 5 Nov 1p + 2o 2.- Cloud Computing Lunes 12 Nov 1p + 2o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-ingesta-de-datos","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- ETL xxx xxx","title":"Bloque Ingesta de Datos"},{"location":"index.html#bloque-big-data-aplicado","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- xxx xxx xxx","title":"Bloque Big Data Aplicado"},{"location":"apuntes/arquitecturas01.html","text":"Arquitecturas Big Data \u00b6 Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. En esta sesi\u00f3n no vamos a entrar al detalle de ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas. Caracter\u00edsticas \u00b6 Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener. Tipos de arquitecturas \u00b6 Debido a que las empresas disponen de un volumen cada vez mayor de datos y a la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming. Procesamiento Batch \u00b6 Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extraci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante. Procesamiento en Streaming \u00b6 Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo. Arquitectura Lambda \u00b6 Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter, y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Para ello, se compone de tres capas: Capa batch : se encarga de (a) gestionar los datos hist\u00f3ricos y (b) recalcular los resultados, por ejemplo, de los modelos de machine learning . De manera espec\u00edfica, la capa batch recibe los datos, los combina con el historico existente y recalcula los resultados iterando sobre todo el conjunto de datos combinado. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: https://www.paradigmadigital.com/techbiz/de-lambda-a-kappa-evolucion-de-las-arquitecturas-big-data/ El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con baja latencia. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos. Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa Arquitectura Kappa \u00b6 https://www.treelogic.com/es/Arquitectura_Kappa.html El t\u00e9rmino Arquitectura Kappa, representada por la letra , fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture. En \u00e9l se\u00f1ala los posibles puntos \u201cd\u00e9biles\u201d de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Su propuesta consiste en eliminar la capa batch dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Como un proceso batch se puede entender como un stream acotado, podr\u00edamos decir que el procesamiento batch es un subconjunto del procesamiento en streaming. Esta evoluci\u00f3n consiste en una simplificaci\u00f3n de la Arquitectura Lambda, en la que se elimina la capa batch y todo el procesamiento se realiza en una sola capa denominada de tiempo real o Real-time Layer, dando soporte a procesamientos tanto batch como en tiempo real. Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream: las operaciones batch son un subconjunto de las operaciones de streaming, por lo que todo puede ser tratado como un stream. Los datos de partida no se modifican: los datos son almacenados sin ser transformados y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos. Casos de uso \u00b6 \u00bfQu\u00e9 arquitectura se adapta mejor a nuestro problema? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Por lo general, no existe una \u00fanica respuesta. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos para poder decidir es, \u00bfel an\u00e1lisis y el procesamiento que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la Arquitectura Kappa. Como ejemplo real de esta arquitectura podr\u00edamos poner un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Arquitectura Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia una Arquitectura Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende libros en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Para finalizar, hay que destacar lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir con nuestras soluciones Big Data, y eso supone que hay que adaptarse a ellos lo antes posible. Cada problema a resolver tiene unos condicionantes particulares y en muchos casos habr\u00e1 que evolucionar la arquitectura que est\u00e1bamos utilizando hasta el momento, o como se suele decir: \u201crenovarse o morir\u201d. Conclusi\u00f3n \u00b6 A la hora de elegir tecnolog\u00edas no te abrumes, ve poco a poco, aqu\u00ed te dejo consejos que te podr\u00e1n ayudar en tu dise\u00f1o: En la ingesta de informaci\u00f3n: eval\u00faa tus tipos de fuentes, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s queutilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. No desv\u00edes tu camino, hay cosas que debemos tener siempre presentes: Seguramente seg\u00fan vayas adquiriendo experiencia en el dise\u00f1o de arquitecturas Big Data, puedas ir aport\u00e1ndome ideas en este tema, pero por el momento desde mi punto de vista, estas son algunas de las principales respuestas que debes conocer antes de comenzar a dise\u00f1ar una soluci\u00f3n. Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad, .. realmente lo necesitas todo? Define tu arquitectura: \u00bfbatch o streaming?\u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes? Referencias \u00b6 Arquitectura Big Data: \u00bfen qu\u00e9 consiste y para qu\u00e9 se utiliza? Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa/ https://medium.com/dataprophet/4-big-data-architectures-data-streaming-lambda-architecture-kappa-architecture-and-unifield-d9bcbf711eb9","title":"1.- Arquitecturas"},{"location":"apuntes/arquitecturas01.html#arquitecturas-big-data","text":"Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. En esta sesi\u00f3n no vamos a entrar al detalle de ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas.","title":"Arquitecturas Big Data"},{"location":"apuntes/arquitecturas01.html#caracteristicas","text":"Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener.","title":"Caracter\u00edsticas"},{"location":"apuntes/arquitecturas01.html#tipos-de-arquitecturas","text":"Debido a que las empresas disponen de un volumen cada vez mayor de datos y a la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming.","title":"Tipos de arquitecturas"},{"location":"apuntes/arquitecturas01.html#procesamiento-batch","text":"Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extraci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante.","title":"Procesamiento Batch"},{"location":"apuntes/arquitecturas01.html#procesamiento-en-streaming","text":"Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo.","title":"Procesamiento en Streaming"},{"location":"apuntes/arquitecturas01.html#arquitectura-lambda","text":"Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter, y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Para ello, se compone de tres capas: Capa batch : se encarga de (a) gestionar los datos hist\u00f3ricos y (b) recalcular los resultados, por ejemplo, de los modelos de machine learning . De manera espec\u00edfica, la capa batch recibe los datos, los combina con el historico existente y recalcula los resultados iterando sobre todo el conjunto de datos combinado. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: https://www.paradigmadigital.com/techbiz/de-lambda-a-kappa-evolucion-de-las-arquitecturas-big-data/ El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con baja latencia. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos. Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa","title":"Arquitectura Lambda"},{"location":"apuntes/arquitecturas01.html#arquitectura-kappa","text":"https://www.treelogic.com/es/Arquitectura_Kappa.html El t\u00e9rmino Arquitectura Kappa, representada por la letra , fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture. En \u00e9l se\u00f1ala los posibles puntos \u201cd\u00e9biles\u201d de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Su propuesta consiste en eliminar la capa batch dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Como un proceso batch se puede entender como un stream acotado, podr\u00edamos decir que el procesamiento batch es un subconjunto del procesamiento en streaming. Esta evoluci\u00f3n consiste en una simplificaci\u00f3n de la Arquitectura Lambda, en la que se elimina la capa batch y todo el procesamiento se realiza en una sola capa denominada de tiempo real o Real-time Layer, dando soporte a procesamientos tanto batch como en tiempo real. Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream: las operaciones batch son un subconjunto de las operaciones de streaming, por lo que todo puede ser tratado como un stream. Los datos de partida no se modifican: los datos son almacenados sin ser transformados y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos.","title":"Arquitectura Kappa"},{"location":"apuntes/arquitecturas01.html#casos-de-uso","text":"\u00bfQu\u00e9 arquitectura se adapta mejor a nuestro problema? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Por lo general, no existe una \u00fanica respuesta. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos para poder decidir es, \u00bfel an\u00e1lisis y el procesamiento que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la Arquitectura Kappa. Como ejemplo real de esta arquitectura podr\u00edamos poner un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Arquitectura Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia una Arquitectura Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende libros en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Para finalizar, hay que destacar lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir con nuestras soluciones Big Data, y eso supone que hay que adaptarse a ellos lo antes posible. Cada problema a resolver tiene unos condicionantes particulares y en muchos casos habr\u00e1 que evolucionar la arquitectura que est\u00e1bamos utilizando hasta el momento, o como se suele decir: \u201crenovarse o morir\u201d.","title":"Casos de uso"},{"location":"apuntes/arquitecturas01.html#conclusion","text":"A la hora de elegir tecnolog\u00edas no te abrumes, ve poco a poco, aqu\u00ed te dejo consejos que te podr\u00e1n ayudar en tu dise\u00f1o: En la ingesta de informaci\u00f3n: eval\u00faa tus tipos de fuentes, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s queutilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. No desv\u00edes tu camino, hay cosas que debemos tener siempre presentes: Seguramente seg\u00fan vayas adquiriendo experiencia en el dise\u00f1o de arquitecturas Big Data, puedas ir aport\u00e1ndome ideas en este tema, pero por el momento desde mi punto de vista, estas son algunas de las principales respuestas que debes conocer antes de comenzar a dise\u00f1ar una soluci\u00f3n. Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad, .. realmente lo necesitas todo? Define tu arquitectura: \u00bfbatch o streaming?\u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes?","title":"Conclusi\u00f3n"},{"location":"apuntes/arquitecturas01.html#referencias","text":"Arquitectura Big Data: \u00bfen qu\u00e9 consiste y para qu\u00e9 se utiliza? Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa/ https://medium.com/dataprophet/4-big-data-architectures-data-streaming-lambda-architecture-kappa-architecture-and-unifield-d9bcbf711eb9","title":"Referencias"},{"location":"apuntes/arquitecturas02.html","text":"Cloud Computing \u00b6 La Nube \u00b6 Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet que hace que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Ventajas \u00b6 As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto. Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx. CapEx vs OpEx \u00b6 Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan costo previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costos asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las ventajas, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Coste total de propiedad \u00b6 El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memor\u00eda): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. Servicios en la nube \u00b6 Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. Todo REVISAR Definici\u00f3n y diferencias. Apuntes Azure https://www.ibm.com/es-es/cloud/learn/cloud-computing-gbl https://docs.microsoft.com/es-es/learn/modules/fundamental-azure-concepts/categories-of-cloud-services IaaS \u00b6 La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que obtener, instalar y configurar un servidor f\u00edsico. PaaS \u00b6 La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. SaaS \u00b6 Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Tanto las aplicaciones como los datos son accesibles desde cualquier sistema conectado. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa. Tipos de arquitectura seg\u00fan la infraestructura \u00b6 Arquitecturas on premise \u00b6 Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware. Arquitecturas cloud \u00b6 Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: p\u00fablica: los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. privada: los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de propia fibra o VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. Arquitecturas h\u00edbridas \u00b6 Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento o requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado, combinada con la integraci\u00f3n estrat\u00e9gica y el uso de servicios cloud p\u00fablico. En realidad, un cloud privado no puede existir aislado del resto de los recursos de TI de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionar\u00e1n para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Aspectos clave de cloud h\u00edbrido: Permite que todas las empresas mantengan las aplicaciones cr\u00edticas y los datos confidenciales en un entorno de centro de datos tradicional o en un cloud privado Permite beneficiarse de los recursos de cloud p\u00fablico como SaaS, para obtener las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible Facilita la portabilidad de datos, aplicaciones y servicios y otras opciones de modelos de despliegue Plataformas Cloud \u00b6 A continuaci\u00f3n, muestro una lista de servicios en la nube clasificados de acuerdo al modelo de servicio. Evidentemente, hay m\u00e1s ejemplos, hasta nos dar\u00eda para escribir un libro. As\u00ed que, mencionar\u00e9 los que probablemente sean m\u00e1s conocidos y m\u00e1s utilizados. Ejemplo de IaaS AWS Microsoft Azure Google Cloud Platform OpenStack Solo por mencionar algunos ejemplos espec\u00edficos de IaaS que encontramos con estas nubes, Amazon EC2 y las m\u00e1quinas virtuales de Azure, pero realmente el cat\u00e1logo de este tipo de servicios es amplio. Ejemplos de PaaS: AWS Elastic Beanstalk Azure App Service Google App Engine Red Hat OpenShift CloudFountry Heroku Ejemplos de SaaS: Microsoft Office 365 Aplicaciones web de Google Servicio de mensajer\u00eda Slack Proveedores de Cloud Computing La demanda de Cloud Computing es enorme, cada vez m\u00e1s empresas est\u00e1n migrando a la nube. En este apartado trataremos los principales proveedores de nube para que conozcas las alternativas disponibles y realizar tus implementaciones. Para estar en contexto, comparto un dato interesante de Synergy Research Group donde se muestra el crecimiento y el posicionamiento competitivo de los proveedores de nube p\u00fablica. Este informe es de octubre del 2020. Posicionamiento competitivo de los proveedores de nube Amazon Amazon Web Service (AWS) fue el primero en ofrecer a las empresas servicios de infraestructura en la nube, siendo un pionero es de los m\u00e1s conocidos y con mayor crecimiento. En la imagen anterior puedes comprobar que sigue en la primera posici\u00f3n. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Si entras a la consola de administraci\u00f3n de AWS, seguramente te sorprender\u00e1 la cantidad de servicios y herramientas disponibles, y lo m\u00e1s importante, no para de crecer. Microsoft Microsoft tard\u00f3 un poco m\u00e1s en entrar al mercado con su soluci\u00f3n, Microsoft Azure. No obstante, ha crecido r\u00e1pidamente y a gran escala. Al igual que AWS, es un proveedor de nube p\u00fablica que ofrece diferentes servicios. Una caracter\u00edstica importante, es que ofrece servicios en las tres capas principales de nube (IaaS, PaaS, SaaS). La competencia es muy buena porque se potencian, siempre intentando sacar nuevos productos o mejorar los existentes. Google Google tambi\u00e9n es un proveedor de nube p\u00fablica y ofrece soluciones Cloud Computing a trav\u00e9s de su plataforma, Google Cloud Platform (GCP). Parece que le ha costado entrar en la competencia con los otros proveedores, pero en los \u00faltimos a\u00f1os ha crecido de forma r\u00e1pida y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. OpenStack Seguramente has notado que OpenStack no aparece en la imagen anterior. Efectivamente no est\u00e1, solo hay proveedores de nube p\u00fablica. Por tanto, me parece importante mencionar a OpenStack en este apartado, ya que estamos tratando todos los tipos de cloud, si quieres implementar una nube privada, OpenStack es una gran opci\u00f3n. Es un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Herramientas de Cloud Computing En este apartado quiero aprovechar para mencionar algunas herramientas importantes para trabajar en la nube y sacarle el m\u00e1ximo provecho, principalmente las que ayudan automatizar procesos y tratar la infraestructura como c\u00f3digo. Hay muchas herramientas, algunas orientadas a un tipo de nube o proveedor especifico, sin embargo, quiero compartir las de mayor alcance, es decir, te valen para cualquier implementaci\u00f3n de un entorno con cualquier proveedor de nube. Si quieres explotar las ventajas de Cloud Computing, debes conocer y aprender a usar estas cuatro herramientas: Terraform Ansible Docker Kubernetes Heroku Infraestructura cloud \u00b6 Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas. Regiones y Zonas de disponibilidad \u00b6 A lo largo de todo el globo terr\u00e1queo, se han construido grandes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ). Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente. Despliegue \u00b6 Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions Seguridad \u00b6 Responsabilidad compartida - Azure Actividades \u00b6 Cuestionario de preguntas cortas, tipo test ... Realizar los m\u00f3dulos 1 (Informaci\u00f3n general sobre los conceptos de la nube) y 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . \u00bfcalculadora de costes? Referencias \u00b6 Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 https://openwebinars.net/blog/tipos-de-cloud-computing/","title":"2.- Cloud Computing"},{"location":"apuntes/arquitecturas02.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"apuntes/arquitecturas02.html#la-nube","text":"Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet que hace que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software.","title":"La Nube"},{"location":"apuntes/arquitecturas02.html#ventajas","text":"As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto. Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx.","title":"Ventajas"},{"location":"apuntes/arquitecturas02.html#capex-vs-opex","text":"Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan costo previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costos asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las ventajas, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten.","title":"CapEx vs OpEx"},{"location":"apuntes/arquitecturas02.html#coste-total-de-propiedad","text":"El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memor\u00eda): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n.","title":"Coste total de propiedad"},{"location":"apuntes/arquitecturas02.html#servicios-en-la-nube","text":"Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. Todo REVISAR Definici\u00f3n y diferencias. Apuntes Azure https://www.ibm.com/es-es/cloud/learn/cloud-computing-gbl https://docs.microsoft.com/es-es/learn/modules/fundamental-azure-concepts/categories-of-cloud-services","title":"Servicios en la nube"},{"location":"apuntes/arquitecturas02.html#iaas","text":"La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que obtener, instalar y configurar un servidor f\u00edsico.","title":"IaaS"},{"location":"apuntes/arquitecturas02.html#paas","text":"La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos.","title":"PaaS"},{"location":"apuntes/arquitecturas02.html#saas","text":"Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Tanto las aplicaciones como los datos son accesibles desde cualquier sistema conectado. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa.","title":"SaaS"},{"location":"apuntes/arquitecturas02.html#tipos-de-arquitectura-segun-la-infraestructura","text":"","title":"Tipos de arquitectura seg\u00fan la infraestructura"},{"location":"apuntes/arquitecturas02.html#arquitecturas-on-premise","text":"Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware.","title":"Arquitecturas on premise"},{"location":"apuntes/arquitecturas02.html#arquitecturas-cloud","text":"Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: p\u00fablica: los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. privada: los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de propia fibra o VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente.","title":"Arquitecturas cloud"},{"location":"apuntes/arquitecturas02.html#arquitecturas-hibridas","text":"Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento o requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado, combinada con la integraci\u00f3n estrat\u00e9gica y el uso de servicios cloud p\u00fablico. En realidad, un cloud privado no puede existir aislado del resto de los recursos de TI de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionar\u00e1n para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Aspectos clave de cloud h\u00edbrido: Permite que todas las empresas mantengan las aplicaciones cr\u00edticas y los datos confidenciales en un entorno de centro de datos tradicional o en un cloud privado Permite beneficiarse de los recursos de cloud p\u00fablico como SaaS, para obtener las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible Facilita la portabilidad de datos, aplicaciones y servicios y otras opciones de modelos de despliegue","title":"Arquitecturas h\u00edbridas"},{"location":"apuntes/arquitecturas02.html#plataformas-cloud","text":"A continuaci\u00f3n, muestro una lista de servicios en la nube clasificados de acuerdo al modelo de servicio. Evidentemente, hay m\u00e1s ejemplos, hasta nos dar\u00eda para escribir un libro. As\u00ed que, mencionar\u00e9 los que probablemente sean m\u00e1s conocidos y m\u00e1s utilizados. Ejemplo de IaaS AWS Microsoft Azure Google Cloud Platform OpenStack Solo por mencionar algunos ejemplos espec\u00edficos de IaaS que encontramos con estas nubes, Amazon EC2 y las m\u00e1quinas virtuales de Azure, pero realmente el cat\u00e1logo de este tipo de servicios es amplio. Ejemplos de PaaS: AWS Elastic Beanstalk Azure App Service Google App Engine Red Hat OpenShift CloudFountry Heroku Ejemplos de SaaS: Microsoft Office 365 Aplicaciones web de Google Servicio de mensajer\u00eda Slack Proveedores de Cloud Computing La demanda de Cloud Computing es enorme, cada vez m\u00e1s empresas est\u00e1n migrando a la nube. En este apartado trataremos los principales proveedores de nube para que conozcas las alternativas disponibles y realizar tus implementaciones. Para estar en contexto, comparto un dato interesante de Synergy Research Group donde se muestra el crecimiento y el posicionamiento competitivo de los proveedores de nube p\u00fablica. Este informe es de octubre del 2020. Posicionamiento competitivo de los proveedores de nube Amazon Amazon Web Service (AWS) fue el primero en ofrecer a las empresas servicios de infraestructura en la nube, siendo un pionero es de los m\u00e1s conocidos y con mayor crecimiento. En la imagen anterior puedes comprobar que sigue en la primera posici\u00f3n. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Si entras a la consola de administraci\u00f3n de AWS, seguramente te sorprender\u00e1 la cantidad de servicios y herramientas disponibles, y lo m\u00e1s importante, no para de crecer. Microsoft Microsoft tard\u00f3 un poco m\u00e1s en entrar al mercado con su soluci\u00f3n, Microsoft Azure. No obstante, ha crecido r\u00e1pidamente y a gran escala. Al igual que AWS, es un proveedor de nube p\u00fablica que ofrece diferentes servicios. Una caracter\u00edstica importante, es que ofrece servicios en las tres capas principales de nube (IaaS, PaaS, SaaS). La competencia es muy buena porque se potencian, siempre intentando sacar nuevos productos o mejorar los existentes. Google Google tambi\u00e9n es un proveedor de nube p\u00fablica y ofrece soluciones Cloud Computing a trav\u00e9s de su plataforma, Google Cloud Platform (GCP). Parece que le ha costado entrar en la competencia con los otros proveedores, pero en los \u00faltimos a\u00f1os ha crecido de forma r\u00e1pida y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. OpenStack Seguramente has notado que OpenStack no aparece en la imagen anterior. Efectivamente no est\u00e1, solo hay proveedores de nube p\u00fablica. Por tanto, me parece importante mencionar a OpenStack en este apartado, ya que estamos tratando todos los tipos de cloud, si quieres implementar una nube privada, OpenStack es una gran opci\u00f3n. Es un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Herramientas de Cloud Computing En este apartado quiero aprovechar para mencionar algunas herramientas importantes para trabajar en la nube y sacarle el m\u00e1ximo provecho, principalmente las que ayudan automatizar procesos y tratar la infraestructura como c\u00f3digo. Hay muchas herramientas, algunas orientadas a un tipo de nube o proveedor especifico, sin embargo, quiero compartir las de mayor alcance, es decir, te valen para cualquier implementaci\u00f3n de un entorno con cualquier proveedor de nube. Si quieres explotar las ventajas de Cloud Computing, debes conocer y aprender a usar estas cuatro herramientas: Terraform Ansible Docker Kubernetes Heroku","title":"Plataformas Cloud"},{"location":"apuntes/arquitecturas02.html#infraestructura-cloud","text":"Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas.","title":"Infraestructura cloud"},{"location":"apuntes/arquitecturas02.html#regiones-y-zonas-de-disponibilidad","text":"A lo largo de todo el globo terr\u00e1queo, se han construido grandes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ). Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente.","title":"Regiones y Zonas de disponibilidad"},{"location":"apuntes/arquitecturas02.html#despliegue","text":"Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions","title":"Despliegue"},{"location":"apuntes/arquitecturas02.html#seguridad","text":"Responsabilidad compartida - Azure","title":"Seguridad"},{"location":"apuntes/arquitecturas02.html#actividades","text":"Cuestionario de preguntas cortas, tipo test ... Realizar los m\u00f3dulos 1 (Informaci\u00f3n general sobre los conceptos de la nube) y 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . \u00bfcalculadora de costes?","title":"Actividades"},{"location":"apuntes/arquitecturas02.html#referencias","text":"Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 https://openwebinars.net/blog/tipos-de-cloud-computing/","title":"Referencias"},{"location":"apuntes/arquitecturas03.html","text":"Amazon Web Services \u00b6 Los servicios de AWS se clasifican en categor\u00edas (ver diapositivas del m\u00f3dulo 1 de AWS) Seguridad en AWS \u00b6 AWS Organizations: Administraci\u00f3n de cuentas basada en pol\u00edticas Administraci\u00f3n de cuentas basada en grupos Interfaces de programaci\u00f3n de aplicaciones (API) que automatizan la administraci\u00f3n de cuentas Facturaci\u00f3n unificada Controle el acceso con AWS Identity and Access Management (IAM) Las pol\u00edticas de IAM le permiten otorgar o denegar el acceso a los servicios de AWS a usuarios, grupos y funciones. Las pol\u00edticas de control de servicios (SCP) le permiten otorgar o denegar el acceso a los servicios de AWS a cuentas individuales o grupales en una unidad organizativa (OU). Configuraci\u00f3n de organizaciones: Crear una organizaci\u00f3n Crear unidades organizativas Crear pol\u00edticas de control de servicios Realizar la prueba de restricciones Actividades \u00b6 Cuestionario de preguntas cortas, tipo test ... Realizar los m\u00f3dulos 3 (Informaci\u00f3n general sobre la infraestructura global de AWS), 4 (Seguridad en la nube) y 5 (Redes y entrega de contenido) del curso ACF de AWS . Referencias \u00b6 Overview of Amazon Web Services","title":"3.- AWS"},{"location":"apuntes/arquitecturas03.html#amazon-web-services","text":"Los servicios de AWS se clasifican en categor\u00edas (ver diapositivas del m\u00f3dulo 1 de AWS)","title":"Amazon Web Services"},{"location":"apuntes/arquitecturas03.html#seguridad-en-aws","text":"AWS Organizations: Administraci\u00f3n de cuentas basada en pol\u00edticas Administraci\u00f3n de cuentas basada en grupos Interfaces de programaci\u00f3n de aplicaciones (API) que automatizan la administraci\u00f3n de cuentas Facturaci\u00f3n unificada Controle el acceso con AWS Identity and Access Management (IAM) Las pol\u00edticas de IAM le permiten otorgar o denegar el acceso a los servicios de AWS a usuarios, grupos y funciones. Las pol\u00edticas de control de servicios (SCP) le permiten otorgar o denegar el acceso a los servicios de AWS a cuentas individuales o grupales en una unidad organizativa (OU). Configuraci\u00f3n de organizaciones: Crear una organizaci\u00f3n Crear unidades organizativas Crear pol\u00edticas de control de servicios Realizar la prueba de restricciones","title":"Seguridad en AWS"},{"location":"apuntes/arquitecturas03.html#actividades","text":"Cuestionario de preguntas cortas, tipo test ... Realizar los m\u00f3dulos 3 (Informaci\u00f3n general sobre la infraestructura global de AWS), 4 (Seguridad en la nube) y 5 (Redes y entrega de contenido) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/arquitecturas03.html#referencias","text":"Overview of Amazon Web Services","title":"Referencias"},{"location":"apuntes/arquitecturas04.html","text":"AWS EC2 \u00b6 Referencias \u00b6 aaa","title":"4.- AWS EC2"},{"location":"apuntes/arquitecturas04.html#aws-ec2","text":"","title":"AWS EC2"},{"location":"apuntes/arquitecturas04.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/arquitecturas05.html","text":"Almacenamiento distribuido: AWS S3 \u00b6 Referencias \u00b6 aaa","title":"5.- AWS S3"},{"location":"apuntes/arquitecturas05.html#almacenamiento-distribuido-aws-s3","text":"","title":"Almacenamiento distribuido: AWS S3"},{"location":"apuntes/arquitecturas05.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/arquitecturas06.html","text":"Azure \u00b6 Referencias \u00b6 aaa","title":"6.- Azure"},{"location":"apuntes/arquitecturas06.html#azure","text":"","title":"Azure"},{"location":"apuntes/arquitecturas06.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado01.html","text":"Hadoop \u00b6 Las nuevas tecnolog\u00edas como Hadoop y Spark facilitan el trabajo y la gesti\u00f3n de un cluster de ordenadores. Hadoop puede escalar hasta miles de ordenadores creando un cluster con un almacenamiento con un orden de petabytes de informaci\u00f3n. Estas tecnolog\u00edas son las que realmente catalizan el Big Data . Apache Hadoop ( http://hadoop.apache.org/ ) es un framework que facilita el trabajo con un cluster de ordenadores. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Escalable: los datos y su procesamiento se distribuyen sobre un cluster de ordenadores (escalado horizontal) Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Componentes \u00b6 El n\u00facleo se compone de: un sistema de ficheros distribuidos ( HDFS ). un gestor de recursos para el manejo del cluster ( YARN ) un sistema para ejecutar programas distribuidos a gran escala ( MapReduce ) Estos elementos permiten trabajar de casi la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. MapReduce \u00b6 Es el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. El siguiente gr\u00e1fico muestra un ejemplo de una empresa de juguete que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color ha de preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura de los ficheros de entrada. Pasar cada linea de forma separada al mapeador. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan las claves. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cluster funcional no es una cosa trivial. Existen gestores de clusters que hacen las cosas un poco menos incom\u00f3das (como son YARN o Apache Mesos), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso del framework Spark, que mejora el rendimiento por una orden de magnitud. Referencias \u00b6 aaa","title":"1.- Hadoop"},{"location":"apuntes/bdaplicado01.html#hadoop","text":"Las nuevas tecnolog\u00edas como Hadoop y Spark facilitan el trabajo y la gesti\u00f3n de un cluster de ordenadores. Hadoop puede escalar hasta miles de ordenadores creando un cluster con un almacenamiento con un orden de petabytes de informaci\u00f3n. Estas tecnolog\u00edas son las que realmente catalizan el Big Data . Apache Hadoop ( http://hadoop.apache.org/ ) es un framework que facilita el trabajo con un cluster de ordenadores. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Escalable: los datos y su procesamiento se distribuyen sobre un cluster de ordenadores (escalado horizontal) Portable: se puede instalar en todo tipos de hardware y sistemas operativos.","title":"Hadoop"},{"location":"apuntes/bdaplicado01.html#componentes","text":"El n\u00facleo se compone de: un sistema de ficheros distribuidos ( HDFS ). un gestor de recursos para el manejo del cluster ( YARN ) un sistema para ejecutar programas distribuidos a gran escala ( MapReduce ) Estos elementos permiten trabajar de casi la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop.","title":"Componentes"},{"location":"apuntes/bdaplicado01.html#mapreduce","text":"Es el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. El siguiente gr\u00e1fico muestra un ejemplo de una empresa de juguete que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color ha de preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura de los ficheros de entrada. Pasar cada linea de forma separada al mapeador. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan las claves. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cluster funcional no es una cosa trivial. Existen gestores de clusters que hacen las cosas un poco menos incom\u00f3das (como son YARN o Apache Mesos), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso del framework Spark, que mejora el rendimiento por una orden de magnitud.","title":"MapReduce"},{"location":"apuntes/bdaplicado01.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado02.html","text":"HDFS \u00b6 Al almacenar datos en HDFS, una buena pr\u00e1ctica es crear un archivo con las cabeceras y otro con los datos Referencias \u00b6 aaa","title":"2.- HDFS"},{"location":"apuntes/bdaplicado02.html#hdfs","text":"Al almacenar datos en HDFS, una buena pr\u00e1ctica es crear un archivo con las cabeceras y otro con los datos","title":"HDFS"},{"location":"apuntes/bdaplicado02.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado03.html","text":"Hive \u00b6 Referencias \u00b6 aaa","title":"3.- Hive"},{"location":"apuntes/bdaplicado03.html#hive","text":"","title":"Hive"},{"location":"apuntes/bdaplicado03.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado04.html","text":"Flume / Sqoop \u00b6 Flume \u00b6 All\u00e1 por el a\u00f1o 2010 se Cloudera present\u00f3 Flume, programa para tratamiento e ingesta de datos masivo. Esto daba la posibilidad de crear desarrollos complejos que permitieran el tratamiento de datos masivos creados en Tiempo Real. En Real Time se utiliza en la etapa de Obtenci\u00f3n de Datos (para conectarnos a fuentes de datos Online). Su arquitectura es sencilla, pues tiene tres componentes principales, muy configurables: Source: Fuente de origen de los datos Channel: la v\u00eda por donde se tratar\u00e1n los datos Sink: persistencia/movimiento de los datos Flume es sencillito apriori, el problema es cuando quieres utilizarlo para obtener datos de manera paralela (o multiplexada) y adem\u00e1s te ves en la necesidad de crear tus propios Sinks, o tus propios interceptores. Entonces la cosa cambia y hay que dedicarle algo m\u00e1s de tiempo. Muy recomendada como ayuda|compa\u00f1ero|alternativa a herramientas como Kettle. Referencias \u00b6 aaa","title":"4.- Flume / Sqoop"},{"location":"apuntes/bdaplicado04.html#flume-sqoop","text":"","title":"Flume / Sqoop"},{"location":"apuntes/bdaplicado04.html#flume","text":"All\u00e1 por el a\u00f1o 2010 se Cloudera present\u00f3 Flume, programa para tratamiento e ingesta de datos masivo. Esto daba la posibilidad de crear desarrollos complejos que permitieran el tratamiento de datos masivos creados en Tiempo Real. En Real Time se utiliza en la etapa de Obtenci\u00f3n de Datos (para conectarnos a fuentes de datos Online). Su arquitectura es sencilla, pues tiene tres componentes principales, muy configurables: Source: Fuente de origen de los datos Channel: la v\u00eda por donde se tratar\u00e1n los datos Sink: persistencia/movimiento de los datos Flume es sencillito apriori, el problema es cuando quieres utilizarlo para obtener datos de manera paralela (o multiplexada) y adem\u00e1s te ves en la necesidad de crear tus propios Sinks, o tus propios interceptores. Entonces la cosa cambia y hay que dedicarle algo m\u00e1s de tiempo. Muy recomendada como ayuda|compa\u00f1ero|alternativa a herramientas como Kettle.","title":"Flume"},{"location":"apuntes/bdaplicado04.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado05.html","text":"Kafka \u00b6 Sencillamente es un servicio de commit log, particionado, replicado y distribuido. En su arquitectura encontramos que disponemos de un modelo Productor/Consumidor, cuyos mensajes se pueden categorizar en algo llamado topics y que funciona como si fuera un cluster. Se suele utilizar como gestor de colas. Se utiliza en la etapa de Almacenamiento de Datos. Referencias \u00b6 aaa","title":"5.- Kafka"},{"location":"apuntes/bdaplicado05.html#kafka","text":"Sencillamente es un servicio de commit log, particionado, replicado y distribuido. En su arquitectura encontramos que disponemos de un modelo Productor/Consumidor, cuyos mensajes se pueden categorizar en algo llamado topics y que funciona como si fuera un cluster. Se suele utilizar como gestor de colas. Se utiliza en la etapa de Almacenamiento de Datos.","title":"Kafka"},{"location":"apuntes/bdaplicado05.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0601.html","text":"Spark \u00b6 Spark es un framework de computaci\u00f3n en cluster similar a MapReduce , pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.1.-Trabajando con Spark"},{"location":"apuntes/bdaplicado0601.html#spark","text":"Spark es un framework de computaci\u00f3n en cluster similar a MapReduce , pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0601.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0602.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.2.-Spark RDD"},{"location":"apuntes/bdaplicado0602.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0602.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0603.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.3.-Spark Avanzado"},{"location":"apuntes/bdaplicado0603.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0603.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0604.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.4.-Spark SQL"},{"location":"apuntes/bdaplicado0604.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0604.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0605.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.5.-Spark Streaming I"},{"location":"apuntes/bdaplicado0605.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0605.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/bdaplicado0606.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"6.6.-Spark Streaming II"},{"location":"apuntes/bdaplicado0606.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"apuntes/bdaplicado0606.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta01.html","text":"Ingesta de Datos \u00b6 Ingesta de datos \u00b6 https://www.futurespace.es/ingesta-es-mas-que-una-mudanza-de-datos/ ETL \u00b6 https://www.franciscojavierpulido.com/2013/11/paradigmas-bigdata-el-procesamiento.html Tipos de datos \u00b6 Parquet Referencias \u00b6 aaa","title":"1.- ETL"},{"location":"apuntes/ingesta01.html#ingesta-de-datos","text":"","title":"Ingesta de Datos"},{"location":"apuntes/ingesta01.html#ingesta-de-datos_1","text":"https://www.futurespace.es/ingesta-es-mas-que-una-mudanza-de-datos/","title":"Ingesta de datos"},{"location":"apuntes/ingesta01.html#etl","text":"https://www.franciscojavierpulido.com/2013/11/paradigmas-bigdata-el-procesamiento.html","title":"ETL"},{"location":"apuntes/ingesta01.html#tipos-de-datos","text":"Parquet","title":"Tipos de datos"},{"location":"apuntes/ingesta01.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta02.html","text":"ETL con SQL / Python \u00b6 Referencias \u00b6 aaa","title":"2.- ETL con SQL/Python"},{"location":"apuntes/ingesta02.html#etl-con-sql-python","text":"","title":"ETL con SQL / Python"},{"location":"apuntes/ingesta02.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta03.html","text":"Pentaho \u00b6 Referencias \u00b6 aaa","title":"3.- Pentaho"},{"location":"apuntes/ingesta03.html#pentaho","text":"","title":"Pentaho"},{"location":"apuntes/ingesta03.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta04.html","text":"Pentaho multidimensional \u00b6 Referencias \u00b6 aaa","title":"4.- Pentaho multidimensional"},{"location":"apuntes/ingesta04.html#pentaho-multidimensional","text":"","title":"Pentaho multidimensional"},{"location":"apuntes/ingesta04.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta05.html","text":"Nifi I \u00b6 https://www.futurespace.es/cafe-con-iot-capitulo-1-los-flujos-de-nifi/ https://www.futurespace.es/apache-nifi-validando-transferencias-de-ficheros-caso-de-uso-real/ https://www.futurespace.es/apache-nifi-flujo-de-extraccion-validacion-transformacion-y-carga-de-ficheros-caso-de-uso-real/ Referencias \u00b6 aaa","title":"5.- Nifi I"},{"location":"apuntes/ingesta05.html#nifi-i","text":"https://www.futurespace.es/cafe-con-iot-capitulo-1-los-flujos-de-nifi/ https://www.futurespace.es/apache-nifi-validando-transferencias-de-ficheros-caso-de-uso-real/ https://www.futurespace.es/apache-nifi-flujo-de-extraccion-validacion-transformacion-y-carga-de-ficheros-caso-de-uso-real/","title":"Nifi I"},{"location":"apuntes/ingesta05.html#referencias","text":"aaa","title":"Referencias"},{"location":"apuntes/ingesta06.html","text":"Nifi II \u00b6 Referencias \u00b6 aaa","title":"6.- Nifi II"},{"location":"apuntes/ingesta06.html#nifi-ii","text":"","title":"Nifi II"},{"location":"apuntes/ingesta06.html#referencias","text":"aaa","title":"Referencias"},{"location":"ejercicios/ej-arquitecturas.html","text":"Ejercicios \u00b6 Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, using Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive Bibliograf\u00eda \u00b6 Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Ejercicios"},{"location":"ejercicios/ej-arquitecturas.html#ejercicios","text":"Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, using Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive","title":"Ejercicios"},{"location":"ejercicios/ej-arquitecturas.html#bibliografia","text":"Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Bibliograf\u00eda"},{"location":"ejercicios/ej-bdaplicado.html","text":"Ejercicios \u00b6 Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, usign Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive Bibliograf\u00eda \u00b6 En Moodle se publican los apuntes de la asignatura, con ejercicios, explicaciones y ejemplos de todos los conceptos estudiados, tanto en teor\u00eda como en pr\u00e1ctica. Para ampliar algunos conceptos se recomiendan las siguientes referencias: Harold Abelson y Gerald Jay Sussman, Structure and Interpretation of Computer Programs , MIT Press, 1996 Enlace a la edici\u00f3n on-line Signatura en la Biblioteca Polit\u00e9cnica: I.06/ABE/STR Apple, The Swift Programming Language The Racket Guide Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Ejercicios"},{"location":"ejercicios/ej-bdaplicado.html#ejercicios","text":"Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, usign Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive","title":"Ejercicios"},{"location":"ejercicios/ej-bdaplicado.html#bibliografia","text":"En Moodle se publican los apuntes de la asignatura, con ejercicios, explicaciones y ejemplos de todos los conceptos estudiados, tanto en teor\u00eda como en pr\u00e1ctica. Para ampliar algunos conceptos se recomiendan las siguientes referencias: Harold Abelson y Gerald Jay Sussman, Structure and Interpretation of Computer Programs , MIT Press, 1996 Enlace a la edici\u00f3n on-line Signatura en la Biblioteca Polit\u00e9cnica: I.06/ABE/STR Apple, The Swift Programming Language The Racket Guide Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Bibliograf\u00eda"},{"location":"ejercicios/ej-ingesta.html","text":"Ejercicios \u00b6 Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, usign Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive Bibliograf\u00eda \u00b6 En Moodle se publican los apuntes de la asignatura, con ejercicios, explicaciones y ejemplos de todos los conceptos estudiados, tanto en teor\u00eda como en pr\u00e1ctica. Para ampliar algunos conceptos se recomiendan las siguientes referencias: Harold Abelson y Gerald Jay Sussman, Structure and Interpretation of Computer Programs , MIT Press, 1996 Enlace a la edici\u00f3n on-line Signatura en la Biblioteca Polit\u00e9cnica: I.06/ABE/STR Apple, The Swift Programming Language The Racket Guide Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Ejercicios"},{"location":"ejercicios/ej-ingesta.html#ejercicios","text":"Propuesta de proyecto: Libro Manning \"Introducing Data Science - Big Data, ML and more, usign Python Apartado 5.2 (pg 125) Herramientas: Hadoop + Spark + Hive + PowerBI Descargar csv de internet mediante Python y Panda Meter dentro de HDFS Utilizar Spark para hacer la limpieza Almacenar los datos con SparkSQL en Hive","title":"Ejercicios"},{"location":"ejercicios/ej-ingesta.html#bibliografia","text":"En Moodle se publican los apuntes de la asignatura, con ejercicios, explicaciones y ejemplos de todos los conceptos estudiados, tanto en teor\u00eda como en pr\u00e1ctica. Para ampliar algunos conceptos se recomiendan las siguientes referencias: Harold Abelson y Gerald Jay Sussman, Structure and Interpretation of Computer Programs , MIT Press, 1996 Enlace a la edici\u00f3n on-line Signatura en la Biblioteca Polit\u00e9cnica: I.06/ABE/STR Apple, The Swift Programming Language The Racket Guide Lenguajes y Paradigmas de Programaci\u00f3n, curso 2020-21 \u00a9 Departamento Ciencia de la Computaci\u00f3n e Inteligencia Artificial, Universidad de Alicante Domingo Gallardo, Cristina Pomares, Antonio Bot\u00eda, Francisco Mart\u00ednez","title":"Bibliograf\u00eda"}]}