{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales. Bloque Cloud Computing y Arquitecturas Big Data \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 15 Nov 1p + 2o 2.- Amazon Web Services Lunes 22 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 29 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 13 Dic 1p + 2o 5.- Datos en la nube Lunes 20 Dic 1p + 2o 6.- Arquitecturas Big Data Lunes 10 Ene 1p + 2o","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"index.html#bloque-cloud-computing-y-arquitecturas-big-data","text":"","title":"Bloque Cloud Computing y Arquitecturas Big Data"},{"location":"index.html#resultados-de-aprendizaje","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 15 Nov 1p + 2o 2.- Amazon Web Services Lunes 22 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 29 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 13 Dic 1p + 2o 5.- Datos en la nube Lunes 20 Dic 1p + 2o 6.- Arquitecturas Big Data Lunes 10 Ene 1p + 2o","title":"Planificaci\u00f3n"},{"location":"apuntes/arquitecturas01.html","text":"Arquitecturas Big Data \u00b6 Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manjar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas. Caracter\u00edsticas \u00b6 Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener. Tipos de arquitecturas \u00b6 Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming. Procesamiento Batch \u00b6 Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origne para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante. Procesamiento en Streaming \u00b6 Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo. Arquitectura Lambda \u00b6 Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streamin ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para pocesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos,el lento y el r\u00e1pido,convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actulizar con datos m\u00e1s precisos provenientes de la capa batch . Paso a paso \u00b6 Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. Arquitectura Kappa \u00b6 https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos camis diferenciados, lo que conlleva a duplica la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps es su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos. Arquitectura por capas \u00b6 La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.como) Capa de ingesti\u00f3n: es la primera capa que recoje los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almacen de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interactuan los usuarios. Tecnolog\u00edas \u00b6 Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka . El almacendamiento de los datos y modelos lo podemos realizar mediante HDFS o S3. Dentro de una arquitectura Lamba, en el sistema batch, mediante algoritmos MapReduce de Hadoop podemos entrenar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB o Redis . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill . Casos de uso \u00b6 \u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejeuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para \u00e1mbas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomdable tenemos un par de casos desarrollados por Ericsson que pode\u00eds leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha. Buenas pr\u00e1cticas \u00b6 En la ingesta de datos: eval\u00faa los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s que utilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. Algunas decisiones que tenemos que tomar a la hora de elegir la arquitectura son: Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad? Define tu arquitectura: \u00bfbatch o streaming? \u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes? Arquitectura en la nube \u00b6 Dentro de las diferentes plataformas cloud, a la hora de dise\u00f1ar e implementar una aplicaci\u00f3n, podemos tener en mente, desde un inicio, que se va a desplegar en la nube, haciendo uso de la gran mayor\u00eda de servicios que hemos estudiado en las sesiones anteriores. De forma complementaria, estas plataformas cloud ofrecen un marco buenas pr\u00e1cticas, principios y decisiones que hemos de tomar a la hora de dise\u00f1ar nuestros sistemas, sean big data o no. Marco de buena arquitectura (WAF) \u00b6 Pilares del Marco de Buena Arquitectura (WAF) El marco de buena arquitectura ( AWS Well-Architected Framework / Azure Well-Architected Framework ) es una gu\u00eda dise\u00f1ada para ayudar a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para las aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas que facilitan la evaluaci\u00f3n e implementaci\u00f3n de nuestras arquitecturas en la nube. Se organiza en cinco pilares que estudiaremos a continuaci\u00f3n: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costes. Excelencia operativa \u00b6 Se centra en la habilidad de ejecutar y monitorizar sistemas para proporcionar valor de negocio, mejorando de forma continua los procesos y procedimientos de soporte. Comprende la capacidad para dar soporte al desarrollo y ejecutar cargas de trabajo de manera eficaz, obtener informaci\u00f3n acerca de las operaciones y mejorar continuamente el soporte a los procesos y los procedimientos para ofrecer valor de negocio. Para ello se recomienda: Realizar operaciones como c\u00f3digo : podemos definir toda la carga de trabajo (aplicaciones, infraestructura) como c\u00f3digo y actualizarla con c\u00f3digo, sin necesidad de utilizar el interfaz gr\u00e1fico. De esta manera, podemos automatizar la ejecuci\u00f3n en respuesta a eventos, adem\u00e1s de limitar la posibilidad de error humano. Realizar cambios peque\u00f1os, reversibles (por si se producen errores) y frecuentes . Refinar los procedimientos de las operativos con frecuencia, revisando de forma periodica su efectividad y conocimiento por parte de los equipos. Preveer los errores : realizar simulacros de fallos, probando los procedimientos de respuesta. Aprender de los errores y eventos operativos : promover mejoras a partir de las lecciones aprendidas de todos los eventos y los errores operativos. Seguridad \u00b6 Antes de dise\u00f1ar cualquier sistema, es imprescindible aplicar pr\u00e1cticas de seguridad. Debemos poder controlar qui\u00e9n puede hacer qu\u00e9. Adem\u00e1s, debe de identificar incidentes de seguridad, proteger los sistemas y servicios, y mantener la confidencialidad y la integridad de los datos mediante la protecci\u00f3n de la informaci\u00f3n. As\u00ed pues, este pilar se centra en la capacidad de proteger la informaci\u00f3n, los sistemas y los recursos, al mismo tiempo que se aporta valor de negocio mediante evaluaciones de riesgo y estrategias de mitigaci\u00f3n, siguiendo los siguientes principios: Implementar una base s\u00f3lida de credenciales : mediante el principio de m\u00ednimo privilegio y aplicando la separaci\u00f3n de obligaciones con la autorizaci\u00f3n apropiada para cada interacci\u00f3n con los recursos de AWS. Habilitar la trazabilidad : Integrando registros y m\u00e9tricas con sistemas para responder y tomar medidas de manera autom\u00e1tica, que facilitan la monitorizaci\u00f3n y el uso de alertar. Aplicar seguridad en todas las capas : por ejemplo, a la red de borde, a la nube virtual privada, a la subred y al balanceador de carga, y a cada instancia, sistema operativo y aplicaci\u00f3n. Automatizar las pr\u00e1cticas recomendadas de seguridad. Proteger los datos en tr\u00e1nsito y en reposo : mediante el cifrado, el uso de tokens y el control de acceso cuando corresponda, asegurando la confidencialidad e integridad de los datos. Mantener a las personas alejadas de los datos : reducir o eliminar el acceso directo o el procesamiento manual de los datos. Prepararse para eventos de seguridad : mediante procesos de administraci\u00f3n de incidencias con automatizaci\u00f3n que d\u00e9 respuesta a incidentes. Fiabilidad \u00b6 Se centra en: la capacidad de un sistema de recuperarse de interrupciones en la infraestructura o el servicio. incorporar din\u00e1micamente recursos inform\u00e1ticos para satisfacer la demanda mitigar las interrupciones, como errores de configuraci\u00f3n o problemas de red temporales. Se recomiendan los siguientes principios para aumentar la fiabilidad: Probar los procedimientos de recuperaci\u00f3n : usar la automatizaci\u00f3n para simular diferentes errores o para volver a crear situaciones que hayan dado lugar a errores antes. Recuperarse autom\u00e1ticamente de los errores: monitorizar los sistemas en busca de indicadores clave de rendimiento y configurar los sistemas para desencadenar procesos de recuperaci\u00f3n automatizado cuando se supere un l\u00edmite. Escalar horizontalmente para aumentar la disponibilidad total del sistema: sustituir un recurso grande por varios recursos m\u00e1s peque\u00f1os, distribuyendo las solicitudes para reducir el impacto de un \u00fanico punto de error. Evitar asumir estimaciones sobre capacidad : monitorizando la demanda y el uso del sistema, y automatizando la incorporaci\u00f3n o eliminaci\u00f3n de recursos para mantener el nivel \u00f3ptimo. Administrar los cambios mediante la automatizaci\u00f3n . Eficiencia del rendimiento \u00b6 Se centra en la capacidad de utilizar recursos inform\u00e1ticos de forma eficiente (s\u00f3lo cuando sean necesarios) para satisfacer los requisitos del sistema y mantener esa eficiencia a medida que cambia la demanda o evolucionan las tecnolog\u00edas. Entre los temas principales se incluyen la selecci\u00f3n de los tipos y tama\u00f1os de recursos adecuados en funci\u00f3n de los requisitos de la carga de trabajo, el monitoreo del rendimiento y la toma de decisiones fundamentadas para mantener la eficiencia a medida que evolucionan las necesidades de la empresa. Se recomiendan los siguientes principios para mejorar la eficiencia del rendimiento: Democratizar las tecnolog\u00edas avanzadas : usando tecnolog\u00edas como servicio (como son los servicios de IA que ofrecen tanto AWS como Azure), que simplifican su uso. Adquirir escala mundial en cuesti\u00f3n de minutos : desplegando sistemas en varias regiones para ofrecer una menor latencia. Utilizar arquitecturas sin servidor : las arquitecturas sin servidor eliminan la carga operativa que supone ejecutar y mantener servidores. Experimentar m\u00e1s a menudo : mediante pruebas comparativas de diferentes tipos de instancias, almacenamiento y/o configuraciones. Diponer de compatibilidad mec\u00e1nica : utilizando el enfoque tecnol\u00f3gico que se ajuste mejor a lo que intenta conseguir (por ejemplo, mediante los patrones de acceso a los datos cuando accedamos a bases de datos o almacenamiento). Optimizaci\u00f3n de costes \u00b6 Se centra en la capacidad de ejecutar sistemas para ofrecer valor de negocio al precio m\u00e1s bajo. Entre los temas principales se incluyen la comprensi\u00f3n y el control de cu\u00e1ndo se est\u00e1 gastando el dinero, la selecci\u00f3n de los tipos de recursos m\u00e1s adecuados en la cantidad correcta, el an\u00e1lisis de los gastos a lo largo del tiempo y el escalado para satisfacer las necesidades de la empresa sin gastos excesivos. Se recomiendan los siguientes principios para optimizar los costes: Adoptar un modelo de consumo : pagando solo por los recursos inform\u00e1ticos que necesitamos. Medir la eficacia general : midiendo la producci\u00f3n comercial de la carga de trabajo y los costes asociados a la entrega. Dejar de gastar dinero en las operaciones de centros de datos : la nube elimina los costes de aprovisionamiento, electricidad, aire acondicionado, seguridad f\u00edsica, etc... Analizar y asignar gastos : la nube facilita la identificaci\u00f3n precisa del uso y los costes del sistema, as\u00ed como el coste de las cargas de trabajo individuales, lo que facilita medir el retorno de la inversi\u00f3n (ROI). Utilizar los servicios administrados para reducir el coste de propiedad: reducen la carga operativa que supone mantener servidores para tareas como el env\u00edo de email o la administraci\u00f3n de bases de datos. Todo falla constantemente Hemos de dise\u00f1ar nuestras arquitecturas con el axioma que en un momento u otro algo fallar\u00e1. Para que nuestros sistemas resistan los errores, los dos factores m\u00e1s cr\u00edticos son la fiabilidad y la disponibilidad. Una forma de medir la fiabilidad es el MTBF : tiempo promedio entre errores, es decir tiempo total en servicio respecto a la cantidad de errores. Otra forma es mediante el porcentaje de tiempo durante el cual el sistema funcional correctamente. Este porcentaje se suele medir en la cantidad de nueves, as\u00ed pues seis nueves implica una disponibilidad del 99,9999%. Un sistema de alta disponibilidad (HA) es aquel que puede soportar cierta medida de degradaci\u00f3n sin dejar de estar disponible. Los tres factores que influyen en la disponibilidad son la tolerancia a errores (gracias a la redundancia, cambiar de recurso cuando uno falla), la escalabilidad (la aplicaci\u00f3n se adaptar a los aumentos de carga) y la capacidad de recuperaci\u00f3n (el servicio se restaura r\u00e1pidamente sin perder datos). AWS Trusted Advisor \u00b6 AWS dispone de la herramienta en l\u00ednea AWS Trusted Advisor que ofrece asesoramiento en tiempo real con las pr\u00e1cticas recomendadas de AWS. Examina todo el entorno AWS y ofrece recomendaciones en cinco categor\u00edas: AWS Trusted Advisor Optimizaci\u00f3n de costes: sugiere recursos no utilizados e inactivos, o bien, posibilidad de realiza una reserva de capacidad. Rendimiento: comprueba los l\u00edmites del servicio y monitoriza para detectar instancias que se est\u00e9n utilizando por encima de su capacidad. Seguridad: examina los permisos para mejorar el nivel de seguridad de la aplicaci\u00f3n. Tolerancia a errores: revisa las capacidades de escalado autom\u00e1tico, las comprobaciones de estado, la implementaci\u00f3n Multi-AZ y las capacidades de copia de seguridad. L\u00edmites del servicio: realiza verificaciones para detectar usos que superen el 80% del l\u00edmite del servicio. Azure Advisor y Azure Score Microsoft, del forma similar, ofrece un par herramientas que nos ayudan a optimizar las implementaciones, como son Azure Advisor , y dentro de ella Advisor Score que puntua las recomendaciones para con un simple vistazo poder priorizar las mejoras sugeridas. Gesti\u00f3n del escalado y la monitorizaci\u00f3n \u00b6 Monitorizaci\u00f3n \u00b6 Al operar en la nube, es importante llevar un seguimiento de las actividades, porque probablemente haya un coste asociado a cada una de ellas. AWS ayuda a monitorizar, registrar e informar sobre el uso de sus servicios proporcionando herramientas para hacerlo. As\u00ed pues, AWS ofrece los siguientes servicios relacionados con la monitorizaci\u00f3n: Amazon CloudTrail : Servicio que registra cada acci\u00f3n que se lleva a cabo en la cuenta de AWS por motivos de seguridad. Esto significa que CloudTrail registra cada vez que alguien carga datos, ejecuta un c\u00f3digo, crea una instancia de EC2 o realiza cualquier otra acci\u00f3n. Amazon Cloudwatch : Servicio de monitorizaci\u00f3n de los recursos de AWS y las aplicaciones que ejecuta en AWS. CloudTrail registra actividades, mientras que CloudWatch las monitoriza. As\u00ed pues, CloudWatch vigila que los servicios cloud se ejecutan si problema y ayuda a no utilizar ni m\u00e1s ni menos recursos de lo esperado, lo que es importante para el seguimiento del presupuesto. AWS Config : Servicio que permite analizar, auditar y evaluar las configuraciones de los recursos de AWS. AWS Config monitoriza y registra de manera continua las configuraciones de recursos de AWS y permite automatizar la evaluaci\u00f3n de las configuraciones registradas con respecto a las deseadas. Amazon SNS ( Amazon Simple Notification Service ) : herramienta que permite enviar textos, correos electr\u00f3nicos y mensajes a otros servicios en la nube y enviar notificaciones al cliente de varias formas desde la nube. Ejemplo Cloudwatch \u00b6 En el siguiente ejemplo vamos a crear una alarma de Cloudwatch para enviar una notificaci\u00f3n con la cuenta haya gastado una cierta cantidad de dinero. La alarma env\u00eda un mensaje a Amazon SNS para posteriormente enviar un correo electr\u00f3nico. El primer paso es crear y subscribirse a un tema ( topic ) SNS. Un tema act\u00faa como un canal de comunicaci\u00f3n donde se recibes los mensajes de las alertas y eventos. Para ello, dentro del servicio SNS, crearemos un tema al que llamaremos AlertaSaldo . Cloudwatch - Creaci\u00f3n del tema A continuaci\u00f3n, vamos a crear una subscripci\u00f3n a ese tema para que cuando se recibe una mensje, lo redirijamos a nuestro tel\u00e9fono o correo electr\u00f3nico. Para ello, dentro de la secci\u00f3n de subscripciones, crearemos una subscripci\u00f3n. En el ARN pondremos el tema AlertaSaldo que acabamos de crear, y en el protocolo, vamos a seleccionar Correo electr\u00f3nico . Finalmente, en el punto de enlace, definimos el email que recibir\u00e1 la alerta. En este momento, Amazon enviar\u00e1 un email a la cuenta que hayamos indicado para confirmar los datos. Cloudwatch - Creaci\u00f3n de la subscripci\u00f3n El siguiente paso es crear la alarma en Cloudwatch . Para ello, una vez dentro de Cloudwatch , dentro de la opci\u00f3n de Alarmas, al crear una nueva, tendremos que elegir la m\u00e9trica, que en nuestro caso seleccionaremos Facturaci\u00f3n -> Cargo total estimado. En la siguiente pantalla, en la secci\u00f3n de Condiciones ..... est\u00e1tico, e indicamos la condici\u00f3n que queremos que se active cuando es superior a 100. Cloudwatch - Condiciones de la alarma En la secci\u00f3n de Notificaci\u00f3n , tras elegir en modo alarma seleccionamos el tema SNS existente (en nuestro caso AlertaSaldo ). Cloudwatch - Notificaciones de la alarma Finalmente, le asignamos el nombre de AlertaSaldoAlarma y tras ver un resumen de todo los configurado, creamos la alarma. De esta manera, cuando se supere el gasto de 100$, autom\u00e1ticamente nos enviar\u00e1 un email a la direcci\u00f3n que le hemos configurado. Escalado y Balanceo de carga \u00b6 Elastic Load Balancing distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2 Adem\u00e1s, le permite obtener tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Auto Scaling permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Puede utilizar Auto Scaling para asegurarse de que se ejecuta la cantidad deseada de instancias de Amazon EC2. Con Auto Scaling, tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costos. Auto Scaling es adecuado para aplicaciones con patrones de demanda estables o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana. Elastic Load Balancing \u00b6 Amazon EC2 Auto Scaling \u00b6 Actividades \u00b6 Si nos basamos en una arquitectura Lambda, clasifica los siguientes elementos: Si nos basamos en una arquitectura Kappa, clasifica los siguientes elementos: Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS . (opcional) Cloudwatch_ Siga estos pasos para crear una ruta en su bucket de Amazon Simple Storage Service (Amazon S3). https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html Esto llevar\u00e1 un seguimiento de todas las acciones que se realizan con ese bucket de S3. Referencias \u00b6 Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Laboratorios de Amazon sobre AWF","title":"6.- Arquitecturas"},{"location":"apuntes/arquitecturas01.html#arquitecturas-big-data","text":"Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manjar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas.","title":"Arquitecturas Big Data"},{"location":"apuntes/arquitecturas01.html#caracteristicas","text":"Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener.","title":"Caracter\u00edsticas"},{"location":"apuntes/arquitecturas01.html#tipos-de-arquitecturas","text":"Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming.","title":"Tipos de arquitecturas"},{"location":"apuntes/arquitecturas01.html#procesamiento-batch","text":"Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origne para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante.","title":"Procesamiento Batch"},{"location":"apuntes/arquitecturas01.html#procesamiento-en-streaming","text":"Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo.","title":"Procesamiento en Streaming"},{"location":"apuntes/arquitecturas01.html#arquitectura-lambda","text":"Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streamin ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para pocesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos,el lento y el r\u00e1pido,convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actulizar con datos m\u00e1s precisos provenientes de la capa batch .","title":"Arquitectura Lambda"},{"location":"apuntes/arquitecturas01.html#paso-a-paso","text":"Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas.","title":"Paso a paso"},{"location":"apuntes/arquitecturas01.html#arquitectura-kappa","text":"https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos camis diferenciados, lo que conlleva a duplica la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps es su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos.","title":"Arquitectura Kappa"},{"location":"apuntes/arquitecturas01.html#arquitectura-por-capas","text":"La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.como) Capa de ingesti\u00f3n: es la primera capa que recoje los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almacen de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interactuan los usuarios.","title":"Arquitectura por capas"},{"location":"apuntes/arquitecturas01.html#tecnologias","text":"Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka . El almacendamiento de los datos y modelos lo podemos realizar mediante HDFS o S3. Dentro de una arquitectura Lamba, en el sistema batch, mediante algoritmos MapReduce de Hadoop podemos entrenar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB o Redis . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill .","title":"Tecnolog\u00edas"},{"location":"apuntes/arquitecturas01.html#casos-de-uso","text":"\u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejeuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para \u00e1mbas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomdable tenemos un par de casos desarrollados por Ericsson que pode\u00eds leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha.","title":"Casos de uso"},{"location":"apuntes/arquitecturas01.html#buenas-practicas","text":"En la ingesta de datos: eval\u00faa los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s que utilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. Algunas decisiones que tenemos que tomar a la hora de elegir la arquitectura son: Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad? Define tu arquitectura: \u00bfbatch o streaming? \u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes?","title":"Buenas pr\u00e1cticas"},{"location":"apuntes/arquitecturas01.html#arquitectura-en-la-nube","text":"Dentro de las diferentes plataformas cloud, a la hora de dise\u00f1ar e implementar una aplicaci\u00f3n, podemos tener en mente, desde un inicio, que se va a desplegar en la nube, haciendo uso de la gran mayor\u00eda de servicios que hemos estudiado en las sesiones anteriores. De forma complementaria, estas plataformas cloud ofrecen un marco buenas pr\u00e1cticas, principios y decisiones que hemos de tomar a la hora de dise\u00f1ar nuestros sistemas, sean big data o no.","title":"Arquitectura en la nube"},{"location":"apuntes/arquitecturas01.html#marco-de-buena-arquitectura-waf","text":"Pilares del Marco de Buena Arquitectura (WAF) El marco de buena arquitectura ( AWS Well-Architected Framework / Azure Well-Architected Framework ) es una gu\u00eda dise\u00f1ada para ayudar a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para las aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas que facilitan la evaluaci\u00f3n e implementaci\u00f3n de nuestras arquitecturas en la nube. Se organiza en cinco pilares que estudiaremos a continuaci\u00f3n: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costes.","title":"Marco de buena arquitectura (WAF)"},{"location":"apuntes/arquitecturas01.html#aws-trusted-advisor","text":"AWS dispone de la herramienta en l\u00ednea AWS Trusted Advisor que ofrece asesoramiento en tiempo real con las pr\u00e1cticas recomendadas de AWS. Examina todo el entorno AWS y ofrece recomendaciones en cinco categor\u00edas: AWS Trusted Advisor Optimizaci\u00f3n de costes: sugiere recursos no utilizados e inactivos, o bien, posibilidad de realiza una reserva de capacidad. Rendimiento: comprueba los l\u00edmites del servicio y monitoriza para detectar instancias que se est\u00e9n utilizando por encima de su capacidad. Seguridad: examina los permisos para mejorar el nivel de seguridad de la aplicaci\u00f3n. Tolerancia a errores: revisa las capacidades de escalado autom\u00e1tico, las comprobaciones de estado, la implementaci\u00f3n Multi-AZ y las capacidades de copia de seguridad. L\u00edmites del servicio: realiza verificaciones para detectar usos que superen el 80% del l\u00edmite del servicio. Azure Advisor y Azure Score Microsoft, del forma similar, ofrece un par herramientas que nos ayudan a optimizar las implementaciones, como son Azure Advisor , y dentro de ella Advisor Score que puntua las recomendaciones para con un simple vistazo poder priorizar las mejoras sugeridas.","title":"AWS Trusted Advisor"},{"location":"apuntes/arquitecturas01.html#gestion-del-escalado-y-la-monitorizacion","text":"","title":"Gesti\u00f3n del escalado y la monitorizaci\u00f3n"},{"location":"apuntes/arquitecturas01.html#monitorizacion","text":"Al operar en la nube, es importante llevar un seguimiento de las actividades, porque probablemente haya un coste asociado a cada una de ellas. AWS ayuda a monitorizar, registrar e informar sobre el uso de sus servicios proporcionando herramientas para hacerlo. As\u00ed pues, AWS ofrece los siguientes servicios relacionados con la monitorizaci\u00f3n: Amazon CloudTrail : Servicio que registra cada acci\u00f3n que se lleva a cabo en la cuenta de AWS por motivos de seguridad. Esto significa que CloudTrail registra cada vez que alguien carga datos, ejecuta un c\u00f3digo, crea una instancia de EC2 o realiza cualquier otra acci\u00f3n. Amazon Cloudwatch : Servicio de monitorizaci\u00f3n de los recursos de AWS y las aplicaciones que ejecuta en AWS. CloudTrail registra actividades, mientras que CloudWatch las monitoriza. As\u00ed pues, CloudWatch vigila que los servicios cloud se ejecutan si problema y ayuda a no utilizar ni m\u00e1s ni menos recursos de lo esperado, lo que es importante para el seguimiento del presupuesto. AWS Config : Servicio que permite analizar, auditar y evaluar las configuraciones de los recursos de AWS. AWS Config monitoriza y registra de manera continua las configuraciones de recursos de AWS y permite automatizar la evaluaci\u00f3n de las configuraciones registradas con respecto a las deseadas. Amazon SNS ( Amazon Simple Notification Service ) : herramienta que permite enviar textos, correos electr\u00f3nicos y mensajes a otros servicios en la nube y enviar notificaciones al cliente de varias formas desde la nube.","title":"Monitorizaci\u00f3n"},{"location":"apuntes/arquitecturas01.html#ejemplo-cloudwatch","text":"En el siguiente ejemplo vamos a crear una alarma de Cloudwatch para enviar una notificaci\u00f3n con la cuenta haya gastado una cierta cantidad de dinero. La alarma env\u00eda un mensaje a Amazon SNS para posteriormente enviar un correo electr\u00f3nico. El primer paso es crear y subscribirse a un tema ( topic ) SNS. Un tema act\u00faa como un canal de comunicaci\u00f3n donde se recibes los mensajes de las alertas y eventos. Para ello, dentro del servicio SNS, crearemos un tema al que llamaremos AlertaSaldo . Cloudwatch - Creaci\u00f3n del tema A continuaci\u00f3n, vamos a crear una subscripci\u00f3n a ese tema para que cuando se recibe una mensje, lo redirijamos a nuestro tel\u00e9fono o correo electr\u00f3nico. Para ello, dentro de la secci\u00f3n de subscripciones, crearemos una subscripci\u00f3n. En el ARN pondremos el tema AlertaSaldo que acabamos de crear, y en el protocolo, vamos a seleccionar Correo electr\u00f3nico . Finalmente, en el punto de enlace, definimos el email que recibir\u00e1 la alerta. En este momento, Amazon enviar\u00e1 un email a la cuenta que hayamos indicado para confirmar los datos. Cloudwatch - Creaci\u00f3n de la subscripci\u00f3n El siguiente paso es crear la alarma en Cloudwatch . Para ello, una vez dentro de Cloudwatch , dentro de la opci\u00f3n de Alarmas, al crear una nueva, tendremos que elegir la m\u00e9trica, que en nuestro caso seleccionaremos Facturaci\u00f3n -> Cargo total estimado. En la siguiente pantalla, en la secci\u00f3n de Condiciones ..... est\u00e1tico, e indicamos la condici\u00f3n que queremos que se active cuando es superior a 100. Cloudwatch - Condiciones de la alarma En la secci\u00f3n de Notificaci\u00f3n , tras elegir en modo alarma seleccionamos el tema SNS existente (en nuestro caso AlertaSaldo ). Cloudwatch - Notificaciones de la alarma Finalmente, le asignamos el nombre de AlertaSaldoAlarma y tras ver un resumen de todo los configurado, creamos la alarma. De esta manera, cuando se supere el gasto de 100$, autom\u00e1ticamente nos enviar\u00e1 un email a la direcci\u00f3n que le hemos configurado.","title":"Ejemplo Cloudwatch"},{"location":"apuntes/arquitecturas01.html#escalado-y-balanceo-de-carga","text":"Elastic Load Balancing distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2 Adem\u00e1s, le permite obtener tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Auto Scaling permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Puede utilizar Auto Scaling para asegurarse de que se ejecuta la cantidad deseada de instancias de Amazon EC2. Con Auto Scaling, tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costos. Auto Scaling es adecuado para aplicaciones con patrones de demanda estables o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana.","title":"Escalado y Balanceo de carga"},{"location":"apuntes/arquitecturas01.html#elastic-load-balancing","text":"","title":"Elastic Load Balancing"},{"location":"apuntes/arquitecturas01.html#amazon-ec2-auto-scaling","text":"","title":"Amazon EC2 Auto Scaling"},{"location":"apuntes/arquitecturas01.html#actividades","text":"Si nos basamos en una arquitectura Lambda, clasifica los siguientes elementos: Si nos basamos en una arquitectura Kappa, clasifica los siguientes elementos: Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS . (opcional) Cloudwatch_ Siga estos pasos para crear una ruta en su bucket de Amazon Simple Storage Service (Amazon S3). https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html Esto llevar\u00e1 un seguimiento de todas las acciones que se realizan con ese bucket de S3.","title":"Actividades"},{"location":"apuntes/arquitecturas01.html#referencias","text":"Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Laboratorios de Amazon sobre AWF","title":"Referencias"},{"location":"apuntes/bdaplicado01hadoop.html","text":"Hadoop \u00b6 Si Big Data es la filosof\u00eda de trabajo para grandes volumenes de datos, Apache Hadoop http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribudos de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillo. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos, donde se distingue entre: FIXME: hardware hadoop Nodos maestros: normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos: entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2. Componentes \u00b6 El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System <--> HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar de casi la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite accede a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores (HiveSQL). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado con columnas para Hadoop. Es de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java e implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, miles de millones de filas de X millones de columnas, sobre un cl\u00faster Hadoop. Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover en Hadoop informaci\u00f3n textual, como ficheros de logs, bloques de twitter/reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo vale para Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuido de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft MapReduce \u00b6 Es el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. El siguiente gr\u00e1fico muestra un ejemplo de una empresa de juguete que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color ha de preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura de los ficheros de entrada. Pasar cada linea de forma separada al mapeador. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan las claves. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso del framework Spark, que mejora el rendimiento por una orden de magnitud. HDFS \u00b6 Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incrementa y sobrevivir a fallos de hardware sin perder datos. En un sistema que se reparte entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos. FIXME: poner gr\u00e1fico Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces. Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para:\u200b Accesos de baja latencia\u200b Ficheros peque\u00f1os (a menos que se agrupen)\u200b M\u00faltiples escritores Modificaciones arbitrarias de ficheros\u200b Bloques \u00b6 Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita.\u200b El tama\u00f1o predeterminado de HDFS son 128 MB, ya que Hadoop est\u00e1 pensado para trabajar con fichero de gran tama\u00f1o.\u200b Todos los ficheros est\u00e1n divididos en bloques.\u200b Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de redundancia, cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3.\u200b Por lo tanto, el archivo de 600MB que teniamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS En HDFS se distinguen las siguientes m\u00e1quinas:\u200b Namenode : \u200bAct\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques.\u200b Tiene control sobre donde est\u00e1n todos los bloques.\u200b Datanode :\u200b Son los esclavos, se limitan a almacenar los bloques que compone cada fichero.\u200b Secondary Namenode :\u200b Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode.\u200b Arquitectura HDFS Namenodes \u00b6 Existen dos tipos. El principal se conoce como Namenode : Solo existe uno.\u200b Gestiona el espacio del sistema de ficheros\u200b Mantiene el \u00e1rbol del Sistema de ficheros y los metadatos para todos los ficheros y directorios en el \u00e1rbol.\u200b Los bloques nunca pasan por el NameNode, se transfieren entre DataNodes o al cliente.\u200b Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso al HDFS\u200b El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog\u200b FsImage : instant\u00e1nea de los metadatos del sistema de archivos.\u200b EditLog : registro de transacciones que contiene los registros de cada cambio que se produce en los metadatos del sistema de * archivos.\u200b No se trata de un nodo de respaldo\u200b Por lo general se ejecuta en una m\u00e1quina distinta\u200b Datanode \u00b6 M\u00e1s de uno\u200b Almacena y lee bloques.\u200b Recuperado por Namenode clientes\u200b Reportan al Namenode la lista de bloques que est\u00e1n almacenando.\u200b Pueden ir en distintos discos\u200b Guarda un checksum del bloque\u200b Verificaci\u00f3n del bloque: lectura\u200b HDFS por dentro HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Si entramos a la carpeta de datos que tenemos configurada en hdfs-site.xml , tendremos una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS DFS Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . Procesos de lectura y escritura \u00b6 Colocar un archivo en HDFS implica los siguientes pasos: Una aplicaci\u00f3n cliente env\u00eda una solicitud al namenode que especifica d\u00f2nde quiere poner el archivo dentro de HDFS. El namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. Esta informaci\u00f3n se devuelve a la aplicaci\u00f3n cliente. La aplicaci\u00f3n cliente se comunica directamente con cada datanode escribiendo los bloques informados en el paso anterior. El datanode replica el bloque de nueva creaci\u00f3n a otros 2 datanodes (suponiendo que el factor de replicaci\u00f3n sea 3). Podemos especificar el tama\u00f1o del bloque para cada archivo mediante la propiedad dfs.blocksize . Si no se indica un tama\u00f1o de bloque a nivel de arhicov, se utiliza el valor global de dfs.blocksize definido en hdfs-site.xml , el cual por defecto es de 128MB. Es importante destacar que los datos nunca pasan por el namenode . El cliente que realiza la carga en HDFS es el que hace las operaci\u00f3n de lectura/escritura directamente con los datanodes . FIXME: revisar, mirar proceso de lectura/escritura del ppt de Teralco FIXME: mirar video y hacer capturas https://www.youtube.com/watch?v=e1-yVYXOTMg Trabajando con HDFS \u00b6 Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual admite requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv : Copia / mueve-renombra un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /datos hdfs dfs -put ejemplo.txt /datos/ hdfs dfs -put ejemplo.txt /datos/ejemploRenombrado.txt hdfs dfs -ls /datos hdfs dfs -count /datos hdfs dfs -mv /datos/ejemploRenombrado.txt /datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp A continuaci\u00f3n vamos a ver como trabajar HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . FIXME: Corregir y revisar o primero que vamos a hacer es crear un directorio dentro del hdfs llamado temporal hdfs dfs -mkdir /user/temporal \uf0b7 Una vez creado subimos el archivo de la carpeta Recurso al directorio de dfs creado en el paso anterior hdfs dfs -put el_quijote.txt /user/temporal \uf0b7 Con el fichero subido nos vamos al hdfs UI: localhost:9870 y comprobamos que el Block Pool ID del block information, coincide con el del directorio de datos del datanode, dentro del directorio current: Dentro de este subdirectorio existe otro current/finalized, donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir()... donde albergar\u00e1 los bloques de datos. En uno aparecen los datos y en el otro los metadatos \uf0b7 Creamos un nuevo directorio llamado temporal1 y copiamos el fichero prueba.txt del directorio temporal a temporal1. \uf0b7 Borramos el directorio temporal \uf0b7 Ahora vamos a crear un fichero grande. Para ello lanzamos este comando que nos va a generar un fichero de 1G en /tmp, llamado giga_test.dat que estar\u00e1 lleno de ceros. \uf0b7 Subimos el fichero a un directorio que creamos conveniente hdfs dfs -put /tmp/giga_test.dat /user/prueba \uf0b7 Una vez subido nos vamos a hdfs UI file browser para ver los bloques que ha creado dd if = /dev/zero of = /tmp/giga_test.dat bs = 1024 count = 1000000 ```` Ahora nos vamos al directorio subdir () de datanode y podremos comprobar todos los bloques ls /datos/datanode/current/BP-1410034788-192.168.0.101- 1618596221610 /current/finalized/subdir0/subdir0 Bloques acabados desde el 20 al 27 ### Administraci\u00f3n Algunas de las opciones m\u00e1s utiles para administrar HDFS son: * ` hdfs dfsadmin -report ` : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web. * ` hdfs fsck ` : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: ` hdfs fsck /datos/prueba ` * ` hdfs dfsadmin -printTopology ` : Muestra la topolog\u00eda que tenemos, identificando los nodos que tenemos y al rack al que pertenece cada nodo * ` hdfs dfsadmin -listOpenFiles ` : Comprueba si hay alg\u00fan fichero abierto ### *Snapshots* Mediante las *snapshots* podemos hacer una foto que indica c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder hacer una recuperaci\u00f3n. El primer paso es activar el uso de *snapshots*, mediante el comando de administraci\u00f3n, indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: ``` bash hdfs dfsadmin -allowSnapshot /datos El siguiente paso es crear una snapshot , para ellos se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /datos /captura1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /datos/.snapshot/captura1/ la cual contendr\u00e1 la informaci\u00f3n de la captura) A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /datos/ejemplo.txt hdfs dfs -ls /datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp /datos/.snapshot/captura1/ejemplo.txt /datos YARN \u00b6 Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gestion de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , Node Manager y ApplicationMaster . La idea es tener un Resource Manager global y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager es la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, el NodeManager es un agente que est\u00e1 en cada nodo de datos y es responsable de monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Resource Manager \u00b6 A su vez se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Los clientes del sistema pueden enviar una aplicaci\u00f3n Yarn soportada para ejecutar al Resource Manager . Node Manager \u00b6 Gestiona los trabajos con las instrucciones del Resource Manager y proporciona los recursos computacionales necesarios para las aplicaciones en forma de contenedores. Implementa Heartbeats para mantener informado del estado al Resource Manager . Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Application Master \u00b6 El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n Yarn en el contenedor correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del APU con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. Instalaci\u00f3n \u00b6 Para trabajar en este y las siguientes sesi\u00f3n, vamos a trabajar con la m\u00e1quina virtual que tenemos compartida en Aules. A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario hadoop y la contrase\u00f1a hadoop . Si quieres instalar el software del curso, se recomiendo crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n 20.04 LTS y la versi\u00f3n 3.2.2 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos. Configuraci\u00f3n \u00b6 Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de fichero, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://dominioNodoMaestro:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /carpetaData/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /carpetaData/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro solo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> dominioNodoMaestro </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> Puesta en marcha \u00b6 Arrancando HDFS Para arrancar Hadoop, hemos de ejecutar el comando startdfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://localhost:9870/ podremos acceder a una interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para arrancar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n como las m\u00e9tricas del cl\u00faster. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obtienendo la siguiente p\u00e1gina: Interfaz Web de YARN Ambari \u00b6 Ambari es un producto que simplifica la gesti\u00f3n de Hadoop y permite configurar, instalar y monitorizar un cluster Hadoop. La instalaci\u00f3n que tenemos no nos sirve, ya que Ambari crea un nueva instalaci\u00f3n tanto de Hadoop y YARN. Para su instalaci\u00f3n desde cero, es recomendable seguir las indicaciones de su p\u00e1gina oficial . Nosotros vamos a resumir los pasos recomendados. Para instalar en Ubuntu el servidor Amabari \u00fanicamente deber\u00edamos ejecutar el siguiente comando (tambi\u00e9n va a instalar PostgreSQL donde almacenar\u00e1 todos los metadados sobre la configuraci\u00f3n de Ambari): apt-get install ./ambari-server*.deb # Instalar\u00e1 tambi\u00e9n PostgreSQL Una vez instalado, solo queda configurarlo (como usuario con permisos root ): ambari-server setup Y finalmente arrancarlo: ambari-server start Ahora tocar\u00eda instalar los agentes en cada uno de los nodos, as\u00ed como configurarlo. Como vamos a trabajar con un modelo pseudodistribuido, en nuestro caso no vamos a hacer ese paso. Finalmente, accederemos al interfaz gr\u00e1fico con el usuario admin/admin y acceder http://localhost:8080. FIXME: captura Al crear un cluster, Ambari se basa en los stacks de HortonWorks HDP para realizar la instalaci\u00f3n de Hadoop. FIXME: captura Revisar nodos Instalar SSH en usuario root Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop Art\u00edculo de Hadoop por dentro Tutorial de Hadoop de Tutorialspoint Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resutlado de cada acci\u00f3n Comandos HDFS Crear carpeta, meter 2 archivos, duplicar, renombrar y sacar FIXME: escribir HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . Realiza un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo. Realiza un checkpoint Vuelve a entrar al modo normal Comprueba que los fimage del namenode son iguales. HDFS / Administraci\u00f3n Ejecutar todos los comandos del apartado haciendo capturas. Sobre el contenido creado en el ejercicio 1, crear una snapshot, borrar un archivo, y recuperarlo de la snapshot FIXME: escribir (opcional) Crea una instalaci\u00f3n desde cero en una nueva m\u00e1quina virtual siguiendo las intrucciones del art\u00edculo recomendado en el apartado de Configuracion . Arranca Hadoop, muestra los procesos y accede al interfaz gr\u00e1fico. (opcional) Crea una instalaci\u00f3n desde cero mediante Ambari. Tras acceder al interfaz gr\u00e1fico, crea un cluster llamado cluster2122 . Instala Hadoop y accede al interfaz gr\u00e1fico.","title":"Hadoop"},{"location":"apuntes/bdaplicado01hadoop.html#hadoop","text":"Si Big Data es la filosof\u00eda de trabajo para grandes volumenes de datos, Apache Hadoop http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribudos de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillo. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos, donde se distingue entre: FIXME: hardware hadoop Nodos maestros: normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos: entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2.","title":"Hadoop"},{"location":"apuntes/bdaplicado01hadoop.html#componentes","text":"El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System <--> HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar de casi la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite accede a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores (HiveSQL). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado con columnas para Hadoop. Es de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java e implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, miles de millones de filas de X millones de columnas, sobre un cl\u00faster Hadoop. Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover en Hadoop informaci\u00f3n textual, como ficheros de logs, bloques de twitter/reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo vale para Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuido de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft","title":"Componentes"},{"location":"apuntes/bdaplicado01hadoop.html#mapreduce","text":"Es el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. El siguiente gr\u00e1fico muestra un ejemplo de una empresa de juguete que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color ha de preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura de los ficheros de entrada. Pasar cada linea de forma separada al mapeador. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan las claves. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso del framework Spark, que mejora el rendimiento por una orden de magnitud.","title":"MapReduce"},{"location":"apuntes/bdaplicado01hadoop.html#hdfs","text":"Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incrementa y sobrevivir a fallos de hardware sin perder datos. En un sistema que se reparte entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos. FIXME: poner gr\u00e1fico Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces. Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para:\u200b Accesos de baja latencia\u200b Ficheros peque\u00f1os (a menos que se agrupen)\u200b M\u00faltiples escritores Modificaciones arbitrarias de ficheros\u200b","title":"HDFS"},{"location":"apuntes/bdaplicado01hadoop.html#bloques","text":"Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita.\u200b El tama\u00f1o predeterminado de HDFS son 128 MB, ya que Hadoop est\u00e1 pensado para trabajar con fichero de gran tama\u00f1o.\u200b Todos los ficheros est\u00e1n divididos en bloques.\u200b Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de redundancia, cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3.\u200b Por lo tanto, el archivo de 600MB que teniamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS En HDFS se distinguen las siguientes m\u00e1quinas:\u200b Namenode : \u200bAct\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques.\u200b Tiene control sobre donde est\u00e1n todos los bloques.\u200b Datanode :\u200b Son los esclavos, se limitan a almacenar los bloques que compone cada fichero.\u200b Secondary Namenode :\u200b Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode.\u200b Arquitectura HDFS","title":"Bloques"},{"location":"apuntes/bdaplicado01hadoop.html#namenodes","text":"Existen dos tipos. El principal se conoce como Namenode : Solo existe uno.\u200b Gestiona el espacio del sistema de ficheros\u200b Mantiene el \u00e1rbol del Sistema de ficheros y los metadatos para todos los ficheros y directorios en el \u00e1rbol.\u200b Los bloques nunca pasan por el NameNode, se transfieren entre DataNodes o al cliente.\u200b Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso al HDFS\u200b El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog\u200b FsImage : instant\u00e1nea de los metadatos del sistema de archivos.\u200b EditLog : registro de transacciones que contiene los registros de cada cambio que se produce en los metadatos del sistema de * archivos.\u200b No se trata de un nodo de respaldo\u200b Por lo general se ejecuta en una m\u00e1quina distinta\u200b","title":"Namenodes"},{"location":"apuntes/bdaplicado01hadoop.html#datanode","text":"M\u00e1s de uno\u200b Almacena y lee bloques.\u200b Recuperado por Namenode clientes\u200b Reportan al Namenode la lista de bloques que est\u00e1n almacenando.\u200b Pueden ir en distintos discos\u200b Guarda un checksum del bloque\u200b Verificaci\u00f3n del bloque: lectura\u200b HDFS por dentro HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Si entramos a la carpeta de datos que tenemos configurada en hdfs-site.xml , tendremos una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS DFS Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits .","title":"Datanode"},{"location":"apuntes/bdaplicado01hadoop.html#procesos-de-lectura-y-escritura","text":"Colocar un archivo en HDFS implica los siguientes pasos: Una aplicaci\u00f3n cliente env\u00eda una solicitud al namenode que especifica d\u00f2nde quiere poner el archivo dentro de HDFS. El namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. Esta informaci\u00f3n se devuelve a la aplicaci\u00f3n cliente. La aplicaci\u00f3n cliente se comunica directamente con cada datanode escribiendo los bloques informados en el paso anterior. El datanode replica el bloque de nueva creaci\u00f3n a otros 2 datanodes (suponiendo que el factor de replicaci\u00f3n sea 3). Podemos especificar el tama\u00f1o del bloque para cada archivo mediante la propiedad dfs.blocksize . Si no se indica un tama\u00f1o de bloque a nivel de arhicov, se utiliza el valor global de dfs.blocksize definido en hdfs-site.xml , el cual por defecto es de 128MB. Es importante destacar que los datos nunca pasan por el namenode . El cliente que realiza la carga en HDFS es el que hace las operaci\u00f3n de lectura/escritura directamente con los datanodes . FIXME: revisar, mirar proceso de lectura/escritura del ppt de Teralco FIXME: mirar video y hacer capturas https://www.youtube.com/watch?v=e1-yVYXOTMg","title":"Procesos de lectura y escritura"},{"location":"apuntes/bdaplicado01hadoop.html#trabajando-con-hdfs","text":"Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual admite requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv : Copia / mueve-renombra un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /datos hdfs dfs -put ejemplo.txt /datos/ hdfs dfs -put ejemplo.txt /datos/ejemploRenombrado.txt hdfs dfs -ls /datos hdfs dfs -count /datos hdfs dfs -mv /datos/ejemploRenombrado.txt /datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp A continuaci\u00f3n vamos a ver como trabajar HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . FIXME: Corregir y revisar o primero que vamos a hacer es crear un directorio dentro del hdfs llamado temporal hdfs dfs -mkdir /user/temporal \uf0b7 Una vez creado subimos el archivo de la carpeta Recurso al directorio de dfs creado en el paso anterior hdfs dfs -put el_quijote.txt /user/temporal \uf0b7 Con el fichero subido nos vamos al hdfs UI: localhost:9870 y comprobamos que el Block Pool ID del block information, coincide con el del directorio de datos del datanode, dentro del directorio current: Dentro de este subdirectorio existe otro current/finalized, donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir()... donde albergar\u00e1 los bloques de datos. En uno aparecen los datos y en el otro los metadatos \uf0b7 Creamos un nuevo directorio llamado temporal1 y copiamos el fichero prueba.txt del directorio temporal a temporal1. \uf0b7 Borramos el directorio temporal \uf0b7 Ahora vamos a crear un fichero grande. Para ello lanzamos este comando que nos va a generar un fichero de 1G en /tmp, llamado giga_test.dat que estar\u00e1 lleno de ceros. \uf0b7 Subimos el fichero a un directorio que creamos conveniente hdfs dfs -put /tmp/giga_test.dat /user/prueba \uf0b7 Una vez subido nos vamos a hdfs UI file browser para ver los bloques que ha creado dd if = /dev/zero of = /tmp/giga_test.dat bs = 1024 count = 1000000 ```` Ahora nos vamos al directorio subdir () de datanode y podremos comprobar todos los bloques ls /datos/datanode/current/BP-1410034788-192.168.0.101- 1618596221610 /current/finalized/subdir0/subdir0 Bloques acabados desde el 20 al 27 ### Administraci\u00f3n Algunas de las opciones m\u00e1s utiles para administrar HDFS son: * ` hdfs dfsadmin -report ` : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web. * ` hdfs fsck ` : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: ` hdfs fsck /datos/prueba ` * ` hdfs dfsadmin -printTopology ` : Muestra la topolog\u00eda que tenemos, identificando los nodos que tenemos y al rack al que pertenece cada nodo * ` hdfs dfsadmin -listOpenFiles ` : Comprueba si hay alg\u00fan fichero abierto ### *Snapshots* Mediante las *snapshots* podemos hacer una foto que indica c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder hacer una recuperaci\u00f3n. El primer paso es activar el uso de *snapshots*, mediante el comando de administraci\u00f3n, indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: ``` bash hdfs dfsadmin -allowSnapshot /datos El siguiente paso es crear una snapshot , para ellos se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /datos /captura1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /datos/.snapshot/captura1/ la cual contendr\u00e1 la informaci\u00f3n de la captura) A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /datos/ejemplo.txt hdfs dfs -ls /datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp /datos/.snapshot/captura1/ejemplo.txt /datos","title":"Trabajando con HDFS"},{"location":"apuntes/bdaplicado01hadoop.html#yarn","text":"Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gestion de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , Node Manager y ApplicationMaster . La idea es tener un Resource Manager global y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager es la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, el NodeManager es un agente que est\u00e1 en cada nodo de datos y es responsable de monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas.","title":"YARN"},{"location":"apuntes/bdaplicado01hadoop.html#resource-manager","text":"A su vez se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Los clientes del sistema pueden enviar una aplicaci\u00f3n Yarn soportada para ejecutar al Resource Manager .","title":"Resource Manager"},{"location":"apuntes/bdaplicado01hadoop.html#node-manager","text":"Gestiona los trabajos con las instrucciones del Resource Manager y proporciona los recursos computacionales necesarios para las aplicaciones en forma de contenedores. Implementa Heartbeats para mantener informado del estado al Resource Manager . Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos.","title":"Node Manager"},{"location":"apuntes/bdaplicado01hadoop.html#application-master","text":"El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n Yarn en el contenedor correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del APU con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo.","title":"Application Master"},{"location":"apuntes/bdaplicado01hadoop.html#instalacion","text":"Para trabajar en este y las siguientes sesi\u00f3n, vamos a trabajar con la m\u00e1quina virtual que tenemos compartida en Aules. A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario hadoop y la contrase\u00f1a hadoop . Si quieres instalar el software del curso, se recomiendo crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n 20.04 LTS y la versi\u00f3n 3.2.2 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.","title":"Instalaci\u00f3n"},{"location":"apuntes/bdaplicado01hadoop.html#configuracion","text":"Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de fichero, que normalmente ser\u00e1 hdfs, indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://dominioNodoMaestro:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /carpetaData/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /carpetaData/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro solo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> dominioNodoMaestro </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration>","title":"Configuraci\u00f3n"},{"location":"apuntes/bdaplicado01hadoop.html#puesta-en-marcha","text":"Arrancando HDFS Para arrancar Hadoop, hemos de ejecutar el comando startdfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://localhost:9870/ podremos acceder a una interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para arrancar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n como las m\u00e9tricas del cl\u00faster. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obtienendo la siguiente p\u00e1gina: Interfaz Web de YARN","title":"Puesta en marcha"},{"location":"apuntes/bdaplicado01hadoop.html#ambari","text":"Ambari es un producto que simplifica la gesti\u00f3n de Hadoop y permite configurar, instalar y monitorizar un cluster Hadoop. La instalaci\u00f3n que tenemos no nos sirve, ya que Ambari crea un nueva instalaci\u00f3n tanto de Hadoop y YARN. Para su instalaci\u00f3n desde cero, es recomendable seguir las indicaciones de su p\u00e1gina oficial . Nosotros vamos a resumir los pasos recomendados. Para instalar en Ubuntu el servidor Amabari \u00fanicamente deber\u00edamos ejecutar el siguiente comando (tambi\u00e9n va a instalar PostgreSQL donde almacenar\u00e1 todos los metadados sobre la configuraci\u00f3n de Ambari): apt-get install ./ambari-server*.deb # Instalar\u00e1 tambi\u00e9n PostgreSQL Una vez instalado, solo queda configurarlo (como usuario con permisos root ): ambari-server setup Y finalmente arrancarlo: ambari-server start Ahora tocar\u00eda instalar los agentes en cada uno de los nodos, as\u00ed como configurarlo. Como vamos a trabajar con un modelo pseudodistribuido, en nuestro caso no vamos a hacer ese paso. Finalmente, accederemos al interfaz gr\u00e1fico con el usuario admin/admin y acceder http://localhost:8080. FIXME: captura Al crear un cluster, Ambari se basa en los stacks de HortonWorks HDP para realizar la instalaci\u00f3n de Hadoop. FIXME: captura Revisar nodos Instalar SSH en usuario root","title":"Ambari"},{"location":"apuntes/bdaplicado01hadoop.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop Art\u00edculo de Hadoop por dentro Tutorial de Hadoop de Tutorialspoint","title":"Referencias"},{"location":"apuntes/bdaplicado01hadoop.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resutlado de cada acci\u00f3n Comandos HDFS Crear carpeta, meter 2 archivos, duplicar, renombrar y sacar FIXME: escribir HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . Realiza un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo. Realiza un checkpoint Vuelve a entrar al modo normal Comprueba que los fimage del namenode son iguales. HDFS / Administraci\u00f3n Ejecutar todos los comandos del apartado haciendo capturas. Sobre el contenido creado en el ejercicio 1, crear una snapshot, borrar un archivo, y recuperarlo de la snapshot FIXME: escribir (opcional) Crea una instalaci\u00f3n desde cero en una nueva m\u00e1quina virtual siguiendo las intrucciones del art\u00edculo recomendado en el apartado de Configuracion . Arranca Hadoop, muestra los procesos y accede al interfaz gr\u00e1fico. (opcional) Crea una instalaci\u00f3n desde cero mediante Ambari. Tras acceder al interfaz gr\u00e1fico, crea un cluster llamado cluster2122 . Instala Hadoop y accede al interfaz gr\u00e1fico.","title":"Actividades"},{"location":"apuntes/bdaplicado02hdfs.html","text":"CLI \u00b6 HDFS por defecto puerto 8020\u200b Interface POSIX\u200b Hadoop fs -help\u200b Referencias \u00b6 aaa https://empresas.blogthinkbig.com/hadoop-por-dentro-ii-hdfs-y-mapreduce/ Actividades \u00b6 A partir de un fichero java, hacer el ejemplo de contar las palabras del quijote .... \u00bfbuscar en Python?","title":"Bdaplicado02hdfs"},{"location":"apuntes/bdaplicado02hdfs.html#cli","text":"HDFS por defecto puerto 8020\u200b Interface POSIX\u200b Hadoop fs -help\u200b","title":"CLI"},{"location":"apuntes/bdaplicado02hdfs.html#referencias","text":"aaa https://empresas.blogthinkbig.com/hadoop-por-dentro-ii-hdfs-y-mapreduce/","title":"Referencias"},{"location":"apuntes/bdaplicado02hdfs.html#actividades","text":"A partir de un fichero java, hacer el ejemplo de contar las palabras del quijote .... \u00bfbuscar en Python?","title":"Actividades"},{"location":"apuntes/bdaplicado03hive.html","text":"Hive \u00b6 Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un c\u00faster de Hadoop . Permite leer, escribir y gestionar grandes dataset (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop. FIXME: poner logo El proyecto comenz\u00f3 en el 2008 por Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os. Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop Relaci\u00f3n de Hive con Hadoop y otras herramientas, Aunque en principio estaba dise\u00f1ado para el procesamiento por lotes ahora se integra con frameworks de tiempo real como Tez y Spark. Dicho de otro modo, Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduc e u otras tecnolog\u00edas. Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema, y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase. . Hive permite evitar la complejidad de escribir trabajos Tez basados en DAG (directed acyclic graphs) o programas MapReduce en un lenguaje de programaci\u00f3n inferior como es Java. Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n puede personalizar el procesamiento de consultas creando un esquema de tabla acorde con sus datos, sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON, cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive ejecuta en varios data nodes en paralelo, con trabajos MapReduce asociados. Estas operaciones son de tipo batch, por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. Hive incorpora Beeline, un cliente basado en JDBC para hacer consultas por l\u00ednea de comandos contra el componente HiveServer , sin necesitar las dependencias de Hive. Por otro lado, tambi\u00e9n incorpora Hive CLI, un cliente basado en Apache Thrift , que usa los mismos drivers que Hive. Hive como Data Warehouse Cuando se empezaba a generalizar el procesamiento de datos de negocio masivos, se usaban las mismas bases de datos para procesar las transacciones y para hacer consultas anal\u00edticas. Sin embargo, las organizaciones pronto empezaron a separar las consultas anal\u00edticas a una base de datos distinta llamada Data Warehouse. Esta base de datos contiene copias de solo lectura de todos los datos en los sistemas transaccionales y operacionales (OLTP). Los datos se extraen peri\u00f3dicamente de las bases de datos OLTP, se transforman y se limpian para adaptarlos esquemas que facilitan la anal\u00edtica y se insertan en el Data Warehouse (OLAP). Este es el proceso conocido como ETL. El modelo OLTP (Online Transaction Processing) requiere operaciones transaccionales. Es una categor\u00eda de procesamiento basada en tareas transaccionales, generalmente consisten en actualizar, insertar o eliminar peque\u00f1os conjuntos de datos. Para mantener su integridad, estas bases de datos deben cumplir las propiedades ACID, que garantizan la Atomicidad, Consistencia, Aislamiento y Durabilidad de las transacciones. Por otra parte, aunque Hive est\u00e1 m\u00e1s cerca de ser una base de datos tipo OLAP (Online Analytical Processing), tampoco satisface la parte en l\u00ednea o la rapidez de respuesta, como hace Apache Kylin. Estas herramientas t\u00edpicamente est\u00e1n optimizadas para consultar grandes conjuntos de datos o todos los registros disponibles. Estructura de datos en Hive \u00b6 Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: Tablas, particiones y buckets. Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Hive permite crear tablas externas, similares a las tablas en una Base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS. Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento de las consultas en el caso de usar filtros con cl\u00e1usula WHERE. Otro concepto importante en Hive son los Buckets. Son particiones hasheadas, en las que los datos se distribuyen en funci\u00f3n de su valor hash. Los Buckets pueden acelerar las operaciones de tipo JOIN si las claves de particionado y de JOIN coinciden. Debido a los beneficios de las particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. ENTIDAD EJEMPLO UBICACI\u00d3N base de datos testdb $WH/testdb.db tabla T $WH[/testdb.db]/T partici\u00f3n fecha=\u201901012020\u2032 $WH[/testdb.db]/T/fecha=01012020 bucket column userid $WH[/testdb.db]/T/fecha=01012020/000000_0 \u2026 $WH[/testdb.db]/T/fecha=01012020/000032_0 Hive tambi\u00e9n permite una operaci\u00f3n de sampling sobre una tabla, por la que se obtienen valores aleatorios o una \u201cmuestra\u201d sobre la que realizar anal\u00edtica o transformaciones sin tener que tratar el dataset completo, que en ocasiones es inviable. La pol\u00edtica del esquema es schema-on-read, de forma que solo se obliga en las operaciones de lectura. Esta propiedad permite a Hive ser m\u00e1s flexible en la lectura de los datos: un mismo dato se puede ajustar a varios esquemas, uno en cada lectura. Los sistemas RDBMS tienen una pol\u00edtica schema-on-write, que obliga a las escrituras a cumplir un esquema. En este caso acelera las lecturas. Componentes \u00b6 En Hive 3 se deja de soportar MapReduce. Apache Tez lo reemplaza como el motor de ejecuci\u00f3n por defecto. Tez es un framework de procesamiento que mejora el rendimiento y ejecuta sobre Hadoop Yarn, que encola y planifica los trabajos en el cl\u00faster. Adem\u00e1s de Tez, Hive tambi\u00e9n puede usar Apache Spark como motor de ejecuci\u00f3n. Hive Server \u00b6 HiveServer 2 (HS2) es la \u00faltima versi\u00f3n del servicio. Se compone de una interfaz que permite a clientes externos ejecutar consultas contra Apache Hive y obtener los resultados. Est\u00e1 basado en Thrift RPC y soporta clientes concurrentes. A este servidor nos conectaremos mediante Beeline (Beeline CLI) herramienta en modo comando. Hive Metastore \u00b6 Es el repositorio central para los metatados de Hive, y se almacena una base de datos relacional como MySQL, PostgreSQL o Apache Derby (embebida). Mantiene los metadatos, las tablas y sus tipos mediante Hive DDL ( Data Definition Language ). Adem\u00e1s, el sistema se puede configurar para que tambi\u00e9n almacene estad\u00edsticas de las operaciones y registros de autorizaci\u00f3n para optimizar las consultas. En las \u00faltimas versiones de Hive, este componente se puede desplegar de forma remota e independiente, para no compartir la misma JVM con HiveServer . Dentro del metastore podemos encontrar el Hive Catalog ( HCatalog ), que permite acceder a sus metadatos, actuando como una API. Al poder desplegarse de forma aislada e independiente, permite que otras aplicaciones hagan uso del Schema sin tener que desplegar el motor de consultas de Hive. As\u00ed pues, al metastore podremos acceder mediante HiveCLI, o a traves del Hive Server, por ejemplo con Beeline. Arquitectura de alto nivel Hive Arquitectura de Apache Hive Hive LLAP Hive LLAP (Low Latency Analytical Processing) fue a\u00f1adido como motor a Hive 2.0. Requiere Tez como motor de ejecuci\u00f3n y aporta funcionalidades de cach\u00e9 de datos y metadatos en memoria, acelerando mucho algunos tipos de consulta. Es especialmente significativo en consultas repetitivas para las que ofrecer tiempos de respuesta menores al segundo. LLAP se compone de un conjunto de demonios que ejecutan partes de consultas Hive. Las tareas de los ejecutores, por tanto, se encuentran dentro de los demonios y no en los contenedores. En este caso, la sesi\u00f3n Tez tendr\u00e1 solamente un contenedor, que corresponde al coordinador de consultas. Esquema de Arquitectura en Hive LLAP Esquema de Arquitectura en Hive LLAP Hive LLAP tiene en cuenta las transacciones ACID y tiene una pol\u00edtica de desalojo de cach\u00e9 personalizable y optimizada para operaciones anal\u00edticas. Tambi\u00e9n soporta federaci\u00f3n de consultas en HDFS, almacenamiento de objetos e integraci\u00f3n con tecnolog\u00edas de streaming y de tiempo real como Apache Kafka y Apache Druid. Es una tecnolog\u00eda similar a Apache Impala pensada para cargas big data. Hive LLAP es ideal en entornos empresariales de Data Warehouse, en los que nos podemos encontrar consultas repetitivas pero muy pesadas en su primera ejecuci\u00f3n, con transformaciones complejas y joins sobre grandes cantidades de datos. Ventajas Reduce la complejidad de la programaci\u00f3n MapReduce al usar HQL como lenguaje de consulta (dialecto de SQL). Est\u00e1 orientado a aplicaciones de tipo Data Warehouse, con datos est\u00e1ticos, poco cambiantes y sin requisitos de tiempos de respuesta r\u00e1pidos. Permite a los usuarios despreocuparse de en qu\u00e9 formato y d\u00f3nde se almacenan los datos. Incorpora Beeline: una herramienta por l\u00ednea de comandos para realizar consultas con HQL. Desventajas Hive no es la mejor opci\u00f3n para consultas en tiempo real o de tipo OLTP (Online Transaction Processing). Hive no est\u00e1 dise\u00f1ado para usarse con actualizaciones de valores al nivel de registro. Soporte SQL limitado: no existen sub-queries. Los tipos de datos de las tablas son muy similares a los de SQL (TINYIN, INT, FLOAT, BOOLEAN, TIMESTAMP, ... ) as\u00ed como tambi\u00e9n tiene soporte para tipos de datos complejos, como ARRAY (conjunto de datos del mismo tipo), MAP( conjunto de pares clave/valor) o STRUCT (conjunto de valores de distinto tipo). Ejemplo de consulta en Apache Hive A continuaci\u00f3n vamos a escribir un ejemplo de consultas en Apache Hive. Para empezar podemos crear una base de datos: CREATE DATABASE mydb; USE mydb; La sintaxis para crear una tabla es la siguiente: CREATE TABLE IF NOT EXISTS mydb.pagina ( view_time INT, user_id BIGINT, page_url STRING, ip STRING PARTITIONED BY (dt STRING) CLUSTERED BY (user_id) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS ORC; Con ello, hemos creado una tabla llamada pagina con 4 columnas. Se ha especificado una partici\u00f3n y la columna user_id se usar\u00e1 para el bucket. El formato indica ORC (Binario). Para insertar datos podemos cargarlos de un fichero o bien insertar los valores desde la propia consulta: LOAD DATA LOCAL INPATH '/tmp/pagina_2020-01-01.txt' OVERWRITE INTO TABLE mydb.pagina PARTITION (dt='2020-01-01'); INSERT INTO TABLE mydb.pagina partition(dt='2020-01-01') values (1,1,'s1','t1'); Especificando LOCAL, indicamos que el fichero se encuentra en el sistema de ficheros local. Por defecto, interpretar\u00e1 que el fichero se encuentra en HDFS. Referencias \u00b6 Tutorial de Hive de TutorialsPoint. Tutorial de Pih de TutorialsPoint. Actividades \u00b6","title":"Hive"},{"location":"apuntes/bdaplicado03hive.html#hive","text":"Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un c\u00faster de Hadoop . Permite leer, escribir y gestionar grandes dataset (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop. FIXME: poner logo El proyecto comenz\u00f3 en el 2008 por Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os. Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop Relaci\u00f3n de Hive con Hadoop y otras herramientas, Aunque en principio estaba dise\u00f1ado para el procesamiento por lotes ahora se integra con frameworks de tiempo real como Tez y Spark. Dicho de otro modo, Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduc e u otras tecnolog\u00edas. Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema, y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase. . Hive permite evitar la complejidad de escribir trabajos Tez basados en DAG (directed acyclic graphs) o programas MapReduce en un lenguaje de programaci\u00f3n inferior como es Java. Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n puede personalizar el procesamiento de consultas creando un esquema de tabla acorde con sus datos, sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON, cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive ejecuta en varios data nodes en paralelo, con trabajos MapReduce asociados. Estas operaciones son de tipo batch, por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. Hive incorpora Beeline, un cliente basado en JDBC para hacer consultas por l\u00ednea de comandos contra el componente HiveServer , sin necesitar las dependencias de Hive. Por otro lado, tambi\u00e9n incorpora Hive CLI, un cliente basado en Apache Thrift , que usa los mismos drivers que Hive. Hive como Data Warehouse Cuando se empezaba a generalizar el procesamiento de datos de negocio masivos, se usaban las mismas bases de datos para procesar las transacciones y para hacer consultas anal\u00edticas. Sin embargo, las organizaciones pronto empezaron a separar las consultas anal\u00edticas a una base de datos distinta llamada Data Warehouse. Esta base de datos contiene copias de solo lectura de todos los datos en los sistemas transaccionales y operacionales (OLTP). Los datos se extraen peri\u00f3dicamente de las bases de datos OLTP, se transforman y se limpian para adaptarlos esquemas que facilitan la anal\u00edtica y se insertan en el Data Warehouse (OLAP). Este es el proceso conocido como ETL. El modelo OLTP (Online Transaction Processing) requiere operaciones transaccionales. Es una categor\u00eda de procesamiento basada en tareas transaccionales, generalmente consisten en actualizar, insertar o eliminar peque\u00f1os conjuntos de datos. Para mantener su integridad, estas bases de datos deben cumplir las propiedades ACID, que garantizan la Atomicidad, Consistencia, Aislamiento y Durabilidad de las transacciones. Por otra parte, aunque Hive est\u00e1 m\u00e1s cerca de ser una base de datos tipo OLAP (Online Analytical Processing), tampoco satisface la parte en l\u00ednea o la rapidez de respuesta, como hace Apache Kylin. Estas herramientas t\u00edpicamente est\u00e1n optimizadas para consultar grandes conjuntos de datos o todos los registros disponibles.","title":"Hive"},{"location":"apuntes/bdaplicado03hive.html#estructura-de-datos-en-hive","text":"Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: Tablas, particiones y buckets. Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Hive permite crear tablas externas, similares a las tablas en una Base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS. Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento de las consultas en el caso de usar filtros con cl\u00e1usula WHERE. Otro concepto importante en Hive son los Buckets. Son particiones hasheadas, en las que los datos se distribuyen en funci\u00f3n de su valor hash. Los Buckets pueden acelerar las operaciones de tipo JOIN si las claves de particionado y de JOIN coinciden. Debido a los beneficios de las particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. ENTIDAD EJEMPLO UBICACI\u00d3N base de datos testdb $WH/testdb.db tabla T $WH[/testdb.db]/T partici\u00f3n fecha=\u201901012020\u2032 $WH[/testdb.db]/T/fecha=01012020 bucket column userid $WH[/testdb.db]/T/fecha=01012020/000000_0 \u2026 $WH[/testdb.db]/T/fecha=01012020/000032_0 Hive tambi\u00e9n permite una operaci\u00f3n de sampling sobre una tabla, por la que se obtienen valores aleatorios o una \u201cmuestra\u201d sobre la que realizar anal\u00edtica o transformaciones sin tener que tratar el dataset completo, que en ocasiones es inviable. La pol\u00edtica del esquema es schema-on-read, de forma que solo se obliga en las operaciones de lectura. Esta propiedad permite a Hive ser m\u00e1s flexible en la lectura de los datos: un mismo dato se puede ajustar a varios esquemas, uno en cada lectura. Los sistemas RDBMS tienen una pol\u00edtica schema-on-write, que obliga a las escrituras a cumplir un esquema. En este caso acelera las lecturas.","title":"Estructura de datos en Hive"},{"location":"apuntes/bdaplicado03hive.html#componentes","text":"En Hive 3 se deja de soportar MapReduce. Apache Tez lo reemplaza como el motor de ejecuci\u00f3n por defecto. Tez es un framework de procesamiento que mejora el rendimiento y ejecuta sobre Hadoop Yarn, que encola y planifica los trabajos en el cl\u00faster. Adem\u00e1s de Tez, Hive tambi\u00e9n puede usar Apache Spark como motor de ejecuci\u00f3n.","title":"Componentes"},{"location":"apuntes/bdaplicado03hive.html#hive-server","text":"HiveServer 2 (HS2) es la \u00faltima versi\u00f3n del servicio. Se compone de una interfaz que permite a clientes externos ejecutar consultas contra Apache Hive y obtener los resultados. Est\u00e1 basado en Thrift RPC y soporta clientes concurrentes. A este servidor nos conectaremos mediante Beeline (Beeline CLI) herramienta en modo comando.","title":"Hive Server"},{"location":"apuntes/bdaplicado03hive.html#hive-metastore","text":"Es el repositorio central para los metatados de Hive, y se almacena una base de datos relacional como MySQL, PostgreSQL o Apache Derby (embebida). Mantiene los metadatos, las tablas y sus tipos mediante Hive DDL ( Data Definition Language ). Adem\u00e1s, el sistema se puede configurar para que tambi\u00e9n almacene estad\u00edsticas de las operaciones y registros de autorizaci\u00f3n para optimizar las consultas. En las \u00faltimas versiones de Hive, este componente se puede desplegar de forma remota e independiente, para no compartir la misma JVM con HiveServer . Dentro del metastore podemos encontrar el Hive Catalog ( HCatalog ), que permite acceder a sus metadatos, actuando como una API. Al poder desplegarse de forma aislada e independiente, permite que otras aplicaciones hagan uso del Schema sin tener que desplegar el motor de consultas de Hive. As\u00ed pues, al metastore podremos acceder mediante HiveCLI, o a traves del Hive Server, por ejemplo con Beeline. Arquitectura de alto nivel Hive Arquitectura de Apache Hive Hive LLAP Hive LLAP (Low Latency Analytical Processing) fue a\u00f1adido como motor a Hive 2.0. Requiere Tez como motor de ejecuci\u00f3n y aporta funcionalidades de cach\u00e9 de datos y metadatos en memoria, acelerando mucho algunos tipos de consulta. Es especialmente significativo en consultas repetitivas para las que ofrecer tiempos de respuesta menores al segundo. LLAP se compone de un conjunto de demonios que ejecutan partes de consultas Hive. Las tareas de los ejecutores, por tanto, se encuentran dentro de los demonios y no en los contenedores. En este caso, la sesi\u00f3n Tez tendr\u00e1 solamente un contenedor, que corresponde al coordinador de consultas. Esquema de Arquitectura en Hive LLAP Esquema de Arquitectura en Hive LLAP Hive LLAP tiene en cuenta las transacciones ACID y tiene una pol\u00edtica de desalojo de cach\u00e9 personalizable y optimizada para operaciones anal\u00edticas. Tambi\u00e9n soporta federaci\u00f3n de consultas en HDFS, almacenamiento de objetos e integraci\u00f3n con tecnolog\u00edas de streaming y de tiempo real como Apache Kafka y Apache Druid. Es una tecnolog\u00eda similar a Apache Impala pensada para cargas big data. Hive LLAP es ideal en entornos empresariales de Data Warehouse, en los que nos podemos encontrar consultas repetitivas pero muy pesadas en su primera ejecuci\u00f3n, con transformaciones complejas y joins sobre grandes cantidades de datos. Ventajas Reduce la complejidad de la programaci\u00f3n MapReduce al usar HQL como lenguaje de consulta (dialecto de SQL). Est\u00e1 orientado a aplicaciones de tipo Data Warehouse, con datos est\u00e1ticos, poco cambiantes y sin requisitos de tiempos de respuesta r\u00e1pidos. Permite a los usuarios despreocuparse de en qu\u00e9 formato y d\u00f3nde se almacenan los datos. Incorpora Beeline: una herramienta por l\u00ednea de comandos para realizar consultas con HQL. Desventajas Hive no es la mejor opci\u00f3n para consultas en tiempo real o de tipo OLTP (Online Transaction Processing). Hive no est\u00e1 dise\u00f1ado para usarse con actualizaciones de valores al nivel de registro. Soporte SQL limitado: no existen sub-queries. Los tipos de datos de las tablas son muy similares a los de SQL (TINYIN, INT, FLOAT, BOOLEAN, TIMESTAMP, ... ) as\u00ed como tambi\u00e9n tiene soporte para tipos de datos complejos, como ARRAY (conjunto de datos del mismo tipo), MAP( conjunto de pares clave/valor) o STRUCT (conjunto de valores de distinto tipo). Ejemplo de consulta en Apache Hive A continuaci\u00f3n vamos a escribir un ejemplo de consultas en Apache Hive. Para empezar podemos crear una base de datos: CREATE DATABASE mydb; USE mydb; La sintaxis para crear una tabla es la siguiente: CREATE TABLE IF NOT EXISTS mydb.pagina ( view_time INT, user_id BIGINT, page_url STRING, ip STRING PARTITIONED BY (dt STRING) CLUSTERED BY (user_id) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS ORC; Con ello, hemos creado una tabla llamada pagina con 4 columnas. Se ha especificado una partici\u00f3n y la columna user_id se usar\u00e1 para el bucket. El formato indica ORC (Binario). Para insertar datos podemos cargarlos de un fichero o bien insertar los valores desde la propia consulta: LOAD DATA LOCAL INPATH '/tmp/pagina_2020-01-01.txt' OVERWRITE INTO TABLE mydb.pagina PARTITION (dt='2020-01-01'); INSERT INTO TABLE mydb.pagina partition(dt='2020-01-01') values (1,1,'s1','t1'); Especificando LOCAL, indicamos que el fichero se encuentra en el sistema de ficheros local. Por defecto, interpretar\u00e1 que el fichero se encuentra en HDFS.","title":"Hive Metastore"},{"location":"apuntes/bdaplicado03hive.html#referencias","text":"Tutorial de Hive de TutorialsPoint. Tutorial de Pih de TutorialsPoint.","title":"Referencias"},{"location":"apuntes/bdaplicado03hive.html#actividades","text":"","title":"Actividades"},{"location":"apuntes/ingesta01.html","text":"Ingesta de Datos \u00b6 Introducci\u00f3n \u00b6 Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, de diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Un pipeline de datos consume datos de un punto de origen, los limpia y los escribe en un nuevo destino. La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de daots puedan contruir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data, para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y que se pretende conseguir con ellos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake asegur\u00e1ndose que los datos son compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n. La ingesta por dentro \u00b6 La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que aplica uno m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almacen de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregacaiones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. Ingesta de datos Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: Servicios de mensajer\u00eda como Apache Kafka Bases de datos relaciones, las cuales se acceden, por ejemplo, JDBC Servicios REST que vuelven los datos en formato JSON Servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: Servicios de mensajer\u00eda como Apache Kafka Bases de datos relaciones Bases de datos NoSQL Servicios de almacenamiento distribuido como HDFS o S3. Plataformas de datos como Snowflake o Databricks. Pipeline de Datos \u00b6 Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean. El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (como un CRM), encargados de capturar y crear datos, y sistemas OLAP (como un Data Warehouse ), encargados de analizar los datos. Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Recogemos los datos y los enviamos a un topic de Apache Kafka. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Ejemplo de pipeline - aprenderbigdata.com Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch, leemos los datos del buffer. Por ejemplo, mediante Spark realizmaos la anal\u00edtica sobre estos datos. Almacenamos el resultado en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuidos como Amazon S3 . Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas. ETL \u00b6 https://www.talend.com/es/resources/what-is-etl/ https://www.informatica.com/resources/articles/what-is-etl.html https://www.informatica.com/blogs/etl-vs-elt-whats-the-difference.html Los procesos ETL, siglas de extracci\u00f3n, transformaci\u00f3n y carga ( load ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto. Extracci\u00f3n \u00b6 Esta fase de un proceso ETL es la encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Los formatos de las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales entre otros formatos distintos. Una parte de la extracci\u00f3n es la de analizar que los datos sean los que se esperaban, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser trasparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se generase en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n Transformaci\u00f3n \u00b6 En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n Eliminar datos duplicados Cruzar diferentes fuentes de datos para obtener una fuente diferente Agregar informaci\u00f3n en funci\u00f3n de alguna variable Tomar parte de los datos para cargarlos Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n Estructurar mejor la informaci\u00f3n Generar indicadores que faciliten el procesamiento y entendimiento Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados los datos, ya estar\u00e1n listos para su carga. Carga \u00b6 Fase encargada de almacenar los datos en el destino, un Data Warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga ha de realizarse buscando minimizar el tiempo de la transacci\u00f3n Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift) Cargadores Propios (HDFS, Teradata, Greenplum) Se pueden realizar acciones para mejorar estos procesos: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s https://www.informatica.com/blogs/etl-vs-elt-whats-the-difference.html https://www.franciscojavierpulido.com/2013/11/paradigmas-bigdata-el-procesamiento.html Herramientas ETL \u00b6 Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD Transaccionales, XML, Access, Teradata, HDFS, Hive, CRM APIs de Aplicaciones de terceros, Logs\u2026 Permiten la planificaci\u00f3n y ejecuci\u00f3n de l\u00f3gica Planificaci\u00f3n por Batch Planificaci\u00f3n por eventos Planificaci\u00f3n en tiempo real Capacidad para transformar los datos Transformaciones Simples: Tipos de datos, cadenas, codificaciones, c\u00e1lculos simples Transformaciones Intermedias: Agregaciones, lookups, Transformaciones Complejas: Algoritmos de IA, Segmentaci\u00f3n, Integraci\u00f3n de c\u00f3digo de terceros, Integraci\u00f3n con otros lenguajes Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL Arquitectura de Ingesta de datos \u00b6 https://ezdatamunch.com/what-is-data-ingestion/ Herramientas de Ingesta de datos \u00b6 Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Apache Nifi : herramienta ETL que se encarga de cargar datos de diferentes fuentes, los pasa por un flujo de procesos para su tratamiento, y los vuelca en otra fuente. Apache Sqoop : transferencia bidireccional de datos entre Hadoop y una bases de datos SQL (datos estructurados) Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados en streaming sobre HDFS o HBase. Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services. Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure. Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud. This stage of the data processing pipeline has some overlap with the Collection stage. Data can be collected by or ingested into AWS services in various ways. The following two managed AWS services\u2014which can be used for ingestion\u2014are included in this course. AWS Glue (Enlaces a un sitio externo.): AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. ETL jobs can be created with a few clicks in the AWS Management Console. AWS Glue can discover data and store the inferred schema in the AWS Glue Data Catalog, which can then be available for ETL. AWS Glue can also act as a remote metadata store for various AWS services like Amazon Athena, AWS Data Pipeline, etc. AWS Data Pipeline (Enlaces a un sitio externo.): Data Pipeline is a managed service that can be used to move data between various data sources in the AWS Cloud, like Amazon S3, Amazon RDS, DynamoDB, Amazon Redshift, and Amazon EMR. It can reduce the complexities of handling data pipelines, and reliably move data from source to destination in a cost-effective way. Consideraciones \u00b6 A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data, hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCual va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos? \u00bfSer\u00e1n datos estructurados o datos sin estructura? \u00bfCu\u00e1l es el volumen de los datos? Volumen diario, y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/dias (mediante procesos batch) o ser real-time (mediante streaming*) Actualizaciones \u00bfLas fuentes origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? * \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes +insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y Cassandra? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / Cassandra ? \u00bfQu\u00e9 procesos de transformaci\u00f3n de datos se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Seguridad de los datos. Si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta? Referencias \u00b6 Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility 17 Enero https://www.xenonstack.com/blog/big-data-ingestion https://streamsets.com/learn/data-ingestion/ https://ezdatamunch.com/what-is-data-ingestion/ https://streamsets.com/learn/etl-or-elt/ https://aprenderbigdata.com/pipeline-de-datos/ https://www.xenonstack.com/blog/big-data-ingestion https://www.xenonstack.com/blog/data-pipeline","title":"Ingesta de Datos"},{"location":"apuntes/ingesta01.html#ingesta-de-datos","text":"","title":"Ingesta de Datos"},{"location":"apuntes/ingesta01.html#introduccion","text":"Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, de diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Un pipeline de datos consume datos de un punto de origen, los limpia y los escribe en un nuevo destino. La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de daots puedan contruir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data, para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y que se pretende conseguir con ellos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake asegur\u00e1ndose que los datos son compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n.","title":"Introducci\u00f3n"},{"location":"apuntes/ingesta01.html#la-ingesta-por-dentro","text":"La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que aplica uno m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almacen de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregacaiones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. Ingesta de datos Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: Servicios de mensajer\u00eda como Apache Kafka Bases de datos relaciones, las cuales se acceden, por ejemplo, JDBC Servicios REST que vuelven los datos en formato JSON Servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: Servicios de mensajer\u00eda como Apache Kafka Bases de datos relaciones Bases de datos NoSQL Servicios de almacenamiento distribuido como HDFS o S3. Plataformas de datos como Snowflake o Databricks.","title":"La ingesta por dentro"},{"location":"apuntes/ingesta01.html#pipeline-de-datos","text":"Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean. El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (como un CRM), encargados de capturar y crear datos, y sistemas OLAP (como un Data Warehouse ), encargados de analizar los datos. Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Recogemos los datos y los enviamos a un topic de Apache Kafka. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Ejemplo de pipeline - aprenderbigdata.com Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch, leemos los datos del buffer. Por ejemplo, mediante Spark realizmaos la anal\u00edtica sobre estos datos. Almacenamos el resultado en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuidos como Amazon S3 . Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas.","title":"Pipeline de Datos"},{"location":"apuntes/ingesta01.html#etl","text":"https://www.talend.com/es/resources/what-is-etl/ https://www.informatica.com/resources/articles/what-is-etl.html https://www.informatica.com/blogs/etl-vs-elt-whats-the-difference.html Los procesos ETL, siglas de extracci\u00f3n, transformaci\u00f3n y carga ( load ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto.","title":"ETL"},{"location":"apuntes/ingesta01.html#extraccion","text":"Esta fase de un proceso ETL es la encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Los formatos de las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales entre otros formatos distintos. Una parte de la extracci\u00f3n es la de analizar que los datos sean los que se esperaban, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser trasparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se generase en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n","title":"Extracci\u00f3n"},{"location":"apuntes/ingesta01.html#transformacion","text":"En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n Eliminar datos duplicados Cruzar diferentes fuentes de datos para obtener una fuente diferente Agregar informaci\u00f3n en funci\u00f3n de alguna variable Tomar parte de los datos para cargarlos Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n Estructurar mejor la informaci\u00f3n Generar indicadores que faciliten el procesamiento y entendimiento Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados los datos, ya estar\u00e1n listos para su carga.","title":"Transformaci\u00f3n"},{"location":"apuntes/ingesta01.html#carga","text":"Fase encargada de almacenar los datos en el destino, un Data Warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga ha de realizarse buscando minimizar el tiempo de la transacci\u00f3n Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift) Cargadores Propios (HDFS, Teradata, Greenplum) Se pueden realizar acciones para mejorar estos procesos: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s https://www.informatica.com/blogs/etl-vs-elt-whats-the-difference.html https://www.franciscojavierpulido.com/2013/11/paradigmas-bigdata-el-procesamiento.html","title":"Carga"},{"location":"apuntes/ingesta01.html#herramientas-etl","text":"Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD Transaccionales, XML, Access, Teradata, HDFS, Hive, CRM APIs de Aplicaciones de terceros, Logs\u2026 Permiten la planificaci\u00f3n y ejecuci\u00f3n de l\u00f3gica Planificaci\u00f3n por Batch Planificaci\u00f3n por eventos Planificaci\u00f3n en tiempo real Capacidad para transformar los datos Transformaciones Simples: Tipos de datos, cadenas, codificaciones, c\u00e1lculos simples Transformaciones Intermedias: Agregaciones, lookups, Transformaciones Complejas: Algoritmos de IA, Segmentaci\u00f3n, Integraci\u00f3n de c\u00f3digo de terceros, Integraci\u00f3n con otros lenguajes Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL","title":"Herramientas ETL"},{"location":"apuntes/ingesta01.html#arquitectura-de-ingesta-de-datos","text":"https://ezdatamunch.com/what-is-data-ingestion/","title":"Arquitectura de Ingesta de datos"},{"location":"apuntes/ingesta01.html#herramientas-de-ingesta-de-datos","text":"Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Apache Nifi : herramienta ETL que se encarga de cargar datos de diferentes fuentes, los pasa por un flujo de procesos para su tratamiento, y los vuelca en otra fuente. Apache Sqoop : transferencia bidireccional de datos entre Hadoop y una bases de datos SQL (datos estructurados) Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados en streaming sobre HDFS o HBase. Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services. Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure. Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud. This stage of the data processing pipeline has some overlap with the Collection stage. Data can be collected by or ingested into AWS services in various ways. The following two managed AWS services\u2014which can be used for ingestion\u2014are included in this course. AWS Glue (Enlaces a un sitio externo.): AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. ETL jobs can be created with a few clicks in the AWS Management Console. AWS Glue can discover data and store the inferred schema in the AWS Glue Data Catalog, which can then be available for ETL. AWS Glue can also act as a remote metadata store for various AWS services like Amazon Athena, AWS Data Pipeline, etc. AWS Data Pipeline (Enlaces a un sitio externo.): Data Pipeline is a managed service that can be used to move data between various data sources in the AWS Cloud, like Amazon S3, Amazon RDS, DynamoDB, Amazon Redshift, and Amazon EMR. It can reduce the complexities of handling data pipelines, and reliably move data from source to destination in a cost-effective way.","title":"Herramientas de Ingesta de datos"},{"location":"apuntes/ingesta01.html#consideraciones","text":"A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data, hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCual va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos? \u00bfSer\u00e1n datos estructurados o datos sin estructura? \u00bfCu\u00e1l es el volumen de los datos? Volumen diario, y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/dias (mediante procesos batch) o ser real-time (mediante streaming*) Actualizaciones \u00bfLas fuentes origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? * \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes +insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y Cassandra? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / Cassandra ? \u00bfQu\u00e9 procesos de transformaci\u00f3n de datos se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Seguridad de los datos. Si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta?","title":"Consideraciones"},{"location":"apuntes/ingesta01.html#referencias","text":"Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility 17 Enero https://www.xenonstack.com/blog/big-data-ingestion https://streamsets.com/learn/data-ingestion/ https://ezdatamunch.com/what-is-data-ingestion/ https://streamsets.com/learn/etl-or-elt/ https://aprenderbigdata.com/pipeline-de-datos/ https://www.xenonstack.com/blog/big-data-ingestion https://www.xenonstack.com/blog/data-pipeline","title":"Referencias"},{"location":"apuntes/ingesta02pentaho.html","text":"Pentaho \u00b6 Kettle es un componente de Pentaho Data Integration (PDI - https://www.hitachivantara.com/en-us/products/data-management-analytics/pentaho/download-pentaho.html ) que a su vez contiene a Spoon . Mediante Spoon se pueden realizar procesos ETL de manera visual, de forma muy f\u00e1cil y r\u00e1pida, como por ejemplo: Conexiones a los datos. Transformaciones (filtrado, limpieza, formateado, ... y posterior almacenamiento en diferentes formatos y destinos). Inserci\u00f3n de f\u00f3rmulas. Desarrollo de data warehouses con estructura en estrella (Hechos/Dimensiones) Y todo esto sin necesidad de programar directamente con c\u00f3digo y sin necesidad de instalar o configurar nada para poder empezar a usarla. Por esto, este tipo de herramientas se conocen como herramientas de metadatos , ya que trabajan a nivel de definici\u00f3n diciendo qu\u00e9 hay que hacer, pero no el detalle del c\u00f3mo se hace, \u00e9ste queda oculto, lo cual resulta muy interesante en la mayor\u00eda de los casos. Pentaho - Ejemplo de flujo ETL Se trata de una herramienta open source multiplatforma que tambi\u00e9n tiene su soporte comercial. La versi\u00f3n open source se puede descargar desde https://sourceforge.net/projects/pentaho/ . En nuestro caso, vamos a trabajar con la versi\u00f3n 9.2 que data de agosto de 2021 (para los apuntes he trabajado indistintamente con la versi\u00f3n 9.1 y la 9.2, tanto en Windows como en Ubuntu). Es importante destacar el requisito que necesitamos tener instalado en el sistema al menos la versi\u00f3n 8 de Java. Instalaci\u00f3n en Ubuntu Si trabajamos con Ubuntu, ser\u00e1 necesario instalar el paquete libwebkitgtk . Para ello, primero tenemos que a\u00f1adir su repositorio: sudo nano /etc/apt/sources.list Y a\u00f1adimos al final la siguiente entrada: deb http://cz.archive.ubuntu.com/ubuntu bionic main universe Y tras actualizar los respositorios con sudo apt-get update , instalaremos el paquete con el comando sudo apt-get install libwebkitgtk-1.0.0 . Una vez descargado el archivo y descomprimirlo, mediante el archivo spoon.bat (o spoon.sh ) lanzaremos la aplicaci\u00f3n. Pantalla de inicio de Pentaho/Spoon Dentro de Spoon Spoon permite dise\u00f1ar las transformaciones y trabajos que se ejecutan con las siguientes herramientas: * Pan es un motor de transformaci\u00f3n de datos que realiza muchas funciones tales como lectura, manipulaci\u00f3n, y escritura de datos hacia y desde varias fuentes de datos. * Kitchen es un programa que ejecuta los trabajos dise\u00f1ados por Spoon en XML o en una base de datos. Para esta sesi\u00f3n, hemos planteado varios casos de uso para ir aprendiendo la herramienta mediante su uso. https://openwebinars.net/blog/que-es-pentaho-data-integraton-pdi/ Elementos \u00b6 En PDI hay dos tipos de elementos: Transformations y Jobs . Se definen Transformations para transformar los datos Se definen Jobs para organizar tareas estableciendo su orden y condiciones de ejecuci\u00f3n. Las transformaciones son un tipo de tarea. Tanto las transformaciones como las tareas, cuando se definen, se almacenan como archivos. Los elementos del interfaz son: Interfaz de Spoon Caso de Uso 0 \u00b6 Para familiarizarnos con el entorno, vamos a crear una transformaci\u00f3n muy b\u00e1sica. Tras seleccionar File -> New Transformation , el primer elemento que vamos a utilizar est\u00e1 dentro de la categor\u00eda Input . Men\u00fa emergente En concreto seleccionamos la transformaci\u00f3n Get system info , la cual nos permite obtener informaci\u00f3n sobre el sistema. La vamos a utilizar para averiguar la versi\u00f3n de PDI que estamos utilizando. As\u00ed pues, la seleccionamos desde el \u00e1rbol de pasos y lo arrastramos a la zona de trabajo. Si dejamos el rat\u00f3n sobre el elemento, nos aparecer\u00e1 un men\u00fa emergente donde podremos conectar una entrada, editar las propiedades, ver el men\u00fa contextual del paso, conectar una salida e inyectar metadatos. Sobre este paso, vamos a editar la informaci\u00f3n que queremos obtener. Para ello, vamos a crear una propiedad con nombre Versi\u00f3n Pentaho y seleccionaremos del desplegable la opci\u00f3n Kettle Version . A continuaci\u00f3n, en la categor\u00eda Utility seleccionamos el icono Write to Log , y lo arrastramos al area de trabajo. Ahora conectamos la salida de Get system info con Write to log , mediante la 4\u00aa opci\u00f3n del menu emergente, quedando una transformaci\u00f3n tal como se ve en la imagen: Caso de Uso 0 - Versi\u00f3n de PDI Finalmente, s\u00f3lo nos queda ejecutar la transformaci\u00f3n mediante el icono del tri\u00e1ngulo ( Run o F9), y ver el resultado en el panel inferior. Caso de Uso 1 - Filtrando datos \u00b6 En este caso de uso, vamos a leer un archivo CSV y vamos a filtrar los datos para quedarnos con un subconjunto de los mismos. Adem\u00e1s, vamos a ver c\u00f3mo podemos gestionar los errores y ejecutar la transformaci\u00f3n desde el terminal. Lectura CSV \u00b6 Tras crear la nueva transformaci\u00f3n (CTRL + N), desde Input arrastraremos el paso de CSV input file para seleccionar el archivo samples\\transformations\\files\\Zipssortedbycitystate.csv dentro de nuestra instalaci\u00f3n de Pentaho. Tras seleccionar el archivo, mediante el bot\u00f3n Get Fields cargaremos y comprobaremos que los campos que vamos a leer son correctos (nombre y tipo de los datos). Caso de Uso 1 - Tras pulsar sobre Get Fields Tras ello, mediante el bot\u00f3n Preview comprobaremos que los datos se leen correctamente. Caso de Uso 1 - Resultado de la opci\u00f3n Preview sobre Ciudades Filtrado de datos \u00b6 Una vez leido, el siguiente paso es filtrar las filas. Para ello, desde la categor\u00eda de Flow , arrastramos el paso Filter , y las conectamos tal como hemos realizado en el caso anterior. Al soltar la flecha, nos mostrar\u00e1 dos opciones: Main output of step : define los pasos con un flujo principal, donde todo funciona bien Error handling of step : define los pasos a seguir en caso de encontrar un error De momento elegimos la primera y configuramos el filtro para solo seleccionar aquellos datos cuyo estado sea NY. Caso de Uso 1 - Configuraci\u00f3n del filtro Para configurar el resultado, seleccionamos el paso del filtro, y bien pulsamos sobre el icono del ojo de la barra de herramientas, o sobre el paso, tras pulsar con el bot\u00f3n derecho, seleccionamos la opci\u00f3n Preview . Caso de Uso 1 - Resultado de hacer Preview sobre Filtro NY Por defecto se precargan 1000 filas. Tras comprobar el resultado, pulsamos sobre Stop para detener el proceso de previsualizaci\u00f3n. Las m\u00e9tricas que aparecen nos informaci\u00f3n del proceso y su rendimiento. Ordenaci\u00f3n \u00b6 El siguiente paso que vamos a realizar es ordenar los datos por su c\u00f3digo Postal Code . Para ello, desde la categor\u00eda de Transform , arrastramos el paso de Sort rows , y conectamos la salida del filtrado con la ordenaci\u00f3n eligiendo la salida principal ( main output of step ). Forzando un error Vamos a forzar un error para comprobar c\u00f3mo lo indica Spoon . Si al elegir el nombre del campo, en vez de POSTAL CODE escribimos CP , cuando previsualizamos el resultado, podremos ver como aparece la marca de prohibido en la esquina superior derecha del paso, y si visualizamos el log y las m\u00e9tricas de los pasos, veremos el error: Caso de Uso 1 - Forzando un error Volvemos a editar el paso, corregimos el nombre del campo (escribimos POSTAL CODE ) y comprobamos que ahora s\u00ed que funciona correctamente Caso de Uso 1 - Ordenando Escritura del resultado \u00b6 Una vez realizados todos los pasos, s\u00f3lo nos queda es enviar el resultado a un fichero para persistir la transformaci\u00f3n. Para ello, desde la categor\u00eda de Output arrastramos el paso Text file output , y lo conectamos desde la salida del paso de ordenaci\u00f3n. Tras ello, editar este paso para indicar el archivo donde almacenar el resultado. Caso de Uso 1 - Guardando el resultado Tras ello, podremos ejecutar la transformaci\u00f3n (icono del tri\u00e1ngulo, men\u00fa Action -> Run o F9) y comprobar el resultado en el fichero: CITY;STATE;POSTALCODE HOLTSVILLE ;NY;501 FISHERS ISLAND ;NY;6390 NEW YORK ;NY;10001 NEW YORK ;NY;10003 NEW YORK ;NY;10005 NEW YORK ;NY;10007 NEW YORK ;NY;10009 Al comprobar el fichero, vemos que se han quedado espacio en blanco a la derecha del nombre de las ciudades, ya que la columna ten\u00eda un tama\u00f1o configurado. Si volvemos a editar el \u00faltimo paso, en la pesta\u00f1a de Fields podemos indicar mediante el bot\u00f3n de Minimal width que reduzca su anchura al m\u00ednimo: Caso de Uso 1 - Anchura m\u00ednima de los campos Y tras volver a ejecutar la transformaci\u00f3n, veremos que ahora s\u00ed que obtenemos los datos que esper\u00e1bamos: CITY;STATE;POSTALCODE HOLTSVILLE;NY;501 FISHERS ISLAND;NY;6390 NEW YORK;NY;10001 NEW YORK;NY;10003 NEW YORK;NY;10005 NEW YORK;NY;10007 NEW YORK;NY;10009 Uso de Pan \u00b6 Mediante la utilidad Pan , podemos ejecutar las transformaciones sin necesidad de arrancar Spoon . Para indicarle el archivo que contiene la transformaci\u00f3n, al comando pan.bat (o pan.sh en el caso de Ubuntu) le pasamos el par\u00e1metro /file=rutaArchivo.ktr . Para comprobar su funcionamiento, vamos a eliminar el fichero generado. A continuaci\u00f3n, ejecutamos pan : pan.bat /file = c:/IABD/caso1filtradoNY.ktr Tras algunos segundos y varias l\u00edneas de debug del arranque de pan, tendremos un mensaje similar al siguiente: 2021/10/24 18:01:42 - Start of run. 2021/10/24 18:01:42 - caso1filtradoNY - Dispatching started for transformation [caso1filtradoNY] 2021/10/24 18:01:42 - Ciudades.0 - Header row skipped in file 'C:\\data-integration\\samples\\transformations\\files\\Zipssortedbycitystate.csv' 2021/10/24 18:01:42 - Ciudades.0 - Finished processing (I=21380, O=0, R=0, W=21379, U=0, E=0) 2021/10/24 18:01:42 - Filtro NY.0 - Finished processing (I=0, O=0, R=21379, W=1146, U=0, E=0) 2021/10/24 18:01:42 - Orden por Codigo Postal.0 - Finished processing (I=0, O=0, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:42 - CiudadesNY.0 - Finished processing (I=0, O=1147, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:43 - Carte - Installing timer to purge stale objects after 1440 minutes. 2021/10/24 18:01:43 - Finished! 2021/10/24 18:01:43 - Start=2021/10/24 18:01:42.424, Stop=2021/10/24 18:01:43.041 2021/10/24 18:01:43 - Processing ended after 0 seconds. 2021/10/24 18:01:43 - caso1filtradoNY - 2021/10/24 18:01:43 - caso1filtradoNY - Step Ciudades.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Filtro NY.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Orden por Codigo Postal.0 ended successfully, processed 1146 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step CiudadesNY.0 ended successfully, processed 1146 lines. ( - lines/s) Y si comprobamos el fichero, veremos que ha vuelto a aparecer. Caso de Uso 2 - Uniendo datos \u00b6 Caso de Uso 2 - Lecturas CSV En este caso, vamos a leer datos de ventas y de productos , y vamos a unirlos de forma similar a un join , para posteriormente, tras agrupar los datos, crear un informe. Merge join \u00b6 Para ello, primero vamos leer de forma separada cada archivo mediante un paso de tipo CSV file input , utilizando el ; como separado. As\u00ed pues, tendremos dos pasos, tal como se puede observar en la imagen de la derecha. En el caso del CSV de Ventas, al tener ventas de diferentes paises, deberemos cambiar el tipo del ZIP (el c\u00f3digo postal) a String . A continuaci\u00f3n, para unir los datos, dentro de la categor\u00eda Join utilizamos el paso Merge join . En este caso tras arrastrar el paso al \u00e1rea de trabajo, vamos a enlazar desde el merge hacia los dos or\u00edgenes, en un caso como left hand side stream of the join y el otro fomo right hand side stream of the join : Caso de Uso 2 - Entradas de Merge A continuaci\u00f3n, editamos el Merge y le decimos que el campo de uni\u00f3n es ProductID . Cuando le damos a aceptar recibimos un mensaje de advertencia avis\u00e1ndonos que si los datos no est\u00e1n ordenados podemos obtener resultados incorrectos. Caso de Uso 2 - Aviso Merge As\u00ed pues, vamos a a\u00f1adir previo al merge un paso de ordenaci\u00f3n a cada entrada, ordenando los datos por ProductID de manera ascendente. Para comprobar su funcionamiento, podemos hacer un preview del merge . Agrupando datos \u00b6 Como el resultado final es un informe de ventas con el total de unidades vendidas y la cantidad recaudada agrupada por pais y categor\u00eda del producto, necesitamos agrupar los datos. Para ello, dentro de la categor\u00eda Statistics , utilizaremos el paso Group by . En nuestro caso queremos agrupar por pa\u00eds ( Country ) y categor\u00eda ( Category ), y los datos que vamos a agregar son la suma de unidades ( Units ) y la suma recaudada ( Revenue ). As\u00ed pues, nuestra agrupaci\u00f3n quedar\u00eda as\u00ed: Caso de Uso 2 - Agrupamos por pa\u00eds y categor\u00eda En este caso, sucede lo mismo que antes, que este paso necesita los datos ordenados. As\u00ed pues, vamos a ordenar las salida del merge por pa\u00eds y categor\u00eda. Caso de Uso 2 - Ordenamos antes de agrupar El \u00faltimo paso que nos queda es exportar los datos a un fichero de texto mediante el paso Text file output . Caso de Uso 3 - Cuestionarios Airbnb \u00b6 Para el siguiente caso de uso, vamos a utilizar datos de los cuestionarios de AirBnb que se pueden descargar desde http://tomslee.net/airbnb-data-collection-get-the-data . En concreto, nos vamos a centrar en los datos de Madrid que podemos descargar desde https://s3.amazonaws.com/tomslee-airbnb-data-2/madrid.zip . Uniendo datos \u00b6 Una vez descargados los datos y descomprimidos, vamos a cargar los tres ficheros en el mismo paso, utilizando dentro de Input la opci\u00f3n de Text File Input : Caso de Uso 3 - Filtrado compuesto Recordad que antes, en la pesta\u00f1a Fields , tenemos que obtener los campos a leer. Nos vamos a quedar con un subconjunto de las columnas y las vamos a renombrar. Para ello, dentro de la categor\u00eda Transform elegimos el paso Select values y elegimos y renombramos los siguentes campos: room_id , room_type , neighborhood , bedrooms , overall_satisfaction , accommodates , y price pasar\u00e1n a ser habitacion_id , habitacion_tipo , barrio , dormitorios , puntuacion , huespedes y precio . Caso de Uso 3 - Selecci\u00f3n y nombrado de campos Filtrado compuesto \u00b6 El siguiente paso que vamos a hacer es quedarnos con aquellos cuestionarios con m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes. As\u00ed pues, con el paso Filter Rows realizaremos: Caso de Uso 3 - Filtrado compuesto Si el filtrado fuese con condiciones m\u00e1s complejas, en ocasiones es m\u00e1s sencillo utilizar el paso Java filter (de la categor\u00eda Flow ), el cual utilizando la notaci\u00f3n de Java, podemos indicar la condici\u00f3n a cumplir. Por ejemplo, vamos filtrar los de m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes, y que su precio sea inferior a 200$: Caso de Uso 3 - Filtrado Java Para comprobar su funcionamiento, vamos a a\u00f1adir un par de pasos dummy (no realizan nada, pero sirven para finalizar tareas). Al ejecutarlo, veremos que nos da un error. Si alg\u00fan dato es nulo, el filtrado Java provocar\u00e1 un error de transformaci\u00f3n. Una posibilidad es que introduzcamos un paso de la categor\u00eda Utility denominado If value is null . Coneste paso, podemos indicar el valor a tomar a todos los campos o hacerlo de forma concreta en los campos que queramos. En nuestro caso, vamos indicar que cambie todos los nulos por -1 . Caso de Uso 3 - Cambiando nulos por -1 Debemos tener en cuenta que como ahora podemos tener precios con -1, para evitar recogerlos en el filtrado Java, deber\u00edamos modificarlo por (dormitorios > 3 || huespedes >=4) && ( precio >= 0 && precio < 200) . Generando JSON \u00b6 Caso de Uso 3 - Configuraci\u00f3n JSON Finalmente queremos almacenar los datos que cumplen el filtro en un fichero JSON. Para ello, sustituimos el dummy del camino exitoso por un paso JSON output , configurando: Filename : La ruta y el nombre del archivo Json bloc name : nombre de la propiedad que contendr\u00e1 un objeto o un array de objetos con los datos. Nr rows in a bloc : Cantidad de datos del archivo. Si ponemos 0, coloca todos los datos en el mismo fichero. Si ponemos 1, generar\u00e1 un fichero por cada registro. En la imagen que tenemos a la derecha puedes comprobar los valores introducidos. Resultado final \u00b6 En la siguiente imagen puedes comprobar la transformaci\u00f3n completa: Caso de Uso 3 - Transformaci\u00f3n final La cual, al ejecutarla, genera los siguientes datos: { \"datos\" :[ { \"dormitorios\" : 4.0 , \"huespedes\" : 10 , \"barrio\" : \"Arg\u00fcelles\" , \"precio\" : 133.0 , \"habitacion_tipo\" : \"Entire home\\/apt\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 23021 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Justicia\" , \"precio\" : 147.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 5.0 , \"habitacion_id\" : 24836 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Bellas Vistas\" , \"precio\" : 44.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 34801 }, ... ] } Caso de Uso 4 - Informe fabricantes en S3 \u00b6 A partir del caso 2, ahora vamos a generar un informe sobre ventas de los fabricantes, y vamos a guardar el informe en S3. Unir fabricantes \u00b6 As\u00ed pues, vamos a abrir el caso de uso 2, y a partir de ah\u00ed, vamos a fusionar los datos con los de fabricantes para obtener el nombre de \u00e9stos. Para ello, creamos una nueva lectura de CSV, ordenaci\u00f3n y posterior Merge . Cabe destacar que antes de hacer el segundo join , debemos ordenar ambas entradas por la clave de uni\u00f3n ( ManufacturerID ): Caso de Uso 4 - Merge fabricantes Aplicar f\u00f3rmulas \u00b6 Tras unir los datos de las ventas con los fabricantes, para poder preparar el informe mediante group-by, del mismo modo que antes, necesitamos ordenar los datos (en este caso por ManufacturerID ). En el informe queremos mostrar para cada fabricante, adem\u00e1s de su c\u00f3digo y nombre (campos de agrupaci\u00f3n), queremos obtener el total de unidades vendidad y la recaudaci\u00f3n total de dicho fabricante. Para ello creamos una nueva agrupaci\u00f3n. Caso de Uso 4 - Agrupamos por fabricante Adem\u00e1s, queremos que nos muestre un nuevo campo que muestre el precio medio obtenido de dividir la recaudaci\u00f3n obtenida entre el total de unidades. Para ello, dentro de la categor\u00eda Transform , elegimos el paso Calculator . Una vez abierto el di\u00e1logo para editar el paso, si desplegamos la columna Calculation puedes observar todas las posibles operaciones y transformaciones (tanto n\u00famericas, como de campos de texto e incluso fecha) que podemos realizar. En nuestro caso, s\u00f3lo necesitamos la divisi\u00f3n entre A y B : Caso de Uso 4 - F\u00f3rmula entre dos campos Guardar en S3 \u00b6 Para almacenar los datos en S3, primero crearemos un bucket p\u00fablico (en mi caso lo he denominado severo2122pdi ) y para facilitar el trabajo, vamos a crear una pol\u00edtica que permita todas las operaciones: { \"Version\" : \"2012-10-17\" , \"Id\" : \"Policy1635323698048\" , \"Statement\" : [ { \"Sid\" : \"Stmt1635323796449\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : \"arn:aws:s3:::severo2122pdi\" } ] } El siguente paso es configurar las credenciales de acceso en nuestro sistema. Recuerda que lo haremos mediante las variables de entorno ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY y AWS_SESSION_TOKEN ) o almacenando las credenciales en el archivo %UserProfile%\\.aws\\credentials . Finalmente, a\u00f1adimos el paso S3 file output indicando: La ruta del archivo, la cual se forma de forma similar a s3n://s3n/bucket/nombreArchivo . En nuestro caso, hemos utilizado el nombre s3n://s3n/severo2122pdi/informeFabricantes . y la extensi\u00f3n: nosotros hemos elegido el formato csv Caso de Uso 4 - Salida a S3 Una vez todo unido, tras ejecutarlo podremos acceder a nuestra consola de AWS y comprobar como ha aparecido el fichero. En resumen, en este caso de uso hemos utilizado la siguiente transformaci\u00f3n: Caso de Uso 4 Caso de Uso 5 - Jobs \u00b6 Ejercicio de Jobs Accede a S3 para ver si el fichero existe Caso de Uso 6 - Interacci\u00f3n con bases de datos \u00b6 En este ejemplo vamos a interactuar con una base de datos relacional que tenemos en RDS (si la quieres realizar en local, funcionar\u00e1 igualmente). Leer pdf Norberto Leer apuntes ugr tutorial itop tutorial pdi hitachivantara https://itop.academy/blog/item/spoon-componente-pentaho-data-integration-kettle.html https://help.hitachivantara.com/Documentation/Pentaho/9.2/Setup/Pentaho_Data_Integration_(PDI)_tutorial The Data Integration perspective of PDI allows you to create two basic file types: transformations and jobs. Transformations describe the data flows for ETL such as reading from a source, transforming data and loading it into a target location. Jobs coordinate ETL activities such as defining the flow and dependencies for what order transformations should be run, or prepare for execution by checking conditions such as, \"Is my source file available?\" or \"Does a table exist in my database?\" The aim of this tutorial is to walk you through the basic concepts and processes involved in building a transformation with PDI in a typical business scenario. In this scenario, you are loading a flat file (CSV) of sales data into a database to generate mailing lists. Several of the customer records are missing postal codes that must be resolved before loading into the database. In the preview feature of PDI, you will use a combination of steps to cleanse, format, standardize, and categorize the sample data. The six basic steps are: Actividades \u00b6 Realiza los casos pr\u00e1cticos de uso del 1 al 5. En la entrega debes adjuntar tanto el archivo .ktr como capturas de pantallas de los flujos de datos. Antes de realizar cada captura, a\u00f1ade una nota donde aparezca vuestro nombre completo (bot\u00f3n derecho -> New Note ). (opcional) Realiza el caso de Referencias \u00b6 Pentaho Data Integration Quick Start Guide de Mar\u00eda Carina Rold\u00e1n. Apuntes de Pentaho , dentro de la asignatura Sistemas Multidimensionales , impartida por Jos\u00e9 Samos Jim\u00e9nez en la Universidad de Granada. Taller sobre integraci\u00f3n de datos (abiertos) / Uso de Pentaho Data Integration , por Jose Norberto Maz\u00f3n (Universidad de Alicante) 24 Enero","title":"Pentaho"},{"location":"apuntes/ingesta02pentaho.html#pentaho","text":"Kettle es un componente de Pentaho Data Integration (PDI - https://www.hitachivantara.com/en-us/products/data-management-analytics/pentaho/download-pentaho.html ) que a su vez contiene a Spoon . Mediante Spoon se pueden realizar procesos ETL de manera visual, de forma muy f\u00e1cil y r\u00e1pida, como por ejemplo: Conexiones a los datos. Transformaciones (filtrado, limpieza, formateado, ... y posterior almacenamiento en diferentes formatos y destinos). Inserci\u00f3n de f\u00f3rmulas. Desarrollo de data warehouses con estructura en estrella (Hechos/Dimensiones) Y todo esto sin necesidad de programar directamente con c\u00f3digo y sin necesidad de instalar o configurar nada para poder empezar a usarla. Por esto, este tipo de herramientas se conocen como herramientas de metadatos , ya que trabajan a nivel de definici\u00f3n diciendo qu\u00e9 hay que hacer, pero no el detalle del c\u00f3mo se hace, \u00e9ste queda oculto, lo cual resulta muy interesante en la mayor\u00eda de los casos. Pentaho - Ejemplo de flujo ETL Se trata de una herramienta open source multiplatforma que tambi\u00e9n tiene su soporte comercial. La versi\u00f3n open source se puede descargar desde https://sourceforge.net/projects/pentaho/ . En nuestro caso, vamos a trabajar con la versi\u00f3n 9.2 que data de agosto de 2021 (para los apuntes he trabajado indistintamente con la versi\u00f3n 9.1 y la 9.2, tanto en Windows como en Ubuntu). Es importante destacar el requisito que necesitamos tener instalado en el sistema al menos la versi\u00f3n 8 de Java. Instalaci\u00f3n en Ubuntu Si trabajamos con Ubuntu, ser\u00e1 necesario instalar el paquete libwebkitgtk . Para ello, primero tenemos que a\u00f1adir su repositorio: sudo nano /etc/apt/sources.list Y a\u00f1adimos al final la siguiente entrada: deb http://cz.archive.ubuntu.com/ubuntu bionic main universe Y tras actualizar los respositorios con sudo apt-get update , instalaremos el paquete con el comando sudo apt-get install libwebkitgtk-1.0.0 . Una vez descargado el archivo y descomprimirlo, mediante el archivo spoon.bat (o spoon.sh ) lanzaremos la aplicaci\u00f3n. Pantalla de inicio de Pentaho/Spoon Dentro de Spoon Spoon permite dise\u00f1ar las transformaciones y trabajos que se ejecutan con las siguientes herramientas: * Pan es un motor de transformaci\u00f3n de datos que realiza muchas funciones tales como lectura, manipulaci\u00f3n, y escritura de datos hacia y desde varias fuentes de datos. * Kitchen es un programa que ejecuta los trabajos dise\u00f1ados por Spoon en XML o en una base de datos. Para esta sesi\u00f3n, hemos planteado varios casos de uso para ir aprendiendo la herramienta mediante su uso. https://openwebinars.net/blog/que-es-pentaho-data-integraton-pdi/","title":"Pentaho"},{"location":"apuntes/ingesta02pentaho.html#elementos","text":"En PDI hay dos tipos de elementos: Transformations y Jobs . Se definen Transformations para transformar los datos Se definen Jobs para organizar tareas estableciendo su orden y condiciones de ejecuci\u00f3n. Las transformaciones son un tipo de tarea. Tanto las transformaciones como las tareas, cuando se definen, se almacenan como archivos. Los elementos del interfaz son: Interfaz de Spoon","title":"Elementos"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-0","text":"Para familiarizarnos con el entorno, vamos a crear una transformaci\u00f3n muy b\u00e1sica. Tras seleccionar File -> New Transformation , el primer elemento que vamos a utilizar est\u00e1 dentro de la categor\u00eda Input . Men\u00fa emergente En concreto seleccionamos la transformaci\u00f3n Get system info , la cual nos permite obtener informaci\u00f3n sobre el sistema. La vamos a utilizar para averiguar la versi\u00f3n de PDI que estamos utilizando. As\u00ed pues, la seleccionamos desde el \u00e1rbol de pasos y lo arrastramos a la zona de trabajo. Si dejamos el rat\u00f3n sobre el elemento, nos aparecer\u00e1 un men\u00fa emergente donde podremos conectar una entrada, editar las propiedades, ver el men\u00fa contextual del paso, conectar una salida e inyectar metadatos. Sobre este paso, vamos a editar la informaci\u00f3n que queremos obtener. Para ello, vamos a crear una propiedad con nombre Versi\u00f3n Pentaho y seleccionaremos del desplegable la opci\u00f3n Kettle Version . A continuaci\u00f3n, en la categor\u00eda Utility seleccionamos el icono Write to Log , y lo arrastramos al area de trabajo. Ahora conectamos la salida de Get system info con Write to log , mediante la 4\u00aa opci\u00f3n del menu emergente, quedando una transformaci\u00f3n tal como se ve en la imagen: Caso de Uso 0 - Versi\u00f3n de PDI Finalmente, s\u00f3lo nos queda ejecutar la transformaci\u00f3n mediante el icono del tri\u00e1ngulo ( Run o F9), y ver el resultado en el panel inferior.","title":"Caso de Uso 0"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-1-filtrando-datos","text":"En este caso de uso, vamos a leer un archivo CSV y vamos a filtrar los datos para quedarnos con un subconjunto de los mismos. Adem\u00e1s, vamos a ver c\u00f3mo podemos gestionar los errores y ejecutar la transformaci\u00f3n desde el terminal.","title":"Caso de Uso 1 - Filtrando datos"},{"location":"apuntes/ingesta02pentaho.html#lectura-csv","text":"Tras crear la nueva transformaci\u00f3n (CTRL + N), desde Input arrastraremos el paso de CSV input file para seleccionar el archivo samples\\transformations\\files\\Zipssortedbycitystate.csv dentro de nuestra instalaci\u00f3n de Pentaho. Tras seleccionar el archivo, mediante el bot\u00f3n Get Fields cargaremos y comprobaremos que los campos que vamos a leer son correctos (nombre y tipo de los datos). Caso de Uso 1 - Tras pulsar sobre Get Fields Tras ello, mediante el bot\u00f3n Preview comprobaremos que los datos se leen correctamente. Caso de Uso 1 - Resultado de la opci\u00f3n Preview sobre Ciudades","title":"Lectura CSV"},{"location":"apuntes/ingesta02pentaho.html#filtrado-de-datos","text":"Una vez leido, el siguiente paso es filtrar las filas. Para ello, desde la categor\u00eda de Flow , arrastramos el paso Filter , y las conectamos tal como hemos realizado en el caso anterior. Al soltar la flecha, nos mostrar\u00e1 dos opciones: Main output of step : define los pasos con un flujo principal, donde todo funciona bien Error handling of step : define los pasos a seguir en caso de encontrar un error De momento elegimos la primera y configuramos el filtro para solo seleccionar aquellos datos cuyo estado sea NY. Caso de Uso 1 - Configuraci\u00f3n del filtro Para configurar el resultado, seleccionamos el paso del filtro, y bien pulsamos sobre el icono del ojo de la barra de herramientas, o sobre el paso, tras pulsar con el bot\u00f3n derecho, seleccionamos la opci\u00f3n Preview . Caso de Uso 1 - Resultado de hacer Preview sobre Filtro NY Por defecto se precargan 1000 filas. Tras comprobar el resultado, pulsamos sobre Stop para detener el proceso de previsualizaci\u00f3n. Las m\u00e9tricas que aparecen nos informaci\u00f3n del proceso y su rendimiento.","title":"Filtrado de datos"},{"location":"apuntes/ingesta02pentaho.html#ordenacion","text":"El siguiente paso que vamos a realizar es ordenar los datos por su c\u00f3digo Postal Code . Para ello, desde la categor\u00eda de Transform , arrastramos el paso de Sort rows , y conectamos la salida del filtrado con la ordenaci\u00f3n eligiendo la salida principal ( main output of step ). Forzando un error Vamos a forzar un error para comprobar c\u00f3mo lo indica Spoon . Si al elegir el nombre del campo, en vez de POSTAL CODE escribimos CP , cuando previsualizamos el resultado, podremos ver como aparece la marca de prohibido en la esquina superior derecha del paso, y si visualizamos el log y las m\u00e9tricas de los pasos, veremos el error: Caso de Uso 1 - Forzando un error Volvemos a editar el paso, corregimos el nombre del campo (escribimos POSTAL CODE ) y comprobamos que ahora s\u00ed que funciona correctamente Caso de Uso 1 - Ordenando","title":"Ordenaci\u00f3n"},{"location":"apuntes/ingesta02pentaho.html#escritura-del-resultado","text":"Una vez realizados todos los pasos, s\u00f3lo nos queda es enviar el resultado a un fichero para persistir la transformaci\u00f3n. Para ello, desde la categor\u00eda de Output arrastramos el paso Text file output , y lo conectamos desde la salida del paso de ordenaci\u00f3n. Tras ello, editar este paso para indicar el archivo donde almacenar el resultado. Caso de Uso 1 - Guardando el resultado Tras ello, podremos ejecutar la transformaci\u00f3n (icono del tri\u00e1ngulo, men\u00fa Action -> Run o F9) y comprobar el resultado en el fichero: CITY;STATE;POSTALCODE HOLTSVILLE ;NY;501 FISHERS ISLAND ;NY;6390 NEW YORK ;NY;10001 NEW YORK ;NY;10003 NEW YORK ;NY;10005 NEW YORK ;NY;10007 NEW YORK ;NY;10009 Al comprobar el fichero, vemos que se han quedado espacio en blanco a la derecha del nombre de las ciudades, ya que la columna ten\u00eda un tama\u00f1o configurado. Si volvemos a editar el \u00faltimo paso, en la pesta\u00f1a de Fields podemos indicar mediante el bot\u00f3n de Minimal width que reduzca su anchura al m\u00ednimo: Caso de Uso 1 - Anchura m\u00ednima de los campos Y tras volver a ejecutar la transformaci\u00f3n, veremos que ahora s\u00ed que obtenemos los datos que esper\u00e1bamos: CITY;STATE;POSTALCODE HOLTSVILLE;NY;501 FISHERS ISLAND;NY;6390 NEW YORK;NY;10001 NEW YORK;NY;10003 NEW YORK;NY;10005 NEW YORK;NY;10007 NEW YORK;NY;10009","title":"Escritura del resultado"},{"location":"apuntes/ingesta02pentaho.html#uso-de-pan","text":"Mediante la utilidad Pan , podemos ejecutar las transformaciones sin necesidad de arrancar Spoon . Para indicarle el archivo que contiene la transformaci\u00f3n, al comando pan.bat (o pan.sh en el caso de Ubuntu) le pasamos el par\u00e1metro /file=rutaArchivo.ktr . Para comprobar su funcionamiento, vamos a eliminar el fichero generado. A continuaci\u00f3n, ejecutamos pan : pan.bat /file = c:/IABD/caso1filtradoNY.ktr Tras algunos segundos y varias l\u00edneas de debug del arranque de pan, tendremos un mensaje similar al siguiente: 2021/10/24 18:01:42 - Start of run. 2021/10/24 18:01:42 - caso1filtradoNY - Dispatching started for transformation [caso1filtradoNY] 2021/10/24 18:01:42 - Ciudades.0 - Header row skipped in file 'C:\\data-integration\\samples\\transformations\\files\\Zipssortedbycitystate.csv' 2021/10/24 18:01:42 - Ciudades.0 - Finished processing (I=21380, O=0, R=0, W=21379, U=0, E=0) 2021/10/24 18:01:42 - Filtro NY.0 - Finished processing (I=0, O=0, R=21379, W=1146, U=0, E=0) 2021/10/24 18:01:42 - Orden por Codigo Postal.0 - Finished processing (I=0, O=0, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:42 - CiudadesNY.0 - Finished processing (I=0, O=1147, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:43 - Carte - Installing timer to purge stale objects after 1440 minutes. 2021/10/24 18:01:43 - Finished! 2021/10/24 18:01:43 - Start=2021/10/24 18:01:42.424, Stop=2021/10/24 18:01:43.041 2021/10/24 18:01:43 - Processing ended after 0 seconds. 2021/10/24 18:01:43 - caso1filtradoNY - 2021/10/24 18:01:43 - caso1filtradoNY - Step Ciudades.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Filtro NY.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Orden por Codigo Postal.0 ended successfully, processed 1146 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step CiudadesNY.0 ended successfully, processed 1146 lines. ( - lines/s) Y si comprobamos el fichero, veremos que ha vuelto a aparecer.","title":"Uso de Pan"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-2-uniendo-datos","text":"Caso de Uso 2 - Lecturas CSV En este caso, vamos a leer datos de ventas y de productos , y vamos a unirlos de forma similar a un join , para posteriormente, tras agrupar los datos, crear un informe.","title":"Caso de Uso 2 - Uniendo datos"},{"location":"apuntes/ingesta02pentaho.html#merge-join","text":"Para ello, primero vamos leer de forma separada cada archivo mediante un paso de tipo CSV file input , utilizando el ; como separado. As\u00ed pues, tendremos dos pasos, tal como se puede observar en la imagen de la derecha. En el caso del CSV de Ventas, al tener ventas de diferentes paises, deberemos cambiar el tipo del ZIP (el c\u00f3digo postal) a String . A continuaci\u00f3n, para unir los datos, dentro de la categor\u00eda Join utilizamos el paso Merge join . En este caso tras arrastrar el paso al \u00e1rea de trabajo, vamos a enlazar desde el merge hacia los dos or\u00edgenes, en un caso como left hand side stream of the join y el otro fomo right hand side stream of the join : Caso de Uso 2 - Entradas de Merge A continuaci\u00f3n, editamos el Merge y le decimos que el campo de uni\u00f3n es ProductID . Cuando le damos a aceptar recibimos un mensaje de advertencia avis\u00e1ndonos que si los datos no est\u00e1n ordenados podemos obtener resultados incorrectos. Caso de Uso 2 - Aviso Merge As\u00ed pues, vamos a a\u00f1adir previo al merge un paso de ordenaci\u00f3n a cada entrada, ordenando los datos por ProductID de manera ascendente. Para comprobar su funcionamiento, podemos hacer un preview del merge .","title":"Merge join"},{"location":"apuntes/ingesta02pentaho.html#agrupando-datos","text":"Como el resultado final es un informe de ventas con el total de unidades vendidas y la cantidad recaudada agrupada por pais y categor\u00eda del producto, necesitamos agrupar los datos. Para ello, dentro de la categor\u00eda Statistics , utilizaremos el paso Group by . En nuestro caso queremos agrupar por pa\u00eds ( Country ) y categor\u00eda ( Category ), y los datos que vamos a agregar son la suma de unidades ( Units ) y la suma recaudada ( Revenue ). As\u00ed pues, nuestra agrupaci\u00f3n quedar\u00eda as\u00ed: Caso de Uso 2 - Agrupamos por pa\u00eds y categor\u00eda En este caso, sucede lo mismo que antes, que este paso necesita los datos ordenados. As\u00ed pues, vamos a ordenar las salida del merge por pa\u00eds y categor\u00eda. Caso de Uso 2 - Ordenamos antes de agrupar El \u00faltimo paso que nos queda es exportar los datos a un fichero de texto mediante el paso Text file output .","title":"Agrupando datos"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-3-cuestionarios-airbnb","text":"Para el siguiente caso de uso, vamos a utilizar datos de los cuestionarios de AirBnb que se pueden descargar desde http://tomslee.net/airbnb-data-collection-get-the-data . En concreto, nos vamos a centrar en los datos de Madrid que podemos descargar desde https://s3.amazonaws.com/tomslee-airbnb-data-2/madrid.zip .","title":"Caso de Uso 3 - Cuestionarios Airbnb"},{"location":"apuntes/ingesta02pentaho.html#uniendo-datos","text":"Una vez descargados los datos y descomprimidos, vamos a cargar los tres ficheros en el mismo paso, utilizando dentro de Input la opci\u00f3n de Text File Input : Caso de Uso 3 - Filtrado compuesto Recordad que antes, en la pesta\u00f1a Fields , tenemos que obtener los campos a leer. Nos vamos a quedar con un subconjunto de las columnas y las vamos a renombrar. Para ello, dentro de la categor\u00eda Transform elegimos el paso Select values y elegimos y renombramos los siguentes campos: room_id , room_type , neighborhood , bedrooms , overall_satisfaction , accommodates , y price pasar\u00e1n a ser habitacion_id , habitacion_tipo , barrio , dormitorios , puntuacion , huespedes y precio . Caso de Uso 3 - Selecci\u00f3n y nombrado de campos","title":"Uniendo datos"},{"location":"apuntes/ingesta02pentaho.html#filtrado-compuesto","text":"El siguiente paso que vamos a hacer es quedarnos con aquellos cuestionarios con m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes. As\u00ed pues, con el paso Filter Rows realizaremos: Caso de Uso 3 - Filtrado compuesto Si el filtrado fuese con condiciones m\u00e1s complejas, en ocasiones es m\u00e1s sencillo utilizar el paso Java filter (de la categor\u00eda Flow ), el cual utilizando la notaci\u00f3n de Java, podemos indicar la condici\u00f3n a cumplir. Por ejemplo, vamos filtrar los de m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes, y que su precio sea inferior a 200$: Caso de Uso 3 - Filtrado Java Para comprobar su funcionamiento, vamos a a\u00f1adir un par de pasos dummy (no realizan nada, pero sirven para finalizar tareas). Al ejecutarlo, veremos que nos da un error. Si alg\u00fan dato es nulo, el filtrado Java provocar\u00e1 un error de transformaci\u00f3n. Una posibilidad es que introduzcamos un paso de la categor\u00eda Utility denominado If value is null . Coneste paso, podemos indicar el valor a tomar a todos los campos o hacerlo de forma concreta en los campos que queramos. En nuestro caso, vamos indicar que cambie todos los nulos por -1 . Caso de Uso 3 - Cambiando nulos por -1 Debemos tener en cuenta que como ahora podemos tener precios con -1, para evitar recogerlos en el filtrado Java, deber\u00edamos modificarlo por (dormitorios > 3 || huespedes >=4) && ( precio >= 0 && precio < 200) .","title":"Filtrado compuesto"},{"location":"apuntes/ingesta02pentaho.html#generando-json","text":"Caso de Uso 3 - Configuraci\u00f3n JSON Finalmente queremos almacenar los datos que cumplen el filtro en un fichero JSON. Para ello, sustituimos el dummy del camino exitoso por un paso JSON output , configurando: Filename : La ruta y el nombre del archivo Json bloc name : nombre de la propiedad que contendr\u00e1 un objeto o un array de objetos con los datos. Nr rows in a bloc : Cantidad de datos del archivo. Si ponemos 0, coloca todos los datos en el mismo fichero. Si ponemos 1, generar\u00e1 un fichero por cada registro. En la imagen que tenemos a la derecha puedes comprobar los valores introducidos.","title":"Generando JSON"},{"location":"apuntes/ingesta02pentaho.html#resultado-final","text":"En la siguiente imagen puedes comprobar la transformaci\u00f3n completa: Caso de Uso 3 - Transformaci\u00f3n final La cual, al ejecutarla, genera los siguientes datos: { \"datos\" :[ { \"dormitorios\" : 4.0 , \"huespedes\" : 10 , \"barrio\" : \"Arg\u00fcelles\" , \"precio\" : 133.0 , \"habitacion_tipo\" : \"Entire home\\/apt\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 23021 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Justicia\" , \"precio\" : 147.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 5.0 , \"habitacion_id\" : 24836 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Bellas Vistas\" , \"precio\" : 44.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 34801 }, ... ] }","title":"Resultado final"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-4-informe-fabricantes-en-s3","text":"A partir del caso 2, ahora vamos a generar un informe sobre ventas de los fabricantes, y vamos a guardar el informe en S3.","title":"Caso de Uso 4 - Informe fabricantes en S3"},{"location":"apuntes/ingesta02pentaho.html#unir-fabricantes","text":"As\u00ed pues, vamos a abrir el caso de uso 2, y a partir de ah\u00ed, vamos a fusionar los datos con los de fabricantes para obtener el nombre de \u00e9stos. Para ello, creamos una nueva lectura de CSV, ordenaci\u00f3n y posterior Merge . Cabe destacar que antes de hacer el segundo join , debemos ordenar ambas entradas por la clave de uni\u00f3n ( ManufacturerID ): Caso de Uso 4 - Merge fabricantes","title":"Unir fabricantes"},{"location":"apuntes/ingesta02pentaho.html#aplicar-formulas","text":"Tras unir los datos de las ventas con los fabricantes, para poder preparar el informe mediante group-by, del mismo modo que antes, necesitamos ordenar los datos (en este caso por ManufacturerID ). En el informe queremos mostrar para cada fabricante, adem\u00e1s de su c\u00f3digo y nombre (campos de agrupaci\u00f3n), queremos obtener el total de unidades vendidad y la recaudaci\u00f3n total de dicho fabricante. Para ello creamos una nueva agrupaci\u00f3n. Caso de Uso 4 - Agrupamos por fabricante Adem\u00e1s, queremos que nos muestre un nuevo campo que muestre el precio medio obtenido de dividir la recaudaci\u00f3n obtenida entre el total de unidades. Para ello, dentro de la categor\u00eda Transform , elegimos el paso Calculator . Una vez abierto el di\u00e1logo para editar el paso, si desplegamos la columna Calculation puedes observar todas las posibles operaciones y transformaciones (tanto n\u00famericas, como de campos de texto e incluso fecha) que podemos realizar. En nuestro caso, s\u00f3lo necesitamos la divisi\u00f3n entre A y B : Caso de Uso 4 - F\u00f3rmula entre dos campos","title":"Aplicar f\u00f3rmulas"},{"location":"apuntes/ingesta02pentaho.html#guardar-en-s3","text":"Para almacenar los datos en S3, primero crearemos un bucket p\u00fablico (en mi caso lo he denominado severo2122pdi ) y para facilitar el trabajo, vamos a crear una pol\u00edtica que permita todas las operaciones: { \"Version\" : \"2012-10-17\" , \"Id\" : \"Policy1635323698048\" , \"Statement\" : [ { \"Sid\" : \"Stmt1635323796449\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : \"arn:aws:s3:::severo2122pdi\" } ] } El siguente paso es configurar las credenciales de acceso en nuestro sistema. Recuerda que lo haremos mediante las variables de entorno ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY y AWS_SESSION_TOKEN ) o almacenando las credenciales en el archivo %UserProfile%\\.aws\\credentials . Finalmente, a\u00f1adimos el paso S3 file output indicando: La ruta del archivo, la cual se forma de forma similar a s3n://s3n/bucket/nombreArchivo . En nuestro caso, hemos utilizado el nombre s3n://s3n/severo2122pdi/informeFabricantes . y la extensi\u00f3n: nosotros hemos elegido el formato csv Caso de Uso 4 - Salida a S3 Una vez todo unido, tras ejecutarlo podremos acceder a nuestra consola de AWS y comprobar como ha aparecido el fichero. En resumen, en este caso de uso hemos utilizado la siguiente transformaci\u00f3n: Caso de Uso 4","title":"Guardar en S3"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-5-jobs","text":"Ejercicio de Jobs Accede a S3 para ver si el fichero existe","title":"Caso de Uso 5 - Jobs"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-6-interaccion-con-bases-de-datos","text":"En este ejemplo vamos a interactuar con una base de datos relacional que tenemos en RDS (si la quieres realizar en local, funcionar\u00e1 igualmente). Leer pdf Norberto Leer apuntes ugr tutorial itop tutorial pdi hitachivantara https://itop.academy/blog/item/spoon-componente-pentaho-data-integration-kettle.html https://help.hitachivantara.com/Documentation/Pentaho/9.2/Setup/Pentaho_Data_Integration_(PDI)_tutorial The Data Integration perspective of PDI allows you to create two basic file types: transformations and jobs. Transformations describe the data flows for ETL such as reading from a source, transforming data and loading it into a target location. Jobs coordinate ETL activities such as defining the flow and dependencies for what order transformations should be run, or prepare for execution by checking conditions such as, \"Is my source file available?\" or \"Does a table exist in my database?\" The aim of this tutorial is to walk you through the basic concepts and processes involved in building a transformation with PDI in a typical business scenario. In this scenario, you are loading a flat file (CSV) of sales data into a database to generate mailing lists. Several of the customer records are missing postal codes that must be resolved before loading into the database. In the preview feature of PDI, you will use a combination of steps to cleanse, format, standardize, and categorize the sample data. The six basic steps are:","title":"Caso de Uso 6 - Interacci\u00f3n con bases de datos"},{"location":"apuntes/ingesta02pentaho.html#actividades","text":"Realiza los casos pr\u00e1cticos de uso del 1 al 5. En la entrega debes adjuntar tanto el archivo .ktr como capturas de pantallas de los flujos de datos. Antes de realizar cada captura, a\u00f1ade una nota donde aparezca vuestro nombre completo (bot\u00f3n derecho -> New Note ). (opcional) Realiza el caso de","title":"Actividades"},{"location":"apuntes/ingesta02pentaho.html#referencias","text":"Pentaho Data Integration Quick Start Guide de Mar\u00eda Carina Rold\u00e1n. Apuntes de Pentaho , dentro de la asignatura Sistemas Multidimensionales , impartida por Jos\u00e9 Samos Jim\u00e9nez en la Universidad de Granada. Taller sobre integraci\u00f3n de datos (abiertos) / Uso de Pentaho Data Integration , por Jose Norberto Maz\u00f3n (Universidad de Alicante) 24 Enero","title":"Referencias"},{"location":"apuntes/nube01.html","text":"Cloud Computing \u00b6 La Nube \u00b6 Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Ventajas \u00b6 As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx. CapEx vs OpEx \u00b6 Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Esta elasticidad facilita que la capacidad de computo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes. Coste total de propiedad \u00b6 El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micos. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala, el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo. Inconvenientes \u00b6 Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...). Servicios en la nube \u00b6 Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. IaaS \u00b6 La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas. PaaS \u00b6 La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos. SaaS \u00b6 Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa. Tipos de arquitectura seg\u00fan la infraestructura \u00b6 Arquitecturas on premise \u00b6 Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware. Arquitecturas cloud \u00b6 Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel (serverless, base de datos administradas...). Arquitecturas h\u00edbridas \u00b6 Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados. Plataformas Cloud \u00b6 En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente, Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: Infraestructura cloud \u00b6 Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas. Regiones y Zonas de disponibilidad \u00b6 A lo largo de todo el globo terr\u00e1queo, se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante enlaces de baja latencia. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestra centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cluster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centro de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que caiga un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n. Ubicaciones de borde \u00b6 Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generar\u00e1n vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Para ello, despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional), a trav\u00e9s de 90 ciudades en 47 paises. El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , que redirige el tr\u00e1fico a los nodos Cloudfront ( https://aws.amazon.com/es/cloudfront/ ). Despliegue \u00b6 Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos. Actividades \u00b6 A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . Referencias \u00b6 Curso Academy Cloud Foundation de Amazon Web Services. Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure","title":"1.- Cloud Computing"},{"location":"apuntes/nube01.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"apuntes/nube01.html#la-nube","text":"Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software.","title":"La Nube"},{"location":"apuntes/nube01.html#ventajas","text":"As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx.","title":"Ventajas"},{"location":"apuntes/nube01.html#capex-vs-opex","text":"Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Esta elasticidad facilita que la capacidad de computo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes.","title":"CapEx vs OpEx"},{"location":"apuntes/nube01.html#coste-total-de-propiedad","text":"El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micos. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala, el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo.","title":"Coste total de propiedad"},{"location":"apuntes/nube01.html#inconvenientes","text":"Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...).","title":"Inconvenientes"},{"location":"apuntes/nube01.html#servicios-en-la-nube","text":"Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo.","title":"Servicios en la nube"},{"location":"apuntes/nube01.html#iaas","text":"La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas.","title":"IaaS"},{"location":"apuntes/nube01.html#paas","text":"La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos.","title":"PaaS"},{"location":"apuntes/nube01.html#saas","text":"Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa.","title":"SaaS"},{"location":"apuntes/nube01.html#tipos-de-arquitectura-segun-la-infraestructura","text":"","title":"Tipos de arquitectura seg\u00fan la infraestructura"},{"location":"apuntes/nube01.html#arquitecturas-on-premise","text":"Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware.","title":"Arquitecturas on premise"},{"location":"apuntes/nube01.html#arquitecturas-cloud","text":"Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel (serverless, base de datos administradas...).","title":"Arquitecturas cloud"},{"location":"apuntes/nube01.html#arquitecturas-hibridas","text":"Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados.","title":"Arquitecturas h\u00edbridas"},{"location":"apuntes/nube01.html#plataformas-cloud","text":"En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente, Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure:","title":"Plataformas Cloud"},{"location":"apuntes/nube01.html#infraestructura-cloud","text":"Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas.","title":"Infraestructura cloud"},{"location":"apuntes/nube01.html#regiones-y-zonas-de-disponibilidad","text":"A lo largo de todo el globo terr\u00e1queo, se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante enlaces de baja latencia. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestra centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cluster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centro de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que caiga un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n.","title":"Regiones y Zonas de disponibilidad"},{"location":"apuntes/nube01.html#ubicaciones-de-borde","text":"Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generar\u00e1n vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Para ello, despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional), a trav\u00e9s de 90 ciudades en 47 paises. El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , que redirige el tr\u00e1fico a los nodos Cloudfront ( https://aws.amazon.com/es/cloudfront/ ).","title":"Ubicaciones de borde"},{"location":"apuntes/nube01.html#despliegue","text":"Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos.","title":"Despliegue"},{"location":"apuntes/nube01.html#actividades","text":"A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube01.html#referencias","text":"Curso Academy Cloud Foundation de Amazon Web Services. Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure","title":"Referencias"},{"location":"apuntes/nube02aws.html","text":"Amazon Web Services \u00b6 Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones. Servicios \u00b6 Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados: Almacenamiento \u00b6 Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo costo para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para su uso con los servicios en la nube de AWS y los recursos en las instalaciones. Est\u00e1 dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Almacenamiento en AWS . Inform\u00e1tica / Computaci\u00f3n \u00b6 Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores Docker en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS. Bases de Datos \u00b6 Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 6.- Datos en AWS . Redes \u00b6 Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS. Seguridad en AWS \u00b6 Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS. Servicios de administraci\u00f3n de costes \u00b6 Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo. Administraci\u00f3n y gobernanza de datos \u00b6 La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorear recursos y aplicaciones. Si CloudTrail monitorea personas, CloudWatch monitorea servicios. CloudWatch es perfecto para asegurar de que sus servicios en la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se puedese pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla: Redes en AWS \u00b6 Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad. Amazon VPC \u00b6 AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutariamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalles, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b VPC Wizard \u00b6 Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad. IP El\u00e1stica \u00b6 Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html Seguridad en la Nube \u00b6 La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero. Modelo de responsabilidad compartida de AWS \u00b6 La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general. Responsabilidad de AWS \u00b6 AWS es responsable de proteger la infraestructura en la que se ejecutan todos los servicios ofrecidos por la nube de AWS (en algunas preguntas de la certificaci\u00f3n se refieren a ellos por servicios de la nube): Seguridad f\u00edsica de los centros de datos con acceso controlado basado en las necesidades en instalaciones sin identificaci\u00f3n, con guardias de seguridad permanentes, autenticaci\u00f3n de dos factores, revisi\u00f3n y registro de accesos, videovigilancia, y destrucci\u00f3n y desmagnetizaci\u00f3n de discos. Infraestructura de hardware , como servidores, dispositivos de almacenamiento y otros dispositivos de los que dependen los servicios de AWS. Infraestructura de software ,que aloja sistemas operativos, aplicaciones de servicios y software de virtualizaci\u00f3n. Infraestructura de red , como routers, conmutadores, balanceadores de carga, firewalls y cables. AWS tambi\u00e9n monitorea la red en l\u00edmites externos, protege los puntos de acceso y proporciona infraestructura redundante con detecci\u00f3n de intrusiones de forma constante Responsabilidad del cliente \u00b6 El cliente es responsable del cifrado de los datos en reposo y los datos en tr\u00e1nsito, de todo lo que se pone en la nube. Los pasos de seguridad que debe tomar depender\u00e1n de los servicios que utilice y de la complejidad del sistema. Si entramos en m\u00e1s detalle, es responsable de: El sistema operativo de la instancia de Amazon EC2 : incluidas las actualizaciones, los parches de seguridad y su mantenimiento. La protecci\u00f3n de las aplicaciones que se lanzan en los recursos AWS: contrase\u00f1as, acceso basado en roles, etc. Configuraci\u00f3n del grupo de seguridad . SO o firewalls basados en host: incluidos los sistemas de detecci\u00f3n o prevenci\u00f3n de intrusiones. Configuraciones de red . Administraci\u00f3n de cuentas : Configuraci\u00f3n de inicio de sesi\u00f3n y credenciales para cada usuario. Respecto al contenido cr\u00edtico, el cliente es responsable de administrar: El contenido que eligen almacenar en AWS. Los servicios de AWS que se utilizan con el contenido. En qu\u00e9 pa\u00eds se almacena ese contenido. El formato y la estructura de ese contenido y si est\u00e1 enmascarado, cifrado o es an\u00f3nimo. Qui\u00e9n tiene acceso a ese contenido y c\u00f3mo se conceden, administran y revocan esos derechos de acceso. AWS IAM \u00b6 AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz. Autenticaci\u00f3n \u00b6 Cuando se define un usuario de IAM se indica qu\u00e9 tipo de acceso puede utilizar el usuario para obtener acceso a los recursos de AWS: acceso mediante programaci\u00f3n: mediante email y clave de acceso secreta cuando realice una llamada a la API de AWS mediante la CLI de AWS, el SDK de AWS o cualquier otra herramienta de desarrollo. acceso a la consola de administraci\u00f3n de AWS: mediante usuario / contrase\u00f1a m\u00e1s el ID/alias de cuenta. Es recomendable activar MFA ( Multi-Factor Authentication ) para a\u00f1adir una capa m\u00e1s de seguridad. acceso mediante ambos tipos Autorizaci\u00f3n \u00b6 Una vez que el usuario se ha autenticado, se ha de determinar qu\u00e9 permisos debe concederse a un usuario, servicio o aplicaci\u00f3n. De forma predeterminada, los usuarios de IAM no tienen permiso para obtener acceso a los recursos o los datos en una cuenta de AWS. En su lugar, debe conceder permisos de forma expl\u00edcita a un usuario, grupo o rol mediante la creaci\u00f3n de una pol\u00edtica de IAM, ya que por defecto, se denegar\u00e1n todas las acciones que no se hayan permitido expl\u00edcitamente. Consejo Seguir el principio de m\u00ednimo privilegio: conceder \u00fanicamente los privilegios de usuario m\u00ednimos que necesita el usuario. El alcance de las configuraciones del servicio de IAM es global, se aplican en todas las regiones de AWS. Pol\u00edticas IAM \u00b6 Una pol\u00edtica de IAM es una instrucci\u00f3n formal mediante un documento JSON con los permisos que se conceder\u00e1 a una entidad. Las entidad es incluyen usuarios, grupos, roles o recursos. Las pol\u00edticas especifican cu\u00e1les son las acciones permitidas, cu\u00e1les son los recursos a los que estas tienen permiso y cu\u00e1l ser\u00e1 el efecto cuando el usuario solicite acceso a los recursos. Info Una sola pol\u00edtica se puede asociar a varias entidades. Una sola entidad puede tener varias pol\u00edticas asociadas a ella. Hay dos tipos de pol\u00edticas de IAM: pol\u00edticas basadas en identidad : controlan qu\u00e9 acciones puede realizar dicha identidad, en qu\u00e9 recursos y en qu\u00e9 condiciones. A su vez se dividen en administradas (asociada a varios usuarios/grupos/roles) o insertadas (un \u00fanico usuario/grupo/rol). pol\u00edticas basadas en recursos : son documentos de pol\u00edtica JSON que se asocia a un recurso (por ejemplo, un bucket de S3). Estas pol\u00edticas controlan qu\u00e9 acciones puede realizar una entidad principal especificada en dicho recurso y en qu\u00e9 condiciones. Destacar que no todos los servicios de AWS soportan este tipo de pol\u00edticas. Pol\u00edticas y permisos \u00b6 El usuario solo podr\u00e1 realizar la acci\u00f3n si la acci\u00f3n solicitada no est\u00e1 denegada de forma expl\u00edcita y est\u00e1 permitida de forma expl\u00edcita Cuando IAM determina si se concede un permiso, primero comprueba la existencia de cualquier pol\u00edtica de denegaci\u00f3n expl\u00edcita aplicable. Si no existe ninguna denegaci\u00f3n expl\u00edcita, comprueba si existe alguna pol\u00edtica de permisos expl\u00edcitos aplicable. Si no existe una pol\u00edtica de denegaci\u00f3n expl\u00edcita ni de permiso expl\u00edcito, IAM vuelve a la forma predeterminada, que consiste en denegar el acceso. Este proceso se denomina denegaci\u00f3n impl\u00edcita . Otros servicios relacionados con la seguridad AWS Organizations : Permite configurar los permisos de una organizaci\u00f3n que contiene varias cuentas de usuario en unidades organizativas (UO), y unificar tanto la seguridad como la facturaci\u00f3n AWS Key Management Service (AWS KMS): servicio que permite crear y administrar claves de cifrado Amazon Cognito : permite controlar el acceso a recursos de AWS desde aplicaciones con una credencial \u00fanica mediante SAML. AWS Shield : servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones ejecutadas en AWS. Pr\u00e1cticas recomendadas \u00b6 Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS. AWS CLI \u00b6 AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema opertivo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola vocareum , tras clickar en el bot\u00f3n azul de Acount Details podr\u00e9is ver los datos de acceso temporales en la ventana Credentials . Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias. AWS Cloudshell \u00b6 Es un shell integrado en el navegador que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operaciones ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Sin permisos Con el usuario de estudiante es una de las herramientas que est\u00e1 deshabilitada. Actividades \u00b6 Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity . (opcional) Realiza el m\u00f3dulo 4 (Seguridad en la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS . Referencias \u00b6 Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS","title":"2.- AWS"},{"location":"apuntes/nube02aws.html#amazon-web-services","text":"Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones.","title":"Amazon Web Services"},{"location":"apuntes/nube02aws.html#servicios","text":"Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados:","title":"Servicios"},{"location":"apuntes/nube02aws.html#almacenamiento","text":"Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo costo para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para su uso con los servicios en la nube de AWS y los recursos en las instalaciones. Est\u00e1 dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Almacenamiento en AWS .","title":"Almacenamiento"},{"location":"apuntes/nube02aws.html#informatica-computacion","text":"Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores Docker en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS.","title":"Inform\u00e1tica / Computaci\u00f3n"},{"location":"apuntes/nube02aws.html#bases-de-datos","text":"Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 6.- Datos en AWS .","title":"Bases de Datos"},{"location":"apuntes/nube02aws.html#redes","text":"Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS.","title":"Redes"},{"location":"apuntes/nube02aws.html#seguridad-en-aws","text":"Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS.","title":"Seguridad en AWS"},{"location":"apuntes/nube02aws.html#servicios-de-administracion-de-costes","text":"Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo.","title":"Servicios de administraci\u00f3n de costes"},{"location":"apuntes/nube02aws.html#administracion-y-gobernanza-de-datos","text":"La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorear recursos y aplicaciones. Si CloudTrail monitorea personas, CloudWatch monitorea servicios. CloudWatch es perfecto para asegurar de que sus servicios en la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se puedese pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla:","title":"Administraci\u00f3n y gobernanza de datos"},{"location":"apuntes/nube02aws.html#redes-en-aws","text":"Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad.","title":"Redes en AWS"},{"location":"apuntes/nube02aws.html#amazon-vpc","text":"AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutariamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalles, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b","title":"Amazon VPC"},{"location":"apuntes/nube02aws.html#vpc-wizard","text":"Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad.","title":"VPC Wizard"},{"location":"apuntes/nube02aws.html#ip-elastica","text":"Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"IP El\u00e1stica"},{"location":"apuntes/nube02aws.html#seguridad-en-la-nube","text":"La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero.","title":"Seguridad en la Nube"},{"location":"apuntes/nube02aws.html#modelo-de-responsabilidad-compartida-de-aws","text":"La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general.","title":"Modelo de responsabilidad compartida de AWS"},{"location":"apuntes/nube02aws.html#aws-iam","text":"AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz.","title":"AWS IAM"},{"location":"apuntes/nube02aws.html#practicas-recomendadas","text":"Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS.","title":"Pr\u00e1cticas recomendadas"},{"location":"apuntes/nube02aws.html#aws-cli","text":"AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema opertivo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola vocareum , tras clickar en el bot\u00f3n azul de Acount Details podr\u00e9is ver los datos de acceso temporales en la ventana Credentials . Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias.","title":"AWS CLI"},{"location":"apuntes/nube02aws.html#aws-cloudshell","text":"Es un shell integrado en el navegador que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operaciones ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Sin permisos Con el usuario de estudiante es una de las herramientas que est\u00e1 deshabilitada.","title":"AWS Cloudshell"},{"location":"apuntes/nube02aws.html#actividades","text":"Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity . (opcional) Realiza el m\u00f3dulo 4 (Seguridad en la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube02aws.html#referencias","text":"Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS","title":"Referencias"},{"location":"apuntes/nube03computacion.html","text":"Servicios de computaci\u00f3n en la nube \u00b6 Introducci\u00f3n \u00b6 Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio. Amazon EC2 \u00b6 Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual. Paso 1: AMI \u00b6 Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde. Tipos de AMI \u00b6 Puede elegir entre los siguientes tipos de AMI: Quick Start : AWS ofrece una serie de AMI predise\u00f1adas, tanto Linux como Windows, para lanzar las instancias. Mis AMI : estas son las AMI que hemos creado nosotros, ya sea a partir de m\u00e1quinas locales que hayamos creado en VmWare, VirtualBox, o una previa que hemos creado en una instancia EC2, configurado y luego exportado. AWS Marketplace : cat\u00e1logo que incluye miles de soluciones de software creadas por empresas terceras (las cuales pueden cobrar por su uso). Estas AMI pueden ofrecer casos de uso espec\u00edficos para que pueda ponerse en marcha r\u00e1pidamente. AMI de la comunidad : estas son AMI creadas por personas de todo el mundo.AWS no controla estas AMI, as\u00ed que deben utilizarse bajo la propia responsabilidad, evitando su uso en entornos corporativos o de producci\u00f3n. Las AMI dependen de la regi\u00f3n Las AMI que creamos se hacen en la regi\u00f3n en la que estamos conectados. Si la necesitamos en otra regi\u00f3n, debemos realizar un proceso de copia. Paso 2: Tipo de instancias \u00b6 El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino. Categor\u00edas \u00b6 Las categor\u00edas de tipos de instancia incluyen instancias de uso general, optimizadas para inform\u00e1tica, optimizadas para memoria, optimizadas para almacenamiento y de inform\u00e1tica acelerada. Categor\u00eda Tipo de instancia Caso de uso Uso general a1, m4, m5, t2, t3 Amplio Computaci\u00f3n c4, c5 Alto rendimiento Memoria r4, r5 , x1, z1 Big Data Inform\u00e1tica acelerada f1, g3, g4, p2, p3 Machine Learning Almacenamiento d2, h1, i3 Sistemas de archivos distribuidos Tipos de instancias \u00b6 Los tipos de instancias ( https://aws.amazon.com/es/ec2/instance-types/ ) ofrecen familias, generaciones y tama\u00f1os . As\u00ed pues, el tipo de instancia t3.large referencia a la familia T , de la tercera generaci\u00f3n y con un tama\u00f1o large . En general, los tipos de instancia que son de una generaci\u00f3n superior son m\u00e1s potentes y ofrecen una mejor relaci\u00f3n calidad/precio. Comparando tipos de instancias Cuando se comparan los tama\u00f1os hay que examinar la parte del coeficiente en la categor\u00eda de tama\u00f1o. Por ejemplo, una instancia t3.2xlarge tiene el doble de CPU virtual y memoria que una t3.xlarge . A su vez, la instancia t3.xlarge tiene el doble de CPU virtual y memoria que una t3.large . Tambi\u00e9n se debe tener en cuenta que el ancho de banda de red tambi\u00e9n est\u00e1 vinculado al tama\u00f1o de la instancia de Amazon EC2. Si ejecutar\u00e1 trabajos que requieren un uso muy intensivo de la red, es posible que deba aumentar las especificaciones de la instancia para que satisfaga sus necesidades. A la hora de elegir un tipo de instancia, nos centraremos en la cantidad de nucleos, el tama\u00f1o de la memoria, el rendimiento de la red y las tecnolog\u00edas de la propia CPU (si tiene habilitada GPU y FPGA) Paso 3: Configuraci\u00f3n de la instancia / red \u00b6 El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita. Asociar un rol de IAM \u00b6 Si necesitamos que nuestras instancias EC2 ejecuten una aplicaci\u00f3n que debe realizar llamadas seguras de la API a otros servicios de AWS, en vez de dejar anotadas las credenciales en el c\u00f3digo de la aplicaci\u00f3n (esto es una muy mala pr\u00e1ctica que puede acarrear problemas de seguridad), debemos asociar un rol de IAM a una instancia EC2. El rol de IAM asociado a una instancia EC2 se almacena en un perfil de instancia . Si creamos el rol desde esta misma pantalla, AWS crear\u00e1 un perfil de instancia autom\u00e1ticamente y le otorgar\u00e1 el mismo nombre que al rol. En el desplegable la lista que se muestra es, en realidad, una lista de nombres de perfiles de instancia. Cuando definimos un rol que una instancia EC2 puede utilizar, estamos configurando qu\u00e9 cuentas o servicios de AWS pueden asumir dicho rol, as\u00ed como qu\u00e9 acciones y recursos de la API puede utilizar la aplicaci\u00f3n despu\u00e9s de asumir el rol. Si cambia un rol, el cambio se extiende a todas las instancias que tengan el rol asociado. La asociaci\u00f3n del rol no est\u00e1 limitada al momento del lanzamiento de la instancia, tambi\u00e9n se puede asociar un rol a una instancia que ya exista. Script de datos de usuario \u00b6 Al momento de crear las instancias EC2, de forma opcional, podemos especificar un script de datos de usuario durante el lanzamiento de la instancia. Los datos de usuario pueden automatizar la finalizaci\u00f3n de las instalaciones y las configuraciones durante el lanzamiento de la instancia. Por ejemplo, un script de datos de usuario podr\u00eda colocar parches en el sistema operativo de la instancia y actualizarlo, recuperar e instalar claves de licencia de software, o instalar sistemas de software adicionales. Por ejemplo, si queremos instalar un servidor de Apache, de manera que arranque autom\u00e1ticamente y que muestre un Hola Mundo podr\u00edamos poner #!/bin/bash yum update -y yum -y install httpd systemctl enable httpd systemctl start httpd echo '<html><h1>Hola Mundo desde el Severo!</h1></html>' > /var/www/html/index.html Script en Windows Si nuestra instancia es de Windows, el script de datos de usuario debe escribirse en un formato que sea compatible con una ventana del s\u00edmbolo del sistema (comandos por lotes) o con Windows PowerShell. De forma predeterminada, los datos de usuario s\u00f3lo se ejecutan la primera vez que se inicia la instancia. Paso 4: Almacenamiento \u00b6 Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias). Paso 5: Etiquetas \u00b6 Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso. Paso 6: Grupo de seguridad \u00b6 Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. !!! \"Servidor Web\" Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. ![Regla HTTP](../imagenes/cloud/03reglaHttp.png) AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad. Paso 7: An\u00e1lisis e identificaci\u00f3n \u00b6 El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Uso de la consola \u00b6 En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a centrarnos en un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-1a2b3c4d --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia c3.large key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s utiles a la hora de trabajar con AWS CLI. Ciclo de vida de las instancias \u00b6 Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de detenci\u00f3n) Terminated(terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping(apag\u00e1ndose) : las instancias que cuentan con el respaldo de Amazon EBS se pueden detener. Stopped(detenida) : no generar\u00e1 los mismos costos que una instancia en el estado running. S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que cuentan con el respaldo de Amazon EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asociandola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se paga por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n. Monitorizaci\u00f3n \u00b6 Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias. Costes de las instancias \u00b6 Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS es en esa modalidad), pero conviene conocer el el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y \u200b apto para la capa gratuita de AWS bajo coste y flexibilidad Cargas de trabajo de corto plazo, con picos o impredecibles\u200b. Tambi\u00e9n para desarrollo o prueba de aplicaciones\u200b spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot.\u200b AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos.\u200b Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda\u200b. Carga de trabajo din\u00e1mica y a gran escala\u200b Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n\u200b. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos\u200b. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional\u200b instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve\u200b. Descuento en el cargo por hora por el uso de la instancia\u200b (hasta 72%). Plazo de 1 o 3 a\u00f1os\u200b Asegura capacidad de c\u00f3mputo disponible cuando se la necesita\u200b Cargas de trabajo de uso predecible o estado estable\u200b. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres\u200b. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n\u200b host reservado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso\u200b Ahorro de dinero en costes de licencia\u200b. Asistencia para cumplir los requisitos normativos y de conformidad\u200b Licencia \u201cBring your own\u201d (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias\u200b. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cutoas mensuales NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/ Optimizaci\u00f3n de costes \u00b6 Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las intancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las intancias EBS que ya no se necesitan o las copias de seguridad ya pasadas. AWS Lambda \u00b6 La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions . Creando una funci\u00f3n \u00b6 Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n, mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda Restricciones \u00b6 Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos. AWS Elastic Beanstalk \u00b6 AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga con los recursos que AWS utilice (isntancia, base de datos, almacenamiento S3, etc...) Actividades \u00b6 Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Realiza el ejemplo de AWS Lambda del siguiente art\u00edculo: https://aws.amazon.com/es/getting-started/hands-on/run-serverless-code/ , y adjunta captura del c\u00f3digo fuente, del saldo antes y despu\u00e9s de ejecutar la funci\u00f3n 10 veces y de las m\u00e9tricas capturadas. Referencias \u00b6 Amazon EC2 Funciones Lambda en AWS","title":"3.- Computaci\u00f3n"},{"location":"apuntes/nube03computacion.html#servicios-de-computacion-en-la-nube","text":"","title":"Servicios de computaci\u00f3n en la nube"},{"location":"apuntes/nube03computacion.html#introduccion","text":"Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio.","title":"Introducci\u00f3n"},{"location":"apuntes/nube03computacion.html#amazon-ec2","text":"Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual.","title":"Amazon EC2"},{"location":"apuntes/nube03computacion.html#paso-1-ami","text":"Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde.","title":"Paso 1: AMI"},{"location":"apuntes/nube03computacion.html#paso-2-tipo-de-instancias","text":"El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino.","title":"Paso 2: Tipo de instancias"},{"location":"apuntes/nube03computacion.html#paso-3-configuracion-de-la-instancia-red","text":"El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita.","title":"Paso 3: Configuraci\u00f3n de la instancia / red"},{"location":"apuntes/nube03computacion.html#paso-4-almacenamiento","text":"Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias).","title":"Paso 4: Almacenamiento"},{"location":"apuntes/nube03computacion.html#paso-5-etiquetas","text":"Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso.","title":"Paso 5: Etiquetas"},{"location":"apuntes/nube03computacion.html#paso-6-grupo-de-seguridad","text":"Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. !!! \"Servidor Web\" Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. ![Regla HTTP](../imagenes/cloud/03reglaHttp.png) AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad.","title":"Paso 6: Grupo de seguridad"},{"location":"apuntes/nube03computacion.html#paso-7-analisis-e-identificacion","text":"El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia.","title":"Paso 7: An\u00e1lisis e identificaci\u00f3n"},{"location":"apuntes/nube03computacion.html#uso-de-la-consola","text":"En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a centrarnos en un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-1a2b3c4d --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia c3.large key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s utiles a la hora de trabajar con AWS CLI.","title":"Uso de la consola"},{"location":"apuntes/nube03computacion.html#ciclo-de-vida-de-las-instancias","text":"Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de detenci\u00f3n) Terminated(terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping(apag\u00e1ndose) : las instancias que cuentan con el respaldo de Amazon EBS se pueden detener. Stopped(detenida) : no generar\u00e1 los mismos costos que una instancia en el estado running. S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que cuentan con el respaldo de Amazon EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asociandola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se paga por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n.","title":"Ciclo de vida de las instancias"},{"location":"apuntes/nube03computacion.html#monitorizacion","text":"Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias.","title":"Monitorizaci\u00f3n"},{"location":"apuntes/nube03computacion.html#costes-de-las-instancias","text":"Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS es en esa modalidad), pero conviene conocer el el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y \u200b apto para la capa gratuita de AWS bajo coste y flexibilidad Cargas de trabajo de corto plazo, con picos o impredecibles\u200b. Tambi\u00e9n para desarrollo o prueba de aplicaciones\u200b spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot.\u200b AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos.\u200b Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda\u200b. Carga de trabajo din\u00e1mica y a gran escala\u200b Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n\u200b. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos\u200b. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional\u200b instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve\u200b. Descuento en el cargo por hora por el uso de la instancia\u200b (hasta 72%). Plazo de 1 o 3 a\u00f1os\u200b Asegura capacidad de c\u00f3mputo disponible cuando se la necesita\u200b Cargas de trabajo de uso predecible o estado estable\u200b. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres\u200b. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n\u200b host reservado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso\u200b Ahorro de dinero en costes de licencia\u200b. Asistencia para cumplir los requisitos normativos y de conformidad\u200b Licencia \u201cBring your own\u201d (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias\u200b. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cutoas mensuales NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/","title":"Costes de las instancias"},{"location":"apuntes/nube03computacion.html#optimizacion-de-costes","text":"Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las intancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las intancias EBS que ya no se necesitan o las copias de seguridad ya pasadas.","title":"Optimizaci\u00f3n de costes"},{"location":"apuntes/nube03computacion.html#aws-lambda","text":"La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions .","title":"AWS Lambda"},{"location":"apuntes/nube03computacion.html#creando-una-funcion","text":"Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n, mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda","title":"Creando una funci\u00f3n"},{"location":"apuntes/nube03computacion.html#restricciones","text":"Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos.","title":"Restricciones"},{"location":"apuntes/nube03computacion.html#aws-elastic-beanstalk","text":"AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga con los recursos que AWS utilice (isntancia, base de datos, almacenamiento S3, etc...)","title":"AWS Elastic Beanstalk"},{"location":"apuntes/nube03computacion.html#actividades","text":"Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Realiza el ejemplo de AWS Lambda del siguiente art\u00edculo: https://aws.amazon.com/es/getting-started/hands-on/run-serverless-code/ , y adjunta captura del c\u00f3digo fuente, del saldo antes y despu\u00e9s de ejecutar la funci\u00f3n 10 veces y de las m\u00e9tricas capturadas.","title":"Actividades"},{"location":"apuntes/nube03computacion.html#referencias","text":"Amazon EC2 Funciones Lambda en AWS","title":"Referencias"},{"location":"apuntes/nube04almacenamiento.html","text":"Almacenamiento en la nube \u00b6 El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar. Amazon EBS \u00b6 Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad. Vol\u00famenes \u00b6 IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS. Instant\u00e1neas \u00b6 Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS. Amazon S3 \u00b6 S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . S3 es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket (los objetos pueden ser de hasta 5TB), le permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivo se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Tambi\u00e9n tenemos la posibilidad de activar el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Clases de almacenamiento \u00b6 S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/*S3*/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas. Buckets \u00b6 Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Puede controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n puede ver registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, videos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar sus datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre severo2122 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato): Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://severo2122.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos. Casos de uso \u00b6 Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada. Costes \u00b6 Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ . Sitio web est\u00e1tico \u00b6 Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo2122web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo2122web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablic M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html S3 Select \u00b6 Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejectuar consulta SQL . Si nos fijamos en la imagen, se crea una tabla fictia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST. Acceso \u00b6 Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo2122cli --region us-east-1 { \"Location\" : \"/severo2122cli\" } Otra forma que veremos m\u00e1s adelante es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremos S3 mediante Python directamente y utilizando AWS Lambda. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Mediante la configuraci\u00f3n de IAM, podemos obtener un control detallado sobre qui\u00e9n puede acceder a los datos. Tambi\u00e9n podemos utilizar las pol\u00edticas de bucket de S3 e, incluso, las listas de control de acceso por objeto (ACL). Seguridad Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Para ello, se recomienda hacer uso de IAM, creando un grupo de usuarios donde definamos los permisos mediante pol\u00edticas. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html . Amazon EFS \u00b6 Amazon Elastic File System (Amazon EFS - https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS. Actividades \u00b6 Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado. (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web. Referencias \u00b6 Amazon EBS Amazon S3 Amazon EFS","title":"4.- Almacenamiento"},{"location":"apuntes/nube04almacenamiento.html#almacenamiento-en-la-nube","text":"El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar.","title":"Almacenamiento en la nube"},{"location":"apuntes/nube04almacenamiento.html#amazon-ebs","text":"Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad.","title":"Amazon EBS"},{"location":"apuntes/nube04almacenamiento.html#volumenes","text":"IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS.","title":"Vol\u00famenes"},{"location":"apuntes/nube04almacenamiento.html#instantaneas","text":"Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS.","title":"Instant\u00e1neas"},{"location":"apuntes/nube04almacenamiento.html#amazon-s3","text":"S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . S3 es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket (los objetos pueden ser de hasta 5TB), le permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivo se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Tambi\u00e9n tenemos la posibilidad de activar el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor.","title":"Amazon S3"},{"location":"apuntes/nube04almacenamiento.html#clases-de-almacenamiento","text":"S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/*S3*/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas.","title":"Clases de almacenamiento"},{"location":"apuntes/nube04almacenamiento.html#buckets","text":"Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Puede controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n puede ver registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, videos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar sus datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre severo2122 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato): Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://severo2122.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos.","title":"Buckets"},{"location":"apuntes/nube04almacenamiento.html#casos-de-uso","text":"Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada.","title":"Casos de uso"},{"location":"apuntes/nube04almacenamiento.html#costes","text":"Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ .","title":"Costes"},{"location":"apuntes/nube04almacenamiento.html#sitio-web-estatico","text":"Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo2122web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo2122web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablic M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html","title":"Sitio web est\u00e1tico"},{"location":"apuntes/nube04almacenamiento.html#s3-select","text":"Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejectuar consulta SQL . Si nos fijamos en la imagen, se crea una tabla fictia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST.","title":"S3 Select"},{"location":"apuntes/nube04almacenamiento.html#acceso","text":"Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo2122cli --region us-east-1 { \"Location\" : \"/severo2122cli\" } Otra forma que veremos m\u00e1s adelante es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremos S3 mediante Python directamente y utilizando AWS Lambda. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Mediante la configuraci\u00f3n de IAM, podemos obtener un control detallado sobre qui\u00e9n puede acceder a los datos. Tambi\u00e9n podemos utilizar las pol\u00edticas de bucket de S3 e, incluso, las listas de control de acceso por objeto (ACL). Seguridad Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Para ello, se recomienda hacer uso de IAM, creando un grupo de usuarios donde definamos los permisos mediante pol\u00edticas. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html .","title":"Acceso"},{"location":"apuntes/nube04almacenamiento.html#amazon-efs","text":"Amazon Elastic File System (Amazon EFS - https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS.","title":"Amazon EFS"},{"location":"apuntes/nube04almacenamiento.html#actividades","text":"Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado. (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web.","title":"Actividades"},{"location":"apuntes/nube04almacenamiento.html#referencias","text":"Amazon EBS Amazon S3 Amazon EFS","title":"Referencias"},{"location":"apuntes/nube05datos.html","text":"Datos en la nube \u00b6 Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PosgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches. Datos relacionales - Amazon RDS \u00b6 AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones. Instancias de bases de datos \u00b6 Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB Alta disponibilidad \u00b6 Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal. R\u00e9plica de lectura \u00b6 RDS tambi\u00e9n admite la creaci\u00f3n de r\u00e9plicas de lectura para MySQL, MariaDB, PostgreSQLy Amazon Aurora. R\u00e9plica de lectura Las actualizaciones que se realizan en la instancia principal se copian de manera as\u00edncrona en la instancia de r\u00e9plica de lectura, de manera que direccionando las consultas a esta nueva r\u00e9plica reduciremos la carga de la instancia principal. Las r\u00e9plicas de lectura tambi\u00e9n pueden convertirse en la instancia de base de datos principal, pero, debido a la replicaci\u00f3n as\u00edncrona, este proceso debe hacerse de forma manual. Las r\u00e9plicas de lectura pueden crearse en una regi\u00f3n diferente a la utilizada por la base de datos principal, lo que puede mejorar la recuperaci\u00f3n de desastres y/o disminuir la latencia al dirigir las lecturas a una r\u00e9plica de lectura lo m\u00e1s cercana al usuario. Casos de uso \u00b6 AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2). Costes \u00b6 El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducirian en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os). Ejemplo RDS \u00b6 A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndard con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL. Amazon Aurora \u00b6 Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, \u200bla implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n \u200bde errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales. Datos NoSQL - DynamoDB \u00b6 DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado\u200b. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una. Componentes y particiones \u00b6 Los componentes principales son: las tablas: son conjuntos de datos, formada por los elementos. los elementos: grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos: elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple que consta de un atributo denominado clave de ordenamiento. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, est\u00e1 conformada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. Infraestructura \u00b6 Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave incluyen las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija (tablas globales), el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Costes \u00b6 Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes. Ejemplo DynamoDB \u00b6 A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos hacer la consulta desde AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL. As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL. Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Info M\u00e1s adelante mediante Python, accederemos a DynamoDB y realizaremos consultas con PartiQL, adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos. Actividades \u00b6 Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instantanea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.m6g.large , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV. Referencias \u00b6 Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"5.- Datos"},{"location":"apuntes/nube05datos.html#datos-en-la-nube","text":"Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PosgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches.","title":"Datos en la nube"},{"location":"apuntes/nube05datos.html#datos-relacionales-amazon-rds","text":"AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones.","title":"Datos relacionales - Amazon RDS"},{"location":"apuntes/nube05datos.html#instancias-de-bases-de-datos","text":"Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB","title":"Instancias de bases de datos"},{"location":"apuntes/nube05datos.html#alta-disponibilidad","text":"Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal.","title":"Alta disponibilidad"},{"location":"apuntes/nube05datos.html#casos-de-uso","text":"AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2).","title":"Casos de uso"},{"location":"apuntes/nube05datos.html#costes","text":"El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducirian en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os).","title":"Costes"},{"location":"apuntes/nube05datos.html#ejemplo-rds","text":"A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndard con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL.","title":"Ejemplo RDS"},{"location":"apuntes/nube05datos.html#amazon-aurora","text":"Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, \u200bla implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n \u200bde errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales.","title":"Amazon Aurora"},{"location":"apuntes/nube05datos.html#datos-nosql-dynamodb","text":"DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado\u200b. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una.","title":"Datos NoSQL - DynamoDB"},{"location":"apuntes/nube05datos.html#componentes-y-particiones","text":"Los componentes principales son: las tablas: son conjuntos de datos, formada por los elementos. los elementos: grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos: elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple que consta de un atributo denominado clave de ordenamiento. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, est\u00e1 conformada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria.","title":"Componentes y particiones"},{"location":"apuntes/nube05datos.html#infraestructura","text":"Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave incluyen las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija (tablas globales), el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos.","title":"Infraestructura"},{"location":"apuntes/nube05datos.html#costes_1","text":"Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes.","title":"Costes"},{"location":"apuntes/nube05datos.html#ejemplo-dynamodb","text":"A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos hacer la consulta desde AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL. As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL. Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Info M\u00e1s adelante mediante Python, accederemos a DynamoDB y realizaremos consultas con PartiQL, adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos.","title":"Ejemplo DynamoDB"},{"location":"apuntes/nube05datos.html#actividades","text":"Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instantanea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.m6g.large , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV.","title":"Actividades"},{"location":"apuntes/nube05datos.html#referencias","text":"Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"Referencias"},{"location":"draft/athena.html","text":"AWS Athena \u00b6 Athena es una herramienta serverless que permite agregar datos en S3 que provienen de fuentes dispares como bases de datos, un flujo de datos, contenido web desestructurado, etc.. Amazon Athena is an interactive query service that makes it easy for you to analyze data directly in Amazon S3 using standard SQL. With a few actions in the AWS Management Console, you can use Athena directly against data assets stored in the data lake and begin using standard SQL to run ad hoc queries and get results in a matter of seconds. Athena is serverless, so there is no infrastructure to set up or manage, and you only pay for the volume of data assets scanned during the queries you run. Athena scales automatically\u2014executing queries in parallel\u2014so results are fast, even with large datasets and complex queries. You can use Athena to process unstructured, semi-structured, and structured data sets. Supported data asset formats include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. Athena integrates with Amazon QuickSight for easy visualization. It can also be used with third-party reporting and business intelligence tools by connecting these tools to Athena with a JDBC driver. En el siguiente supuesto, vamos a crear una aplicaci\u00f3n Athena , definiremos una base de datos, crearemos una tabla con sus columnas y tipos de datos, y ejecutaremos consultas sencillas y compuestas. Amazon Athena es un servicio de consulta interactivo que se puede utilizar para extraer informaci\u00f3n de datos almacenados en S3. Athena almacena metadatos sobre las fuentes de datos, as\u00ed como las consultas para poder reutilizarlas o compartirlas con otros usuarios. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically\u2014running queries in parallel\u2014so results are fast, even with large datasets and complex queries. Amazon Athena is a fast, cost-effective, interactive query service that makes it easy to analyze petabytes of data in S3 with no data warehouses or clusters to manage. Seleccionar el data set , identificando en S3 donde est\u00e1n los datos. Athena permite consultar los datos CSV, TSV, JSON, Parquet y formato ORC. Crear la tabla, mediante el asistente de crear tabla o utilizanos la sintaxis DDL de Hive. Consultar los datos, mediante SQL. Antes de empezar con Athena , necesitamos crear un bucket donde almacenar los resultados de nuestras consultas. As\u00ed pues, vamos a crear un bucket que llamaremos severo2122athena . Una vez creado, ya podemos acceder a Athena, y en la pesta\u00f1a de Settings indicar el bucket donde vamos a guardar los resultados: Athena - Configuraci\u00f3n inicial A continuaci\u00f3n, entramos al Query editor , y a lado de la secci\u00f3n Tables and vies , desplegamos el men\u00fa Create y creamos una tabla a partir de datos S3: Athena - Opci\u00f3n para crear la tabla a partir de S3 En la siguiente pantalla, crearemos la tabla yellow en una nueva base de datos que denominamos taxidata y le indicamos que coja los datos S3 de s3://aws-tc-largeobjects/CUR-TF-200-ACBDFO-1/Lab2/yellow/ . El formato de los datos es CSV . El siguiente paso es mediante Bulk add columns indicar los nombre y tipos de las columnas: vendor string , pickup timestamp , dropoff timestamp , count int , distance int , ratecode string , storeflag string , pulocid string , dolocid string , paytype string , fare decimal , extra decimal , mta_tax decimal , tip decimal , tolls decimal , surcharge decimal , total decimal Finalmente, veremos a modo de resumen una instrucci\u00f3n create table similar a la siguiente y ya podemos pulsar sobre Create table : `` sql CREATE EXTERNAL TABLE IF NOT EXISTS taxidata . yellow ( vendor string, pickup timestamp, dropoff timestamp, count int, distance int, ratecode string, storeflag string pulocid string, dolocid string, paytype string, fare decimal, extra decimal, mta_tax decimal, tip decimal, tolls decimal, surcharge decimal, total` decimal ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = ',', 'field.delim' = ',' ) LOCATION 's3://aws-tc-largeobjects/CUR-TF-200-ACBDFO-1/Lab2/yellow/' TBLPROPERTIES ('has_encrypted_data'='false'); FIXME: mirar y hacer https://docs.aws.amazon.com/athena/latest/ug/getting-started.html AWS Glue \u00b6 Es la herramienta ETL completamente administrado que ofrece Amazon Web Services. Cuando el esquema de los datos es desconocido, AWS Glue permite inferirlo. Para ello, hemos de construir un rastreador ( crawler ) para descubrir su estructura. . AWS Glue builds a catalog that contains metadata about the various data sources. AWS Glue is similar to Amazon Athena in that the actual data you analyze remains in the data source. The key difference is that you can build a crawler with AWS Glue to discover the schema. Creaci\u00f3n \u00b6 Ejecuci\u00f3n \u00b6 Se puede ejecutar bajo demanda o planificar su ejecuci\u00f3n de forma diaria, por horas, etc... Una vez lanzado, al cabo de un minuto, veremos que ha finalizado y en Base de Datos --> Tablas podremos ver la tabla que ha creado (en nuestro caso la tabla csv ), junto con la estructura que ha inferido respecto a los datos cargados. El siguiente paso es editar el esquema y ponerle nombres significativos a los campos: Una vez los datos est\u00e1n cargados, ya podemos realizar consular con AWS Athena.","title":"Athena"},{"location":"draft/athena.html#aws-athena","text":"Athena es una herramienta serverless que permite agregar datos en S3 que provienen de fuentes dispares como bases de datos, un flujo de datos, contenido web desestructurado, etc.. Amazon Athena is an interactive query service that makes it easy for you to analyze data directly in Amazon S3 using standard SQL. With a few actions in the AWS Management Console, you can use Athena directly against data assets stored in the data lake and begin using standard SQL to run ad hoc queries and get results in a matter of seconds. Athena is serverless, so there is no infrastructure to set up or manage, and you only pay for the volume of data assets scanned during the queries you run. Athena scales automatically\u2014executing queries in parallel\u2014so results are fast, even with large datasets and complex queries. You can use Athena to process unstructured, semi-structured, and structured data sets. Supported data asset formats include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. Athena integrates with Amazon QuickSight for easy visualization. It can also be used with third-party reporting and business intelligence tools by connecting these tools to Athena with a JDBC driver. En el siguiente supuesto, vamos a crear una aplicaci\u00f3n Athena , definiremos una base de datos, crearemos una tabla con sus columnas y tipos de datos, y ejecutaremos consultas sencillas y compuestas. Amazon Athena es un servicio de consulta interactivo que se puede utilizar para extraer informaci\u00f3n de datos almacenados en S3. Athena almacena metadatos sobre las fuentes de datos, as\u00ed como las consultas para poder reutilizarlas o compartirlas con otros usuarios. Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically\u2014running queries in parallel\u2014so results are fast, even with large datasets and complex queries. Amazon Athena is a fast, cost-effective, interactive query service that makes it easy to analyze petabytes of data in S3 with no data warehouses or clusters to manage. Seleccionar el data set , identificando en S3 donde est\u00e1n los datos. Athena permite consultar los datos CSV, TSV, JSON, Parquet y formato ORC. Crear la tabla, mediante el asistente de crear tabla o utilizanos la sintaxis DDL de Hive. Consultar los datos, mediante SQL. Antes de empezar con Athena , necesitamos crear un bucket donde almacenar los resultados de nuestras consultas. As\u00ed pues, vamos a crear un bucket que llamaremos severo2122athena . Una vez creado, ya podemos acceder a Athena, y en la pesta\u00f1a de Settings indicar el bucket donde vamos a guardar los resultados: Athena - Configuraci\u00f3n inicial A continuaci\u00f3n, entramos al Query editor , y a lado de la secci\u00f3n Tables and vies , desplegamos el men\u00fa Create y creamos una tabla a partir de datos S3: Athena - Opci\u00f3n para crear la tabla a partir de S3 En la siguiente pantalla, crearemos la tabla yellow en una nueva base de datos que denominamos taxidata y le indicamos que coja los datos S3 de s3://aws-tc-largeobjects/CUR-TF-200-ACBDFO-1/Lab2/yellow/ . El formato de los datos es CSV . El siguiente paso es mediante Bulk add columns indicar los nombre y tipos de las columnas: vendor string , pickup timestamp , dropoff timestamp , count int , distance int , ratecode string , storeflag string , pulocid string , dolocid string , paytype string , fare decimal , extra decimal , mta_tax decimal , tip decimal , tolls decimal , surcharge decimal , total decimal Finalmente, veremos a modo de resumen una instrucci\u00f3n create table similar a la siguiente y ya podemos pulsar sobre Create table : `` sql CREATE EXTERNAL TABLE IF NOT EXISTS taxidata . yellow ( vendor string, pickup timestamp, dropoff timestamp, count int, distance int, ratecode string, storeflag string pulocid string, dolocid string, paytype string, fare decimal, extra decimal, mta_tax decimal, tip decimal, tolls decimal, surcharge decimal, total` decimal ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES ( 'serialization.format' = ',', 'field.delim' = ',' ) LOCATION 's3://aws-tc-largeobjects/CUR-TF-200-ACBDFO-1/Lab2/yellow/' TBLPROPERTIES ('has_encrypted_data'='false'); FIXME: mirar y hacer https://docs.aws.amazon.com/athena/latest/ug/getting-started.html","title":"AWS Athena"},{"location":"draft/athena.html#aws-glue","text":"Es la herramienta ETL completamente administrado que ofrece Amazon Web Services. Cuando el esquema de los datos es desconocido, AWS Glue permite inferirlo. Para ello, hemos de construir un rastreador ( crawler ) para descubrir su estructura. . AWS Glue builds a catalog that contains metadata about the various data sources. AWS Glue is similar to Amazon Athena in that the actual data you analyze remains in the data source. The key difference is that you can build a crawler with AWS Glue to discover the schema.","title":"AWS Glue"},{"location":"draft/athena.html#creacion","text":"","title":"Creaci\u00f3n"},{"location":"draft/athena.html#ejecucion","text":"Se puede ejecutar bajo demanda o planificar su ejecuci\u00f3n de forma diaria, por horas, etc... Una vez lanzado, al cabo de un minuto, veremos que ha finalizado y en Base de Datos --> Tablas podremos ver la tabla que ha creado (en nuestro caso la tabla csv ), junto con la estructura que ha inferido respecto a los datos cargados. El siguiente paso es editar el esquema y ponerle nombres significativos a los campos: Una vez los datos est\u00e1n cargados, ya podemos realizar consular con AWS Athena.","title":"Ejecuci\u00f3n"},{"location":"draft/bdaplicado04flume.html","text":"Flume / Sqoop \u00b6 Flume \u00b6 All\u00e1 por el a\u00f1o 2010 se Cloudera present\u00f3 Flume, programa para tratamiento e ingesta de datos masivo. Esto daba la posibilidad de crear desarrollos complejos que permitieran el tratamiento de datos masivos creados en Tiempo Real. En Real Time se utiliza en la etapa de Obtenci\u00f3n de Datos (para conectarnos a fuentes de datos Online). Su arquitectura es sencilla, pues tiene tres componentes principales, muy configurables: Source: Fuente de origen de los datos Channel: la v\u00eda por donde se tratar\u00e1n los datos Sink: persistencia/movimiento de los datos Flume es sencillito apriori, el problema es cuando quieres utilizarlo para obtener datos de manera paralela (o multiplexada) y adem\u00e1s te ves en la necesidad de crear tus propios Sinks, o tus propios interceptores. Entonces la cosa cambia y hay que dedicarle algo m\u00e1s de tiempo. Muy recomendada como ayuda|compa\u00f1ero|alternativa a herramientas como Kettle. Sqoop \u00b6 https://www.geeksforgeeks.org/overview-of-sqoop-in-hadoop/ Referencias \u00b6 aaa","title":"Flume / Sqoop"},{"location":"draft/bdaplicado04flume.html#flume-sqoop","text":"","title":"Flume / Sqoop"},{"location":"draft/bdaplicado04flume.html#flume","text":"All\u00e1 por el a\u00f1o 2010 se Cloudera present\u00f3 Flume, programa para tratamiento e ingesta de datos masivo. Esto daba la posibilidad de crear desarrollos complejos que permitieran el tratamiento de datos masivos creados en Tiempo Real. En Real Time se utiliza en la etapa de Obtenci\u00f3n de Datos (para conectarnos a fuentes de datos Online). Su arquitectura es sencilla, pues tiene tres componentes principales, muy configurables: Source: Fuente de origen de los datos Channel: la v\u00eda por donde se tratar\u00e1n los datos Sink: persistencia/movimiento de los datos Flume es sencillito apriori, el problema es cuando quieres utilizarlo para obtener datos de manera paralela (o multiplexada) y adem\u00e1s te ves en la necesidad de crear tus propios Sinks, o tus propios interceptores. Entonces la cosa cambia y hay que dedicarle algo m\u00e1s de tiempo. Muy recomendada como ayuda|compa\u00f1ero|alternativa a herramientas como Kettle.","title":"Flume"},{"location":"draft/bdaplicado04flume.html#sqoop","text":"https://www.geeksforgeeks.org/overview-of-sqoop-in-hadoop/","title":"Sqoop"},{"location":"draft/bdaplicado04flume.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/bdaplicado05kafka.html","text":"Kafka \u00b6 Apache Kafka Fundamentals LiveLessons https://learning.oreilly.com/videos/apache-kafka-fundamentals/9780134833682/ TIME TO COMPLETE: 3h 49m https://learning.oreilly.com/videos/apache-kafka-a-z/9781801077569/ Apache Kafka A-Z with Hands-On Learning TIME TO COMPLETE: 9h 36m Transient Storage Sencillamente es un servicio de commit log, particionado, replicado y distribuido. En su arquitectura encontramos que disponemos de un modelo Productor/Consumidor, cuyos mensajes se pueden categorizar en algo llamado topics y que funciona como si fuera un cluster. Se suele utilizar como gestor de colas. Se utiliza en la etapa de Almacenamiento de Datos. Amazon Kinesis \u00b6 Amazon Kinesis facilita la recopilaci\u00f3n, el procesamiento y el an\u00e1lisis de datos de streaming en tiempo real para obtener datos de manera oportuna y reaccionar r\u00e1pidamente ante informaci\u00f3n nueva. Amazon Kinesis ofrece capacidades clave para procesar de manera rentable datos de streaming a cualquier escala, adem\u00e1s de la flexibilidad para elegir las herramientas m\u00e1s adecuadas para los requisitos de su aplicaci\u00f3n. Con Amazon Kinesis, puede incorporar datos en tiempo real, como videos, audios, registros de aplicaciones, secuencias de clics de sitios web y datos de telemetr\u00eda de IoT para aprendizaje autom\u00e1tico, an\u00e1lisis y otras aplicaciones. Amazon Kinesis le permite procesar y analizar datos a medida que se reciben y responder instant\u00e1neamente en vez de tener que esperar a que los datos se recopilen antes de que el procesamiento pueda comenzar. Referencias \u00b6 aaa","title":"Kafka"},{"location":"draft/bdaplicado05kafka.html#kafka","text":"Apache Kafka Fundamentals LiveLessons https://learning.oreilly.com/videos/apache-kafka-fundamentals/9780134833682/ TIME TO COMPLETE: 3h 49m https://learning.oreilly.com/videos/apache-kafka-a-z/9781801077569/ Apache Kafka A-Z with Hands-On Learning TIME TO COMPLETE: 9h 36m Transient Storage Sencillamente es un servicio de commit log, particionado, replicado y distribuido. En su arquitectura encontramos que disponemos de un modelo Productor/Consumidor, cuyos mensajes se pueden categorizar en algo llamado topics y que funciona como si fuera un cluster. Se suele utilizar como gestor de colas. Se utiliza en la etapa de Almacenamiento de Datos.","title":"Kafka"},{"location":"draft/bdaplicado05kafka.html#amazon-kinesis","text":"Amazon Kinesis facilita la recopilaci\u00f3n, el procesamiento y el an\u00e1lisis de datos de streaming en tiempo real para obtener datos de manera oportuna y reaccionar r\u00e1pidamente ante informaci\u00f3n nueva. Amazon Kinesis ofrece capacidades clave para procesar de manera rentable datos de streaming a cualquier escala, adem\u00e1s de la flexibilidad para elegir las herramientas m\u00e1s adecuadas para los requisitos de su aplicaci\u00f3n. Con Amazon Kinesis, puede incorporar datos en tiempo real, como videos, audios, registros de aplicaciones, secuencias de clics de sitios web y datos de telemetr\u00eda de IoT para aprendizaje autom\u00e1tico, an\u00e1lisis y otras aplicaciones. Amazon Kinesis le permite procesar y analizar datos a medida que se reciben y responder instant\u00e1neamente en vez de tener que esperar a que los datos se recopilen antes de que el procesamiento pueda comenzar.","title":"Amazon Kinesis"},{"location":"draft/bdaplicado05kafka.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/ingesta02sql.html","text":"ETL con SQL / Python \u00b6 24 Enero AWS CLI \u00b6 Para poder acceder a los recursos de AWS, necesitamos preparar nuestro entorno de trabajo. AWS desde Python \u00b6 Para acceder a AWS, Amazon ofrece la librer\u00eda Boto3. Para poder utilizarla, la instalaremos mediante pip install boto3 https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html Ejemplo cloud9 y DynamoDB con Python https://aws-dojo.com/excercises/excercise29/ Cargar datos de CSV y cargar en DynamoDB Consultar de RDS e insertar en DDB (Sqoop?) Referencias \u00b6 Python, Boto3, and AWS S3: Demystified: https://realpython.com/python-boto3-aws-s3/","title":"ETL con SQL / Python"},{"location":"draft/ingesta02sql.html#etl-con-sql-python","text":"24 Enero","title":"ETL con SQL / Python"},{"location":"draft/ingesta02sql.html#aws-cli","text":"Para poder acceder a los recursos de AWS, necesitamos preparar nuestro entorno de trabajo.","title":"AWS CLI"},{"location":"draft/ingesta02sql.html#aws-desde-python","text":"Para acceder a AWS, Amazon ofrece la librer\u00eda Boto3. Para poder utilizarla, la instalaremos mediante pip install boto3 https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html Ejemplo cloud9 y DynamoDB con Python https://aws-dojo.com/excercises/excercise29/ Cargar datos de CSV y cargar en DynamoDB Consultar de RDS e insertar en DDB (Sqoop?)","title":"AWS desde Python"},{"location":"draft/ingesta02sql.html#referencias","text":"Python, Boto3, and AWS S3: Demystified: https://realpython.com/python-boto3-aws-s3/","title":"Referencias"},{"location":"draft/ingesta04multidimesional.html","text":"ETL Distribuido \u00b6 https://www.xenonstack.com/use-cases/data-ingestion-platform Referencias \u00b6 aaa","title":"ETL Distribuido"},{"location":"draft/ingesta04multidimesional.html#etl-distribuido","text":"https://www.xenonstack.com/use-cases/data-ingestion-platform","title":"ETL Distribuido"},{"location":"draft/ingesta04multidimesional.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/ingesta05nifi1.html","text":"Nifi I \u00b6 https://www.futurespace.es/cafe-con-iot-capitulo-1-los-flujos-de-nifi/ https://www.futurespace.es/apache-nifi-validando-transferencias-de-ficheros-caso-de-uso-real/ https://www.futurespace.es/apache-nifi-flujo-de-extraccion-validacion-transformacion-y-carga-de-ficheros-caso-de-uso-real/ Introduction to Apache NiFi (Hortonworks DataFlow - HDF 2.0) https://learning.oreilly.com/videos/introduction-to-apache/9781789346084/ TIME TO COMPLETE: 1h 47m Referencias \u00b6 aaa","title":"Nifi I"},{"location":"draft/ingesta05nifi1.html#nifi-i","text":"https://www.futurespace.es/cafe-con-iot-capitulo-1-los-flujos-de-nifi/ https://www.futurespace.es/apache-nifi-validando-transferencias-de-ficheros-caso-de-uso-real/ https://www.futurespace.es/apache-nifi-flujo-de-extraccion-validacion-transformacion-y-carga-de-ficheros-caso-de-uso-real/ Introduction to Apache NiFi (Hortonworks DataFlow - HDF 2.0) https://learning.oreilly.com/videos/introduction-to-apache/9781789346084/ TIME TO COMPLETE: 1h 47m","title":"Nifi I"},{"location":"draft/ingesta05nifi1.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/ingesta06nifi2.html","text":"Nifi II \u00b6 Referencias \u00b6 aaa","title":"Nifi II"},{"location":"draft/ingesta06nifi2.html#nifi-ii","text":"","title":"Nifi II"},{"location":"draft/ingesta06nifi2.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark01.html","text":"Spark \u00b6 Spark es un framework de computaci\u00f3n en cluster similar a MapReduce , pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark01.html#spark","text":"Spark es un framework de computaci\u00f3n en cluster similar a MapReduce , pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark01.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark02.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark02.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark02.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark03.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark03.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark03.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark04.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark04.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark04.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark05.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark05.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark05.html#referencias","text":"aaa","title":"Referencias"},{"location":"draft/spark06.html","text":"Spark \u00b6 WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7. Referencias \u00b6 aaa","title":"Spark"},{"location":"draft/spark06.html#spark","text":"WHAT IS SPARK? Spark is a cluster computing framework similar to MapReduce. Spark, however, doesn\u2019t handle the storage of files on the (distributed) file system itself, nor does it handle the resource management. For this it relies on systems such as the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary systems. For testing and development, you can even run Spark on your local system. HOW DOES SPARK SOLVE THE PROBLEMS OF MAPREDUCE? While we oversimplify things a bit for the sake of clarity, Spark creates a kind of shared RAM memory between the computers of your cluster. This allows the different workers to share variables (and their state) and thus eliminates the need to write the intermediate results to disk. More technically and more correctly if you\u2019re into that: Spark uses Resilient Distributed Datasets (RDD), which are a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a faulttolerant way.1 Because it\u2019s an in-memory system, it avoids costly disk operations. THE DIFFERENT COMPONENTS OF THE SPARK ECOSYSTEM Spark core provides a NoSQL environment well suited for interactive, exploratory analysis. Spark can be run in batch and interactive mode and supports Python. Spark has four other large components, as listed below and depicted in figure 5.5. 1 Spark streaming is a tool for real-time analysis. 2 Spark SQL provides a SQL interface to work with Spark. 3 MLLib is a tool for machine learning inside the Spark framework. 4 GraphX is a graph database for Spark. We\u2019ll go deeper into graph databases in chapter 7.","title":"Spark"},{"location":"draft/spark06.html#referencias","text":"aaa","title":"Referencias"}]}