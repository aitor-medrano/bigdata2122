{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales. Bloque Cloud Computing y Arquitecturas Big Data \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 15 Nov 1p + 2o 2.- Amazon Web Services Lunes 22 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 29 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 13 Dic 1p + 2o 5.- Datos en la nube Lunes 10 Ene 1p + 2o 6.- Arquitecturas Big Data Lunes 17 Ene 1p + 2o Bloque Ingesta de Datos \u00b6 Resultados de aprendizaje \u00b6 M\u00f3dulo Sistemas de Big Data : Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Ingesta de datos. ETL Lunes 24 Ene 1p + 2o 2.- Pentaho Data Integration Lunes 31 Ene 1p + 2o 3.- Nifi Lunes 28 Feb 1p + 2o 4.- Nifi Avanzado Lunes 7 Mar 1p + 2o 5.- Ingesta en AWS mediante Python Lunes 7 Feb 1p + 2o Bloque Big Data Aplicado \u00b6 Resultados de aprendizaje \u00b6 M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Hadoop Mi\u00e9rcoles 9 Feb 2p + 3o 2.- HDFS Mi\u00e9rcoles 2 Mar 2p + 3o 3.- Sqoop y Flume Mi\u00e9rcoles 9 Mar 2p + 3o 4.- Hive Mi\u00e9rcoles 16 Mar 2p + 3o 5.- Kafka Mi\u00e9rcoles 23 Mar 2p + 3o Bloque Anal\u00edtica de Datos \u00b6 Resultados de aprendizaje \u00b6 M\u00f3dulo Sistemas de Big Data : Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Spark. RDD Mi\u00e9rcoles 30 Mar / Mi\u00e9rcoles 6 Abr 4p + 6o 2.- Spark DataFrame Mi\u00e9rcoles 27 Abr 2p + 3o 3.- Spark SQL Mi\u00e9rcoles 27 Abr 2p + 3o 4.- Spark Streaming Mi\u00e9rcoles 11 May 2p + 3o 5.- Spark ML Mi\u00e9rcoles 18 May 2p + 3o","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"index.html#bloque-cloud-computing-y-arquitecturas-big-data","text":"","title":"Bloque Cloud Computing y Arquitecturas Big Data"},{"location":"index.html#resultados-de-aprendizaje","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 15 Nov 1p + 2o 2.- Amazon Web Services Lunes 22 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 29 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 13 Dic 1p + 2o 5.- Datos en la nube Lunes 10 Ene 1p + 2o 6.- Arquitecturas Big Data Lunes 17 Ene 1p + 2o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-ingesta-de-datos","text":"","title":"Bloque Ingesta de Datos"},{"location":"index.html#resultados-de-aprendizaje_1","text":"M\u00f3dulo Sistemas de Big Data : Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion_1","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Ingesta de datos. ETL Lunes 24 Ene 1p + 2o 2.- Pentaho Data Integration Lunes 31 Ene 1p + 2o 3.- Nifi Lunes 28 Feb 1p + 2o 4.- Nifi Avanzado Lunes 7 Mar 1p + 2o 5.- Ingesta en AWS mediante Python Lunes 7 Feb 1p + 2o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-big-data-aplicado","text":"","title":"Bloque Big Data Aplicado"},{"location":"index.html#resultados-de-aprendizaje_2","text":"M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion_2","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Hadoop Mi\u00e9rcoles 9 Feb 2p + 3o 2.- HDFS Mi\u00e9rcoles 2 Mar 2p + 3o 3.- Sqoop y Flume Mi\u00e9rcoles 9 Mar 2p + 3o 4.- Hive Mi\u00e9rcoles 16 Mar 2p + 3o 5.- Kafka Mi\u00e9rcoles 23 Mar 2p + 3o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-analitica-de-datos","text":"","title":"Bloque Anal\u00edtica de Datos"},{"location":"index.html#resultados-de-aprendizaje_3","text":"M\u00f3dulo Sistemas de Big Data : Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. Gestiona y almacena datos facilitando la b\u00fasqueda de respuestas en grandes conjuntos de datos. M\u00f3dulo de Big Data Aplicado : Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion_3","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Spark. RDD Mi\u00e9rcoles 30 Mar / Mi\u00e9rcoles 6 Abr 4p + 6o 2.- Spark DataFrame Mi\u00e9rcoles 27 Abr 2p + 3o 3.- Spark SQL Mi\u00e9rcoles 27 Abr 2p + 3o 4.- Spark Streaming Mi\u00e9rcoles 11 May 2p + 3o 5.- Spark ML Mi\u00e9rcoles 18 May 2p + 3o","title":"Planificaci\u00f3n"},{"location":"apuntes/arquitecturas01.html","text":"Arquitecturas Big Data \u00b6 Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manejar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda concreta, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas. Caracter\u00edsticas \u00b6 Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central ( SPOF ). Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener. Tipos de arquitecturas \u00b6 Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming. Procesamiento Batch \u00b6 Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande con m\u00faltiples relaciones, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origen para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante. Procesamiento en Streaming \u00b6 Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instant\u00e1neo. Arquitectura Lambda \u00b6 Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streaming ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la propia capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para procesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos, el lento y el r\u00e1pido, convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actualizar con datos m\u00e1s precisos provenientes de la capa batch . Paso a paso \u00b6 Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. Arquitectura Kappa \u00b6 El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos caminos diferenciados, lo que conlleva a duplicar la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps en su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos. Arquitectura por capas \u00b6 Adem\u00e1s de las dos soluciones que acabamos de conocer, otra forma de dise\u00f1ar las capas de una arquitectura big data consiste en separar las diferentes fases del dato en capa diferenciadas. La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.como) Capa de ingesti\u00f3n: es la primera capa que recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almac\u00e9n de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interact\u00faan los usuarios. Tecnolog\u00edas \u00b6 Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka y/o un servicio de flujo de datos como Apache Nifi . El almacenamiento de los datos y modelos lo podemos realizar mediante HDFS o S3. Dentro de una arquitectura Lamba, en el sistema batch, mediante algoritmos MapReduce de Hadoop podemos entrenar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB , Redis o AWS Dynamo DB . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill . Casos de uso \u00b6 \u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfCu\u00e1l encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejecuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para ambas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomendable tenemos un par de casos desarrollados por Ericsson que pod\u00e9is leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha. Buenas pr\u00e1cticas \u00b6 En la ingesta de datos: evaluar los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente de datos, y en alg\u00fan caso lo mejor es combinar varias herramientas para cubrir todo el abanico. En el procesamiento: analizar si el sistema debe ser streaming o batch. Algunos sistemas que no se definen como puramente streaming, es decir, utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: al trabajar con multitud de herramientas es importante utilizar herramienta para controlar, monitorizar y gestionar la arquitectura. Algunas decisiones que debemos tomar a la hora de elegir la arquitectura son: Enfocar los casos de uso. Cuando tengamos los objetivos claros sabremos qu\u00e9 parte debemos fortalecer en la arquitectura. \u00bfVolumen, variedad, velocidad? Definir la arquitectura: \u00bfbatch o streaming? \u00bfRealmente es necesario que nuestra arquitectura soporte streaming? Evaluar las fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que se utilizan? Arquitectura en la nube \u00b6 Dentro de las diferentes plataformas cloud, a la hora de dise\u00f1ar e implementar una aplicaci\u00f3n, podemos tener en mente, desde un inicio, que se va a desplegar en la nube, haciendo uso de la gran mayor\u00eda de servicios que hemos estudiado en las sesiones anteriores. De forma complementaria, estas plataformas cloud ofrecen un marco buenas pr\u00e1cticas, principios y decisiones que hemos de tomar a la hora de dise\u00f1ar nuestros sistemas, sean big data o no. Marco de buena arquitectura (WAF) \u00b6 Pilares del Marco de Buena Arquitectura (WAF) El marco de buena arquitectura ( AWS Well-Architected Framework / Azure Well-Architected Framework ) es una gu\u00eda dise\u00f1ada para ayudar a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para las aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas que facilitan la evaluaci\u00f3n e implementaci\u00f3n de nuestras arquitecturas en la nube. Se organiza en cinco pilares que estudiaremos a continuaci\u00f3n: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costes. Excelencia operativa \u00b6 Se centra en la habilidad de ejecutar y monitorizar sistemas para proporcionar valor de negocio, mejorando de forma continua los procesos y procedimientos de soporte. Comprende la capacidad para dar soporte al desarrollo y ejecutar cargas de trabajo de manera eficaz, obtener informaci\u00f3n acerca de las operaciones y mejorar continuamente el soporte a los procesos y los procedimientos para ofrecer valor de negocio. Para ello se recomienda: Realizar operaciones como c\u00f3digo : podemos definir toda la carga de trabajo (aplicaciones, infraestructura) como c\u00f3digo y actualizarla con c\u00f3digo, sin necesidad de utilizar el interfaz gr\u00e1fico. De esta manera, podemos automatizar la ejecuci\u00f3n en respuesta a eventos, adem\u00e1s de limitar la posibilidad de error humano. Realizar cambios peque\u00f1os, reversibles (por si se producen errores) y frecuentes . Refinar los procedimientos de las operativos con frecuencia, revisando de forma peri\u00f3dica su efectividad y conocimiento por parte de los equipos. Prever los errores : realizar simulacros de fallos, probando los procedimientos de respuesta. Aprender de los errores y eventos operativos : promover mejoras a partir de las lecciones aprendidas de todos los eventos y los errores operativos. Seguridad \u00b6 Antes de dise\u00f1ar cualquier sistema, es imprescindible aplicar pr\u00e1cticas de seguridad. Debemos poder controlar qui\u00e9n puede hacer qu\u00e9. Adem\u00e1s, debe de identificar incidentes de seguridad, proteger los sistemas y servicios, y mantener la confidencialidad y la integridad de los datos mediante la protecci\u00f3n de la informaci\u00f3n. As\u00ed pues, este pilar se centra en la capacidad de proteger la informaci\u00f3n, los sistemas y los recursos, al mismo tiempo que se aporta valor de negocio mediante evaluaciones de riesgo y estrategias de mitigaci\u00f3n, siguiendo los siguientes principios: Implementar una base s\u00f3lida de credenciales : mediante el principio de m\u00ednimo privilegio y aplicando la separaci\u00f3n de obligaciones con la autorizaci\u00f3n apropiada para cada interacci\u00f3n con los recursos de AWS. Habilitar la trazabilidad : Integrando registros y m\u00e9tricas con sistemas para responder y tomar medidas de manera autom\u00e1tica, que facilitan la monitorizaci\u00f3n y el uso de alertar. Aplicar seguridad en todas las capas : por ejemplo, a la red de borde, a la nube virtual privada, a la subred y al balanceador de carga, y a cada instancia, sistema operativo y aplicaci\u00f3n. Automatizar las pr\u00e1cticas recomendadas de seguridad. Proteger los datos en tr\u00e1nsito y en reposo : mediante el cifrado, el uso de tokens y el control de acceso cuando corresponda, asegurando la confidencialidad e integridad de los datos. Mantener a las personas alejadas de los datos : reducir o eliminar el acceso directo o el procesamiento manual de los datos. Prepararse para eventos de seguridad : mediante procesos de administraci\u00f3n de incidencias con automatizaci\u00f3n que d\u00e9 respuesta a incidentes. Fiabilidad \u00b6 Se centra en: la capacidad de un sistema de recuperarse de interrupciones en la infraestructura o el servicio. incorporar din\u00e1micamente recursos inform\u00e1ticos para satisfacer la demanda mitigar las interrupciones, como errores de configuraci\u00f3n o problemas de red temporales. Se recomiendan los siguientes principios para aumentar la fiabilidad: Probar los procedimientos de recuperaci\u00f3n : usar la automatizaci\u00f3n para simular diferentes errores o para volver a crear situaciones que hayan dado lugar a errores antes. Recuperarse autom\u00e1ticamente de los errores: monitorizar los sistemas en busca de indicadores clave de rendimiento y configurar los sistemas para desencadenar procesos de recuperaci\u00f3n automatizado cuando se supere un l\u00edmite. Escalar horizontalmente para aumentar la disponibilidad total del sistema: sustituir un recurso grande por varios recursos m\u00e1s peque\u00f1os, distribuyendo las solicitudes para reducir el impacto de un \u00fanico punto de error. Evitar asumir estimaciones sobre capacidad : monitorizando la demanda y el uso del sistema, y automatizando la incorporaci\u00f3n o eliminaci\u00f3n de recursos para mantener el nivel \u00f3ptimo. Administrar los cambios mediante la automatizaci\u00f3n . Eficiencia del rendimiento \u00b6 Se centra en la capacidad de utilizar recursos inform\u00e1ticos de forma eficiente (s\u00f3lo cuando sean necesarios) para satisfacer los requisitos del sistema y mantener esa eficiencia a medida que cambia la demanda o evolucionan las tecnolog\u00edas. Entre los temas principales se incluyen la selecci\u00f3n de los tipos y tama\u00f1os de recursos adecuados en funci\u00f3n de los requisitos de la carga de trabajo, el monitoreo del rendimiento y la toma de decisiones fundamentadas para mantener la eficiencia a medida que evolucionan las necesidades de la empresa. Se recomiendan los siguientes principios para mejorar la eficiencia del rendimiento: Democratizar las tecnolog\u00edas avanzadas : usando tecnolog\u00edas como servicio (como son los servicios de IA que ofrecen tanto AWS como Azure), que simplifican su uso. Adquirir escala mundial en cuesti\u00f3n de minutos : desplegando sistemas en varias regiones para ofrecer una menor latencia. Utilizar arquitecturas sin servidor : las arquitecturas sin servidor eliminan la carga operativa que supone ejecutar y mantener servidores. Experimentar m\u00e1s a menudo : mediante pruebas comparativas de diferentes tipos de instancias, almacenamiento y/o configuraciones. Disponer de compatibilidad mec\u00e1nica : utilizando el enfoque tecnol\u00f3gico que se ajuste mejor a lo que intenta conseguir (por ejemplo, mediante los patrones de acceso a los datos cuando accedamos a bases de datos o almacenamiento). Optimizaci\u00f3n de costes \u00b6 Se centra en la capacidad de ejecutar sistemas para ofrecer valor de negocio al precio m\u00e1s bajo. Entre los temas principales se incluyen la comprensi\u00f3n y el control de cu\u00e1ndo se est\u00e1 gastando el dinero, la selecci\u00f3n de los tipos de recursos m\u00e1s adecuados en la cantidad correcta, el an\u00e1lisis de los gastos a lo largo del tiempo y el escalado para satisfacer las necesidades de la empresa sin gastos excesivos. Se recomiendan los siguientes principios para optimizar los costes: Adoptar un modelo de consumo : pagando solo por los recursos inform\u00e1ticos que necesitamos. Medir la eficacia general : midiendo la producci\u00f3n comercial de la carga de trabajo y los costes asociados a la entrega. Dejar de gastar dinero en las operaciones de centros de datos : la nube elimina los costes de aprovisionamiento, electricidad, aire acondicionado, seguridad f\u00edsica, etc... Analizar y asignar gastos : la nube facilita la identificaci\u00f3n precisa del uso y los costes del sistema, as\u00ed como el coste de las cargas de trabajo individuales, lo que facilita medir el retorno de la inversi\u00f3n (ROI). Utilizar los servicios administrados para reducir el coste de propiedad: reducen la carga operativa que supone mantener servidores para tareas como el env\u00edo de email o la administraci\u00f3n de bases de datos. Todo falla constantemente Hemos de dise\u00f1ar nuestras arquitecturas con el axioma que en un momento u otro algo fallar\u00e1. Para que nuestros sistemas resistan los errores, los dos factores m\u00e1s cr\u00edticos son la fiabilidad y la disponibilidad. Una forma de medir la fiabilidad es el MTBF : tiempo promedio entre errores, es decir tiempo total en servicio respecto a la cantidad de errores. Otra forma es mediante el porcentaje de tiempo durante el cual el sistema funcional correctamente. Este porcentaje se suele medir en la cantidad de nueves, as\u00ed pues seis nueves implica una disponibilidad del 99,9999%. Un sistema de alta disponibilidad (HA) es aquel que puede soportar cierta medida de degradaci\u00f3n sin dejar de estar disponible. Los tres factores que influyen en la disponibilidad son la tolerancia a errores (gracias a la redundancia, cambiar de recurso cuando uno falla), la escalabilidad (la aplicaci\u00f3n se adaptar a los aumentos de carga) y la capacidad de recuperaci\u00f3n (el servicio se restaura r\u00e1pidamente sin perder datos). AWS Trusted Advisor \u00b6 AWS dispone de la herramienta en l\u00ednea AWS Trusted Advisor que ofrece asesoramiento en tiempo real con las pr\u00e1cticas recomendadas de AWS. Examina todo el entorno AWS y ofrece recomendaciones en cinco categor\u00edas: AWS Trusted Advisor Optimizaci\u00f3n de costes: sugiere recursos no utilizados e inactivos, o bien, posibilidad de realiza una reserva de capacidad. Rendimiento: comprueba los l\u00edmites del servicio y monitoriza para detectar instancias que se est\u00e9n utilizando por encima de su capacidad. Seguridad: examina los permisos para mejorar el nivel de seguridad de la aplicaci\u00f3n. Tolerancia a errores: revisa las capacidades de escalado autom\u00e1tico, las comprobaciones de estado, la implementaci\u00f3n Multi-AZ y las capacidades de copia de seguridad. L\u00edmites del servicio: realiza verificaciones para detectar usos que superen el 80% del l\u00edmite del servicio. Azure Advisor y Azure Score Microsoft, del forma similar, ofrece un par herramientas que nos ayudan a optimizar las implementaciones, como son Azure Advisor , y dentro de ella Advisor Score que punt\u00faa las recomendaciones para con un simple vistazo poder priorizar las mejoras sugeridas. Gesti\u00f3n del escalado y la monitorizaci\u00f3n \u00b6 Monitorizaci\u00f3n \u00b6 Al operar en la nube, es importante llevar un seguimiento de las actividades, porque probablemente haya un coste asociado a cada una de ellas. AWS ayuda a monitorizar, registrar e informar sobre el uso de sus servicios proporcionando herramientas para hacerlo. As\u00ed pues, AWS ofrece los siguientes servicios relacionados con la monitorizaci\u00f3n: Amazon CloudTrail : Servicio que registra cada acci\u00f3n que se lleva a cabo en la cuenta de AWS por motivos de seguridad. Esto significa que CloudTrail registra cada vez que alguien carga datos, ejecuta un c\u00f3digo, crea una instancia de EC2 o realiza cualquier otra acci\u00f3n. Amazon Cloudwatch : Servicio de monitorizaci\u00f3n en tiempo real de los recursos de AWS y las aplicaciones que ejecutamos en AWS. CloudTrail registra actividades, mientras que CloudWatch las monitoriza. As\u00ed pues, CloudWatch vigila que los servicios cloud se ejecutan sin problema y ayuda a no utilizar ni m\u00e1s ni menos recursos de lo esperado, lo que es importante para el seguimiento del presupuesto. Permite: Recopilar y hacer un seguimiento de las m\u00e9tricas est\u00e1ndar y personalizadas Establecer alarmas para enviar notificaciones autom\u00e1ticas a SNS o efectuar acciones de AutoScaling EC2 en funci\u00f3n del valor de las m\u00e9tricas obtenidas. AWS Config : Servicio que permite analizar, auditar y evaluar las configuraciones de los recursos de AWS. Monitoriza y registra de manera continua las configuraciones de recursos de AWS y permite automatizar la evaluaci\u00f3n de las configuraciones registradas respecto a las deseadas. Amazon SNS ( Amazon Simple Notification Service ) : herramienta que permite enviar textos, correos electr\u00f3nicos y mensajes a otros servicios en la nube y enviar notificaciones al cliente de varias formas desde la nube. Ejemplo Cloudwatch \u00b6 En el siguiente ejemplo vamos a crear una alarma de Cloudwatch para enviar una notificaci\u00f3n cuando nuestra cuenta haya gastado una cierta cantidad de dinero. La alarma env\u00eda un mensaje a Amazon SNS para posteriormente enviar un correo electr\u00f3nico. El primer paso es crear y subscribirse a un tema ( topic ) SNS. Un tema act\u00faa como un canal de comunicaci\u00f3n donde se recibes los mensajes de las alertas y eventos. As\u00ed pues, entramos al servicio SNS ( Simple Notification Service ) y creamos un tema al que llamaremos AlertaSaldo . Cloudwatch - Creaci\u00f3n del tema A continuaci\u00f3n, vamos a crear una suscripci\u00f3n a ese tema para que cuando se recibe una mensaje, lo redirijamos a nuestro tel\u00e9fono o correo electr\u00f3nico. Para ello, dentro de la secci\u00f3n de suscripciones, crearemos una suscripci\u00f3n. En el ARN pondremos el tema AlertaSaldo que acabamos de crear, y en el protocolo, vamos a seleccionar Correo electr\u00f3nico . Finalmente, en el punto de enlace, definimos el email que recibir\u00e1 la alerta. En este momento, Amazon enviar\u00e1 un email a la cuenta que hayamos indicado para confirmar los datos. Cloudwatch - Creaci\u00f3n de la subscripci\u00f3n El siguiente paso es crear la alarma en Cloudwatch . Para ello, una vez dentro de Cloudwatch , dentro de la opci\u00f3n de Alarmas, al crear una nueva, tendremos que elegir la m\u00e9trica, que en nuestro caso seleccionaremos Facturaci\u00f3n -> Cargo total estimado . En la siguiente pantalla, en la secci\u00f3n de Condiciones ..... est\u00e1tico, e indicamos la condici\u00f3n que queremos que se active cuando es superior a 100. Cloudwatch - Condiciones de la alarma En la secci\u00f3n de Notificaci\u00f3n , tras elegir en modo alarma seleccionamos el tema SNS existente (en nuestro caso AlertaSaldo ). Cloudwatch - Notificaciones de la alarma Finalmente, le asignamos el nombre de AlertaSaldoAlarma y tras ver un resumen de todo los configurado, creamos la alarma. De esta manera, cuando se supere el gasto de 100$, autom\u00e1ticamente nos enviar\u00e1 un email a la direcci\u00f3n que le hemos configurado. Escalado y Balanceo de carga \u00b6 Elastic Load Balancing (ELB) distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2. Adem\u00e1s, mejora la tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Admite tres tipos de balanceadores de carga: balanceador de carga de aplicaciones : balanceo basado en aplicaciones con tr\u00e1fico HTTP y HTTPS, ofreciendo enrutamiento avanzado. Opera en la capa de aplicaci\u00f3n. balanceador de carga de red : para rendimiento ultra (millones de peticiones por segundo), opera a nivel de la capa de conexi\u00f3n (protocolos TCP, UDP, TLS) balanceador de carga gateway , para aplicaciones de terceros que soportan el protocolo Geneve . Opera a nivel de capa de red. Un servicio complementario es AWS Auto Scaling , el cual permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Podemos utilizar Auto Scaling para asegurarnos que se ejecutan la cantidad deseada de instancias EC2, agregando o eliminando instancias de forma autom\u00e1tica seg\u00fan las cargas de trabajo. Mediante Auto Scaling , tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costes. Otro caso de uso es en las aplicaciones con patrones de demanda estables (escalado predictivo) o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana. Para ello, se crea un grupo Auto Scaling , el cual es una colecci\u00f3n de instancias EC2, indicando la cantidad m\u00ednima y m\u00e1xima de instancias a desplegar. Si queremos tener un escalado din\u00e1mico podemos usar conjuntamente EC2 AutoScaling , Amazon CloudWatch y Elastic Load Balancing . Ejemplos Escalado \u00b6 Para este ejemplo, vamos a crear dos instancias EC2 que est\u00e9n en la misma VPC, y vamos a utilizar un balanceador de carga para que tras cada petici\u00f3n, responda una instancia diferente. El primer paso ser\u00e1 crear un grupo de seguridad que van a compartir nuestras instancias. En nuestro caso, lo hemos llamado Servidor Web y la \u00fanica regla de entrada que tiene permite todo el tr\u00e1fico HTTP desde cualquier IP. Otra posibilidad es utilizar el grupo de seguridad iabd-front que creamos en sesiones anteriores. A continuaci\u00f3n, vamos a crear y lanzar la primera instancia. Para ello, en EC2 creamos una instancia del tipo que queramos, con el AMI de Amazon y en el campo de Datos de usuario , vamos a indicarle la siguiente informaci\u00f3n para que inicie un servidor web con una p\u00e1gina est\u00e1tica: #!/bin/bash yum update -y yum -y install httpd systemctl enable httpd systemctl start httpd echo '<html><h1>Hola Severo! Este es el servidor 1.</h1></html>' > /var/www/html/index.html A continuaci\u00f3n le a\u00f1adimos una etiqueta Nombre con el valor Servidor Web 1 y seleccionamos el grupo de seguridad que hab\u00edamos creado previamente. Finalmente, la lanzamos (ya sea con nuestro par de claves o sin indicar ninguno). Ahora deber\u00edamos poder acceder tanto a la IP p\u00fablica como al DNS con la informaci\u00f3n que tengamos de la instancia. Ten en cuenta que al pegar la direcci\u00f3n en el navegador, est\u00e1s accediendo por el protocolo HTTP y no HTTPS. El siguiente paso es repetir los mismos pasos, creando una nueva instancia pero cambiando tanto la etiqueta como el script de Datos de usuario para que muestre el n\u00famero 2. Antes de crear el balanceador, debemos comprobar las zonas de disponibilidad de nuestras instancias. En mi caso, ambas est\u00e1n en us-east-1d . As\u00ed pues, el siguiente paso es crear el balanceador de carga. Para ello, desde el panel lateral, seleccionamos la opci\u00f3n de Balanceadores de carga y pulsamos sobre Crear balanceador de carga de tipo aplicaci\u00f3n, y tras introducir el nombre indicamos las zonas de disponibilidad de nuestras instancias y configuraremos el mismo grupo de seguridad que hemos definido antes para tener abiertas las conexiones HTTP. M\u00e1s abajo, en la secci\u00f3n de Listener and routing , vamos a crear un grupo de destino, pulsando sobre el enlace de create target group del listener del protocolo HTTP del puerto 80. Se nos abrir\u00e1 una nueva pesta\u00f1a, donde tras indicarle el tipo (en nuestro caso b\u00e1sico) y asignarle un nombre al grupo de destino nuevo, en el check de salud, indicaremos el recurso /index.html . En la siguiente pantalla seleccionamos las dos instancias de servidor web: Elastic Load Balancing - Grupos de destino Una vez creado, volvemos a la pesta\u00f1a anterior, y ya podemos configurar en el listener el grupo de destino reci\u00e9n creado: Elastic Load Balancing - Listener Solo nos queda finalizar la creaci\u00f3n, y tras un par de minutos, cuando en estado aparezca Activo , podemos copiar el DNS en otra pesta\u00f1a del navegador, y acceder varias veces para comprobar como cada vez responde un servidor web diferente. Actividades \u00b6 Completa el cuestionario que tienes en Aules sobre las arquitecturas Lamba y Kappa. Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS . (opcional) Realiza el ejemplo de Cloudwatch que tienes m\u00e1s arriba pero con una alerta al superar uno o dos euros (depende de tu saldo actual) y adjunta una captura de pantalla tanto de la alarma creada, como del email recibido por parte de AWS. Referencias \u00b6 Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Data processing architectures \u2013 Lambda and Kappa Laboratorios de Amazon sobre AWF","title":"6.- Arquitecturas"},{"location":"apuntes/arquitecturas01.html#arquitecturas-big-data","text":"Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. Una arquitectura de big data se dise\u00f1a para manejar la ingesti\u00f3n, el procesamiento y el an\u00e1lisis de los datos que son demasiado grandes o complejos para un sistema tradicional de base de datos. En esta sesi\u00f3n no vamos a profundizar en ninguna tecnolog\u00eda concreta, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas.","title":"Arquitecturas Big Data"},{"location":"apuntes/arquitecturas01.html#caracteristicas","text":"Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central ( SPOF ). Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener.","title":"Caracter\u00edsticas"},{"location":"apuntes/arquitecturas01.html#tipos-de-arquitecturas","text":"Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming.","title":"Tipos de arquitecturas"},{"location":"apuntes/arquitecturas01.html#procesamiento-batch","text":"Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos un conjunto de datos muy grande con m\u00faltiples relaciones, puede llevarnos del orden de horas ejecutar las consultas que necesita el cliente, y por tanto, no se pueden ejecutar en tiempo real y necesitan de algoritmos paralelos (como por ejemplo, Map Reduce ). En estos casos, los resultados se almacenan en un lugar diferente al de origen para posteriores consultas. Otro ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realizar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extracci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante.","title":"Procesamiento Batch"},{"location":"apuntes/arquitecturas01.html#procesamiento-en-streaming","text":"Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Para ello, se utilizan diferentes sistemas basados en el uso de colas de mensajes. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instant\u00e1neo.","title":"Procesamiento en Streaming"},{"location":"apuntes/arquitecturas01.html#arquitectura-lambda","text":"Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter , y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto y tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Todos los datos que llegan al sistema van a ir por dos caminos, uno el lento (capa batch ) y otro el r\u00e1pido (capa streaming ), que finalmente confluyen en la capa de consultas. As\u00ed pues, se compone de tres capas: Capa batch : se encarga de gestionar los datos hist\u00f3ricos y recalcular los resultados. De manera espec\u00edfica, la capa batch recibe todos los datos en crudo, los almacena de forma inmutable y los combina con el hist\u00f3rico existente (se a\u00f1aden a los datos existente y los datos previos nunca se sobreescriben) y recalcula los resultados iterando sobre todo el conjunto de datos combinado. Cualquier cambio en un dato se almacena como un nuevo registro, no modifica nada, para as\u00ed poder seguir el linaje de los datos. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable, a coste de perder algo de precisi\u00f3n. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores, en base a las vistas batch que rellenan las capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: Arquitectura Lambda Los datos que fluyen por la capa de velocidad/ streaming tienen la restricci\u00f3n de latencia que impone la propia capa para poder procesar los datos todo lo r\u00e1pido que sea posible. Normalmente, este requisito choca con la precisi\u00f3n de los datos. Por ejemplo, en un escenario IoT donde se leen un gran n\u00famero de sensores de temperatura que env\u00edan datos de telemetr\u00eda, la capa de velocidad se puede utilizar para procesar una ventana temporal de los datos que entran (por ejemplo, los diez primeros segundos de cada minuto). Los datos que fluyen por el camino lento, no est\u00e1n sujeto a los mismos requisitos de latencia, lo que permite una mayor precisi\u00f3n computacional sobre grandes conjuntos de datos, que pueden conllevar mucho tiempo de procesamiento. Finalmente, ambos caminos, el lento y el r\u00e1pido, convergen en las aplicaciones anal\u00edticas del cliente. Si el cliente necesita informaci\u00f3n constante (cercana al tiempo real) aunque menos precisa, obtendr\u00e1 los datos del camino r\u00e1pido. Si no, lo har\u00e1 a partir de los datos de la capa batch . Dicho de otro modo, el camino r\u00e1pido tiene los datos de una peque\u00f1a ventana temporal, la cual se puede actualizar con datos m\u00e1s precisos provenientes de la capa batch .","title":"Arquitectura Lambda"},{"location":"apuntes/arquitecturas01.html#paso-a-paso","text":"Arquitectura Lambda El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas.","title":"Paso a paso"},{"location":"apuntes/arquitecturas01.html#arquitectura-kappa","text":"El t\u00e9rmino Arquitectura Kappa fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos d\u00e9biles de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Uno de los mayores inconveniente de la arquitectura Lambda es su complejidad. El procesamiento de los datos se realiza en dos caminos diferenciados, lo que conlleva a duplicar la l\u00f3gica de computaci\u00f3n y la gesti\u00f3n de la arquitectura de ambos caminos. Lo que se\u00f1ala Jay Kreps en su propuesta es que todos los datos fluyan por un \u00fanico camino, eliminando la capa batch y dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Arquitectura Kappa Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream : las operaciones batch son un subconjunto de las operaciones de streaming , por lo que todo puede ser tratado como un stream . Los datos de partida no se modifican: los datos se almacenan sin ser transformados, por tanto son inmutables, y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Tenemos la posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Arquitectura Kappa Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos.","title":"Arquitectura Kappa"},{"location":"apuntes/arquitecturas01.html#arquitectura-por-capas","text":"Adem\u00e1s de las dos soluciones que acabamos de conocer, otra forma de dise\u00f1ar las capas de una arquitectura big data consiste en separar las diferentes fases del dato en capa diferenciadas. La arquitectura por capas da soporte tanto al procesamiento batch como por streaming . La arquitectura consiste en 6 capas que aseguran un flujo seguro de los datos: Arquitectura por capas (xenonstack.como) Capa de ingesti\u00f3n: es la primera capa que recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas. Capa de colecci\u00f3n: Centrada en el transporte de los datos desde la ingesta al resto del pipeline de datos. En esta capa los datos se deshacen para facilitar la anal\u00edtica posterior. Capa de procesamiento: Esta es la capa principal. Se procesan los datos recogidos en las capas anteriores (ya sea mediante procesos batch , streaming o modelos h\u00edbridos), y se clasifican para decidir hac\u00eda qu\u00e9 capa se dirige. Capa de almacenamiento: Se centra en decidir donde almacenar de forma eficiente la enorme cantidad de datos. Normalmente en un almac\u00e9n de archivos distribuido, que da pie al concepto de data lake . Capa de consulta: capa donde se realiza el procesado anal\u00edtico, centr\u00e1ndose en obtener valor a partir de los datos. Capa de visualizaci\u00f3n: tambi\u00e9n conocida como capa de presentaci\u00f3n, es con la que interact\u00faan los usuarios.","title":"Arquitectura por capas"},{"location":"apuntes/arquitecturas01.html#tecnologias","text":"Por ejemplo, la ingesta de datos hacia las arquitecturas Lambda y Kappa se pueden realizar mediante un sistema de mensajer\u00eda de colas publish/subscribe como Apache Kafka y/o un servicio de flujo de datos como Apache Nifi . El almacenamiento de los datos y modelos lo podemos realizar mediante HDFS o S3. Dentro de una arquitectura Lamba, en el sistema batch, mediante algoritmos MapReduce de Hadoop podemos entrenar modelos. Para la capa de streaming (tanto para Lambda como Kappa) se pueden utilizar otras tecnolog\u00edas como Apache Storm , Apache Samza o Spark Streaming para modificar modelos de forma incremental. De forma alternativa, Apache Spark se puede utilizar como plataforma com\u00fan para desarrollar las capas batch y streaming de la arquitectura Lambda. De ah\u00ed su amplia aceptaci\u00f3n y uso a d\u00eda de hoy en la industria, se codifica una vez y se comparte en ambas capas La capa de serving se puede implementar mediante una base de datos NoSQL como pueda ser Apache HBase , MongoDB , Redis o AWS Dynamo DB . Tambi\u00e9n se pueden utilizar motores de consultas como Apache Drill .","title":"Tecnolog\u00edas"},{"location":"apuntes/arquitecturas01.html#casos-de-uso","text":"\u00bfQu\u00e9 arquitectura se adapta mejor a los requerimientos que nos traslada el cliente? \u00bfLambda o Kappa ? \u00bfCu\u00e1l encaja mejor en nuestro modelo de negocio?. Depende. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos es, \u00bfel an\u00e1lisis y el procesamiento (sus algoritmos) que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la arquitectura Kappa . Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning . En estos casos, los algoritmos batch se pueden optimizar ya que acceden al dataset hist\u00f3rico completo. El decidir entre Lamba y Kappa al final es una decisi\u00f3n entre favorecer el rendimiento de ejecuci\u00f3n de un proceso batch sobre la simplicidad de compartir c\u00f3digo para ambas capas. Casos reales Un ejemplo real de una arquitectura Kappa ser\u00eda un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende pel\u00edculas en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Como lectura recomendable tenemos un par de casos desarrollados por Ericsson que pod\u00e9is leer en https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Es muy importante siempre tener en mente lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir y el mercado del Big Data, lo que implica la necesidad de adaptarse a ellos lo antes posible, modificando la arquitectura sobre la marcha.","title":"Casos de uso"},{"location":"apuntes/arquitecturas01.html#buenas-practicas","text":"En la ingesta de datos: evaluar los tipos de fuentes de datos, no todas las herramientas sirven para cualquier fuente de datos, y en alg\u00fan caso lo mejor es combinar varias herramientas para cubrir todo el abanico. En el procesamiento: analizar si el sistema debe ser streaming o batch. Algunos sistemas que no se definen como puramente streaming, es decir, utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: al trabajar con multitud de herramientas es importante utilizar herramienta para controlar, monitorizar y gestionar la arquitectura. Algunas decisiones que debemos tomar a la hora de elegir la arquitectura son: Enfocar los casos de uso. Cuando tengamos los objetivos claros sabremos qu\u00e9 parte debemos fortalecer en la arquitectura. \u00bfVolumen, variedad, velocidad? Definir la arquitectura: \u00bfbatch o streaming? \u00bfRealmente es necesario que nuestra arquitectura soporte streaming? Evaluar las fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que se utilizan?","title":"Buenas pr\u00e1cticas"},{"location":"apuntes/arquitecturas01.html#arquitectura-en-la-nube","text":"Dentro de las diferentes plataformas cloud, a la hora de dise\u00f1ar e implementar una aplicaci\u00f3n, podemos tener en mente, desde un inicio, que se va a desplegar en la nube, haciendo uso de la gran mayor\u00eda de servicios que hemos estudiado en las sesiones anteriores. De forma complementaria, estas plataformas cloud ofrecen un marco buenas pr\u00e1cticas, principios y decisiones que hemos de tomar a la hora de dise\u00f1ar nuestros sistemas, sean big data o no.","title":"Arquitectura en la nube"},{"location":"apuntes/arquitecturas01.html#marco-de-buena-arquitectura-waf","text":"Pilares del Marco de Buena Arquitectura (WAF) El marco de buena arquitectura ( AWS Well-Architected Framework / Azure Well-Architected Framework ) es una gu\u00eda dise\u00f1ada para ayudar a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para las aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas que facilitan la evaluaci\u00f3n e implementaci\u00f3n de nuestras arquitecturas en la nube. Se organiza en cinco pilares que estudiaremos a continuaci\u00f3n: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costes.","title":"Marco de buena arquitectura (WAF)"},{"location":"apuntes/arquitecturas01.html#aws-trusted-advisor","text":"AWS dispone de la herramienta en l\u00ednea AWS Trusted Advisor que ofrece asesoramiento en tiempo real con las pr\u00e1cticas recomendadas de AWS. Examina todo el entorno AWS y ofrece recomendaciones en cinco categor\u00edas: AWS Trusted Advisor Optimizaci\u00f3n de costes: sugiere recursos no utilizados e inactivos, o bien, posibilidad de realiza una reserva de capacidad. Rendimiento: comprueba los l\u00edmites del servicio y monitoriza para detectar instancias que se est\u00e9n utilizando por encima de su capacidad. Seguridad: examina los permisos para mejorar el nivel de seguridad de la aplicaci\u00f3n. Tolerancia a errores: revisa las capacidades de escalado autom\u00e1tico, las comprobaciones de estado, la implementaci\u00f3n Multi-AZ y las capacidades de copia de seguridad. L\u00edmites del servicio: realiza verificaciones para detectar usos que superen el 80% del l\u00edmite del servicio. Azure Advisor y Azure Score Microsoft, del forma similar, ofrece un par herramientas que nos ayudan a optimizar las implementaciones, como son Azure Advisor , y dentro de ella Advisor Score que punt\u00faa las recomendaciones para con un simple vistazo poder priorizar las mejoras sugeridas.","title":"AWS Trusted Advisor"},{"location":"apuntes/arquitecturas01.html#gestion-del-escalado-y-la-monitorizacion","text":"","title":"Gesti\u00f3n del escalado y la monitorizaci\u00f3n"},{"location":"apuntes/arquitecturas01.html#monitorizacion","text":"Al operar en la nube, es importante llevar un seguimiento de las actividades, porque probablemente haya un coste asociado a cada una de ellas. AWS ayuda a monitorizar, registrar e informar sobre el uso de sus servicios proporcionando herramientas para hacerlo. As\u00ed pues, AWS ofrece los siguientes servicios relacionados con la monitorizaci\u00f3n: Amazon CloudTrail : Servicio que registra cada acci\u00f3n que se lleva a cabo en la cuenta de AWS por motivos de seguridad. Esto significa que CloudTrail registra cada vez que alguien carga datos, ejecuta un c\u00f3digo, crea una instancia de EC2 o realiza cualquier otra acci\u00f3n. Amazon Cloudwatch : Servicio de monitorizaci\u00f3n en tiempo real de los recursos de AWS y las aplicaciones que ejecutamos en AWS. CloudTrail registra actividades, mientras que CloudWatch las monitoriza. As\u00ed pues, CloudWatch vigila que los servicios cloud se ejecutan sin problema y ayuda a no utilizar ni m\u00e1s ni menos recursos de lo esperado, lo que es importante para el seguimiento del presupuesto. Permite: Recopilar y hacer un seguimiento de las m\u00e9tricas est\u00e1ndar y personalizadas Establecer alarmas para enviar notificaciones autom\u00e1ticas a SNS o efectuar acciones de AutoScaling EC2 en funci\u00f3n del valor de las m\u00e9tricas obtenidas. AWS Config : Servicio que permite analizar, auditar y evaluar las configuraciones de los recursos de AWS. Monitoriza y registra de manera continua las configuraciones de recursos de AWS y permite automatizar la evaluaci\u00f3n de las configuraciones registradas respecto a las deseadas. Amazon SNS ( Amazon Simple Notification Service ) : herramienta que permite enviar textos, correos electr\u00f3nicos y mensajes a otros servicios en la nube y enviar notificaciones al cliente de varias formas desde la nube.","title":"Monitorizaci\u00f3n"},{"location":"apuntes/arquitecturas01.html#ejemplo-cloudwatch","text":"En el siguiente ejemplo vamos a crear una alarma de Cloudwatch para enviar una notificaci\u00f3n cuando nuestra cuenta haya gastado una cierta cantidad de dinero. La alarma env\u00eda un mensaje a Amazon SNS para posteriormente enviar un correo electr\u00f3nico. El primer paso es crear y subscribirse a un tema ( topic ) SNS. Un tema act\u00faa como un canal de comunicaci\u00f3n donde se recibes los mensajes de las alertas y eventos. As\u00ed pues, entramos al servicio SNS ( Simple Notification Service ) y creamos un tema al que llamaremos AlertaSaldo . Cloudwatch - Creaci\u00f3n del tema A continuaci\u00f3n, vamos a crear una suscripci\u00f3n a ese tema para que cuando se recibe una mensaje, lo redirijamos a nuestro tel\u00e9fono o correo electr\u00f3nico. Para ello, dentro de la secci\u00f3n de suscripciones, crearemos una suscripci\u00f3n. En el ARN pondremos el tema AlertaSaldo que acabamos de crear, y en el protocolo, vamos a seleccionar Correo electr\u00f3nico . Finalmente, en el punto de enlace, definimos el email que recibir\u00e1 la alerta. En este momento, Amazon enviar\u00e1 un email a la cuenta que hayamos indicado para confirmar los datos. Cloudwatch - Creaci\u00f3n de la subscripci\u00f3n El siguiente paso es crear la alarma en Cloudwatch . Para ello, una vez dentro de Cloudwatch , dentro de la opci\u00f3n de Alarmas, al crear una nueva, tendremos que elegir la m\u00e9trica, que en nuestro caso seleccionaremos Facturaci\u00f3n -> Cargo total estimado . En la siguiente pantalla, en la secci\u00f3n de Condiciones ..... est\u00e1tico, e indicamos la condici\u00f3n que queremos que se active cuando es superior a 100. Cloudwatch - Condiciones de la alarma En la secci\u00f3n de Notificaci\u00f3n , tras elegir en modo alarma seleccionamos el tema SNS existente (en nuestro caso AlertaSaldo ). Cloudwatch - Notificaciones de la alarma Finalmente, le asignamos el nombre de AlertaSaldoAlarma y tras ver un resumen de todo los configurado, creamos la alarma. De esta manera, cuando se supere el gasto de 100$, autom\u00e1ticamente nos enviar\u00e1 un email a la direcci\u00f3n que le hemos configurado.","title":"Ejemplo Cloudwatch"},{"location":"apuntes/arquitecturas01.html#escalado-y-balanceo-de-carga","text":"Elastic Load Balancing (ELB) distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2. Adem\u00e1s, mejora la tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Admite tres tipos de balanceadores de carga: balanceador de carga de aplicaciones : balanceo basado en aplicaciones con tr\u00e1fico HTTP y HTTPS, ofreciendo enrutamiento avanzado. Opera en la capa de aplicaci\u00f3n. balanceador de carga de red : para rendimiento ultra (millones de peticiones por segundo), opera a nivel de la capa de conexi\u00f3n (protocolos TCP, UDP, TLS) balanceador de carga gateway , para aplicaciones de terceros que soportan el protocolo Geneve . Opera a nivel de capa de red. Un servicio complementario es AWS Auto Scaling , el cual permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Podemos utilizar Auto Scaling para asegurarnos que se ejecutan la cantidad deseada de instancias EC2, agregando o eliminando instancias de forma autom\u00e1tica seg\u00fan las cargas de trabajo. Mediante Auto Scaling , tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costes. Otro caso de uso es en las aplicaciones con patrones de demanda estables (escalado predictivo) o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana. Para ello, se crea un grupo Auto Scaling , el cual es una colecci\u00f3n de instancias EC2, indicando la cantidad m\u00ednima y m\u00e1xima de instancias a desplegar. Si queremos tener un escalado din\u00e1mico podemos usar conjuntamente EC2 AutoScaling , Amazon CloudWatch y Elastic Load Balancing .","title":"Escalado y Balanceo de carga"},{"location":"apuntes/arquitecturas01.html#ejemplos-escalado","text":"Para este ejemplo, vamos a crear dos instancias EC2 que est\u00e9n en la misma VPC, y vamos a utilizar un balanceador de carga para que tras cada petici\u00f3n, responda una instancia diferente. El primer paso ser\u00e1 crear un grupo de seguridad que van a compartir nuestras instancias. En nuestro caso, lo hemos llamado Servidor Web y la \u00fanica regla de entrada que tiene permite todo el tr\u00e1fico HTTP desde cualquier IP. Otra posibilidad es utilizar el grupo de seguridad iabd-front que creamos en sesiones anteriores. A continuaci\u00f3n, vamos a crear y lanzar la primera instancia. Para ello, en EC2 creamos una instancia del tipo que queramos, con el AMI de Amazon y en el campo de Datos de usuario , vamos a indicarle la siguiente informaci\u00f3n para que inicie un servidor web con una p\u00e1gina est\u00e1tica: #!/bin/bash yum update -y yum -y install httpd systemctl enable httpd systemctl start httpd echo '<html><h1>Hola Severo! Este es el servidor 1.</h1></html>' > /var/www/html/index.html A continuaci\u00f3n le a\u00f1adimos una etiqueta Nombre con el valor Servidor Web 1 y seleccionamos el grupo de seguridad que hab\u00edamos creado previamente. Finalmente, la lanzamos (ya sea con nuestro par de claves o sin indicar ninguno). Ahora deber\u00edamos poder acceder tanto a la IP p\u00fablica como al DNS con la informaci\u00f3n que tengamos de la instancia. Ten en cuenta que al pegar la direcci\u00f3n en el navegador, est\u00e1s accediendo por el protocolo HTTP y no HTTPS. El siguiente paso es repetir los mismos pasos, creando una nueva instancia pero cambiando tanto la etiqueta como el script de Datos de usuario para que muestre el n\u00famero 2. Antes de crear el balanceador, debemos comprobar las zonas de disponibilidad de nuestras instancias. En mi caso, ambas est\u00e1n en us-east-1d . As\u00ed pues, el siguiente paso es crear el balanceador de carga. Para ello, desde el panel lateral, seleccionamos la opci\u00f3n de Balanceadores de carga y pulsamos sobre Crear balanceador de carga de tipo aplicaci\u00f3n, y tras introducir el nombre indicamos las zonas de disponibilidad de nuestras instancias y configuraremos el mismo grupo de seguridad que hemos definido antes para tener abiertas las conexiones HTTP. M\u00e1s abajo, en la secci\u00f3n de Listener and routing , vamos a crear un grupo de destino, pulsando sobre el enlace de create target group del listener del protocolo HTTP del puerto 80. Se nos abrir\u00e1 una nueva pesta\u00f1a, donde tras indicarle el tipo (en nuestro caso b\u00e1sico) y asignarle un nombre al grupo de destino nuevo, en el check de salud, indicaremos el recurso /index.html . En la siguiente pantalla seleccionamos las dos instancias de servidor web: Elastic Load Balancing - Grupos de destino Una vez creado, volvemos a la pesta\u00f1a anterior, y ya podemos configurar en el listener el grupo de destino reci\u00e9n creado: Elastic Load Balancing - Listener Solo nos queda finalizar la creaci\u00f3n, y tras un par de minutos, cuando en estado aparezca Activo , podemos copiar el DNS en otra pesta\u00f1a del navegador, y acceder varias veces para comprobar como cada vez responde un servidor web diferente.","title":"Ejemplos Escalado"},{"location":"apuntes/arquitecturas01.html#actividades","text":"Completa el cuestionario que tienes en Aules sobre las arquitecturas Lamba y Kappa. Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS . (opcional) Realiza el ejemplo de Cloudwatch que tienes m\u00e1s arriba pero con una alerta al superar uno o dos euros (depende de tu saldo actual) y adjunta una captura de pantalla tanto de la alarma creada, como del email recibido por parte de AWS.","title":"Actividades"},{"location":"apuntes/arquitecturas01.html#referencias","text":"Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa Data processing architectures \u2013 Lambda and Kappa Laboratorios de Amazon sobre AWF","title":"Referencias"},{"location":"apuntes/bdaplicado01hadoop.html","text":"Hadoop \u00b6 Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). La filosof\u00eda de Hadoop es almacenar todos los datos en un lugar y procesar los datos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos/ workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Cada vez que a\u00f1adimos un nuevo nodo esclavo, aumentamos tanto la capacidad como el rendimiento de nuestro sistema. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2. Componentes y Ecosistema \u00b6 El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop, como ficheros de logs, bloques de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft HDFS \u00b6 Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google explicando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos. Bloques \u00b6 Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS Namenode \u00b6 Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento. Datanode \u00b6 De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS MapReduce \u00b6 Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos al final del curso), que mejora el rendimiento por una orden de magnitud. YARN \u00b6 Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados. Componentes en YARN Resource Manager \u00b6 El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles. Node Manager \u00b6 Contenedores en NodeManager Arranca el Application Masters tras petici\u00f3n del Resource Manager , iniciando las tareas/jobs que le indique el Application Master . Gestiona los trabajos en contenedores proporcionado los recursos computacionales necesarios para las aplicaciones. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Tambi\u00e9n implementa heartbeats para mantener informado del estado al Resource Manager . Finalmente, almacena los logs de aplicaci\u00f3n en HDFS. Application Master \u00b6 El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ). Instalaci\u00f3n \u00b6 Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos. Configuraci\u00f3n \u00b6 Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> Puesta en marcha \u00b6 Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN Hola Mundo \u00b6 El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -01-15 12 :59:49,015 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -01-15 12 :59:49,844 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1642247847632_0001 2022 -01-15 12 :59:51,042 INFO input.FileInputFormat: Total input files to process : 1 2022 -01-15 12 :59:51,669 INFO mapreduce.JobSubmitter: number of splits:1 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642247847632_0001 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -01-15 12 :59:52,351 INFO conf.Configuration: resource-types.xml not found 2022 -01-15 12 :59:52,355 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -01-15 12 :59:53,142 INFO impl.YarnClientImpl: Submitted application application_1642247847632_0001 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1642247847632_0001/ 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: Running job: job_1642247847632_0001 2022 -01-15 13 :00:08,894 INFO mapreduce.Job: Job job_1642247847632_0001 running in uber mode : false 2022 -01-15 13 :00:08,932 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -01-15 13 :00:32,985 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -01-15 13 :00:47,344 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -01-15 13 :00:48,373 INFO mapreduce.Job: Job job_1642247847632_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -01-15 13 :00:48,679 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241519 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 20353 Total time spent by all reduces in occupied slots ( ms )= 12093 Total time spent by all map tasks ( ms )= 20353 Total time spent by all reduce tasks ( ms )= 12093 Total vcore-milliseconds taken by all map tasks = 20353 Total vcore-milliseconds taken by all reduce tasks = 12093 Total megabyte-milliseconds taken by all map tasks = 20841472 Total megabyte-milliseconds taken by all reduce tasks = 12383232 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 394 CPU time spent ( ms )= 7470 Physical memory ( bytes ) snapshot = 384565248 Virtual memory ( bytes ) snapshot = 5007564800 Total committed heap usage ( bytes )= 295571456 Peak Map Physical memory ( bytes )= 247332864 Peak Map Virtual memory ( bytes )= 2500415488 Peak Reduce Physical memory ( bytes )= 137232384 Peak Reduce Virtual memory ( bytes )= 2507149312 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/quijote/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el j ob de MapReduce* se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC MapReduce en Python \u00b6 El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python . Mapper \u00b6 Primero creamos el mapeador , el cual se encarga de parsear l\u00ednea a l\u00ednea el fragmento de documento que reciba, y va a generar una nueva salida con todas las palabras de manera que cada nueva l\u00ednea la compongan una tupla formada por la palabra, un tabulador y el n\u00famero 1 (hay una ocurrencia de dicha palabra) mapper.py #!/usr/bin/python3 import sys for linea in sys . stdin : # eliminamos los espacios de delante y de detr\u00e1s linea = linea . strip () # dividimos la l\u00ednea en palabras palabras = linea . split () # creamos tuplas de (palabra, 1) for palabra in palabras : print ( palabra , \" \\t 1\" ) Si queremos probar el mapper, podr\u00edamos ejecutar el siguiente comando: cat el_quijote.txt | python3 mapper.py Obteniendo un resultado similar a: ... gritos 1 al 1 cielo 1 alli\u0301 1 se 1 renovaron 1 las 1 maldiciones 1 ... Reducer \u00b6 A continuaci\u00f3n, en el reducer , vamos a recibir la salida del mapper y parseamos la cadena para separar la palabra del contador. Para llevar la cuenta de las palabras, vamos a meterlas dentro de un diccionario para incrementar las ocurrencias encontradas. Cuidado con la memoria En un caso real, hemos de evitar almacenar todos los datos que recibimos en memoria, ya que es posible que al trabajar con big data no quepa en la RAM de cada datanode . Para ello, se recomienda el uso de la librer\u00eda itertools , por ejemplo, utilizando la funci\u00f3n groupby() . Finalmente, volvemos a crear tuplas de palabra, tabulador y cantidad de ocurrencias. reducer.py #!/usr/bin/python3 import sys # inicializamos el diccionario dictPalabras = {} for linea in sys . stdin : # quitamos espacios de sobra linea = linea . strip () # parseamos la entrada de mapper.py palabra , cuenta = linea . split ( ' \\t ' , 1 ) # convertimos cuenta de string a int try : cuenta = int ( cuenta ) except ValueError : # cuenta no era un numero, descartamos la linea continue try : dictPalabras [ palabra ] += cuenta except : dictPalabras [ palabra ] = cuenta for palabra in dictPalabras . keys (): print ( palabra , \" \\t \" , dictPalabras [ palabra ]) Para probar el proceso completo, ejecutaremos el siguiente comando: cat el_quijote.txt | python3 mapper.py | python3 reducer.py > salida.tsv Si abrimos el fichero, podemos ver el resultado: salida.tsv don 1072 quijote 812 de 9035 la 5014 mancha 50 miguel 3 cervantes 3 ... Hadoop Streaming \u00b6 Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende, codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming envia los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <parametros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-r-00000 Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint . Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Realizar el ejemplo de MapReduce con el fichero de El Quijote utilizando el proceso que ofrece Hadoop . Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabr\u00e1s de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. (opcional) Desarrolla el ejemplo del art\u00edculo Creaci\u00f3n y ejecuci\u00f3n de un programa Python para Hadoop Map Reduce en Linux . Adjunta los scripts, el fichero de datos, y el fichero de resultado. Versi\u00f3n de Python En el art\u00edculo, en el encabezado del mapper y del reducer , utilizan como interprete de Python la ruta #!/usr/bin/python cuando en nuestra m\u00e1quina virtual ser\u00eda #!/usr/bin/python3","title":"1.- Hadoop"},{"location":"apuntes/bdaplicado01hadoop.html#hadoop","text":"Logo de Apache Hadoop Si Big Data es la filosof\u00eda de trabajo para grandes vol\u00famenes de datos, Apache Hadoop ( http://hadoop.apache.org/ ) es la tecnolog\u00eda catalizadora. Hadoop puede escalar hasta miles de ordenadores creando un cl\u00faster con un almacenamiento del orden de petabytes de informaci\u00f3n. M\u00e1s que un producto, es un proyecto open source que aglutina una serie de herramientas para el procesamiento distribuido de grandes conjuntos de datos a trav\u00e9s de cl\u00fasters de ordenadores utilizando modelos de programaci\u00f3n sencillos. Sus caracter\u00edsticas son: Confiable: crea m\u00faltiples copias de los datos de manera autom\u00e1tica y, en caso de fallo, vuelve a desplegar la l\u00f3gica de procesamiento. Tolerante a fallos: tras detectar un fallo aplica una recuperaci\u00f3n autom\u00e1tica. Cuando un componente se recupera, vuelve a formar parte del cl\u00faster. En Hadoop los fallos de hardware se tratan como una regla, no como una excepci\u00f3n. Escalable: los datos y su procesamiento se distribuyen sobre un cl\u00faster de ordenadores (escalado horizontal), desde un \u00fanico servidor a miles de m\u00e1quinas, cada uno ofreciendo computaci\u00f3n y almacenamiento local. Portable: se puede instalar en todo tipos de hardware y sistemas operativos. Hadoop est\u00e1 dise\u00f1ado para ejecutar sistemas de procesamiento en el mismo cl\u00faster que almacena los datos ( data local computing ). La filosof\u00eda de Hadoop es almacenar todos los datos en un lugar y procesar los datos en el mismo lugar, esto es, mover el procesamiento al almac\u00e9n de datos y no mover los datos al sistema de procesamiento. Esto lo logra mediante un entorno distribuido de datos y procesos. El procesamiento se realiza en paralelo a trav\u00e9s de nodos de datos en un sistema de ficheros distribuidos (HDFS), donde se distingue entre: Nodos maestros: encargados de los procesos de gesti\u00f3n global. Normalmente se necesitan 3. Su hardware tiene mayores requisitos. Nodos esclavos/ workers : tratan con los datos locales y los procesos de aplicaci\u00f3n. Su n\u00famero depender\u00e1 de las necesidad de nuestros sistemas, pero pueden estar comprendido entre 4 y 10.000. Su hardware es relativamente barato ( commodity hardware ) mediante servidores X86. Cada vez que a\u00f1adimos un nuevo nodo esclavo, aumentamos tanto la capacidad como el rendimiento de nuestro sistema. En la actualidad se ha impuesto Hadoop v3 (la \u00faltima versi\u00f3n a d\u00eda de hoy es la 3.3.1), aunque todav\u00eda existe mucho c\u00f3digo para Hadoop v2.","title":"Hadoop"},{"location":"apuntes/bdaplicado01hadoop.html#componentes-y-ecosistema","text":"El n\u00facleo se compone de: un conjunto de utilidades comunes ( Hadoop Common ) un sistema de ficheros distribuidos ( Hadoop Distributed File System \u2194 HDFS ). un gestor de recursos para el manejo del cl\u00faster y la planificaci\u00f3n de procesos ( YARN ) un sistema para procesamiento paralelo de grandes conjuntos de datos ( MapReduce ) Estos elementos permiten trabajar casi de la misma forma que si tuvi\u00e9ramos un sistema de fichero locales en nuestro ordenador personal, pero realmente los datos est\u00e1n repartidos entre miles de servidores. Las aplicaciones se desarrollan a alto nivel, sin tener constancia de las caracter\u00edsticas de la red. De esta manera, los cient\u00edficos de datos se centran en la anal\u00edtica y no en la programaci\u00f3n distribuida. Sobre este conjunto de herramientas existe un ecosistema \"infinito\" con tecnolog\u00edas que facilitan el acceso, gesti\u00f3n y extensi\u00f3n del propio Hadoop. Las m\u00e1s utilizadas son: Hive : Permite acceder a HDFS como si fuera una Base de datos, ejecutando comandos muy parecido a SQL para recuperar valores ( HiveSQL ). Simplifica enormemente el desarrollo y la gesti\u00f3n con Hadoop . HBase : Es el sistema de almacenamiento NoSQL basado en columnas para Hadoop . Es una base de datos de c\u00f3digo abierto, distribuida y escalable para el almacenamiento de Big Data. Escrita en Java, implementa y proporciona capacidades similares sobre Hadoop y HDFS. El objetivo de este proyecto es el de trabajar con grandes tablas, de miles de millones de filas de millones de columnas, sobre un cl\u00faster Hadoop . Pig : Lenguaje de alto de nivel para analizar grandes vol\u00famenes de datos. Trabaja en paralelo, lo que permite gestionar gran cantidad de informaci\u00f3n. Realmente es un compilador que genera comandos MapReduce, mediante el lenguaje textual denominado Pig Latin . Sqoop : Permite transferir un gran volumen de datos de manera eficiente entre Hadoop y gestores de datos estructurados. Flume : Servicio distribuido y altamente eficiente para distribuir, agregar y recolectar grandes cantidades de informaci\u00f3n. Es \u00fatil para cargar y mover informaci\u00f3n en Hadoop, como ficheros de logs, bloques de Twitter/Reddit, etc. Utiliza una arquitectura de tipo streaming con un flujo de datos muy potente y personalizables ZooKeeper : Servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas. No s\u00f3lo se utiliza en Hadoop, pero es muy \u00fatil en esa arquitectura, eliminando la complejidad de la gesti\u00f3n distribuida de la plataforma. Spark : Es un motor muy eficiente de procesamiento de datos a gran escala. Implementa procesamiento en tiempo real al contrario que MapReduce, lo que provoca que sea m\u00e1s r\u00e1pido. Para ello, en vez de almacenar los datos en disco, trabaja de forma masiva en memoria. Puede trabajar de forma aut\u00f3noma, sin necesidad de Hadoop. Ambari es una herramienta para instalar, configurar, mantener y monitorizar Hadoop. Si queremos empezar a utilizar Hadoop y todo su ecosistema, disponemos de diversas distribuciones con toda la arquitectura, herramientas y configuraci\u00f3n ya preparadas. Las m\u00e1s rese\u00f1ables son: Amazon Elastic MapReduce (EMR) de AWS. CDH de Cloudera Azure HDInsight de Microsoft","title":"Componentes y Ecosistema"},{"location":"apuntes/bdaplicado01hadoop.html#hdfs","text":"Es la capa de almacenamiento de Hadoop, y como tal, es un sistema de ficheros distribuido y tolerante a fallos que puede almacenar gran cantidad de datos, escalar de forma incremental y sobrevivir a fallos de hardware sin perder datos. Se basa en el paper que public\u00f3 Google explicando su Google File System en 2003. En un sistema que reparte los datos entre todos los nodos del cl\u00faster de Hadoop, dividiendo los ficheros en bloques (cada bloque por defecto es de 128MB) y almacenando copias duplicadas a trav\u00e9s de los nodos. Por defecto se replica en 3 nodos distintos (esto se conoce como el factor de replicaci\u00f3n ). HDFS asegura que se puedan a\u00f1adir servidores para incrementar el tama\u00f1o de almacenamiento de forma lineal, de manera que al introducir un nuevo nodo, se incrementa tanto la redundancia como la capacidad de almacenamiento. Est\u00e1 planteado para escribir los datos una vez y leerlos muchos veces ( WORM / Write Once, Read Many ). Las escrituras se pueden realizar a mano, o desde herramientas como Flume y Sqoop , que estudiaremos m\u00e1s adelante. No ofrece buen rendimiento para: Accesos de baja latencia. Realmente se utiliza para almacenar datos de entrada necesarios para procesos de computaci\u00f3n. Ficheros peque\u00f1os (a menos que se agrupen). Funciona mejor con grandes cantidades de ficheros grandes, es decir, mejor millones de ficheros de 100MB que billones de ficheros de 1MB. M\u00faltiples escritores. Modificaciones arbitrarias de ficheros. As\u00ed pues, los datos, una vez escritos en HDFS son immutables. Cada fichero de HDFS solo permite a\u00f1adir contenido ( append-only ). Una vez se ha creado y escrito en \u00e9l, solo podemos a\u00f1adir contenido o eliminarlo. Es decir, a priori, no podemos modificar los datos. HBase / Hive Tanto HBase como Hive ofrecen una capa por encima de HDFS para dar soporte a la modificaci\u00f3n de los datos, como en cualquier base de datos.","title":"HDFS"},{"location":"apuntes/bdaplicado01hadoop.html#bloques","text":"Un bloque es la cantidad m\u00ednima de datos que puede ser le\u00edda o escrita. El tama\u00f1o predeterminado de HDFS son 128 MB, ya que como hemos comentado, Hadoop est\u00e1 pensado para trabajar con ficheros de gran tama\u00f1o. Todos los ficheros est\u00e1n divididos en bloques. Esto quiere decir que si subimos un fichero de 600MB, lo dividir\u00e1 en 5 bloques de 128MB. Estos bloques se distribuyen por todos los nodos de datos del cl\u00faster de Hadoop. A partir del factor de replicaci\u00f3n , cada bloque se almacena varias veces en m\u00e1quinas distintas. El valor por defecto es 3. Por lo tanto, el archivo de 600MB que ten\u00edamos dividido en 5 bloques de 128MB, si lo replicamos tres veces, lo tendremos repartido en 15 bloques entre todos los nodos del cl\u00faster. Factor de replicaci\u00f3n HDFS Respecto a los permisos de lectura y escritura de los ficheros, sigue la misma filosof\u00eda de asignaci\u00f3n de usuarios y grupos que se realiza en los sistemas Posix . Es una buena pr\u00e1ctica crear una carpeta /user/ en el ra\u00edz de HDFS, de forma similar al /home/ de Linux. En HDFS se distinguen las siguientes m\u00e1quinas: Namenode : Act\u00faa como m\u00e1ster y almacena todos los metadatos necesarios para construir el sistema de ficheros a partir de sus bloques. Tiene control sobre d\u00f3nde est\u00e1n todos los bloques. Datanode : Son los esclavos, se limitan a almacenar los bloques que compone cada fichero. Secondary Namenode : Su funci\u00f3n principal es tomar puntos de control de los metadatos del sistema de archivos presentes en namenode. Arquitectura HDFS","title":"Bloques"},{"location":"apuntes/bdaplicado01hadoop.html#namenode","text":"Tal como hemos comentado, existen dos tipos de nodos. El principal se conoce como Namenode : Solo existe uno, y hace de servidor principal. Nodo al que se tienen que conectar los clientes para realizar las lecturas / escrituras. Mantiene el \u00e1rbol del sistema de archivos ( espacio de nombre ) y los metadatos para todos los ficheros y directorios en el \u00e1rbol, de manera que sabe en qu\u00e9 nodo del cl\u00faster est\u00e1 cada bloque de informaci\u00f3n ( mapa de bloques ) Los metadatos se almacenan tanto en memoria (para acelerar su uso) como en disco a la vez, por lo que es un nodo que requiere de mucha memoria RAM. Los bloques nunca pasan por el NameNode , se transfieren entre DataNodes y/o el cliente. Es decir, el Namenode no es responsable de almacenar o transferir los datos. Si se cae, no hay acceso a HDFS, por lo que es cr\u00edtico el mantenimiento de copias de seguridad. El segundo tipo es el Secondary Namenode : Su funci\u00f3n principal es guardar una copia de FsImage y EditLog : FsImage : instant\u00e1nea de los metadatos del sistema de archivos. EditLog : registro de transacciones que contiene los registros de cada cambio ( deltas ) que se produce en los metadatos del sistema de archivos. No se trata de un nodo de respaldo Por lo general se ejecuta en una m\u00e1quina distinta Adem\u00e1s de distribuir los bloques entre distintos nodos de datos, tambi\u00e9n los replica (con un factor de replicaci\u00f3n igual a tres, los replicar\u00eda en 3 nodos diferentes, 2 en el mismo rack y 1 en otro diferente) para evitar p\u00e9rdidas de informaci\u00f3n si alguno de los nodos falla. Cuando una aplicaci\u00f3n cliente necesita leer o modificar un bloque de datos, el Namenode le indica en qu\u00e9 nodo se localiza esa informaci\u00f3n. Tambi\u00e9n se asegura de que los nodos no est\u00e9n ca\u00eddos y que la informaci\u00f3n est\u00e9 replicada, para asegurar su disponibilidad a\u00fan en estos casos. Para hacernos una idea, independientemente del cloud, Facebook utiliza un cl\u00faster de 1100 m\u00e1quinas, con 8800 nodos y cerca de 12 PB de almacenamiento.","title":"Namenode"},{"location":"apuntes/bdaplicado01hadoop.html#datanode","text":"De este tipo de nodo habr\u00e1 m\u00e1s de uno en cada cl\u00faster. Por cada Namenode podemos tener miles de Datanodes Almacena y lee bloques de datos. Recuperado por Namenode clientes. Reportan al Namenode la lista de bloques que est\u00e1n almacenando. Pueden ir en distintos discos. Guarda un checksum del bloque. Relaci\u00f3n entre Namenodes y Datanodes HDFS","title":"Datanode"},{"location":"apuntes/bdaplicado01hadoop.html#mapreduce","text":"Se trata de un paradigma de programaci\u00f3n funcional en dos fases, la de mapeo y la de reducci\u00f3n, y define el algoritmo que utiliza Hadoop para paralelizar las tareas. Un algoritmo MapReduce divide los datos, los procesa en paralelo, los reordena, combina y agrega de vuelta los resultados mediante un formato clave/valor. Sin embargo, este algoritmo no casa bien con el an\u00e1lisis interactivo o programas iterativos, ya que persiste los datos en disco entre cada uno de los pasos del mismo, lo que con grandes datasets conlleva una penalizaci\u00f3n en el rendimiento. Un job de MapReduce se compone de m\u00faltiples tareas MapReduce , donde la salida de una tarea es la entrada de la siguiente. El siguiente gr\u00e1fico muestra un ejemplo de una empresa que fabrica juguetes de colores. Cuando un cliente compra un juguete desde la p\u00e1gina web, el pedido se almacena como un fichero en Hadoop con los colores de los juguetes adquiridos. Para averiguar cuantas unidades de cada color debe preparar la f\u00e1brica, se emplea un algoritmo MapReduce para contar los colores: Como sugiere el nombre, el proceso se divide principalmente en dos fases: Fase de mapeo ( Map ) \u2014 Los documentos se parten en pares de clave/valor. Hasta que no se reduzca, podemos tener muchos duplicados. Fase de reducci\u00f3n ( Reduce ) \u2014 Es en cierta medida similar a un \"group by\" de SQL. Las ocurrencias similares se agrupan, y dependiendo de la funci\u00f3n de reducci\u00f3n, se puede crear un resultado diferente. En nuestro ejemplo queremos contar los colores, y eso es lo que devuelve nuestra funci\u00f3n. Realmente, es un proceso m\u00e1s complicado: Lectura desde HDFS de los ficheros de entrada como pares clave/valor. Pasar cada l\u00ednea de forma separada al mapeador, teniendo tantos mapeadores como bloques de datos tengamos. El mapeador parsea los colores (claves) de cada fichero y produce un nuevo fichero para cada color con el n\u00famero de ocurrencias encontradas (valor), es decir, mapea una clave (color) con un valor (n\u00famero de ocurrencias). Para facilitar la agregaci\u00f3n, se ordenan y/o barajan los datos a partir de la clave. La fase de reducci\u00f3n suma las ocurrencias de cada color y genera un fichero por clave con el total de cada color. Las claves se unen en un \u00fanico fichero de salida que se persiste en HDFS. No es oro todo lo que reluce Hadoop facilita el trabajo con grandes vol\u00famenes de datos, pero montar un cl\u00faster funcional no es una cosa trivial. Existen gestores de cl\u00fasters que hacen las cosas un poco menos inc\u00f3modas (como son Apache Ambari o Apache Mesos ), aunque la tendencia es utilizar una soluci\u00f3n cloud que nos evita toda la instalaci\u00f3n y configuraci\u00f3n. Tal como comentamos al inicio, uno de los puntos d\u00e9biles de Hadoop es el trabajo con algoritmos iterativos, los cuales son fundamentales en la parte de IA. La soluci\u00f3n es el uso de Spark (que estudiaremos al final del curso), que mejora el rendimiento por una orden de magnitud.","title":"MapReduce"},{"location":"apuntes/bdaplicado01hadoop.html#yarn","text":"Yet Another Resource Negotiator es un distribuidor de datos y gestor de recursos distribuidos. Forma parte de Hadoop desde la versi\u00f3n 2, y abstrae la gesti\u00f3n de recursos de los procesos MapReduce lo que implica una asignaci\u00f3n de recursos m\u00e1s efectiva. YARN soporta varios frameworks de procesamiento distribuido, como MapReduce v2 , Tez , Impala , Spark , etc.. YARN y Hadoop El objetivo principal de YARN es separar en dos servicios las funcionalidades de gesti\u00f3n de recursos de la monitorizaci\u00f3n/planificaci\u00f3n de tareas. Se divide en tres componentes principales: un Resource Manager , m\u00faltiples Node Manager y varios ApplicationMaster . La idea es tener un Resource Manager por cl\u00faster y un Application Master por aplicaci\u00f3n, considerando una aplicaci\u00f3n tanto un \u00fanico job como un conjunto de jobs c\u00edclicos. El Resource Manager y el Node Manager componen el framework de computaci\u00f3n de datos. En concreto, el ResourceManager controla el arranque de la aplicaci\u00f3n, siendo la autoridad que orquesta los recursos entre todas las aplicaciones del sistema. A su vez, tendremos tantos NodeManager como datanodes tenga nuestro cl\u00faster, siendo responsables de gestionar y monitorizar los recursos de cada nodo (CPU, memoria, disco y red) y reportar estos datos al Resource Manager . El Application Master es una librer\u00eda espec\u00edfica encargada de negociar los recursos con el ResourceManager y de trabajar con los Node Manager para ejecutar y monitorizar las tareas. Finalmente, en nuestro cl\u00faster, tendremos corriendo un Job History Server encargado de archivar los fichero de log de los jobs . Aunque es un proceso opcional, se recomienda su uso para monitorizar los jobs ejecutados. Componentes en YARN","title":"YARN"},{"location":"apuntes/bdaplicado01hadoop.html#resource-manager","text":"El gestor de recursos, a su vez, se divide en dos componentes: El Scheduler o planificador es el encargado de gestionar la distribuci\u00f3n de los recursos del cl\u00faster de YARN. Adem\u00e1s, las aplicaciones usan los recursos que el Resource Manager les ha proporcionado en funci\u00f3n de sus criterios de planificaci\u00f3n. Este planificador no monitoriza el estado de ninguna aplicaci\u00f3n ni les ofrece garant\u00edas de ejecuci\u00f3n, ni recuperaci\u00f3n por fallos de la aplicaci\u00f3n o el hardware, s\u00f3lo planifica. Este componente realiza su planificaci\u00f3n a partir de los requisitos de recursos necesarios por las aplicaciones (CPU, memoria, disco y red). Applications Manager : es el componente del Resource Manager responsable de aceptar las peticiones de trabajos, negociar el contenedor con los recursos necesarios en el que ejecutar la Application Master y proporcionar reinicios de los trabajos en caso de que fuera necesario debido a errores. El Resource Manager mantiene un listado de los Node Manager activos y de sus recursos disponibles.","title":"Resource Manager"},{"location":"apuntes/bdaplicado01hadoop.html#node-manager","text":"Contenedores en NodeManager Arranca el Application Masters tras petici\u00f3n del Resource Manager , iniciando las tareas/jobs que le indique el Application Master . Gestiona los trabajos en contenedores proporcionado los recursos computacionales necesarios para las aplicaciones. Los contenedores YARN tienen una asignaci\u00f3n de recursos (CPU, memoria, disco y red) fija de un host del cl\u00faster y el Node Manager es el encargado de monitorizar esta asignaci\u00f3n. Si un proceso sobrepasase los recursos asignados, por ejemplo, ser\u00eda el encargado de detenerlo. Adem\u00e1s, mapean las variables de entorno necesarias, las dependencias y los servicios necesarios para crear los procesos. Tambi\u00e9n implementa heartbeats para mantener informado del estado al Resource Manager . Finalmente, almacena los logs de aplicaci\u00f3n en HDFS.","title":"Node Manager"},{"location":"apuntes/bdaplicado01hadoop.html#application-master","text":"El Application Master es el responsable de negociar los recursos apropiados con el Resource Manager y monitorizar su estado y su progreso. Tambi\u00e9n coordina la ejecuci\u00f3n de todas las tareas en las que puede dividirse su aplicaci\u00f3n. Podemos ver la secuencia de trabajo y colaboraci\u00f3n de estos componentes en el siguiente gr\u00e1fico: Secuencia de trabajo YARN El cliente env\u00eda una aplicaci\u00f3n YARN. Resource Manager reserva los recursos en un contenedor para su ejecuci\u00f3n. El Application Manager se registra con el Resource Manager y pide los recursos necesarios. El Application Manager notifica al Node Manager la ejecuci\u00f3n de los contenedores. Se ejecuta la aplicaci\u00f3n YARN en el/los contenedor/es correspondiente. El Application Master monitoriza la ejecuci\u00f3n y reporta el estado al Resource Manager y al Application Manager . Al terminar la ejecuci\u00f3n, el Application Manager lo notifica al Resource Manager . YARN soporta la reserva de recursos mediante el Reservation System , un componente que permite a los usuarios especificar un perfil de recurso y restricciones temporales ( deadlines ) y posteriormente reservar recursos para asegurar la ejecuci\u00f3n predecibles de las tareas importantes. Este sistema registra los recursos a lo largo del tiempo, realiza control de admisi\u00f3n para las reservas, e informa din\u00e1micamente al planificador para asegurarse que se produce la reserva. Para conseguir una alta escalabilidad (del orden de miles de nodos), YARN ofrece el concepto de Federaci\u00f3n . Esta funcionalidad permite conectar varios cl\u00fasteres YARN y hacerlos visibles como un cl\u00faster \u00fanico. De esta forma puede ejecutar trabajos muy pesados y distribuidos. Hadoop v1 MapReduce en hadoop-2.x mantiene la compatibilidad del API con versiones previas (hadoop-1.x). De esta manera, todo los jobs de MapReduce funcionan perfectamente con YARN s\u00f3lo recompilando el c\u00f3digo. En Hadoop v1 los componentes encargados de realizar el procesamiento eran el JobTracker (situado en el namenode ) y los TaskTracker (situados en los datanodes ).","title":"Application Master"},{"location":"apuntes/bdaplicado01hadoop.html#instalacion","text":"Para trabajar en esta y las siguientes sesiones, vamos a utilizar la m\u00e1quina virtual que tenemos compartida en Aules . A partir de la OVA de VirtualBox, podr\u00e1s entrar con el usuario iabd y la contrase\u00f1a iabd . Si quieres instalar el software del curso, se recomienda crear una m\u00e1quina virtual con cualquier distribuci\u00f3n Linux. En mi caso, yo lo he probado en la versi\u00f3n Lubuntu 20.04 LTS y la versi\u00f3n 3.3.1 de Hadoop . Puedes seguir las instrucciones del art\u00edculo C\u00f3mo instalar y configurar Hadoop en Ubuntu 20.04 LTS . Para trabajar en local tenemos montada una soluci\u00f3n que se conoce como pseudo-distribuida , porque es al mismo tiempo maestro y esclavo. En el mundo real o si utilizamos una soluci\u00f3n cloud tendremos un nodo maestro y m\u00faltiples nodos esclavos.","title":"Instalaci\u00f3n"},{"location":"apuntes/bdaplicado01hadoop.html#configuracion","text":"Los archivos que vamos a revisar a continuaci\u00f3n se encuentran dentro de la carpeta $HADOOP_HOME/etc/hadoop . El archivo que contiene la configuraci\u00f3n general del cl\u00faster es el archivo core-site.xml . En \u00e9l se configura cual ser\u00e1 el sistema de ficheros, que normalmente ser\u00e1 hdfs , indicando el dominio del nodo que ser\u00e1 el maestro de datos ( namenode ) de la arquitectura. Por ejemplo, su contenido ser\u00e1 similar al siguiente: core-site.xml <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> </configuration> El siguiente paso es configurar el archivo hdfs-site.xml donde se indica tanto el factor de replicaci\u00f3n como la ruta donde se almacenan tanto los metadatos ( namenode ) como los datos en s\u00ed ( datanode ): hdfs-site.xml <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop-data/hdfs/namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop-data/hdfs/datanode </value> </property> </configuration> Recuerda Si tuvi\u00e9semos un cl\u00faster, en el nodo maestro s\u00f3lo configurar\u00edamos la ruta del namenode y en cada uno de los nodos esclavos, \u00fanicamente la ruta del datanode . Para configurar YARN, primero editaremos el archivo yarn-site.xml para indicar quien va a ser el nodo maestro, as\u00ed como el manejador y la gesti\u00f3n para hacer el MapReduce : yarn-site.xml <configuration> <property> <name> yarn.resourcemanager.hostname </name> <value> iabd-virtualbox </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.aux-services.mapreduce_shuffle.class </name> <value> org.apache.hadoop.mapred.ShuffleHandler </value> </property> </configuration> Y finalmente el archivo mapred-site.xml para indicar que utilice YARN como framework MapReduce : <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration>","title":"Configuraci\u00f3n"},{"location":"apuntes/bdaplicado01hadoop.html#puesta-en-marcha","text":"Arrancando HDFS Para arrancar Hadoop/HDFS, hemos de ejecutar el comando start-dfs.sh . Al finalizar, veremos que ha arrancado el namenode , los datanodes , y el secondary namenode . Si en cualquier momento queremos comprobar el estado de los servicios y procesos en ejecuci\u00f3n, tenemos el comando jps . Si accedemos a http://iabd-virtualbox:9870/ podremos visualizar su interfaz web. Interfaz Web de Hadoop Arrancando YARN Para arrancar YARN utilizaremos el comando start-yarn.sh para lanzar el Resource Manager y el Node Manager : Y a su vez, YARN tambi\u00e9n ofrece un interfaz web para obtener informaci\u00f3n relativa a los jobs ejecutados. Nos conectaremos con el nombre del nodo principal y el puerto 8088 . En nuestro caso lo hemos realizado a http://hadoop-virtualbox:8088 obteniendo la siguiente p\u00e1gina: Interfaz Web de YARN","title":"Puesta en marcha"},{"location":"apuntes/bdaplicado01hadoop.html#hola-mundo","text":"El primer ejemplo que se realiza como Hola Mundo en Hadoop suele ser una aplicaci\u00f3n que cuente las ocurrencias de cada palabra que aparece en un documento de texto. En nuestro caso, vamos a contar las palabras del libro de El Quijote , el cual podemos descargar desde https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 . Una vez arrancado Hadoop y YARN , vamos a colocar el libro dentro de HDFS (estos comandos los estudiaremos en profundidad en la siguiente sesi\u00f3n): hdfs dfs -put el_quijote.txt /user/iabd/ Hadoop tiene una serie de ejemplos ya implementados para demostrar el uso de MapReduce en la carpeta $HADOOP_HOME/share/hadoop/mapreduce . As\u00ed pues, podemos ejecutar el programa wordcount de la siguiente manera: hadoop jar $HADOOP_HOME /share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar \\ wordcount /user/iabd/el_quijote.txt /user/iabd/salidaWC Si nos fijamos en la salida del comando podremos ver una traza del proceso MapReduce : 2022 -01-15 12 :59:49,015 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2022 -01-15 12 :59:49,844 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1642247847632_0001 2022 -01-15 12 :59:51,042 INFO input.FileInputFormat: Total input files to process : 1 2022 -01-15 12 :59:51,669 INFO mapreduce.JobSubmitter: number of splits:1 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1642247847632_0001 2022 -01-15 12 :59:51,968 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2022 -01-15 12 :59:52,351 INFO conf.Configuration: resource-types.xml not found 2022 -01-15 12 :59:52,355 INFO resource.ResourceUtils: Unable to find 'resource-types.xml' . 2022 -01-15 12 :59:53,142 INFO impl.YarnClientImpl: Submitted application application_1642247847632_0001 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1642247847632_0001/ 2022 -01-15 12 :59:53,360 INFO mapreduce.Job: Running job: job_1642247847632_0001 2022 -01-15 13 :00:08,894 INFO mapreduce.Job: Job job_1642247847632_0001 running in uber mode : false 2022 -01-15 13 :00:08,932 INFO mapreduce.Job: map 0 % reduce 0 % 2022 -01-15 13 :00:32,985 INFO mapreduce.Job: map 100 % reduce 0 % 2022 -01-15 13 :00:47,344 INFO mapreduce.Job: map 100 % reduce 100 % 2022 -01-15 13 :00:48,373 INFO mapreduce.Job: Job job_1642247847632_0001 completed successfully Podemos observar como se crea un job que se env\u00eda a YARN, el cual ejecuta el proceso MapReduce , el cual tarda alrededor de 40 segundos. A continuaci\u00f3n aparecen estad\u00edsticas del proceso: 2022 -01-15 13 :00:48,679 INFO mapreduce.Job: Counters: 54 File System Counters FILE: Number of bytes read = 347063 FILE: Number of bytes written = 1241519 FILE: Number of read operations = 0 FILE: Number of large read operations = 0 FILE: Number of write operations = 0 HDFS: Number of bytes read = 1060376 HDFS: Number of bytes written = 257233 HDFS: Number of read operations = 8 HDFS: Number of large read operations = 0 HDFS: Number of write operations = 2 HDFS: Number of bytes read erasure-coded = 0 Job Counters Launched map tasks = 1 Launched reduce tasks = 1 Data-local map tasks = 1 Total time spent by all maps in occupied slots ( ms )= 20353 Total time spent by all reduces in occupied slots ( ms )= 12093 Total time spent by all map tasks ( ms )= 20353 Total time spent by all reduce tasks ( ms )= 12093 Total vcore-milliseconds taken by all map tasks = 20353 Total vcore-milliseconds taken by all reduce tasks = 12093 Total megabyte-milliseconds taken by all map tasks = 20841472 Total megabyte-milliseconds taken by all reduce tasks = 12383232 Map-Reduce Framework Map input records = 2186 Map output records = 187018 Map output bytes = 1808330 Map output materialized bytes = 347063 Input split bytes = 117 Combine input records = 187018 Combine output records = 22938 Reduce input groups = 22938 Reduce shuffle bytes = 347063 Reduce input records = 22938 Reduce output records = 22938 Spilled Records = 45876 Shuffled Maps = 1 Failed Shuffles = 0 Merged Map outputs = 1 GC time elapsed ( ms )= 394 CPU time spent ( ms )= 7470 Physical memory ( bytes ) snapshot = 384565248 Virtual memory ( bytes ) snapshot = 5007564800 Total committed heap usage ( bytes )= 295571456 Peak Map Physical memory ( bytes )= 247332864 Peak Map Virtual memory ( bytes )= 2500415488 Peak Reduce Physical memory ( bytes )= 137232384 Peak Reduce Virtual memory ( bytes )= 2507149312 Shuffle Errors BAD_ID = 0 CONNECTION = 0 IO_ERROR = 0 WRONG_LENGTH = 0 WRONG_MAP = 0 WRONG_REDUCE = 0 File Input Format Counters Bytes Read = 1060259 File Output Format Counters Bytes Written = 257233 Para poder obtener toda la informaci\u00f3n de un job necesitamos arrancar el Job History Server : mapred --daemon start historyserver De manera que si accedemos a la URL que se visualiza en el log, podremos ver de forma gr\u00e1fica la informaci\u00f3n obtenida: Resultado del History Server Si accedemos al interfaz gr\u00e1fico de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/quijote/salidaWC ), podremos ver c\u00f3mo se ha creado la carpeta salidaWC y dentro contiene dos archivos: _SUCCESS : indica que el j ob de MapReduce* se ha ejecutado correctamente part-r-00000 : bloque de datos con el resultado Contenido HDFS de salidaWC","title":"Hola Mundo"},{"location":"apuntes/bdaplicado01hadoop.html#mapreduce-en-python","text":"El API de MapReduce est\u00e1 escrito en Java , pero mediante Hadoop Streaming podemos utilizar MapReduce con cualquier lenguaje compatible con el sistema de tuber\u00edas Unix ( | ). Para entender c\u00f3mo funciona, vamos a reproducir el ejemplo anterior mediante Python .","title":"MapReduce en Python"},{"location":"apuntes/bdaplicado01hadoop.html#hadoop-streaming","text":"Una vez comprobados que los algoritmos de mapeo y reducci\u00f3n funcionan, vamos a procesarlos dentro de Hadoop para aprovechar la computaci\u00f3n distribuida. Para ello, haremos uso de Hadoop Streaming , el cual permite ejecutar jobs Map/Reduce con cualquier script (y por ende, codificados en cualquier lenguaje de programaci\u00f3n) que pueda leer de la entrada est\u00e1ndar ( stdin ) y escribir a la salida est\u00e1ndar ( stdout ). De este manera, Hadoop Streaming envia los datos en crudo al mapper v\u00eda stdin y tras procesarlos, se los pasa al reducer v\u00eda stdout . La sintaxis para ejecutar los jobs es: mapred streaming \\ -input miCarpetaEntradaHDFS \\ -output miCarpetaSalidaHDFS \\ -mapper scriptMapper \\ -reducer scriptReducer Versiones 1.x En versiones m\u00e1s antiguas de Hadoop, en vez de utilizar el comando mapred , se utiliza el comando hadoop jar rutaDeHadoopStreaming.jar <parametros> , siendo normalmente la ruta del jar $HADOOP_HOME/share/hadoop/tools/lib . As\u00ed pues, en nuestro caso ejecutar\u00edamos el siguiente comando si tuvi\u00e9semos los archivos (tanto los datos como los scripts) dentro de HDFS: mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py \\ -reducer reducer.py Permisos de ejecuci\u00f3n Recuerda darle permisos de ejecuci\u00f3n a ambos scripts ( chmod u+x mapper.py y chmod u+x reducer.py ) para que Hadoop Streaming los pueda ejecutar Como queremos usar los archivos que tenemos en local, debemos indicar cada uno de los elementos mediante el par\u00e1metro -file : mapred streaming \\ -input el_quijote.txt \\ -output salidaPy \\ -mapper mapper.py -file mapper.py \\ -reducer reducer.py -file reducer.py Una vez finalizado el job , podemos comprobar c\u00f3mo se han generado el resultado en HDFS mediante: hdfs dfs -head /user/iabd/salidaPy/part-r-00000","title":"Hadoop Streaming"},{"location":"apuntes/bdaplicado01hadoop.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly Art\u00edculo de Hadoop por dentro . Tutorial de Hadoop de Tutorialspoint .","title":"Referencias"},{"location":"apuntes/bdaplicado01hadoop.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Realizar el ejemplo de MapReduce con el fichero de El Quijote utilizando el proceso que ofrece Hadoop . Vuelve a contar las palabras que tiene El Quijote , pero haciendo usos de los scripts Python , teniendo en cuenta que el proceso de mapeo va a limpiar las palabr\u00e1s de signos ortogr\u00e1ficos (quitar puntos, comas, par\u00e9ntesis) y en el reducer vamos a considerar que las palabras en may\u00fasculas y min\u00fasculas son la misma palabra. Tip : para la limpieza, puedes utilizar el m\u00e9todo de string translate de manera que elimine las string.punctuation . Entra en Hadoop UI y en YARN , y visualiza los procesos que se han ejecutado en las actividades 1 y 2. (opcional) Desarrolla el ejemplo del art\u00edculo Creaci\u00f3n y ejecuci\u00f3n de un programa Python para Hadoop Map Reduce en Linux . Adjunta los scripts, el fichero de datos, y el fichero de resultado. Versi\u00f3n de Python En el art\u00edculo, en el encabezado del mapper y del reducer , utilizan como interprete de Python la ruta #!/usr/bin/python cuando en nuestra m\u00e1quina virtual ser\u00eda #!/usr/bin/python3","title":"Actividades"},{"location":"apuntes/bdaplicado02hdfs.html","text":"HDFS \u00b6 Funcionamiento de HDFS \u00b6 En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet . Procesos de lectura \u00b6 Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster. Proceso de escritura \u00b6 El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura). HDFS por dentro \u00b6 HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log . Trabajando con HDFS \u00b6 Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp Bloques \u00b6 A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451 Administraci\u00f3n \u00b6 Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos. Snapshots \u00b6 Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos HDFS UI \u00b6 En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd . HDFS y Python \u00b6 Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas . Formatos de datos \u00b6 En el primer bloque ya vimos una peque\u00f1a introducci\u00f3n a los diferentes formatos de datos . Las propiedades que ha de tener un formato de datos son: independiente del lenguaje expresivo, con soporte para estructuras complejas y anidadas eficiente, r\u00e1pido y reducido din\u00e1mico, de manera que los programas puedan procesar y definir nuevos tipos de datos. formato de fichero standalone y que permita dividirlo y comprimirlo. Para que Hadoop pueda procesar documento, es imprescindible que el formato del fichero permita su divisi\u00f3n en fragmentos ( splittable in chunks ). Si los clasificamos respecto al formato de almacenamiento tenemos: texto (m\u00e1s lentos, ocupan m\u00e1s pero son m\u00e1s expresivos y permiten su interoperabilidad): CSV, XML, JSON, etc... binarios (mejor rendimiento, ocupan menos, menos expresivos): Avro, Parquet, ORC, etc... Si comparamos los formatos m\u00e1s empleados a partir de las propiedades descritas tenemos: Caracter\u00edstica CSV XML / JSON SequenceFile Avro Independencia del lenguaje Expresivo Eficiente Din\u00e1mico Standalone Dividible Las ventajas de elegir el formato correcto son: Mayor rendimiento en la lectura y/o escritura Ficheros trozeables ( splittables ) Soporte para esquemas que evolucionan Soporte para compresi\u00f3n de los datos (por ejemplo, mediante Snappy ). Filas vs Columnas \u00b6 Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnas los datos del mismo tipo se agrupan, lo que mejor el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV, con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet, lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes. Avro \u00b6 Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Avro se basa en esquemas, los cuales se realizan mediante JSON para definir los tipos de datos y protocolos. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos. Schema \u00b6 Los esquemas se componen de tipos primitivos ( null , boolean , int , long , float , double , bytes , y string ) y compuestos ( record , enum , array , map , union , y fixed ). Un ejemplo de esquema podr\u00eda ser: empleado.avsc { \"type\" : \"record\" , \"namespace\" : \"SeveroOchoa\" , \"name\" : \"Empleado\" , \"fields\" : [ { \"name\" : \"Nombre\" , \"type\" : \"string\" }, { \"name\" : \"Altura\" , \"type\" : \"float\" } { \"name\" : \"Edad\" , \"type\" : \"int\" } ] } Python \u00b6 Para poder serializar y deserializar documentos Avro mediante Python , previamente debemos instalar la librer\u00eda avro : pip install avro-python3 # o si utilizamos Anaconda conda install -c conda-forge avro-python3 Vamos a realizar un ejemplo donde primero leemos un esquema de un archivo Avro , y con dicho esquema, escribiremos nuevos datos en un fichero. A continuaci\u00f3n, abrimos el fichero escrito y leemos y mostramos los datos: C\u00f3digo Python Resultado import avro import copy import json from avro.datafile import DataFileReader , DataFileWriter from avro.io import DatumReader , DatumWriter # abrimos el fichero en modo binario y leemos el esquema schema = avro . schema . parse ( open ( \"empleado.avsc\" , \"rb\" ) . read ()) # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleados.avro' , 'wb' ) as f : writer = DataFileWriter ( f , DatumWriter (), schema ) writer . append ({ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }) writer . append ({ \"nombre\" : \"Juan\" , \"altura\" : 175 }) writer . close () # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleados.avro\" , \"rb\" ) as f : reader = DataFileReader ( f , DatumReader ()) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . meta ) # obtenemos el schema del fichero le\u00eddo schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] reader . close () print ( f 'Schema de empleado.avsc: \\n { schema } ' ) print ( f 'Schema del fichero empleados.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { \"type\" : \"record\" , \"name\" : \"empleado\" , \"namespace\" : \"SeveroOchoa\" , \"fields\" : [{ \"type\" : \"string\" , \"name\" : \"nombre\" }, { \"type\" : \"int\" , \"name\" : \"altura\" }, { \"type\" : [ \"null\" , \"int\" ], \"name\" : \"edad\" , \"default\" : null }]} Schema del f ichero empleados.avro : { ' t ype' : 'record' , ' na me' : 'empleado' , ' na mespace' : 'SeveroOchoa' , ' f ields' : [{ ' t ype' : 's tr i n g' , ' na me' : ' n ombre' }, { ' t ype' : 'i nt ' , ' na me' : 'al tura ' }, { ' t ype' : [ ' null ' , 'i nt ' ], ' na me' : 'edad' , 'de fault ' : No ne }]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Fastavro \u00b6 Para trabajar con Avro y grandes vol\u00famenes de datos, se utiliza la librer\u00eda Fastavro ( https://github.com/fastavro/fastavro ) la cual ofrece un rendimiento mucho mejor (en vez de estar codificada en Python puro, tiene algunos fragmentos realizados mediante Cython ). Primero, hemos de instalar la librer\u00eda: pip install fastavro # o si utilizamos Anaconda conda install -c conda-forge fastavro Como pod\u00e9is observar a continuaci\u00f3n, hemos repetido el ejemplo y el c\u00f3digo es muy similar: C\u00f3digo Python Resultado import fastavro import copy import json from fastavro import reader # abrimos el fichero en modo binario y leemos el esquema with open ( \"empleado.avsc\" , \"rb\" ) as f : schemaJSON = json . load ( f ) schemaDict = fastavro . parse_schema ( schemaJSON ) empleados = [{ \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 }, { \"nombre\" : \"Juan\" , \"altura\" : 175 }] # escribimos un fichero a partir del esquema le\u00eddo with open ( 'empleadosf.avro' , 'wb' ) as f : fastavro . writer ( f , schemaDict , empleados ) # abrimos el archivo creado, lo leemos y mostramos l\u00ednea a l\u00ednea with open ( \"empleadosf.avro\" , \"rb\" ) as f : reader = fastavro . reader ( f ) # copiamos los metadatos del fichero le\u00eddo metadata = copy . deepcopy ( reader . metadata ) # obtenemos el schema del fichero le\u00eddo schemaReader = copy . deepcopy ( reader . writer_schema ) schemaFromFile = json . loads ( metadata [ 'avro.schema' ]) # recuperamos los empleados empleados = [ empleado for empleado in reader ] print ( f 'Schema de empleado.avsc: \\n { schemaDict } ' ) print ( f 'Schema del fichero empleadosf.avro: \\n { schemaFromFile } ' ) print ( f 'Empleados: \\n { empleados } ' ) Schema de empleado.avsc : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}], '__ fasta vro_parsed' : True , '__ na med_schemas' : { 'SeveroOchoa.empleado' : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]}}} Schema del f ichero empleados f .avro : { ' t ype' : 'record' , ' na me' : 'SeveroOchoa.empleado' , ' f ields' : [{ ' na me' : ' n ombre' , ' t ype' : 's tr i n g' }, { ' na me' : 'al tura ' , ' t ype' : 'i nt ' }, { 'de fault ' : No ne , ' na me' : 'edad' , ' t ype' : [ ' null ' , 'i nt ' ]}]} Empleados : [{ ' n ombre' : 'Carlos' , 'al tura ' : 180 , 'edad' : 44 }, { ' n ombre' : 'Jua n ' , 'al tura ' : 175 , 'edad' : No ne }] Fastavro y Pandas \u00b6 Finalmente, vamos a realizar un \u00faltimo ejemplo con las dos librer\u00edas m\u00e1s utilizadas. Vamos a leer un fichero CSV de ventas (que ya utilizamos en las sesiones de Pentaho) mediante Pandas, y tras limpiar los datos y quedarnos \u00fanicamente con las ventas de Alemania, almacenaremos el resultado del procesamiento en Avro. Acceso Local Acceso HDFS import pandas as pd from fastavro import writer , parse_schema # Leemos el csv mediante pandas df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 1. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 2. Convertimos el dataframe a una lista de diccionarios records = df . to_dict ( 'records' ) # 3. Persistimos en un fichero avro with open ( 'sales.avro' , 'wb' ) as f : writer ( f , schemaParseado , records ) import pandas as pd from fastavro import parse_schema from hdfs import InsecureClient from hdfs.ext.avro import AvroWriter from hdfs.ext.dataframe import write_dataframe # 1. Nos conectamos a HDFS HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # 2. Leemos el dataframe with hdfs_client . read ( '/user/iabd/pdi_sales.csv' ) as reader : df = pd . read_csv ( reader , sep = ';' ) # Limpiamos los datos (strip a los c\u00f3digos postales) y nos quedamos con Alemania df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # 3. Definimos el esquema schema = { 'name' : 'Sales' , 'namespace' : 'SeveroOchoa' , 'type' : 'record' , 'fields' : [ { 'name' : 'ProductID' , 'type' : 'int' }, { 'name' : 'Date' , 'type' : 'string' }, { 'name' : 'Zip' , 'type' : 'string' }, { 'name' : 'Units' , 'type' : 'int' }, { 'name' : 'Revenue' , 'type' : 'float' }, { 'name' : 'Country' , 'type' : 'string' } ] } schemaParseado = parse_schema ( schema ) # 4a. Persistimos en un fichero avro dentro de HDFS mediante la extension AvroWriter de hdfs with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado ) as writer : records = df . to_dict ( 'records' ) # diccionario for record in records : writer . write ( record ) # 4b. O directamente persistimos el dataframe mediante la extension write_dataframe de hdfs write_dataframe ( hdfs_client , '/user/iabd/sales2.avro' , df ) # infiere el esquema write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado ) Para el acceso HDFS hemos utilizados las extensiones Fastavro y Pandas de la librer\u00eda HDFS del apartado anterior. Comprimiendo los datos \u00b6 \u00bfY s\u00ed comprimimos los datos para ocupen menos espacio en nuestro cl\u00faster y por tanto, nos cuesten menos dinero? Fastavro soporta dos tipos de compresi\u00f3n: gzip (mediante el algoritmo deflate ) y snappy . Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data, la cual hemos de instalar previamente mediante pip install python-snappy . Para indicar el tipo de compresi\u00f3n, \u00fanicamente hemos de a\u00f1adir un par\u00e1metros extra con el algoritmo de compresi\u00f3n en la funci\u00f3n/constructor de persistencia: Fastavro y gzip AvroWriter y snappy write_dataframe y snappy writer ( f , schemaParseado , records , 'deflate' ) with AvroWriter ( hdfs_client , '/user/iabd/sales.avro' , schemaParseado , 'snappy' ) as writer : write_dataframe ( hdfs_client , '/user/iabd/sales3.avro' , df , schema = schemaParseado , codec = 'snappy' ) Comparando algoritmos de compresi\u00f3n Respecto a la compresi\u00f3n, sobre un fichero de 100GB, podemos considerar media si ronda los 50GB y alta si baja a los 40GB. Algoritmo Velocidad Compresi\u00f3n Gzip Media Media Bzip2 Lenta Alta Snappy Alta Media M\u00e1s que un tema de espacio, necesitamos que los procesos sean eficientes y por eso priman los algoritmos que son m\u00e1s r\u00e1pidos. Si te interesa el tema, es muy interesante el art\u00edculo Data Compression in Hadoop . Por ejemplo, si realizamos el ejemplo de Fast Avro y Pandas con acceso local obtenemos los siguientes tama\u00f1os: Sin compresi\u00f3n: 6,9 MiB Gzip: 1,9 MiB Snappy: 2,8 MiB Parquet \u00b6 Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas. Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc... Parquet y Python \u00b6 Para interactuar con el formato Parquet mediante Python, la librer\u00eda m\u00e1s utilizada es la que ofrece Apache Arrow , en concreto la librer\u00eda PyArrow . As\u00ed pues, la instalamos mediante pip: pip install pyarrow Apache Arrow usa un tipo de estructura denominada tabla para almacenar los datos bidimentsional (ser\u00eda muy similar a un dataframe de Pandas ). La documentaci\u00f3n de PyArrow dispone de un libro de recetas con ejemplos con c\u00f3digo para los diferentes casos de uso que se nos puedan plantear. Vamos a simular el mismo ejemplo que hemos realizado previamente mediante Avro , y vamos a crear un fichero en formato JSON con empleados, y tras persistirlo en formato Parquet , lo vamos a recuperar: Empleados en columnas Empleados en Filas dict-parquet.py import pyarrow.parquet as pq import pyarrow as pa # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Almacenamos los empleados por columnas empleados = { \"nombre\" : [ \"Carlos\" , \"Juan\" ], \"altura\" : [ 180 , 44 ], \"edad\" : [ None , 34 ]} # 3.- Creamos una tabla Arrow y la persistimos mediante Parquet tabla = pa . Table . from_pydict ( empleados , schema ) pq . write_table ( tabla , 'empleados.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) Para que pyarrow pueda leer los empleados como documentos JSON, a d\u00eda de hoy s\u00f3lo puede hacerlo leyendo documentos individuales almacenados en fichero: Por lo tanto, creamos el fichero empleados.json con la siguiente informaci\u00f3n: empleados.json { \"nombre\" : \"Carlos\" , \"altura\" : 180 , \"edad\" : 44 } { \"nombre\" : \"Juan\" , \"altura\" : 175 } De manera que podemos leer los datos JSON y persistirlos en Parquet del siguiente modo: json-parquet.py import pyarrow.parquet as pq import pyarrow as pa from pyarrow import json # 1.- Definimos el esquema schema = pa . schema ([ ( 'nombre' , pa . string ()), ( 'altura' , pa . int32 ()), ( 'edad' , pa . int32 ()) ]) # 2.- Leemos los empleados tabla = json . read_json ( \"empleados.json\" ) # 3.- Persistimos la tabla en Parquet pq . write_table ( tabla , 'empleados-json.parquet' ) # 4.- Leemos el fichero generado table2 = pq . read_table ( 'empleados-json.parquet' ) schemaFromFile = table2 . schema print ( f 'Schema del fichero empleados-json.parquet: \\n { schemaFromFile } \\n ' ) print ( f 'Tabla de Empleados: \\n { table2 } ' ) En ambos casos obtendr\u00edamos algo similar a: Schema del fichero empleados.parquet: nombre: string altura: int32 edad: int32 Tabla de Empleados: pyarrow.Table nombre: string altura: int32 edad: int32 ---- nombre: [[\"Carlos\",\"Juan\"]] altura: [[180,44]] edad: [[null,34]] Parquet y Pandas \u00b6 En el caso del uso de Pandas el c\u00f3digo todav\u00eda se simplifica m\u00e1s. Si reproducimos el mismo ejemplo que hemos realizado con Avro tenemos que los Dataframes ofrecen el m\u00e9todo to_parquet para exportar a un fichero Parquet : csv-parquet.py import pandas as pd df = pd . read_csv ( 'pdi_sales.csv' , sep = ';' ) df [ 'Zip' ] = df [ 'Zip' ] . str . strip () filtro = df . Country == \"Germany\" df = df [ filtro ] # A partir de un DataFrame, persistimos los datos df . to_parquet ( 'sales.parquet' ) Si quisi\u00e9ramos almacenar el archivo directamente en HDFS, necesitamos indicarle a Pandas la direcci\u00f3n del sistema de archivos que tenemos configurado en core-site.xml : core-site.ml <property> <name> fs.defaultFS </name> <value> hdfs://iabd-virtualbox:9000 </value> </property> As\u00ed pues, \u00fanicamente necesitamos modificar el nombre del archivo donde serializamos los datos a Parquet : df . to_parquet ( 'hdfs://iabd-virtualbox:9000/sales.parquet' ) Comparando tama\u00f1os \u00b6 Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiB Hue \u00b6 Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue Referencias \u00b6 Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python Actividades \u00b6 Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Explica paso a paso el proceso de lectura que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. (opcional) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/instantaneas . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/instantaneas as\u00ed como de sus snapshots . (opcional) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales. Mediante Python , carga los datos de los taxis que hemos almacenado en HDFS en el apartado de Bloques y crea dentro de /user/iabd/datos los siguientes archivos con el formato adecuado: taxis.avro : la fecha ( tpep_pickup_datetime ), el VendorID y el coste de cada viaje ( total_amount ) (opcional) taxis.parquet con los mismos atributos.","title":"2.- HDFS"},{"location":"apuntes/bdaplicado02hdfs.html#hdfs","text":"","title":"HDFS"},{"location":"apuntes/bdaplicado02hdfs.html#funcionamiento-de-hdfs","text":"En la sesi\u00f3n anterior hemos estudiado los diferentes componentes que forman parte de HDFS: namenode y datanodes . En esta sesi\u00f3n veremos los procesos de lectura y escritura, aprenderemos a interactuar con HDFS mediante comandos, el uso de instant\u00e1neas y practicaremos con los formatos de datos m\u00e1s empleados en Hadoop , como son Avro y Parquet .","title":"Funcionamiento de HDFS"},{"location":"apuntes/bdaplicado02hdfs.html#procesos-de-lectura","text":"Vamos a entender como fluyen los datos en un proceso de lectura entre el cliente y HDFS a partir de la siguiente imagen: Proceso de lectura El cliente abre el fichero que quiere leer mediante el m\u00e9todo open() del sistema de archivos distribuido. \u00c9ste llama al namenode mediante una RPC (llamada a procedimiento remoto) el cual le indica la localizaci\u00f3n del primer bloque del fichero. Para cada bloque, el namenode devuelve la direcci\u00f3n de los datanodes que tienen una copia de ese bloque. Adem\u00e1s, los datanodes se ordenan respecto a su proximidad con el cliente (depende de la topolog\u00eda de la red y despliegue en datacenter/rack/nodo ). Si el cliente en s\u00ed es un datanode , la lectura la realizar\u00e1 desde su propio sistema local. El sistema de ficheros distribuido devuelve al cliente un FSDataInputStream (un flujo de entrada que soporta la b\u00fasqueda de ficheros), sobre el cual se invoca la lectura mediante el m\u00e9todo read() . Este flujo, que contiene las direcciones de los datanodes para los primeros bloques del fichero, conecta con el datanode m\u00e1s cercano para la lectura del primer bloque. Los datos se leen desde el datanode con llamadas al m\u00e9todo read() . Cuando se haya le\u00eddo el bloque completo, el flujo de entrada cerrar\u00e1 la conexi\u00f3n con el datanode actual y buscar\u00e1 el mejor datanode para el siguiente bloque. Se repite el paso anterior (siempre de manera transparente para el cliente, el cual solo est\u00e1 leyendo datos desde un flujo de datos continuo). Cuando el cliente finaliza la lectura, cierra la conexi\u00f3n con el flujo de datos. Durante la lectura, si el flujo encuentra un error al comunicarse con un datanode (o un error de checksum ), intentar\u00e1 el proceso con el siguiente nodo m\u00e1s cercano (adem\u00e1s, recordar\u00e1 los nodos que han fallado para no realizar reintentos en futuros bloques y/o informar\u00e1 de los bloque corruptos al namenode ) Namenode sin datos Recordad que los datos nunca pasan por el namenode . El cliente que realiza la conexi\u00f3n con HDFS es el que hace las operaciones de lectura/escritura directamente con los datanodes . Este dise\u00f1o permite que HDFS escale de manera adecuada, ya que el tr\u00e1fico de los clientes se esparce por todos los datanodes de nuestro cl\u00faster.","title":"Procesos de lectura"},{"location":"apuntes/bdaplicado02hdfs.html#proceso-de-escritura","text":"El proceso de escritura en HDFS sigue un planteamiento similar. Vamos a analizar la creaci\u00f3n, escritura y cierre de un archivo con la siguiente imagen: Proceso de escritura El cliente crea el fichero mediante la llamada al m\u00e9todo create() del DistributedFileSystem . Este realiza una llamada RPC al namenode para crear el fichero en el sistema de ficheros del namenode , sin ning\u00fan bloque asociado a \u00e9l. El namenode realiza varias comprobaciones para asegurar que el fichero no existe previamente y que el usuario tiene los permisos necesarios para su creaci\u00f3n. Tras ello, el namenode determina la forma en que va a dividir los datos en bloques y qu\u00e9 datanodes utilizar\u00e1 para almacenar los bloques. El DistributedFileSystem devuelve un FSDataOutputStream el cual gestiona la comunicaci\u00f3n con los datanodes y el namenode para que el cliente comience a escribir los datos de cada bloque en el namenode apropiado. Conforme el cliente escribe los datos, el flujo obtiene del namenode una lista de datanodes candidatos para almacenar las r\u00e9plicas. La lista de nodos forman un pipeline , de manera que si el factor de replicaci\u00f3n es 3, habr\u00e1 3 nodos en el pipeline . El flujo env\u00eda los paquete al primer datanode del pipeline, el cual almacena cada paquete y los reenv\u00eda al segundo datanode del pipeline . Y as\u00ed sucesivamente con el resto de nodos del pipeline. Cuando todos los nodos han confirmado la recepci\u00f3n y almacenamiento de los paquetes, env\u00eda un paquete de confirmaci\u00f3n al flujo. Cuando el cliente finaliza con la escritura de los datos, cierra el flujo mediante el m\u00e9todo close() el cual libera los paquetes restantes al pipeline de datanodes y queda a la espera de recibir las confirmaciones. Una vez confirmado, le indica al namenode que la escritura se ha completado, informando de los bloques finales que conforman el fichero (puede que hayan cambiado respecto al paso 2 si ha habido alg\u00fan error de escritura).","title":"Proceso de escritura"},{"location":"apuntes/bdaplicado02hdfs.html#hdfs-por-dentro","text":"HDFS utiliza de un conjunto de ficheros que gestionan los cambios que se producen en el cl\u00faster. Primero entramos en $HADOOP_HOME/etc/hadoop y averiguamos la carpeta de datos que tenemos configurada en hdfs-site.xml para el namenode : hdfs-site.xml <property> <name> dfs.name.dir </name> <value> file:///opt/hadoop-data/hdfs/namenode </value> </property> Desde nuestro sistema de archivos, accedemos a dicha carpeta y vemos que existe una carpeta current que contendr\u00e1 un conjunto de ficheros cuyos prefijos son: edits_000NNN : hist\u00f3rico de cambios que se van produciendo. edits_inprogress_NNN : cambios actuales en memoria que no se han persistido. fsimagen_000NNN : snapshot en el tiempo del sistema de ficheros. HDFS por dentro Al arrancar HDFS se carga en memoria el \u00faltimo fichero fsimage disponible junto con los edits que no han sido procesados. Mediante el secondary namenode , cuando se llena un bloque, se ir\u00e1n sincronizando los cambios que se producen en edits_inprogress creando un nuevo fsimage y un nuevo edits . As\u00ed pues, cada vez que se reinicie el namenode , se realizar\u00e1 el merge de los archivos fsimage y edits log .","title":"HDFS por dentro"},{"location":"apuntes/bdaplicado02hdfs.html#trabajando-con-hdfs","text":"Para interactuar con el almacenamiento desde un terminal, se utiliza el comando hdfs . Este comando admite un segundo par\u00e1metro con diferentes opciones. Antes la duda, es recomendable consultar la documentaci\u00f3n oficial hdfs comando hadoop fs HDFS DFS hadoop fs se relaciona con un sistema de archivos gen\u00e9rico que puede apuntar a cualquier sistema de archivos como local, HDFS, FTP, S3, etc. En versiones anteriores se utilizaba el comando hadoop dfs para acceder a HDFS, pero ya quedado obsoleto en favor de hdfs dfs . En el caso concreto de interactuar con el sistema de ficheros de Hadoop se utiliza el comando dfs , el cual requiere de otro argumento (empezando con un gui\u00f3n) el cual ser\u00e1 uno de los comandos Linux para interactuar con el shell. Pod\u00e9is consultar la lista de comandos en la documentaci\u00f3n oficial . hdfs dfs -comandosLinux Por ejemplo, para mostrar todos los archivos que tenemos en el ra\u00edz har\u00edamos: hdfs dfs -ls Los comandos m\u00e1s utilizados son: put : Coloca un archivo dentro de HDFS get : Recupera un archivo de HDFS y lo lleva a nuestro sistema host . cat / text / head / tail : Visualiza el contenido de un archivo. mkdir / rmdir : Crea / borra una carpeta. count : Cuenta el n\u00famero de elementos (n\u00famero de carpetas, ficheros, tama\u00f1o y ruta). cp / mv / rm : Copia / mueve-renombra / elimina un archivo. Autoevaluaci\u00f3n \u00bfSabes qu\u00e9 realiza cada uno de los siguientes comandos? hdfs dfs -mkdir /user/iabd/datos hdfs dfs -put ejemplo.txt /user/iabd/datos/ hdfs dfs -put ejemplo.txt /user/iabd/datos/ejemploRenombrado.txt hdfs dfs -ls datos hdfs dfs -count datos hdfs dfs -mv datos/ejemploRenombrado.txt /user/iabd/datos/otroNombre.json hdfs dfs -get /datos/otroNombre.json /tmp","title":"Trabajando con HDFS"},{"location":"apuntes/bdaplicado02hdfs.html#bloques","text":"A continuaci\u00f3n vamos a ver c\u00f3mo trabaja internamente HDFS con los bloques. Para el siguiente ejemplo, vamos a trabajar con un archivo que ocupe m\u00e1s de un bloque, como puede ser El registro de taxis amarillos de Nueva York - Enero 2020 . Comenzaremos creando un directorio dentro de HDFS llamado prueba-hdfs : hdfs dfs -mkdir /user/iabd/prueba-hdfs Una vez creado subimos el archivo con los taxis: hdfs dfs -put yellow_tripdata_2020-01.csv /user/iabd/prueba-hdfs Con el fichero subido nos vamos al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ), localizamos el archivo y obtenemos el Block Pool ID del block information : Identificador de bloque Si desplegamos el combo de block information , podremos ver c\u00f3mo ha partido el archivo CSV en 5 bloques (566 MB que ocupa el fichero CSV / 128 del tama\u00f1o del bloque). As\u00ed pues, con el c\u00f3digo del Block Pool Id , podemos confirmar que debe existir el directorio current del datanode donde almacena la informaci\u00f3n nuestro servidor (en `/opt/hadoop-data/): ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current Dentro de este subdirectorio existe otro finalized , donde Hadoop ir\u00e1 creando una estructura de subdirectorios subdir donde albergar\u00e1 los bloques de datos: ls /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0 Una vez en este nivel, vamos a buscar el archivo que coincide con el block id poni\u00e9ndole como prefijo blk_ : find -name blk_1073743451 En mi caso devuelve ./subdir6/blk_1073743451 . De manera que ya podemos comprobar como el inicio del documento se encuentra en dicho archivo: head /opt/hadoop-data/hdfs/datanode/current/BP-481169443-127.0.1.1-1639217848073/current/finalized/subdir0/subdir6/blk_1073743451","title":"Bloques"},{"location":"apuntes/bdaplicado02hdfs.html#administracion","text":"Algunas de las opciones m\u00e1s \u00fatiles para administrar HDFS son: hdfs dfsadmin -report : Realiza un resumen del sistema HDFS, similar al que aparece en el interfaz web, donde podemos comprobar el estado de los diferentes nodos. hdfs fsck : Comprueba el estado del sistema de ficheros. Si queremos comprobar el estado de un determinado directorio, lo indicamos mediante un segundo par\u00e1metro: hdfs fsck /datos/prueba hdfs dfsadmin -printTopology : Muestra la topolog\u00eda, identificando los nodos que tenemos y al rack al que pertenece cada nodo. hdfs dfsadmin -listOpenFiles : Comprueba si hay alg\u00fan fichero abierto. hdfs dfsadmin -safemode enter : Pone el sistema en modo seguro el cual evita la modificaci\u00f3n de los recursos del sistema de archivos.","title":"Administraci\u00f3n"},{"location":"apuntes/bdaplicado02hdfs.html#snapshots","text":"Mediante las snapshots podemos crear una instant\u00e1nea que almacena c\u00f3mo est\u00e1 en un determinado momento nuestro sistema de ficheros, a modo de copia de seguridad de los datos, para en un futuro poder realizar una recuperaci\u00f3n. El primer paso es activar el uso de snapshots , mediante el comando de administraci\u00f3n indicando sobre qu\u00e9 carpeta vamos a habilitar su uso: hdfs dfsadmin -allowSnapshot /user/iabd/datos El siguiente paso es crear una snapshot , para ello se indica tanto la carpeta como un nombre para la captura (es un comando que se realiza sobre el sistema de archivos): hdfs dfs -createSnapshot /user/iabd/datos snapshot1 Esta captura se crear\u00e1 dentro de una carpeta oculta dentro de la ruta indicada (en nuestro caso crear\u00e1 la carpeta /user/iabd/datos/.snapshot/snapshot1/ la cual contendr\u00e1 la informaci\u00f3n de la instant\u00e1nea). A continuaci\u00f3n, vamos a borrar uno de los archivo creados anteriormente y comprobar que ya no existe: hdfs dfs -rm /user/iabd/datos/ejemplo.txt hdfs dfs -ls /user/iabd/datos Para comprobar el funcionamiento de los snapshots , vamos a recuperar el archivo desde la captura creada anteriormente. hdfs dfs -cp \\ /user/iabd/datos/.snapshot/snapshot1/ejemplo.txt \\ /user/iabd/datos Si queremos saber que carpetas soportan las instant\u00e1neas: hdfs lsSnapshottableDir Finalmente, si queremos deshabilitar las snapshots de una determinada carpeta, primero hemos de eliminarlas y luego deshabilitarlas: hdfs dfs -deleteSnapshot /user/iabd/datos snapshot1 hdfs dfsadmin -disallowSnapshot /user/iabd/datos","title":"Snapshots"},{"location":"apuntes/bdaplicado02hdfs.html#hdfs-ui","text":"En la sesi\u00f3n anterior ya vimos que pod\u00edamos acceder al interfaz gr\u00e1fico de Hadoop ( http://iabd-virtualbox:9870/explorer.html#/ ) y navegar por las carpetas de HDFS. Si intentamos crear una carpeta o eliminar alg\u00fan archivo recibimos un mensaje del tipo Permission denied: user=dr.who, access=WRITE, inode=\"/\":iabd:supergroup:drwxr-xr-x . Por defecto, los recursos via web los crea el usuario dr.who . Error al crear un directorio mediante Hadoop UI Si queremos habilitar los permisos para que desde este IU podamos crear/modificar/eliminar recursos, podemos cambiar permisos a la carpeta: hdfs dfs -mkdir /user/iabd/pruebas hdfs dfs -chmod 777 /user/iabd/pruebas Si ahora accedemos al interfaz, s\u00ed que podremos trabajar con la carpeta pruebas via web, teniendo en cuenta que las operaciones las realiza el usuario dr.who que pertenece al grupo supergroup . Otra posibilidad es modificar el archivo de configuraci\u00f3n core-site.xml y a\u00f1adir una propiedad para modificar el usuario est\u00e1tico: core-site.xml <property> <name> hadoop.http.staticuser.user </name> <value> iabd </value> </property> Tras reiniciar Hadoop , ya podremos crear los recursos como el usuario iabd .","title":"HDFS UI"},{"location":"apuntes/bdaplicado02hdfs.html#hdfs-y-python","text":"Para el acceso mediante Python a HDFS podemos utilizar la librer\u00eda HdfsCLI ( https://hdfscli.readthedocs.io/en/latest/ ). Primero hemos de instalarla mediante pip : pip install hdfs Vamos a ver un sencillo ejemplo de lectura y escritura en HDFS: from hdfs import InsecureClient # Datos de conexi\u00f3n HDFS_HOSTNAME = 'iabd-virtualbox' HDFSCLI_PORT = 9870 HDFSCLI_CONNECTION_STRING = f 'http:// { HDFS_HOSTNAME } : { HDFSCLI_PORT } ' # En nuestro caso, al no usar Kerberos, creamos una conexi\u00f3n no segura hdfs_client = InsecureClient ( HDFSCLI_CONNECTION_STRING ) # Leemos el fichero de 'El quijote' que tenemos en HDFS fichero = '/user/iabd/el_quijote.txt' with hdfs_client . read ( fichero ) as reader : texto = reader . read () print ( texto ) # Creamos una cadena con formato CSV y la almacenamos en HDFS datos = \"nombre,apellidos \\n Aitor,Medrano \\n Pedro,Casas\" hdfs_client . write ( \"/user/iabd/datos.csv\" , datos ) En el mundo real, los formatos de los archivos normalmente ser\u00e1n Avro y/o Parquet , y el acceso lo realizaremos en gran medida mediante la librer\u00eda de Pandas .","title":"HDFS y Python"},{"location":"apuntes/bdaplicado02hdfs.html#formatos-de-datos","text":"En el primer bloque ya vimos una peque\u00f1a introducci\u00f3n a los diferentes formatos de datos . Las propiedades que ha de tener un formato de datos son: independiente del lenguaje expresivo, con soporte para estructuras complejas y anidadas eficiente, r\u00e1pido y reducido din\u00e1mico, de manera que los programas puedan procesar y definir nuevos tipos de datos. formato de fichero standalone y que permita dividirlo y comprimirlo. Para que Hadoop pueda procesar documento, es imprescindible que el formato del fichero permita su divisi\u00f3n en fragmentos ( splittable in chunks ). Si los clasificamos respecto al formato de almacenamiento tenemos: texto (m\u00e1s lentos, ocupan m\u00e1s pero son m\u00e1s expresivos y permiten su interoperabilidad): CSV, XML, JSON, etc... binarios (mejor rendimiento, ocupan menos, menos expresivos): Avro, Parquet, ORC, etc... Si comparamos los formatos m\u00e1s empleados a partir de las propiedades descritas tenemos: Caracter\u00edstica CSV XML / JSON SequenceFile Avro Independencia del lenguaje Expresivo Eficiente Din\u00e1mico Standalone Dividible Las ventajas de elegir el formato correcto son: Mayor rendimiento en la lectura y/o escritura Ficheros trozeables ( splittables ) Soporte para esquemas que evolucionan Soporte para compresi\u00f3n de los datos (por ejemplo, mediante Snappy ).","title":"Formatos de datos"},{"location":"apuntes/bdaplicado02hdfs.html#filas-vs-columnas","text":"Los formatos con los que estamos m\u00e1s familiarizados, como son CSV o JSON, se basan en filas, donde cada registro se almacena en una fila o documento. Estos formatos son m\u00e1s lentos en ciertas consultas y su almacenamiento no es \u00f3ptimo. En un formato basado en columnas, cada fila almacena toda la informaci\u00f3n de una columna. Al basarse en columnas, ofrece mejor rendimiento para consultas de determinadas columnas y/o agregaciones, y el almacenamiento es m\u00e1s \u00f3ptimo (como todos los datos de una columna son del mismo tipo, la compresi\u00f3n es mayor). Supongamos que tenemos los siguientes datos: Ejemplo de tabla Dependiendo del almacenamiento en filas o columnas tendr\u00edamos la siguiente representaci\u00f3n: Comparaci\u00f3n filas y columnas En un formato columnas los datos del mismo tipo se agrupan, lo que mejor el rendimiento de acceso y reduce el tama\u00f1o: Comparaci\u00f3n filas y columnas El art\u00edculo Apache Parquet: How to be a hero with the open-source columnar data format compara un formato basado en filas, como CSV, con uno basado en columnas como Parquet, en base al tiempo y el coste de su lectura en AWS (por ejemplo, AWS Athena cobra 5$ por cada TB escaneado): Comparaci\u00f3n CSV y Parquet En la tabla podemos observar como 1TB de un fichero CSV en texto plano pasa a ocupar s\u00f3lo 130GB mediante Parquet, lo que provoca que las posteriores consultas tarden menos y, en consecuencia, cuesten menos. En la siguiente tabla comparamos un fichero CSV compuesto de cuatro columnas almacenado en S3 mediante tres formatos: Comparaci\u00f3n filas y columnas Queda claro que la elecci\u00f3n del formato de los datos y la posibilidad de elegir el formato dependiendo de sus futuros casos de uso puede conllevar un importante ahorro en tiempo y costes.","title":"Filas vs Columnas"},{"location":"apuntes/bdaplicado02hdfs.html#avro","text":"Logo de Apache Avro Apache Avro es un formato de almacenamiento basado en filas para Hadoop , utilizado para la serializaci\u00f3n de datos, ya que es m\u00e1s r\u00e1pido y ocupa menos espacio que JSON, debido a que la serializaci\u00f3n de los datos se realiza en un formato binario compacto. Avro se basa en esquemas, los cuales se realizan mediante JSON para definir los tipos de datos y protocolos. Cuando los datos .avro son le\u00eddos siempre est\u00e1 presente el esquema con el que han sido escritos.","title":"Avro"},{"location":"apuntes/bdaplicado02hdfs.html#parquet","text":"Logo de Apache Parquet Apache Parquet es un formato de almacenamiento basado en columnas para Hadoop , con soporte para todos los frameworks de procesamiento de datos, as\u00ed como lenguajes de programaci\u00f3n. De la misma forma que Avro , se trata de un formato de datos auto-descriptivo, de manera que embebe el esquema o estructura de los datos con los propios datos en s\u00ed. Parquet es id\u00f3neo para analizar datasets que contienen muchas columnas. Formato de un archivo Parquet Cada fichero Parquet almacena los datos en binario organizados en grupos de filas. Para cada grupo de filas ( row group ), los valores de los datos se organizan en columnas, lo que facilita la compresi\u00f3n a nivel de columna. La columna de metadatos de un fichero Parquet se almacena al final del fichero, lo que permite que las escrituras sean r\u00e1pidas con una \u00fanica pasada. Los metadatos pueden incluir informaci\u00f3n como los tipos de datos, esquemas de codificaci\u00f3n/compresi\u00f3n, estad\u00edsticas, nombre de los elementos, etc...","title":"Parquet"},{"location":"apuntes/bdaplicado02hdfs.html#comparando-tamanos","text":"Si comparamos los tama\u00f1os de los archivos respecto al formato de datos empleado con \u00fanicamente las ventas de Alemania tendr\u00edamos: ger_sales.csv : 9,7 MiB ger_sales.avro : 6,9 MiB ger_sales-gzip.avro : 1,9 MiB ger_sales-snappy.avro : 2,8 MiB ger_sales.parquet : 2,3 MiB ger_sales-gzip.parquet : 1,6 MiB ger_sales-snappy.parquet : 2,3 MiB","title":"Comparando tama\u00f1os"},{"location":"apuntes/bdaplicado02hdfs.html#hue","text":"Hue ( Hadoop User Experience ) es una interfaz gr\u00e1fica de c\u00f3digo abierto basada en web para su uso con Apache Hadoop . Hue act\u00faa como front-end para las aplicaciones que se ejecutan en el cl\u00faster, lo que permite interactuar con las aplicaciones mediante una interfaz m\u00e1s amigable que el interfaz de comandos. En nuestra m\u00e1quina virtual ya lo tenemos instalado y configurado para que funcione con HDFS y Hive. La ruta de instalaci\u00f3n es /opt/hue-4.10.0 y desde all\u00ed, arrancaremos Hue: ./build/env/bin/hue runserver Tras arrancarlo, nos dirigimos a http://127.0.0.1:8000/ y visualizaremos el formulario de entrada, el cual entraremos con el usuario iabd y la contrase\u00f1a iabd : Login en Hue Una vez dentro, por ejemplo, podemos visualizar e interactuar con HDFS: HDFS en Hue","title":"Hue"},{"location":"apuntes/bdaplicado02hdfs.html#referencias","text":"Documentaci\u00f3n de Apache Hadoop . Hadoop: The definitive Guide, 4th Ed - de Tom White - O'Reilly HDFS Commands, HDFS Permissions and HDFS Storage Introduction to Data Serialization in Apache Hadoop Handling Avro files in Python Native Hadoop file system (HDFS) connectivity in Python","title":"Referencias"},{"location":"apuntes/bdaplicado02hdfs.html#actividades","text":"Para los siguientes ejercicios, copia el comando y/o haz una captura de pantalla donde se muestre el resultado de cada acci\u00f3n. Explica paso a paso el proceso de lectura que realiza HDFS si queremos leer el archivo /logs/101213.log : Proceso de lectura HDFS En este ejercicio vamos a practicar los comandos b\u00e1sicos de HDFS. Una vez arrancado Hadoop : Crea la carpeta /user/iabd/ejercicios . Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote2.txt . Recupera el principio del fichero el_quijote2.txt . Renombra el_quijote2.txt a el_quijote_copia.txt . Adjunta una captura desde el interfaz web donde se vean ambos archivos. Vuelve al terminal y elimina la carpeta con los archivos contenidos mediante un \u00fanico comando. (opcional) Vamos a practicar los comandos de gesti\u00f3n de instant\u00e1neas y administraci\u00f3n de HDFS. Para ello: Crea la carpeta /user/iabd/instantaneas . Habilita las snapshots sobre la carpeta creada. Sube el archivo el_quijote.txt a la carpeta creada. Crea una copia en HDFS y ll\u00e1mala el_quijote_snapshot.txt . Crea una instant\u00e1nea de la carpeta llamada ss1 . Elimina ambos ficheros del quijote. Comprueba que la carpeta est\u00e1 vac\u00eda. Recupera desde ss el archivo el_quijote.txt . Crea una nueva instant\u00e1nea de la carpeta llamada ss2 . Muestra el contenido de la carpeta /user/iabd/instantaneas as\u00ed como de sus snapshots . (opcional) HDFS por dentro Accede al archivo de configuraci\u00f3n hdfs-site.xml y averigua la carpeta donde se almacena el namenode . Muestra los archivos que contiene la carpeta current dentro del namenode Comprueba el id del archivo VERSION . En los siguientes pasos vamos a realizar un checkpoint manual para sincronizar el sistema de ficheros. Para ello entramos en modo safe con el comando hdfs dfsadmin -safemode enter , de manera que impedamos que se trabaje con el sistema de ficheros mientras lanzamos el checkpoint . Comprueba mediante el interfaz gr\u00e1fico que el modo seguro est\u00e1 activo ( Safe mode is ON ). Ahora realiza el checkpoint con el comando hdfs dfsadmin -saveNamespace Vuelve a entrar al modo normal (saliendo del modo seguro mediante hdfs dfsadmin -safemode leave ) Accede a la carpeta del namenode y comprueba que los fsimage del namenode son iguales. Mediante Python , carga los datos de los taxis que hemos almacenado en HDFS en el apartado de Bloques y crea dentro de /user/iabd/datos los siguientes archivos con el formato adecuado: taxis.avro : la fecha ( tpep_pickup_datetime ), el VendorID y el coste de cada viaje ( total_amount ) (opcional) taxis.parquet con los mismos atributos.","title":"Actividades"},{"location":"apuntes/bdaplicado03flume.html","text":"Sqoop / Flume \u00b6 Las dos herramientas principales utilizadas para importar/exportar datos en HDFS son Sqoop y Flume, las cuales vamos a estudiar a continuaci\u00f3n. Sqoop \u00b6 Logo de Apache Sqoop Apache Sqoop ( https://sqoop.apache.org ) es una herramienta dise\u00f1ada para transferir de forma eficiente datos crudos entre un cluster de Hadoop y un almacenamiento estructurado, como una base de datos relacional. Sin continuidad Desde Junio de 2021, el proyecto Sqoop ha dejado de mantenerse como proyecto de Apache y forma parte del \u00e1tico . A\u00fan as\u00ed, creemos conveniente conocer su uso en el estado actual. Gran parte de las funcionalidad que ofrece Sqoop se pueden realizar mediante Nifi o Spark . Un caso t\u00edpico de uso es el de cargar los datos en un data lake (ya sea en HDFS o en S3) con datos que importaremos desde una base de datos, como MariaDB , PostgreSQL o MongoDB . Sqoop utiliza una arquitectura basada en conectores, con soporte para plugins que ofrecen la conectividad a los sistemas externos, como pueden ser Oracle o SqlServer . Internamente, Sqoop utiliza los algoritmos MapReduce para importar y exportar los datos. Por defecto, todos los trabajos Sqoop ejecutan cuatro mapas de trabajo, de manera que los datos se dividen en cuatro nodos de Hadoop. Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos ya tenemos tanto Hadoop como Sqoop instalados, podemos descargar la \u00faltima versi\u00f3n desde http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz . Se recomienda seguir las instrucciones resumidas que tenemos en https://www.tutorialspoint.com/sqoop/sqoop_installation.htm o las de https://riptutorial.com/sqoop . Un par de aspectos que hemos tenido que modificar en nuestra m\u00e1quina virtual son: Copiar el driver de MySQL en $SQOOP_HOME/lib Copiar la librer\u00eda commons-langs-2.6 en $SQOOP_HOME/lib Una vez configurado, podemos comprobar que funciona, por ejemplo, consultando las bases de datos que tenemos en MariaDB (aparecen mensajes de warning por no tener instalados/configurados algunos productos): sqoop list-databases --connect jdbc:mysql://localhost --username=iabd --password=iabd Importando datos \u00b6 La sintaxis b\u00e1sica de Sqoop para importar datos en HDFS es la siguiente: sqoop import -connect jdbc:mysql://host/nombredb -table <nombreTabla> \\ --username <usuarioMariaDB> --password <passwordMariaDB> -m 2 El \u00fanico par\u00e1metro que conviene explicar es -m 2 , el cual est\u00e1 indicando que utilice dos mappers en paralelo para importar los datos. Si no le indicamos este par\u00e1metro, como hemos comentado antes, Sqoop siempre utilizar\u00e1 cuatro mappers . La importaci\u00f3n se realiza en dos pasos: Sqoop escanea la base de datos y colecta los metadatos de la tabla a importar. Env\u00eda un job y transfiere los datos reales utilizando los metadatos necesarios. De forma paralela, cada uno de los mappers se encarga de cargar en HDFS una parte proporcional de los datos. Arquitectura de trabajo de Sqoop Los datos importados se almacenan en carpetas de HDFS, pudiendo especificar otras carpetas, as\u00ed como los caracteres separadores o de terminaci\u00f3n de registro. Adem\u00e1s, podemos utilizar diferentes formatos, como son Avro, ORC, Parquet, ficheros secuenciales o de tipo texto, para almacenar los datos en HDFS. Caso 1 - Importando datos desde MariaDB \u00b6 En el siguiente caso de uso vamos a importar datos que tenemos en una base de datos de MariaDB a HDFS. Sqoop y las zonas horarias Cuando se lanza Sqoop captura los timestamps de nuestra base de datos origen y las convierte a la hora del sistema servidor por lo que tenemos que especificar en nuestra base de datos la zona horaria. Para realizar estos ajustes simplemente editamos el fichero mysqld.cnf que se encuentra en /etc/mysql/my.cnf/ y a\u00f1adimos la siguiente propiedad para asignarle nuestra zona horaria: [mariabd] default_time_zone = 'Europe/Madrid' Primero, vamos a preparar nuestro entorno. Una vez conectados a MariaDB , creamos una base de datos que contenga una tabla con informaci\u00f3n sobre profesores: create database sqoopCaso1 ; use sqoopCaso1 ; CREATE TABLE profesores ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Insertamos datos en la tabla profesores: INSERT INTO profesores ( nombre , edad , materia ) VALUES ( \"Carlos\" , 24 , \"Matem\u00e1ticas\" ), ( \"Pedro\" , 32 , \"Ingl\u00e9s\" ), ( \"Juan\" , 35 , \"Tecnolog\u00eda\" ), ( \"Jose\" , 48 , \"Matem\u00e1ticas\" ), ( \"Paula\" , 24 , \"Inform\u00e1tica\" ), ( \"Susana\" , 32 , \"Inform\u00e1tica\" ), ( \"Lorena\" , 54 , \"Inform\u00e1tica\" ); A continuaci\u00f3n, arrancamos HDFS y YARN : start-dfs.sh start-yarn.sh Con el comando sqoop list-tables listamos todas las tablas de la base de datos sqoopCaso1 : sqoop list-tables --connect jdbc:mysql://localhost/sqoopCaso1 --username = iabd --password = iabd Y finalmente importamos los datos mediante el comando sqoop import : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_hdfs \\ --fields-terminated-by = ',' --lines-terminated-by '\\n' En la primera l\u00ednea, indicamos que vamos a importar datos desde un conexi\u00f3n JDBC, donde se indica el SGBD ( mysql ), el host ( localhost ) y el nombre de la base de datos ( sqoopCaso1 ). En la l\u00ednea dos, se configuran tanto el usuario como la contrase\u00f1a del usuario ( iabd / iabd ) que se conecta a la base de datos. En la tercera l\u00ednea, indicamos la tabla que vamos a leer ( profesores ) y el driver que utilizamos. En la cuarta l\u00ednea configuramos el destino HDFS donde se van a importar los datos. Finalmente, en la \u00faltima l\u00ednea, indicamos el separador de los campos y el car\u00e1cter para separar las l\u00edneas. Si queremos que en el caso de que ya existe la carpeta de destino la borre previamente, a\u00f1adiremos la opci\u00f3n --delete-target-dir . Unhealthy node Nuestra m\u00e1quina virtual tiene el espacio limitado a 30GB, y es probable que en alg\u00fan momento se llene el disco. Adem\u00e1s de eliminar archivos no necesarios, una opci\u00f3n es configurar YARN mediante el archivo yarn-site.xml y configurar las siguientes propiedades para ser m\u00e1s permisivos con la falta de espacio: <property> <name> yarn.nodemanager.disk-health-checker.min-healthy-disks </name> <value> 0.0 </value> </property> <property> <name> yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage </name> <value> 100.0 </value> </property> El resultado que aparece en consola es: 2021-12-14 17:19:04,684 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 2021-12-14 17:19:04,806 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 2021-12-14 17:19:05,057 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time. 2021-12-14 17:19:05,087 INFO manager.SqlManager: Using default fetchSize of 1000 2021-12-14 17:19:05,087 INFO tool.CodeGenTool: Beginning code generation 2021-12-14 17:19:05,793 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,798 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,877 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-3.3.1 Note: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 2021-12-14 17:19:12,153 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.jar 2021-12-14 17:19:12,235 INFO mapreduce.ImportJobBase: Beginning import of profesores 2021-12-14 17:19:12,240 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address 2021-12-14 17:19:12,706 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 2021-12-14 17:19:12,714 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:14,330 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 2021-12-14 17:19:14,608 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2021-12-14 17:19:16,112 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1639498733738_0001 2021-12-14 17:19:22,016 INFO db.DBInputFormat: Using read commited transaction isolation 2021-12-14 17:19:22,018 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM profesores 2021-12-14 17:19:22,022 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 1 to: 7 2021-12-14 17:19:22,214 INFO mapreduce.JobSubmitter: number of splits:4 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639498733738_0001 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2021-12-14 17:19:23,390 INFO conf.Configuration: resource-types.xml not found 2021-12-14 17:19:23,391 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'. 2021-12-14 17:19:24,073 INFO impl.YarnClientImpl: Submitted application application_1639498733738_0001 2021-12-14 17:19:24,300 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1639498733738_0001/ 2021-12-14 17:19:24,303 INFO mapreduce.Job: Running job: job_1639498733738_0001 2021-12-14 17:19:44,015 INFO mapreduce.Job: Job job_1639498733738_0001 running in uber mode : false 2021-12-14 17:19:44,017 INFO mapreduce.Job: map 0% reduce 0% 2021-12-14 17:20:21,680 INFO mapreduce.Job: map 50% reduce 0% 2021-12-14 17:20:23,707 INFO mapreduce.Job: map 100% reduce 0% 2021-12-14 17:20:24,736 INFO mapreduce.Job: Job job_1639498733738_0001 completed successfully 2021-12-14 17:20:24,960 INFO mapreduce.Job: Counters: 34 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=1125124 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=377 HDFS: Number of bytes written=163 HDFS: Number of read operations=24 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 HDFS: Number of bytes read erasure-coded=0 Job Counters Killed map tasks=1 Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=139377 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=139377 Total vcore-milliseconds taken by all map tasks=139377 Total megabyte-milliseconds taken by all map tasks=142722048 Map-Reduce Framework Map input records=7 Map output records=7 Input split bytes=377 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=1218 CPU time spent (ms)=5350 Physical memory (bytes) snapshot=560439296 Virtual memory (bytes) snapshot=10029588480 Total committed heap usage (bytes)=349175808 Peak Map Physical memory (bytes)=142544896 Peak Map Virtual memory (bytes)=2507415552 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=163 2021-12-14 17:20:24,979 INFO mapreduce.ImportJobBase: Transferred 163 bytes in 70,589 seconds (2,3091 bytes/sec) 2021-12-14 17:20:24,986 INFO mapreduce.ImportJobBase: Retrieved 7 records. Vamos a repasar la salida del log para entender el proceso: En la l\u00ednea 5 vemos como se lanza el generador de c\u00f3digo. En las l\u00edneas 6, 7 y 15 vemos como ejecuta la consulta para obtener todos los datos de profesores. En la l\u00ednea 20 obtiene los valores m\u00ednimo y m\u00e1ximo para calcular como dividir los datos. De las l\u00edneas 29 a la 34 se ejecuta el proceso MapReduce . En el resto se puede observar un resumen estad\u00edstico. Si accedemos al interfaz gr\u00e1fico de YARN (en http://iabd-virtualbox:8088/cluster ) podemos ver c\u00f3mo aparece el proceso como realizado: Estado de YARN tras la importaci\u00f3n Si accedemos al interfaz gr\u00e1fico de Hadoop (recuerda que puedes acceder a \u00e9l mediante http://localhost:9870 ) podremos comprobar en el directorio /user/iabd/sqoop que ha creado el directorio que hemos especificado junto con los siguientes archivos: Contenido de /user/iabd/sqoop/profesores_hdfs Si entramos a ver los datos, podemos visualizar el contenido del primer fragmento que contiene los primeros datos de la tabla: Contenido de part-m-0000 Caso 2 - Exportando datos a MariaDB \u00b6 Ahora vamos a hacer el paso contrario, desde HDFS vamos a exportar los ficheros a otra tabla. As\u00ed pues, primero vamos a crear la nueva tabla en una nueva base de datos (aunque pod\u00edamos haber reutilizado la base de datos): create database sqoopCaso2 ; use sqoopCaso2 ; CREATE TABLE profesores2 ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Para exportar los datos de HDFS y cargarlos en esta nueva tabla lanzamos la siguiente orden: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --export-dir = /user/iabd/sqoop/profesores_hdfs Formatos Avro y Parquet \u00b6 Sqoop permite trabajar con diferentes formatos, tanto Avro como Parquet . Avro es un formato de almacenamiento basado en filas para Hadoop que se usa ampliamente como formato de serializaci\u00f3n. Recuerda que Avro almacena la estructura en formato JSON y los datos en binario. Parquet a su vez es un formato de almacenamiento binario basado en columnas que puede almacenar estructuras de datos anidados. Avro y Hadoop Para que funcione la serializaci\u00f3n con Avro hay que copiar el fichero .jar que viene en el directorio de Sqoop para Avro como librer\u00eda de Hadoop , mediante el siguiente comando: cp $SQOOP_HOME /lib/avro-1.8.1.jar $HADOOP_HOME /share/hadoop/common/lib/ rm $HADOOP_HOME /share/hadoop/common/lib/avro-1.7.7.jar En nuestra m\u00e1quina virtual este paso ya est\u00e1 realizado. Para importar los datos en formato Avro , a\u00f1adiremos la opci\u00f3n --as-avrodatafile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_avro --as-avrodatafile Si en vez de Avro , queremos importar los datos en formato Parquet cambiamos el \u00faltimo par\u00e1metro por --as-parquetfile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_parquet --as-parquetfile Si queremos comprobar los archivos, podemos acceder via HDFS y la opci\u00f3n -ls : hdfs dfs -ls /user/iabd/sqoop/profesores_avro Obteniendo: Found 5 items -rw-r--r-- 1 iabd supergroup 0 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/_SUCCESS -rw-r--r-- 1 iabd supergroup 568 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00000.avro -rw-r--r-- 1 iabd supergroup 569 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00001.avro -rw-r--r-- 1 iabd supergroup 547 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00002.avro -rw-r--r-- 1 iabd supergroup 574 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00003.avro Si queremos ver el contenido de una de las partes, utilizamos la opci\u00f3n -text : hdfs dfs -text /user/iabd/sqoop/profesores_avro/part-m-00000.avro Obteniendo el esquema y los datos en formato Avro : {\"id\":{\"int\":1},\"nombre\":{\"string\":\"Carlos\"},\"edad\":{\"int\":24},\"materia\":{\"string\":\"Matem\u00e1ticas\"}} {\"id\":{\"int\":2},\"nombre\":{\"string\":\"Pedro\"},\"edad\":{\"int\":32},\"materia\":{\"string\":\"Ingl\u00e9s\"}} Autoevaluaci\u00f3n \u00bfQu\u00e9 sucede si ejectuamos el comando hdfs dfs -tail /user/iabd/sqoop/profesores_avro/part-m-00000.avro ? \u00bfPor qu\u00e9 aparece contenido en binario? En el caso de ficheros Parquet , primero listamos los archivos generados: hdfs dfs -ls /user/iabd/sqoop/profesores_parquet Obteniendo: Found 6 items drwxr-xr-x - iabd supergroup 0 2021-12-15 16:13 /user/iabd/sqoop/profesores_parquet/.metadata drwxr-xr-x - iabd supergroup 0 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/.signals -rw-r--r-- 1 iabd supergroup 1094 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet -rw-r--r-- 1 iabd supergroup 1114 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/1e12aaad-98c6-4508-9c41-e1599e698385.parquet -rw-r--r-- 1 iabd supergroup 1097 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/6a803503-f3e0-4f2a-8546-a337f7f90e73.parquet -rw-r--r-- 1 iabd supergroup 1073 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/eda459b2-1da4-4790-b649-0f2f8b83ab06.parquet Podemos usar las parquet-tools para ver su contenido. Si la instalamos mediante pip install parquet-tools , podremos acceder a ficheros locales y almacenados en S3. Si queremos acceder de forma remota via HDFS, podemos descargar la versi\u00f3n Java y utilizarla mediante hadoop (aunque da problemas entre las versiones de Sqoop y Parquet): hadoop jar parquet-tools-1.11.2.jar head -n5 hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet Si queremos obtener informaci\u00f3n sobre los documentos, usaremos la opci\u00f3n meta : hadoop jar parquet-tools-1.11.2.jar meta hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet M\u00e1s informaci\u00f3n sobre parquet-tools en https://pypi.org/project/parquet-tools/ . Trabajando con datos comprimidos \u00b6 En un principio, vamos a trabajar siempre con los datos sin comprimir. Cuando tengamos datos que vamos a utilizar durante mucho tiempo (del orden de varios a\u00f1os) es cuando nos plantearemos comprimir los datos. Por defecto, podemos comprimir mediante el formato gzip : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_gzip \\ --compress Si en cambio queremos comprimirlo con formato bzip2 : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_bzip \\ --compress --compression-codec bzip2 Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data. As\u00ed pues, para utilizarlo lo indicaremos mediante el codec snappy : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_snappy \\ --compress --compression-codec snappy Importando con filtros \u00b6 Adem\u00e1s de poder importar todos los datos de una tabla, podemos filtrar los datos. Por ejemplo, podemos indicar mediante la opci\u00f3n --where el filtro a ejecutar en la consulta: sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_materia_info \\ --where \"materia='Inform\u00e1tica'\" Tambi\u00e9n podemos restringir las columnas que queremos recuperar mediante la opci\u00f3n --columns : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_cols \\ --columns \"nombre,materia\" Finalmente, podemos especificar una consulta con clave de particionado (en este caso, ya no indicamos el nombre de la tabla): sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_query \\ --query \"select * from profesores where edad > 40 AND \\$CONDITIONS\" \\ --split-by \"id\" En la consulta, hemos de a\u00f1adir el token \\$CONDITIONS , el cual Hadoop substituir\u00e1 por la columna por la que realiza el particionado. Importaci\u00f3n incremental \u00b6 Si utilizamos procesos batch , es muy com\u00fan realizar importaciones incrementales tras una carga de datos. Para ello, utilizaremos las opciones --incremental append junto con la columna a comprobar mediante --check-column y el \u00faltimo registro cargado mediante --last-value : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_inc \\ --incremental append \\ --check-column id \\ --last-value 4 Despu\u00e9s de ejecutarlo, si vemos la informaci\u00f3n que nos devuelve, en las \u00faltimas l\u00edneas, podemos copiar los par\u00e1metros que tenemos que utilizar para posteriores importaciones. ... 2021-12-15 19:10:59,348 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments: 2021-12-15 19:10:59,348 INFO tool.ImportTool: --incremental append 2021-12-15 19:10:59,348 INFO tool.ImportTool: --check-column id 2021-12-15 19:10:59,348 INFO tool.ImportTool: --last-value 7 2021-12-15 19:10:59,349 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create') Trabajando con Hive \u00b6 Podemos importar los datos en HDFS para que luego puedan ser consultables desde Hive . Para ello hemos de utilizar el par\u00e1metro --hive-import e indicar el nombre de la base de datos mediante --hive-database as\u00ed como la opci\u00f3n de --create-hive-table para que cree la tabla indicada en el par\u00e1metro hive-table . Es importante destacar que ya no ponemos destino con target-dir : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database default \\ --create-hive-table --hive-table profesores_mariadb Para comprobar el resultado, dentro de Hive ejecutaremos el comando: describe formatted profesores_mariadb Para exportar los datos, de forma similar haremos: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --driver = com.mysql.jdbc.Driver \\ --h-catalog-table profesores_mariadb Flume \u00b6 Logo de Apache Flume All\u00e1 por el a\u00f1o 2010 Cloudera present\u00f3 Flume que posteriormente pas\u00f3 a formar parte de Apache ( https://flume.apache.org/ ) como un software para tratamiento e ingesta de datos masivo. Flume permite crear desarrollos complejos que permiten el tratamiento en streaming de datos masivos. Flume funciona como un buffer entre los productores de datos y el destino final. Al utilizar un buffer, evitamos que un productor sature a un consumidor, sin necesidad de preocuparnos de que alg\u00fan destino est\u00e9 inalcanzable o inoperable (por ejemplo, en el caso de que haya ca\u00eddo HDFS), etc... Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos tambi\u00e9n tenemos instalado Flume , podemos descargar la \u00faltima versi\u00f3n desde http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz . A nivel de configuraci\u00f3n s\u00f3lo hemos definido la variable de entorno $FLUME_HOME que apunta a /opt/flume-1.9.0 . Arquitectura \u00b6 Su arquitectura es sencilla, y se basa en el uso de agentes que se dividen en tres componentes los cuales podemos configurar: Source (fuente): Fuente de origen de los datos, ya sea Twitter , Kafka , una petici\u00f3n Http , etc... Las fuentes son un componente activo que recibe datos desde otra aplicaci\u00f3n que produce datos (aunque tambi\u00e9n existen fuentes que pueden producir datos por s\u00ed mismos, cuyo objetivo es poder probar ciertos flujos de datos). Las fuentes puedes escuchar uno o m\u00e1s puertos de red para recibir o leer datos del sistema de arhcivos. Cada fuente debe conectar a al menos un canal. Una fuente puede escribir en varios canales, replicando los eventos a todos o algunos canales en base a alg\u00fan criterio. Channel (canal): la v\u00eda por donde se tratar\u00e1n los datos. Un canal es un componente pasivo que almacena los datos como un buffer. Se comportan como colas, donde las fuentes publican y los sumideros consumen los datos. M\u00faltiples fuentes pueden escribir de forma segura en el mismo canal, y m\u00faltiples sumideros pueden leer desde el mismo canal. Sin embargo, cada sumidero s\u00f3lo puede leer de un \u00fanico canal. Si m\u00faltiples sumideros leen del mismo canal, s\u00f3lo uno de ellos leer\u00e1 el dato. Sink (sumidero): persistencia/movimiento de los datos, a ficheros / base de datos. Toma eventos del canal de manera continua leyendo y eliminando los eventos. A continuaci\u00f3n, los transmite hacia el siguiente componente, ya sea a HDFS, Hive, etc... Una vez los datos han llegado al siguiente destino, el sumidero informa al canal mediante un commit transaccional para que elimine dichos eventos del canal. Arquitectura Flume - imagen extra\u00edda de https://www.diegocalvo.es/flume/ Es muy recomendable acceder a la gu\u00eda de usuario oficial para consultar todas las fuentes de datos, canales y sumideros disponibles en la actualidad. A continuaci\u00f3n se nombran algunos de los m\u00e1s destacados: Sources Channels Sinks Avro Source Memory Channel HDFS Sink Thrift Source JDBC Channel Hive Sink Exec Source Kafka Channel Logger Sink JMS Source File Channel Avro Sink Spooling Directory Source Spillable Memory Channel Thrift Sink Twitter 1% firehose Source Pseudo Transaction Channel Kafka Sink Kafka Source File Roll Sink NetCat Source Null Sink Sequence Generator Source HBaseSink Syslog Sources AsyncHBaseSink HTTP Source MorphlineSolrSink Multiport Syslog TCP Source ElasticSearchSink Syslog UDP Source Kite Dataset Sink Flume se complica cuando queremos utilizarlo para obtener datos de manera paralela (o multiplexada) y/o necesitamos crear nuestros propios sumideros o interceptores. Pero por lo general, su uso es sencillo y se trata de una herramienta muy recomendada como ayuda/alternativa a herramientas como Pentaho . Algunas de sus caracter\u00edsticas son: Dise\u00f1o flexible basado en flujos de datos de transmisi\u00f3n. Resistente a fallos y robusto con m\u00faltiples conmutaciones por error y mecanismos de recuperaci\u00f3n. Lleva datos desde origen a destino: incluidos HDFS y HBase . Agentes \u00b6 Un agente es la unidad m\u00e1s sencilla con la que trabaja Flume , permitiendo conectar un agente Flume a uno o m\u00e1s agentes, encanden\u00e1ndolos. Adem\u00e1s, un agente puede recibir datos de uno o m\u00e1s agentes. Conectando m\u00faltiples agentes entre s\u00ed podemos crear un flujo de datos para mover datos de un lugar a otro, de aplicaciones que producen datos a HDFS, HBase o donde necesitemos. Para lanzar un tarea en Flume , debemos definir un agente, el cual funciona como un contenedor para alojar subcomponentes que permiten mover los datos. Estos agentes tienen cuatro partes bien diferenciadas asociadas a la arquitectura de Flume. En la primera parte, definiremos los componente del agente ( sources , channels y sinks ), y luego, para cada uno de ellos, configuraremos sus propiedades: sources : responsables de colocar los eventos/datos en el agente channels : buffer que almacena los eventos/datos recibidos por los sources hasta que un sink lo saca para enviarlo al siguiente destino. sinks : responsable de sacar los eventos/datos del agente y reenviarlo al siguiente agente (HDFS, HBase, etc...) Evento \u00b6 El evento es la unidad m\u00e1s peque\u00f1a del procesamiento de eventos de Flume. Cuando Flume lee una fuente de datos, envuelve una fila de datos (es decir, encuentra los saltos de l\u00ednea) en un evento. Un evento es una estructura de datos que se compone de dos partes: Encabezado, se utiliza principalmente para registrar informaci\u00f3n mediante un mapa en forma de clave y valor. No transfieren datos, pero contienen informaci\u00f3n util para el enrutado y gesti\u00f3n de la prioridad o importancia de los mensajes. Cuerpo: array de bytes que almacena los datos reales. Probando Flume \u00b6 Por ejemplo, vamos a crear un agente el cual llamaremos ExecLoggerAgent el cual va a ejecutar un comando y mostrar\u00e1 el resultado por el log de Flume . Para ello, creamos la configuraci\u00f3n del agente en el fichero agente.conf (todas las propiedades comenzar\u00e1n con el nombre del agente): agente.conf # Nombramos los componentes del agente ExecLoggerAgent.sources = Exec ExecLoggerAgent.channels = MemChannel ExecLoggerAgent.sinks = LoggerSink # Describimos el tipo de origen ExecLoggerAgent.sources.Exec.type = exec ExecLoggerAgent.sources.Exec.command = ls /home/iabd/ # Describimos el destino ExecLoggerAgent.sinks.LoggerSink.type = logger # Describimos la configuraci\u00f3n del canal ExecLoggerAgent.channels.MemChannel.type = memory ExecLoggerAgent.channels.MemChannel.capacity = 1000 ExecLoggerAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal ExecLoggerAgent.sources.Exec.channels = MemChannel ExecLoggerAgent.sinks.LoggerSink.channel = MemChannel Antes de lanzar el agente Flume, recuerda que debes arrancar tanto Hadoop como YARN , por ejemplo, mediante el comando start-all.sh . A continuaci\u00f3n ya podemos lanzar Flume con el agente mediante el comando (la opci\u00f3n -n sirve para indicar el nombre del agente, y con -f indicamos el nombre del archivo de configuraci\u00f3n): flume-ng agent -n ExecLoggerAgent -f agente.conf Configurando un agente \u00b6 Si te has fijado en el ejemplo anterior, los ficheros de configuraci\u00f3n de los agentes siguen el mismo formato. Para definir un flujo dentro de un agente, necesitamos enlazar las fuentes y los sumideros con un canal. Para ello, listaremos las fuentes, sumideros y canales del agente, y entonces apuntaremos la fuente y el sumidero a un canal. 1 - N - 1 Una fuente puede indicar m\u00faltiples canales, pero un sumidero s\u00f3lo puede indicar un \u00fanico canal. As\u00ed pues, el formato ser\u00e1 similar al siguiente archivo: # Listamos las fuentes, sumideros y canales <Agent>.sources = <Source> <Agent>.sinks = <Sink> <Agent>.channels = <Channel1> <Channel2> # Configuramos los canales de la fuente <Agent>.sources.<Source>.channels = <Channel1> <Channel2> ... # Configuramos el canal para el sumidero <Agent>.sinks.<Sink>.channel = <Channel1> Adem\u00e1s de definir el flujo, es necesario configurar las propiedades de cada fuente, sumidero y canal. Para elo se sigue la misma nomenclatura donde fijamos el tipo de componente (mediante la propiedad type ) y el resto de propiedades espec\u00edficas de cada componente: # Propiedades de las fuentes <Agent>.sources.<Source>.<someProperty> = <someValue> # Propiedades de los canales <Agent>.channel.<Channel>.<someProperty> = <someValue> # Propiedades de los sumideros <Agent>.sources.<Sink>.<someProperty> = <someValue> Para cada tipo de fuente , canal y sumidero es recomendable revisar la documentaci\u00f3n para validar todas las propiedades disponibles. Caso 3a - Almacenando en HDFS \u00b6 En este caso de uso vamos generar datos de forma secuencial y los vamos a ingestar en HDFS . Una buena pr\u00e1ctica es colocar los archivos de configuraci\u00f3n dentro de $FLUME_HOME/conf . As\u00ed pues, vamos a crear el agente SeqGenAgent y almacenar la configuraci\u00f3n en el fichero seqgen.conf : seqgen.conf # Nombramos a los componentes del agente SeqGenAgent.sources = SeqSource SeqGenAgent.channels = MemChannel SeqGenAgent.sinks = HDFS # Describimos el tipo de origen SeqGenAgent.sources.SeqSource.type = seq # Describimos el destino SeqGenAgent.sinks.HDFS.type = hdfs SeqGenAgent.sinks.HDFS.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/seqgen_data/ SeqGenAgent.sinks.HDFS.hdfs.filePrefix = flume-caso3-seqgen SeqGenAgent.sinks.HDFS.hdfs.rollInterval = 0 SeqGenAgent.sinks.HDFS.hdfs.rollCount = 1000 SeqGenAgent.sinks.HDFS.hdfs.fileType = DataStream # Describimos la configuraci\u00f3n del canal SeqGenAgent.channels.MemChannel.type = memory SeqGenAgent.channels.MemChannel.capacity = 1000 SeqGenAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal SeqGenAgent.sources.SeqSource.channels = MemChannel SeqGenAgent.sinks.HDFS.channel = MemChannel Ejecutamos el siguiente comando desde $FLUME_HOME y a los pocos segundo lo paramos mediante CTRL + C para que detenga la generaci\u00f3n de n\u00fameros, ya que si no seguir\u00e1 generando archivos en HDFS: ./bin/flume-ng agent --conf ./conf/ --conf-file conf/seqgen.conf \\ --name SeqGenAgent \\ -Dflume.root.logger = INFO,console Vaciando HDFS Si queremos eliminar los ficheros generados en HDFS, recuerda que puedes realizar un borrado recursivo mediante el comando: hdfs dfs -rm -r /user/iabd/flume Si comprobamos por ejemplo el contenido de la carpeta ( hdfs dfs -ls /user/iabd/flume/seqgen_data ) veremos que se han generado m\u00faltiples archivos: Found 10 items -rw-r--r-- 1 iabd supergroup 1402 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 -rw-r--r-- 1 iabd supergroup 1368 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740934 -rw-r--r-- 1 iabd supergroup 1350 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740935 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740936 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740937 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740938 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740939 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740940 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740941 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740942 Y si comprobamos el contenido del primero ( hdfs dfs -cat /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 ) veremos como contiene la secuencia generada: 0 1 2 3 ... Caso 3b - De Netcat a HDFS \u00b6 Ahora vamos a crear otro ejemplo de generaci\u00f3n de informaci\u00f3n, pero esta vez, en vez que utilizar la memoria del servidor como canal, vamos a utilizar el sistema de archivos. Adem\u00e1s, para generar la informaci\u00f3n nos basamos en una fuente Netcat , en la cual debemos especificar un puerto de escucha. Mediante esta fuente, Flume quedar\u00e1 a la escucha en dicho puerto y recibir\u00e1 cada l\u00ednea introducida como un evento individual que transferir\u00e1 al canal especificado. En el mismo directorio $FLUME_HOME\\conf , creamos un nuevo fichero con el nombre netcat.conf y creamos otro agente que se va a encargar de generar informaci\u00f3n: netcat.conf # Nombramos a los componentes del agente NetcatAgent.sources = Netcat NetcatAgent.channels = FileChannel NetcatAgent.sinks = HdfsSink # Describimos el origen netcat en localhost:44444 NetcatAgent.sources.Netcat.type = netcat NetcatAgent.sources.Netcat.bind = localhost NetcatAgent.sources.Netcat.port = 44444 NetcatAgent.sources.Netcat.channels = FileChannel # Describimos el destino en HDFS NetcatAgent.sinks.HdfsSink.type = hdfs NetcatAgent.sinks.HdfsSink.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/net_data/ NetcatAgent.sinks.HdfsSink.hdfs.writeFormat = Text NetcatAgent.sinks.HdfsSink.hdfs.fileType = DataStream NetcatAgent.sinks.HdfsSink.channel = FileChannel # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAgent.channels.FileChannel.type = file NetcatAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint Lanzamos al agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat.conf \\ --name NetcatAgent \\ -Dflume.root.logger = INFO,console En una nueva pesta\u00f1a introducimos el siguiente comando y escribimos curl telnet://localhost:44444 Una vez conectados, escribimos varias frases con saltos de l\u00ednea. Por cada vez que pulsamos Enter , nos aparecer\u00e1 un OK . Probando Netcat OK Esto parece que funciona m\u00e1s o menos OK A continuaci\u00f3n, nos vamos al navegador web de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/flume/net_data ) y comprobamos que se ha creado el fichero: Resultado del flujo Netcat-HDFS Caso 4 - Flujos encadenados \u00b6 Es muy com\u00fan definir un pipeline de flujos encadenados, uniendo la salida de un agente a la entrada de otro. Para ello, utilizaremos como enlace un sink - source de tipo Avro . Este dise\u00f1o tambi\u00e9n se conoce como flujo Multi-hop : Encadenando flujos En este caso, vamos a crear un primer agente ( NetcatAvroAgent ) que ingeste datos desde Netcat y los coloque en un sink de tipo Avro . Para ello, creamos el agente netcat-avro.conf : netcat-avro.conf # Nombramos a los componentes del agente NetcatAvroAgent.sources = Netcat NetcatAvroAgent.channels = FileChannel NetcatAvroAgent.sinks = AvroSink # Describimos el origen netcat en localhost:44444 NetcatAvroAgent.sources.Netcat.type = netcat NetcatAvroAgent.sources.Netcat.bind = localhost NetcatAvroAgent.sources.Netcat.port = 44444 # Describimos el destino como Avro en localhost:10003 NetcatAvroAgent.sinks.AvroSink.type = avro NetcatAvroAgent.sinks.AvroSink.hostname = localhost NetcatAvroAgent.sinks.AvroSink.port = 10003 # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAvroAgent.sources.Netcat.channels = FileChannel NetcatAvroAgent.sinks.AvroSink.channel = FileChannel NetcatAvroAgent.channels.FileChannel.type = file NetcatAvroAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAvroAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint A continuaci\u00f3n, creamos un segundo agente ( AvroHdfsAgent ) que utilice como fuente Avro y que almacene los eventos recibidos en HDFS. Para ello, creamos el agente avro-hdfs.conf : avro-hdfs.conf # Nombramos a los componentes del agente AvroHdfsAgent.sources = AvroSource AvroHdfsAgent.channels = MemChannel AvroHdfsAgent.sinks = HdfsSink # Describimos el origen como Avro en localhost:10003 AvroHdfsAgent.sources.AvroSource.type = avro AvroHdfsAgent.sources.AvroSource.bind = localhost AvroHdfsAgent.sources.AvroSource.port = 10003 # Describimos el destino HDFS AvroHdfsAgent.sinks.HdfsSink.type = hdfs AvroHdfsAgent.sinks.HdfsSink.hdfs.path = /user/iabd/flume/avro_data/ AvroHdfsAgent.sinks.HdfsSink.hdfs.fileType = DataStream AvroHdfsAgent.sinks.HdfsSink.hdfs.writeFormat = Text # Unimos el origen y el destino AvroHdfsAgent.sources.AvroSource.channels = MemChannel AvroHdfsAgent.sinks.HdfsSink.channel = MemChannel AvroHdfsAgent.channels.MemChannel.type = memory Primero lanzamos este \u00faltimo agente, para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en nueva pesta\u00f1a, lanzamos el primer agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat-avro.conf \\ --name NetcatAvroAgent \\ -Dflume.root.logger = INFO,console Finalmente, en otro terminal, escribimos mensajes Netcat accediendo a curl telnet://localhost:44444 . Si acced\u00e9is a la carpeta /user/iabd/flume/avro_data en HDFS podremos comprobar c\u00f3mo se van creando archivos que agrupan los mensajes enviados. Caso 5 - Flujo multi-agente \u00b6 Para demostrar como varios agentes pueden conectarse entre s\u00ed, vamos a realizar un caso de uso donde vamos a leer informaci\u00f3n de tres fuentes distintas: una fuente de Netcat con un canal basado en ficheros, otra que realice spooling de una carpeta (vigile una carpeta y cuando haya alg\u00fan archivo, lo ingeste y lo elimine) utilizando un canal en memoria y un tercero que ejecute un comando utilizando tambi\u00e9n un canal en memoria. Como agente de consolidaci\u00f3n que una la informaci\u00f3n de las tres fuentes de datos, vamos a reutilizar el agente AvroHdfsAgent que hemos creado en el caso de uso anterior. Consolidando flujos Para ello, vamos a definir los agentes en siguiente fichero de configuraci\u00f3n multiagent-avro.conf ): multiagent-avro.conf # Nombramos las tres fuentes con sus tres sumideros MultiAgent.sources = Netcat Spooldir Exec MultiAgent.channels = FileChannel MemChannel1 MemChannel2 MultiAgent.sinks = AvroSink1 AvroSink2 AvroSink3 # Describimos el primer agente MultiAgent.sources.Netcat.type = netcat MultiAgent.sources.Netcat.bind = localhost MultiAgent.sources.Netcat.port = 10004 # Describimos el segundo agente MultiAgent.sources.Spooldir.type = spooldir MultiAgent.sources.Spooldir.spoolDir = /home/iabd/flume/spoolDir MultiAgent.sources.Spooldir.deletePolicy = immediate # Describimos el tercer agente MultiAgent.sources.Exec.type = exec MultiAgent.sources.Exec.command = cat /home/iabd/datos/empleados.txt # Describimos los tres destinos como Avro en localhost:10003 MultiAgent.sinks.AvroSink1.type = avro MultiAgent.sinks.AvroSink1.hostname = localhost MultiAgent.sinks.AvroSink1.port = 10003 MultiAgent.sinks.AvroSink2.type = avro MultiAgent.sinks.AvroSink2.hostname = localhost MultiAgent.sinks.AvroSink2.port = 10003 MultiAgent.sinks.AvroSink3.type = avro MultiAgent.sinks.AvroSink3.hostname = localhost MultiAgent.sinks.AvroSink3.port = 10003 # Describimos los canales MultiAgent.channels.FileChannel.type = file MultiAgent.channels.FileChannel.dataDir = /home/iabd/flume/data MultiAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint MultiAgent.channels.MemChannel1.type = memory MultiAgent.channels.MemChannel2.type = memory # Unimos los or\u00edgenes y destinos MultiAgent.sources.Netcat.channels = FileChannel MultiAgent.sources.Spooldir.channels = MemChannel1 MultiAgent.sources.Exec.channels = MemChannel2 MultiAgent.sinks.AvroSink1.channel = FileChannel MultiAgent.sinks.AvroSink2.channel = MemChannel1 MultiAgent.sinks.AvroSink3.channel = MemChannel2 Preparaci\u00f3n Antes de arrancar los agentes, aseg\u00farate de tener creada la carpeta /home/iabd/flume/spoolDir y disponible el recurso /home/iabd/datos/empleados.txt . Igual que en el caso de uso anterior, primero lanzamos el agente consolidador para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en una nueva pesta\u00f1a, lanzamos el multi agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/multiagent-avro.conf \\ --name MultiAgent \\ -Dflume.root.logger = INFO,console Interceptores Podemos utilizar interceptores para modificar o borrar eventos al vuelo a partir del timestamp , nombre del host , uuid , etc... incluso mediante el uso de una expresi\u00f3n regular. Si quieres profundizar en el tema, el siguiente art\u00edculo detalla los diferentes tipos y configuraciones: https://data-flair.training/blogs/flume-interceptors/ En este caso, para poder probarlo, adem\u00e1s de enviar comandos Netstat en curl telnet://localhost:10004 , prueba a colocar un archivo de texto (por ejemplo, un documento CSV) en /home/iabd/flume/spoolDir . Actividades \u00b6 Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Haciendo uso de Sqoop y la base de datos retail_db , importa todos los pedidos de la tabla orders cuyo campo order_status sea COMPLETE . Coloca los datos en user/iabd/sqoop/orders/datos_parquet en formato Parquet, utilizando el tabulador como delimitador de campos y utilizando la compresi\u00f3n Snappy. Deber\u00e1s recuperar 22.899 (\u00bfo 22.902?) registros. Haciendo uso de Sqoop y la base de datos retail_db , importa todos los clientes de la tabla customers cuyo campo state sea CA . Coloca los datos en user/iabd/sqoop/customers/datos_avro en formato Avro, utilizando la compresi\u00f3n bzip2. Deber\u00e1s recuperar las columnas customer_id, customer_fname, customer_lname, customer_state . El resultado contendr\u00e1 2012 registros. Mediante Flume , realiza los caso de uso 3, 4 y 5. (opcional) Haciendo uso de Flume, recupera informaci\u00f3n de Twitter y almac\u00e9nala en HDFS. Para ello, utiliza el Twitter 1% Firehouse source y el HDFS sink . Para ello, necesitar\u00e9is las claves de desarrollo que ya creamos en las sesiones sobre Nifi . Adjunta una captura de pantalla donde se visualice el contenido de uno de los bloques de HDFS. Cuidado con el espacio de almacenamiento Una vez lances el agente, detenlo a los tres segundos para no llenar de datos HDFS. Referencias \u00b6 P\u00e1gina oficial de Sqoop Sqoop User Guide Sqoop Tutorial en Tutorialspoint P\u00e1gina oficial de Flume Flume User Guide","title":"3.- Sqoop / Flume"},{"location":"apuntes/bdaplicado03flume.html#sqoop-flume","text":"Las dos herramientas principales utilizadas para importar/exportar datos en HDFS son Sqoop y Flume, las cuales vamos a estudiar a continuaci\u00f3n.","title":"Sqoop / Flume"},{"location":"apuntes/bdaplicado03flume.html#sqoop","text":"Logo de Apache Sqoop Apache Sqoop ( https://sqoop.apache.org ) es una herramienta dise\u00f1ada para transferir de forma eficiente datos crudos entre un cluster de Hadoop y un almacenamiento estructurado, como una base de datos relacional. Sin continuidad Desde Junio de 2021, el proyecto Sqoop ha dejado de mantenerse como proyecto de Apache y forma parte del \u00e1tico . A\u00fan as\u00ed, creemos conveniente conocer su uso en el estado actual. Gran parte de las funcionalidad que ofrece Sqoop se pueden realizar mediante Nifi o Spark . Un caso t\u00edpico de uso es el de cargar los datos en un data lake (ya sea en HDFS o en S3) con datos que importaremos desde una base de datos, como MariaDB , PostgreSQL o MongoDB . Sqoop utiliza una arquitectura basada en conectores, con soporte para plugins que ofrecen la conectividad a los sistemas externos, como pueden ser Oracle o SqlServer . Internamente, Sqoop utiliza los algoritmos MapReduce para importar y exportar los datos. Por defecto, todos los trabajos Sqoop ejecutan cuatro mapas de trabajo, de manera que los datos se dividen en cuatro nodos de Hadoop. Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos ya tenemos tanto Hadoop como Sqoop instalados, podemos descargar la \u00faltima versi\u00f3n desde http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz . Se recomienda seguir las instrucciones resumidas que tenemos en https://www.tutorialspoint.com/sqoop/sqoop_installation.htm o las de https://riptutorial.com/sqoop . Un par de aspectos que hemos tenido que modificar en nuestra m\u00e1quina virtual son: Copiar el driver de MySQL en $SQOOP_HOME/lib Copiar la librer\u00eda commons-langs-2.6 en $SQOOP_HOME/lib Una vez configurado, podemos comprobar que funciona, por ejemplo, consultando las bases de datos que tenemos en MariaDB (aparecen mensajes de warning por no tener instalados/configurados algunos productos): sqoop list-databases --connect jdbc:mysql://localhost --username=iabd --password=iabd","title":"Sqoop"},{"location":"apuntes/bdaplicado03flume.html#importando-datos","text":"La sintaxis b\u00e1sica de Sqoop para importar datos en HDFS es la siguiente: sqoop import -connect jdbc:mysql://host/nombredb -table <nombreTabla> \\ --username <usuarioMariaDB> --password <passwordMariaDB> -m 2 El \u00fanico par\u00e1metro que conviene explicar es -m 2 , el cual est\u00e1 indicando que utilice dos mappers en paralelo para importar los datos. Si no le indicamos este par\u00e1metro, como hemos comentado antes, Sqoop siempre utilizar\u00e1 cuatro mappers . La importaci\u00f3n se realiza en dos pasos: Sqoop escanea la base de datos y colecta los metadatos de la tabla a importar. Env\u00eda un job y transfiere los datos reales utilizando los metadatos necesarios. De forma paralela, cada uno de los mappers se encarga de cargar en HDFS una parte proporcional de los datos. Arquitectura de trabajo de Sqoop Los datos importados se almacenan en carpetas de HDFS, pudiendo especificar otras carpetas, as\u00ed como los caracteres separadores o de terminaci\u00f3n de registro. Adem\u00e1s, podemos utilizar diferentes formatos, como son Avro, ORC, Parquet, ficheros secuenciales o de tipo texto, para almacenar los datos en HDFS.","title":"Importando datos"},{"location":"apuntes/bdaplicado03flume.html#caso-1-importando-datos-desde-mariadb","text":"En el siguiente caso de uso vamos a importar datos que tenemos en una base de datos de MariaDB a HDFS. Sqoop y las zonas horarias Cuando se lanza Sqoop captura los timestamps de nuestra base de datos origen y las convierte a la hora del sistema servidor por lo que tenemos que especificar en nuestra base de datos la zona horaria. Para realizar estos ajustes simplemente editamos el fichero mysqld.cnf que se encuentra en /etc/mysql/my.cnf/ y a\u00f1adimos la siguiente propiedad para asignarle nuestra zona horaria: [mariabd] default_time_zone = 'Europe/Madrid' Primero, vamos a preparar nuestro entorno. Una vez conectados a MariaDB , creamos una base de datos que contenga una tabla con informaci\u00f3n sobre profesores: create database sqoopCaso1 ; use sqoopCaso1 ; CREATE TABLE profesores ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Insertamos datos en la tabla profesores: INSERT INTO profesores ( nombre , edad , materia ) VALUES ( \"Carlos\" , 24 , \"Matem\u00e1ticas\" ), ( \"Pedro\" , 32 , \"Ingl\u00e9s\" ), ( \"Juan\" , 35 , \"Tecnolog\u00eda\" ), ( \"Jose\" , 48 , \"Matem\u00e1ticas\" ), ( \"Paula\" , 24 , \"Inform\u00e1tica\" ), ( \"Susana\" , 32 , \"Inform\u00e1tica\" ), ( \"Lorena\" , 54 , \"Inform\u00e1tica\" ); A continuaci\u00f3n, arrancamos HDFS y YARN : start-dfs.sh start-yarn.sh Con el comando sqoop list-tables listamos todas las tablas de la base de datos sqoopCaso1 : sqoop list-tables --connect jdbc:mysql://localhost/sqoopCaso1 --username = iabd --password = iabd Y finalmente importamos los datos mediante el comando sqoop import : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_hdfs \\ --fields-terminated-by = ',' --lines-terminated-by '\\n' En la primera l\u00ednea, indicamos que vamos a importar datos desde un conexi\u00f3n JDBC, donde se indica el SGBD ( mysql ), el host ( localhost ) y el nombre de la base de datos ( sqoopCaso1 ). En la l\u00ednea dos, se configuran tanto el usuario como la contrase\u00f1a del usuario ( iabd / iabd ) que se conecta a la base de datos. En la tercera l\u00ednea, indicamos la tabla que vamos a leer ( profesores ) y el driver que utilizamos. En la cuarta l\u00ednea configuramos el destino HDFS donde se van a importar los datos. Finalmente, en la \u00faltima l\u00ednea, indicamos el separador de los campos y el car\u00e1cter para separar las l\u00edneas. Si queremos que en el caso de que ya existe la carpeta de destino la borre previamente, a\u00f1adiremos la opci\u00f3n --delete-target-dir . Unhealthy node Nuestra m\u00e1quina virtual tiene el espacio limitado a 30GB, y es probable que en alg\u00fan momento se llene el disco. Adem\u00e1s de eliminar archivos no necesarios, una opci\u00f3n es configurar YARN mediante el archivo yarn-site.xml y configurar las siguientes propiedades para ser m\u00e1s permisivos con la falta de espacio: <property> <name> yarn.nodemanager.disk-health-checker.min-healthy-disks </name> <value> 0.0 </value> </property> <property> <name> yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage </name> <value> 100.0 </value> </property> El resultado que aparece en consola es: 2021-12-14 17:19:04,684 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 2021-12-14 17:19:04,806 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 2021-12-14 17:19:05,057 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time. 2021-12-14 17:19:05,087 INFO manager.SqlManager: Using default fetchSize of 1000 2021-12-14 17:19:05,087 INFO tool.CodeGenTool: Beginning code generation 2021-12-14 17:19:05,793 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,798 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:05,877 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-3.3.1 Note: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 2021-12-14 17:19:12,153 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-iabd/compile/585dc8a5a92b80ebbd22c9f597dd1928/profesores.jar 2021-12-14 17:19:12,235 INFO mapreduce.ImportJobBase: Beginning import of profesores 2021-12-14 17:19:12,240 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address 2021-12-14 17:19:12,706 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 2021-12-14 17:19:12,714 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM profesores AS t WHERE 1=0 2021-12-14 17:19:14,330 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 2021-12-14 17:19:14,608 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at iabd-virtualbox/127.0.1.1:8032 2021-12-14 17:19:16,112 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/iabd/.staging/job_1639498733738_0001 2021-12-14 17:19:22,016 INFO db.DBInputFormat: Using read commited transaction isolation 2021-12-14 17:19:22,018 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(id), MAX(id) FROM profesores 2021-12-14 17:19:22,022 INFO db.IntegerSplitter: Split size: 1; Num splits: 4 from: 1 to: 7 2021-12-14 17:19:22,214 INFO mapreduce.JobSubmitter: number of splits:4 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1639498733738_0001 2021-12-14 17:19:22,707 INFO mapreduce.JobSubmitter: Executing with tokens: [] 2021-12-14 17:19:23,390 INFO conf.Configuration: resource-types.xml not found 2021-12-14 17:19:23,391 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'. 2021-12-14 17:19:24,073 INFO impl.YarnClientImpl: Submitted application application_1639498733738_0001 2021-12-14 17:19:24,300 INFO mapreduce.Job: The url to track the job: http://iabd-virtualbox:8088/proxy/application_1639498733738_0001/ 2021-12-14 17:19:24,303 INFO mapreduce.Job: Running job: job_1639498733738_0001 2021-12-14 17:19:44,015 INFO mapreduce.Job: Job job_1639498733738_0001 running in uber mode : false 2021-12-14 17:19:44,017 INFO mapreduce.Job: map 0% reduce 0% 2021-12-14 17:20:21,680 INFO mapreduce.Job: map 50% reduce 0% 2021-12-14 17:20:23,707 INFO mapreduce.Job: map 100% reduce 0% 2021-12-14 17:20:24,736 INFO mapreduce.Job: Job job_1639498733738_0001 completed successfully 2021-12-14 17:20:24,960 INFO mapreduce.Job: Counters: 34 File System Counters FILE: Number of bytes read=0 FILE: Number of bytes written=1125124 FILE: Number of read operations=0 FILE: Number of large read operations=0 FILE: Number of write operations=0 HDFS: Number of bytes read=377 HDFS: Number of bytes written=163 HDFS: Number of read operations=24 HDFS: Number of large read operations=0 HDFS: Number of write operations=8 HDFS: Number of bytes read erasure-coded=0 Job Counters Killed map tasks=1 Launched map tasks=4 Other local map tasks=4 Total time spent by all maps in occupied slots (ms)=139377 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=139377 Total vcore-milliseconds taken by all map tasks=139377 Total megabyte-milliseconds taken by all map tasks=142722048 Map-Reduce Framework Map input records=7 Map output records=7 Input split bytes=377 Spilled Records=0 Failed Shuffles=0 Merged Map outputs=0 GC time elapsed (ms)=1218 CPU time spent (ms)=5350 Physical memory (bytes) snapshot=560439296 Virtual memory (bytes) snapshot=10029588480 Total committed heap usage (bytes)=349175808 Peak Map Physical memory (bytes)=142544896 Peak Map Virtual memory (bytes)=2507415552 File Input Format Counters Bytes Read=0 File Output Format Counters Bytes Written=163 2021-12-14 17:20:24,979 INFO mapreduce.ImportJobBase: Transferred 163 bytes in 70,589 seconds (2,3091 bytes/sec) 2021-12-14 17:20:24,986 INFO mapreduce.ImportJobBase: Retrieved 7 records. Vamos a repasar la salida del log para entender el proceso: En la l\u00ednea 5 vemos como se lanza el generador de c\u00f3digo. En las l\u00edneas 6, 7 y 15 vemos como ejecuta la consulta para obtener todos los datos de profesores. En la l\u00ednea 20 obtiene los valores m\u00ednimo y m\u00e1ximo para calcular como dividir los datos. De las l\u00edneas 29 a la 34 se ejecuta el proceso MapReduce . En el resto se puede observar un resumen estad\u00edstico. Si accedemos al interfaz gr\u00e1fico de YARN (en http://iabd-virtualbox:8088/cluster ) podemos ver c\u00f3mo aparece el proceso como realizado: Estado de YARN tras la importaci\u00f3n Si accedemos al interfaz gr\u00e1fico de Hadoop (recuerda que puedes acceder a \u00e9l mediante http://localhost:9870 ) podremos comprobar en el directorio /user/iabd/sqoop que ha creado el directorio que hemos especificado junto con los siguientes archivos: Contenido de /user/iabd/sqoop/profesores_hdfs Si entramos a ver los datos, podemos visualizar el contenido del primer fragmento que contiene los primeros datos de la tabla: Contenido de part-m-0000","title":"Caso 1 - Importando datos desde MariaDB"},{"location":"apuntes/bdaplicado03flume.html#caso-2-exportando-datos-a-mariadb","text":"Ahora vamos a hacer el paso contrario, desde HDFS vamos a exportar los ficheros a otra tabla. As\u00ed pues, primero vamos a crear la nueva tabla en una nueva base de datos (aunque pod\u00edamos haber reutilizado la base de datos): create database sqoopCaso2 ; use sqoopCaso2 ; CREATE TABLE profesores2 ( id MEDIUMINT NOT NULL AUTO_INCREMENT , nombre CHAR ( 30 ) NOT NULL , edad INTEGER ( 30 ), materia CHAR ( 30 ), PRIMARY KEY ( id ) ); Para exportar los datos de HDFS y cargarlos en esta nueva tabla lanzamos la siguiente orden: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --export-dir = /user/iabd/sqoop/profesores_hdfs","title":"Caso 2 - Exportando datos a MariaDB"},{"location":"apuntes/bdaplicado03flume.html#formatos-avro-y-parquet","text":"Sqoop permite trabajar con diferentes formatos, tanto Avro como Parquet . Avro es un formato de almacenamiento basado en filas para Hadoop que se usa ampliamente como formato de serializaci\u00f3n. Recuerda que Avro almacena la estructura en formato JSON y los datos en binario. Parquet a su vez es un formato de almacenamiento binario basado en columnas que puede almacenar estructuras de datos anidados. Avro y Hadoop Para que funcione la serializaci\u00f3n con Avro hay que copiar el fichero .jar que viene en el directorio de Sqoop para Avro como librer\u00eda de Hadoop , mediante el siguiente comando: cp $SQOOP_HOME /lib/avro-1.8.1.jar $HADOOP_HOME /share/hadoop/common/lib/ rm $HADOOP_HOME /share/hadoop/common/lib/avro-1.7.7.jar En nuestra m\u00e1quina virtual este paso ya est\u00e1 realizado. Para importar los datos en formato Avro , a\u00f1adiremos la opci\u00f3n --as-avrodatafile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_avro --as-avrodatafile Si en vez de Avro , queremos importar los datos en formato Parquet cambiamos el \u00faltimo par\u00e1metro por --as-parquetfile : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_parquet --as-parquetfile Si queremos comprobar los archivos, podemos acceder via HDFS y la opci\u00f3n -ls : hdfs dfs -ls /user/iabd/sqoop/profesores_avro Obteniendo: Found 5 items -rw-r--r-- 1 iabd supergroup 0 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/_SUCCESS -rw-r--r-- 1 iabd supergroup 568 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00000.avro -rw-r--r-- 1 iabd supergroup 569 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00001.avro -rw-r--r-- 1 iabd supergroup 547 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00002.avro -rw-r--r-- 1 iabd supergroup 574 2021-12-14 17:56 /user/iabd/sqoop/profesores_avro/part-m-00003.avro Si queremos ver el contenido de una de las partes, utilizamos la opci\u00f3n -text : hdfs dfs -text /user/iabd/sqoop/profesores_avro/part-m-00000.avro Obteniendo el esquema y los datos en formato Avro : {\"id\":{\"int\":1},\"nombre\":{\"string\":\"Carlos\"},\"edad\":{\"int\":24},\"materia\":{\"string\":\"Matem\u00e1ticas\"}} {\"id\":{\"int\":2},\"nombre\":{\"string\":\"Pedro\"},\"edad\":{\"int\":32},\"materia\":{\"string\":\"Ingl\u00e9s\"}} Autoevaluaci\u00f3n \u00bfQu\u00e9 sucede si ejectuamos el comando hdfs dfs -tail /user/iabd/sqoop/profesores_avro/part-m-00000.avro ? \u00bfPor qu\u00e9 aparece contenido en binario? En el caso de ficheros Parquet , primero listamos los archivos generados: hdfs dfs -ls /user/iabd/sqoop/profesores_parquet Obteniendo: Found 6 items drwxr-xr-x - iabd supergroup 0 2021-12-15 16:13 /user/iabd/sqoop/profesores_parquet/.metadata drwxr-xr-x - iabd supergroup 0 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/.signals -rw-r--r-- 1 iabd supergroup 1094 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet -rw-r--r-- 1 iabd supergroup 1114 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/1e12aaad-98c6-4508-9c41-e1599e698385.parquet -rw-r--r-- 1 iabd supergroup 1097 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/6a803503-f3e0-4f2a-8546-a337f7f90e73.parquet -rw-r--r-- 1 iabd supergroup 1073 2021-12-15 16:14 /user/iabd/sqoop/profesores_parquet/eda459b2-1da4-4790-b649-0f2f8b83ab06.parquet Podemos usar las parquet-tools para ver su contenido. Si la instalamos mediante pip install parquet-tools , podremos acceder a ficheros locales y almacenados en S3. Si queremos acceder de forma remota via HDFS, podemos descargar la versi\u00f3n Java y utilizarla mediante hadoop (aunque da problemas entre las versiones de Sqoop y Parquet): hadoop jar parquet-tools-1.11.2.jar head -n5 hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet Si queremos obtener informaci\u00f3n sobre los documentos, usaremos la opci\u00f3n meta : hadoop jar parquet-tools-1.11.2.jar meta hdfs://iabd-virtualbox:9000/user/iabd/sqoop/profesores_parquet/12205ee4-6e63-4c0d-8e64-751882d60179.parquet M\u00e1s informaci\u00f3n sobre parquet-tools en https://pypi.org/project/parquet-tools/ .","title":"Formatos Avro y Parquet"},{"location":"apuntes/bdaplicado03flume.html#trabajando-con-datos-comprimidos","text":"En un principio, vamos a trabajar siempre con los datos sin comprimir. Cuando tengamos datos que vamos a utilizar durante mucho tiempo (del orden de varios a\u00f1os) es cuando nos plantearemos comprimir los datos. Por defecto, podemos comprimir mediante el formato gzip : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_gzip \\ --compress Si en cambio queremos comprimirlo con formato bzip2 : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_bzip \\ --compress --compression-codec bzip2 Snappy es una biblioteca de compresi\u00f3n y descompresi\u00f3n de datos de gran rendimiento que se utiliza con frecuencia en proyectos Big Data. As\u00ed pues, para utilizarlo lo indicaremos mediante el codec snappy : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_snappy \\ --compress --compression-codec snappy","title":"Trabajando con datos comprimidos"},{"location":"apuntes/bdaplicado03flume.html#importando-con-filtros","text":"Adem\u00e1s de poder importar todos los datos de una tabla, podemos filtrar los datos. Por ejemplo, podemos indicar mediante la opci\u00f3n --where el filtro a ejecutar en la consulta: sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_materia_info \\ --where \"materia='Inform\u00e1tica'\" Tambi\u00e9n podemos restringir las columnas que queremos recuperar mediante la opci\u00f3n --columns : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_cols \\ --columns \"nombre,materia\" Finalmente, podemos especificar una consulta con clave de particionado (en este caso, ya no indicamos el nombre de la tabla): sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_query \\ --query \"select * from profesores where edad > 40 AND \\$CONDITIONS\" \\ --split-by \"id\" En la consulta, hemos de a\u00f1adir el token \\$CONDITIONS , el cual Hadoop substituir\u00e1 por la columna por la que realiza el particionado.","title":"Importando con filtros"},{"location":"apuntes/bdaplicado03flume.html#importacion-incremental","text":"Si utilizamos procesos batch , es muy com\u00fan realizar importaciones incrementales tras una carga de datos. Para ello, utilizaremos las opciones --incremental append junto con la columna a comprobar mediante --check-column y el \u00faltimo registro cargado mediante --last-value : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --target-dir = /user/iabd/sqoop/profesores_inc \\ --incremental append \\ --check-column id \\ --last-value 4 Despu\u00e9s de ejecutarlo, si vemos la informaci\u00f3n que nos devuelve, en las \u00faltimas l\u00edneas, podemos copiar los par\u00e1metros que tenemos que utilizar para posteriores importaciones. ... 2021-12-15 19:10:59,348 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments: 2021-12-15 19:10:59,348 INFO tool.ImportTool: --incremental append 2021-12-15 19:10:59,348 INFO tool.ImportTool: --check-column id 2021-12-15 19:10:59,348 INFO tool.ImportTool: --last-value 7 2021-12-15 19:10:59,349 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')","title":"Importaci\u00f3n incremental"},{"location":"apuntes/bdaplicado03flume.html#trabajando-con-hive","text":"Podemos importar los datos en HDFS para que luego puedan ser consultables desde Hive . Para ello hemos de utilizar el par\u00e1metro --hive-import e indicar el nombre de la base de datos mediante --hive-database as\u00ed como la opci\u00f3n de --create-hive-table para que cree la tabla indicada en el par\u00e1metro hive-table . Es importante destacar que ya no ponemos destino con target-dir : sqoop import --connect jdbc:mysql://localhost/sqoopCaso1 \\ --username = iabd --password = iabd \\ --table = profesores --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database default \\ --create-hive-table --hive-table profesores_mariadb Para comprobar el resultado, dentro de Hive ejecutaremos el comando: describe formatted profesores_mariadb Para exportar los datos, de forma similar haremos: sqoop export --connect jdbc:mysql://localhost/sqoopCaso2 \\ --username = iabd --password = iabd \\ --table = profesores2 --driver = com.mysql.jdbc.Driver \\ --h-catalog-table profesores_mariadb","title":"Trabajando con Hive"},{"location":"apuntes/bdaplicado03flume.html#flume","text":"Logo de Apache Flume All\u00e1 por el a\u00f1o 2010 Cloudera present\u00f3 Flume que posteriormente pas\u00f3 a formar parte de Apache ( https://flume.apache.org/ ) como un software para tratamiento e ingesta de datos masivo. Flume permite crear desarrollos complejos que permiten el tratamiento en streaming de datos masivos. Flume funciona como un buffer entre los productores de datos y el destino final. Al utilizar un buffer, evitamos que un productor sature a un consumidor, sin necesidad de preocuparnos de que alg\u00fan destino est\u00e9 inalcanzable o inoperable (por ejemplo, en el caso de que haya ca\u00eddo HDFS), etc... Instalaci\u00f3n Aunque en la m\u00e1quina virtual con la que trabajamos tambi\u00e9n tenemos instalado Flume , podemos descargar la \u00faltima versi\u00f3n desde http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz . A nivel de configuraci\u00f3n s\u00f3lo hemos definido la variable de entorno $FLUME_HOME que apunta a /opt/flume-1.9.0 .","title":"Flume"},{"location":"apuntes/bdaplicado03flume.html#arquitectura","text":"Su arquitectura es sencilla, y se basa en el uso de agentes que se dividen en tres componentes los cuales podemos configurar: Source (fuente): Fuente de origen de los datos, ya sea Twitter , Kafka , una petici\u00f3n Http , etc... Las fuentes son un componente activo que recibe datos desde otra aplicaci\u00f3n que produce datos (aunque tambi\u00e9n existen fuentes que pueden producir datos por s\u00ed mismos, cuyo objetivo es poder probar ciertos flujos de datos). Las fuentes puedes escuchar uno o m\u00e1s puertos de red para recibir o leer datos del sistema de arhcivos. Cada fuente debe conectar a al menos un canal. Una fuente puede escribir en varios canales, replicando los eventos a todos o algunos canales en base a alg\u00fan criterio. Channel (canal): la v\u00eda por donde se tratar\u00e1n los datos. Un canal es un componente pasivo que almacena los datos como un buffer. Se comportan como colas, donde las fuentes publican y los sumideros consumen los datos. M\u00faltiples fuentes pueden escribir de forma segura en el mismo canal, y m\u00faltiples sumideros pueden leer desde el mismo canal. Sin embargo, cada sumidero s\u00f3lo puede leer de un \u00fanico canal. Si m\u00faltiples sumideros leen del mismo canal, s\u00f3lo uno de ellos leer\u00e1 el dato. Sink (sumidero): persistencia/movimiento de los datos, a ficheros / base de datos. Toma eventos del canal de manera continua leyendo y eliminando los eventos. A continuaci\u00f3n, los transmite hacia el siguiente componente, ya sea a HDFS, Hive, etc... Una vez los datos han llegado al siguiente destino, el sumidero informa al canal mediante un commit transaccional para que elimine dichos eventos del canal. Arquitectura Flume - imagen extra\u00edda de https://www.diegocalvo.es/flume/ Es muy recomendable acceder a la gu\u00eda de usuario oficial para consultar todas las fuentes de datos, canales y sumideros disponibles en la actualidad. A continuaci\u00f3n se nombran algunos de los m\u00e1s destacados: Sources Channels Sinks Avro Source Memory Channel HDFS Sink Thrift Source JDBC Channel Hive Sink Exec Source Kafka Channel Logger Sink JMS Source File Channel Avro Sink Spooling Directory Source Spillable Memory Channel Thrift Sink Twitter 1% firehose Source Pseudo Transaction Channel Kafka Sink Kafka Source File Roll Sink NetCat Source Null Sink Sequence Generator Source HBaseSink Syslog Sources AsyncHBaseSink HTTP Source MorphlineSolrSink Multiport Syslog TCP Source ElasticSearchSink Syslog UDP Source Kite Dataset Sink Flume se complica cuando queremos utilizarlo para obtener datos de manera paralela (o multiplexada) y/o necesitamos crear nuestros propios sumideros o interceptores. Pero por lo general, su uso es sencillo y se trata de una herramienta muy recomendada como ayuda/alternativa a herramientas como Pentaho . Algunas de sus caracter\u00edsticas son: Dise\u00f1o flexible basado en flujos de datos de transmisi\u00f3n. Resistente a fallos y robusto con m\u00faltiples conmutaciones por error y mecanismos de recuperaci\u00f3n. Lleva datos desde origen a destino: incluidos HDFS y HBase .","title":"Arquitectura"},{"location":"apuntes/bdaplicado03flume.html#probando-flume","text":"Por ejemplo, vamos a crear un agente el cual llamaremos ExecLoggerAgent el cual va a ejecutar un comando y mostrar\u00e1 el resultado por el log de Flume . Para ello, creamos la configuraci\u00f3n del agente en el fichero agente.conf (todas las propiedades comenzar\u00e1n con el nombre del agente): agente.conf # Nombramos los componentes del agente ExecLoggerAgent.sources = Exec ExecLoggerAgent.channels = MemChannel ExecLoggerAgent.sinks = LoggerSink # Describimos el tipo de origen ExecLoggerAgent.sources.Exec.type = exec ExecLoggerAgent.sources.Exec.command = ls /home/iabd/ # Describimos el destino ExecLoggerAgent.sinks.LoggerSink.type = logger # Describimos la configuraci\u00f3n del canal ExecLoggerAgent.channels.MemChannel.type = memory ExecLoggerAgent.channels.MemChannel.capacity = 1000 ExecLoggerAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal ExecLoggerAgent.sources.Exec.channels = MemChannel ExecLoggerAgent.sinks.LoggerSink.channel = MemChannel Antes de lanzar el agente Flume, recuerda que debes arrancar tanto Hadoop como YARN , por ejemplo, mediante el comando start-all.sh . A continuaci\u00f3n ya podemos lanzar Flume con el agente mediante el comando (la opci\u00f3n -n sirve para indicar el nombre del agente, y con -f indicamos el nombre del archivo de configuraci\u00f3n): flume-ng agent -n ExecLoggerAgent -f agente.conf","title":"Probando Flume"},{"location":"apuntes/bdaplicado03flume.html#configurando-un-agente","text":"Si te has fijado en el ejemplo anterior, los ficheros de configuraci\u00f3n de los agentes siguen el mismo formato. Para definir un flujo dentro de un agente, necesitamos enlazar las fuentes y los sumideros con un canal. Para ello, listaremos las fuentes, sumideros y canales del agente, y entonces apuntaremos la fuente y el sumidero a un canal. 1 - N - 1 Una fuente puede indicar m\u00faltiples canales, pero un sumidero s\u00f3lo puede indicar un \u00fanico canal. As\u00ed pues, el formato ser\u00e1 similar al siguiente archivo: # Listamos las fuentes, sumideros y canales <Agent>.sources = <Source> <Agent>.sinks = <Sink> <Agent>.channels = <Channel1> <Channel2> # Configuramos los canales de la fuente <Agent>.sources.<Source>.channels = <Channel1> <Channel2> ... # Configuramos el canal para el sumidero <Agent>.sinks.<Sink>.channel = <Channel1> Adem\u00e1s de definir el flujo, es necesario configurar las propiedades de cada fuente, sumidero y canal. Para elo se sigue la misma nomenclatura donde fijamos el tipo de componente (mediante la propiedad type ) y el resto de propiedades espec\u00edficas de cada componente: # Propiedades de las fuentes <Agent>.sources.<Source>.<someProperty> = <someValue> # Propiedades de los canales <Agent>.channel.<Channel>.<someProperty> = <someValue> # Propiedades de los sumideros <Agent>.sources.<Sink>.<someProperty> = <someValue> Para cada tipo de fuente , canal y sumidero es recomendable revisar la documentaci\u00f3n para validar todas las propiedades disponibles.","title":"Configurando un agente"},{"location":"apuntes/bdaplicado03flume.html#caso-3a-almacenando-en-hdfs","text":"En este caso de uso vamos generar datos de forma secuencial y los vamos a ingestar en HDFS . Una buena pr\u00e1ctica es colocar los archivos de configuraci\u00f3n dentro de $FLUME_HOME/conf . As\u00ed pues, vamos a crear el agente SeqGenAgent y almacenar la configuraci\u00f3n en el fichero seqgen.conf : seqgen.conf # Nombramos a los componentes del agente SeqGenAgent.sources = SeqSource SeqGenAgent.channels = MemChannel SeqGenAgent.sinks = HDFS # Describimos el tipo de origen SeqGenAgent.sources.SeqSource.type = seq # Describimos el destino SeqGenAgent.sinks.HDFS.type = hdfs SeqGenAgent.sinks.HDFS.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/seqgen_data/ SeqGenAgent.sinks.HDFS.hdfs.filePrefix = flume-caso3-seqgen SeqGenAgent.sinks.HDFS.hdfs.rollInterval = 0 SeqGenAgent.sinks.HDFS.hdfs.rollCount = 1000 SeqGenAgent.sinks.HDFS.hdfs.fileType = DataStream # Describimos la configuraci\u00f3n del canal SeqGenAgent.channels.MemChannel.type = memory SeqGenAgent.channels.MemChannel.capacity = 1000 SeqGenAgent.channels.MemChannel.transactionCapacity = 100 # Unimos el origen y el destino a trav\u00e9s del canal SeqGenAgent.sources.SeqSource.channels = MemChannel SeqGenAgent.sinks.HDFS.channel = MemChannel Ejecutamos el siguiente comando desde $FLUME_HOME y a los pocos segundo lo paramos mediante CTRL + C para que detenga la generaci\u00f3n de n\u00fameros, ya que si no seguir\u00e1 generando archivos en HDFS: ./bin/flume-ng agent --conf ./conf/ --conf-file conf/seqgen.conf \\ --name SeqGenAgent \\ -Dflume.root.logger = INFO,console Vaciando HDFS Si queremos eliminar los ficheros generados en HDFS, recuerda que puedes realizar un borrado recursivo mediante el comando: hdfs dfs -rm -r /user/iabd/flume Si comprobamos por ejemplo el contenido de la carpeta ( hdfs dfs -ls /user/iabd/flume/seqgen_data ) veremos que se han generado m\u00faltiples archivos: Found 10 items -rw-r--r-- 1 iabd supergroup 1402 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 -rw-r--r-- 1 iabd supergroup 1368 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740934 -rw-r--r-- 1 iabd supergroup 1350 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740935 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740936 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740937 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740938 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740939 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740940 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740941 -rw-r--r-- 1 iabd supergroup 1280 2021-12-22 16:42 /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740942 Y si comprobamos el contenido del primero ( hdfs dfs -cat /user/iabd/flume/seqgen_data/flume-caso3-seqgen.1640187740933 ) veremos como contiene la secuencia generada: 0 1 2 3 ...","title":"Caso 3a - Almacenando en HDFS"},{"location":"apuntes/bdaplicado03flume.html#caso-3b-de-netcat-a-hdfs","text":"Ahora vamos a crear otro ejemplo de generaci\u00f3n de informaci\u00f3n, pero esta vez, en vez que utilizar la memoria del servidor como canal, vamos a utilizar el sistema de archivos. Adem\u00e1s, para generar la informaci\u00f3n nos basamos en una fuente Netcat , en la cual debemos especificar un puerto de escucha. Mediante esta fuente, Flume quedar\u00e1 a la escucha en dicho puerto y recibir\u00e1 cada l\u00ednea introducida como un evento individual que transferir\u00e1 al canal especificado. En el mismo directorio $FLUME_HOME\\conf , creamos un nuevo fichero con el nombre netcat.conf y creamos otro agente que se va a encargar de generar informaci\u00f3n: netcat.conf # Nombramos a los componentes del agente NetcatAgent.sources = Netcat NetcatAgent.channels = FileChannel NetcatAgent.sinks = HdfsSink # Describimos el origen netcat en localhost:44444 NetcatAgent.sources.Netcat.type = netcat NetcatAgent.sources.Netcat.bind = localhost NetcatAgent.sources.Netcat.port = 44444 NetcatAgent.sources.Netcat.channels = FileChannel # Describimos el destino en HDFS NetcatAgent.sinks.HdfsSink.type = hdfs NetcatAgent.sinks.HdfsSink.hdfs.path = hdfs://iabd-virtualbox:9000/user/iabd/flume/net_data/ NetcatAgent.sinks.HdfsSink.hdfs.writeFormat = Text NetcatAgent.sinks.HdfsSink.hdfs.fileType = DataStream NetcatAgent.sinks.HdfsSink.channel = FileChannel # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAgent.channels.FileChannel.type = file NetcatAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint Lanzamos al agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat.conf \\ --name NetcatAgent \\ -Dflume.root.logger = INFO,console En una nueva pesta\u00f1a introducimos el siguiente comando y escribimos curl telnet://localhost:44444 Una vez conectados, escribimos varias frases con saltos de l\u00ednea. Por cada vez que pulsamos Enter , nos aparecer\u00e1 un OK . Probando Netcat OK Esto parece que funciona m\u00e1s o menos OK A continuaci\u00f3n, nos vamos al navegador web de HDFS ( http://iabd-virtualbox:9870/explorer.html#/user/iabd/flume/net_data ) y comprobamos que se ha creado el fichero: Resultado del flujo Netcat-HDFS","title":"Caso 3b - De Netcat a HDFS"},{"location":"apuntes/bdaplicado03flume.html#caso-4-flujos-encadenados","text":"Es muy com\u00fan definir un pipeline de flujos encadenados, uniendo la salida de un agente a la entrada de otro. Para ello, utilizaremos como enlace un sink - source de tipo Avro . Este dise\u00f1o tambi\u00e9n se conoce como flujo Multi-hop : Encadenando flujos En este caso, vamos a crear un primer agente ( NetcatAvroAgent ) que ingeste datos desde Netcat y los coloque en un sink de tipo Avro . Para ello, creamos el agente netcat-avro.conf : netcat-avro.conf # Nombramos a los componentes del agente NetcatAvroAgent.sources = Netcat NetcatAvroAgent.channels = FileChannel NetcatAvroAgent.sinks = AvroSink # Describimos el origen netcat en localhost:44444 NetcatAvroAgent.sources.Netcat.type = netcat NetcatAvroAgent.sources.Netcat.bind = localhost NetcatAvroAgent.sources.Netcat.port = 44444 # Describimos el destino como Avro en localhost:10003 NetcatAvroAgent.sinks.AvroSink.type = avro NetcatAvroAgent.sinks.AvroSink.hostname = localhost NetcatAvroAgent.sinks.AvroSink.port = 10003 # Unimos el origen y el destino a trav\u00e9s del canal de fichero NetcatAvroAgent.sources.Netcat.channels = FileChannel NetcatAvroAgent.sinks.AvroSink.channel = FileChannel NetcatAvroAgent.channels.FileChannel.type = file NetcatAvroAgent.channels.FileChannel.dataDir = /home/iabd/flume/data NetcatAvroAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint A continuaci\u00f3n, creamos un segundo agente ( AvroHdfsAgent ) que utilice como fuente Avro y que almacene los eventos recibidos en HDFS. Para ello, creamos el agente avro-hdfs.conf : avro-hdfs.conf # Nombramos a los componentes del agente AvroHdfsAgent.sources = AvroSource AvroHdfsAgent.channels = MemChannel AvroHdfsAgent.sinks = HdfsSink # Describimos el origen como Avro en localhost:10003 AvroHdfsAgent.sources.AvroSource.type = avro AvroHdfsAgent.sources.AvroSource.bind = localhost AvroHdfsAgent.sources.AvroSource.port = 10003 # Describimos el destino HDFS AvroHdfsAgent.sinks.HdfsSink.type = hdfs AvroHdfsAgent.sinks.HdfsSink.hdfs.path = /user/iabd/flume/avro_data/ AvroHdfsAgent.sinks.HdfsSink.hdfs.fileType = DataStream AvroHdfsAgent.sinks.HdfsSink.hdfs.writeFormat = Text # Unimos el origen y el destino AvroHdfsAgent.sources.AvroSource.channels = MemChannel AvroHdfsAgent.sinks.HdfsSink.channel = MemChannel AvroHdfsAgent.channels.MemChannel.type = memory Primero lanzamos este \u00faltimo agente, para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en nueva pesta\u00f1a, lanzamos el primer agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/netcat-avro.conf \\ --name NetcatAvroAgent \\ -Dflume.root.logger = INFO,console Finalmente, en otro terminal, escribimos mensajes Netcat accediendo a curl telnet://localhost:44444 . Si acced\u00e9is a la carpeta /user/iabd/flume/avro_data en HDFS podremos comprobar c\u00f3mo se van creando archivos que agrupan los mensajes enviados.","title":"Caso 4 - Flujos encadenados"},{"location":"apuntes/bdaplicado03flume.html#caso-5-flujo-multi-agente","text":"Para demostrar como varios agentes pueden conectarse entre s\u00ed, vamos a realizar un caso de uso donde vamos a leer informaci\u00f3n de tres fuentes distintas: una fuente de Netcat con un canal basado en ficheros, otra que realice spooling de una carpeta (vigile una carpeta y cuando haya alg\u00fan archivo, lo ingeste y lo elimine) utilizando un canal en memoria y un tercero que ejecute un comando utilizando tambi\u00e9n un canal en memoria. Como agente de consolidaci\u00f3n que una la informaci\u00f3n de las tres fuentes de datos, vamos a reutilizar el agente AvroHdfsAgent que hemos creado en el caso de uso anterior. Consolidando flujos Para ello, vamos a definir los agentes en siguiente fichero de configuraci\u00f3n multiagent-avro.conf ): multiagent-avro.conf # Nombramos las tres fuentes con sus tres sumideros MultiAgent.sources = Netcat Spooldir Exec MultiAgent.channels = FileChannel MemChannel1 MemChannel2 MultiAgent.sinks = AvroSink1 AvroSink2 AvroSink3 # Describimos el primer agente MultiAgent.sources.Netcat.type = netcat MultiAgent.sources.Netcat.bind = localhost MultiAgent.sources.Netcat.port = 10004 # Describimos el segundo agente MultiAgent.sources.Spooldir.type = spooldir MultiAgent.sources.Spooldir.spoolDir = /home/iabd/flume/spoolDir MultiAgent.sources.Spooldir.deletePolicy = immediate # Describimos el tercer agente MultiAgent.sources.Exec.type = exec MultiAgent.sources.Exec.command = cat /home/iabd/datos/empleados.txt # Describimos los tres destinos como Avro en localhost:10003 MultiAgent.sinks.AvroSink1.type = avro MultiAgent.sinks.AvroSink1.hostname = localhost MultiAgent.sinks.AvroSink1.port = 10003 MultiAgent.sinks.AvroSink2.type = avro MultiAgent.sinks.AvroSink2.hostname = localhost MultiAgent.sinks.AvroSink2.port = 10003 MultiAgent.sinks.AvroSink3.type = avro MultiAgent.sinks.AvroSink3.hostname = localhost MultiAgent.sinks.AvroSink3.port = 10003 # Describimos los canales MultiAgent.channels.FileChannel.type = file MultiAgent.channels.FileChannel.dataDir = /home/iabd/flume/data MultiAgent.channels.FileChannel.checkpointDir = /home/iabd/flume/checkpoint MultiAgent.channels.MemChannel1.type = memory MultiAgent.channels.MemChannel2.type = memory # Unimos los or\u00edgenes y destinos MultiAgent.sources.Netcat.channels = FileChannel MultiAgent.sources.Spooldir.channels = MemChannel1 MultiAgent.sources.Exec.channels = MemChannel2 MultiAgent.sinks.AvroSink1.channel = FileChannel MultiAgent.sinks.AvroSink2.channel = MemChannel1 MultiAgent.sinks.AvroSink3.channel = MemChannel2 Preparaci\u00f3n Antes de arrancar los agentes, aseg\u00farate de tener creada la carpeta /home/iabd/flume/spoolDir y disponible el recurso /home/iabd/datos/empleados.txt . Igual que en el caso de uso anterior, primero lanzamos el agente consolidador para que Flume quede a la espera de mensajes Avro en localhost:10003 : ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/avro-hdfs.conf \\ --name AvroHdfsAgent \\ -Dflume.root.logger = INFO,console Una vez ha arrancado, en una nueva pesta\u00f1a, lanzamos el multi agente: ./bin/flume-ng agent --conf ./conf/ --conf-file ./conf/multiagent-avro.conf \\ --name MultiAgent \\ -Dflume.root.logger = INFO,console Interceptores Podemos utilizar interceptores para modificar o borrar eventos al vuelo a partir del timestamp , nombre del host , uuid , etc... incluso mediante el uso de una expresi\u00f3n regular. Si quieres profundizar en el tema, el siguiente art\u00edculo detalla los diferentes tipos y configuraciones: https://data-flair.training/blogs/flume-interceptors/ En este caso, para poder probarlo, adem\u00e1s de enviar comandos Netstat en curl telnet://localhost:10004 , prueba a colocar un archivo de texto (por ejemplo, un documento CSV) en /home/iabd/flume/spoolDir .","title":"Caso 5 - Flujo multi-agente"},{"location":"apuntes/bdaplicado03flume.html#actividades","text":"Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Haciendo uso de Sqoop y la base de datos retail_db , importa todos los pedidos de la tabla orders cuyo campo order_status sea COMPLETE . Coloca los datos en user/iabd/sqoop/orders/datos_parquet en formato Parquet, utilizando el tabulador como delimitador de campos y utilizando la compresi\u00f3n Snappy. Deber\u00e1s recuperar 22.899 (\u00bfo 22.902?) registros. Haciendo uso de Sqoop y la base de datos retail_db , importa todos los clientes de la tabla customers cuyo campo state sea CA . Coloca los datos en user/iabd/sqoop/customers/datos_avro en formato Avro, utilizando la compresi\u00f3n bzip2. Deber\u00e1s recuperar las columnas customer_id, customer_fname, customer_lname, customer_state . El resultado contendr\u00e1 2012 registros. Mediante Flume , realiza los caso de uso 3, 4 y 5. (opcional) Haciendo uso de Flume, recupera informaci\u00f3n de Twitter y almac\u00e9nala en HDFS. Para ello, utiliza el Twitter 1% Firehouse source y el HDFS sink . Para ello, necesitar\u00e9is las claves de desarrollo que ya creamos en las sesiones sobre Nifi . Adjunta una captura de pantalla donde se visualice el contenido de uno de los bloques de HDFS. Cuidado con el espacio de almacenamiento Una vez lances el agente, detenlo a los tres segundos para no llenar de datos HDFS.","title":"Actividades"},{"location":"apuntes/bdaplicado03flume.html#referencias","text":"P\u00e1gina oficial de Sqoop Sqoop User Guide Sqoop Tutorial en Tutorialspoint P\u00e1gina oficial de Flume Flume User Guide","title":"Referencias"},{"location":"apuntes/bdaplicado04hive.html","text":"Hive \u00b6 Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un cl\u00faster de Hadoop . Permite leer, escribir y gestionar grandes datasets (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop . Logo de Apache Hive El proyecto lo inici\u00f3 Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os. Hive y Hadoop \u00b6 Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop , Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduce u otras tecnolog\u00edas. Aunque en principio estaba dise\u00f1ado para el procesamiento batch , ahora se integra con frameworks en streaming como Tez y Spark . Ecosistema Hadoop Caracter\u00edsticas \u00b6 Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema , y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase . Para interactuar con Hive utilizaremos HiveQL , el cual es un dialecto de SQL (recuerda que SQL no es sensible a las may\u00fasculas, excepto en la comparaci\u00f3n de cadenas). Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n podemos personalizar el procesamiento de consultas creando un esquema de tabla acorde con nuestros datos, pero sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON o cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive se ejecuta en varios datanodes en paralelo, con varios trabajos MapReduce asociados. Estas operaciones son de tipo batch , por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets. Ventajas \u00b6 Las ventajas de utilizar Hive son: Reduce la complejidad de la programaci\u00f3n MapReduce al usar HiveQL como lenguaje de consulta. Est\u00e1 orientado a aplicaciones de tipo Data Warehouse , con datos est\u00e1ticos, poco cambiantes y sin requisitos de tiempos de respuesta r\u00e1pidos. Permite a los usuarios despreocuparse de en qu\u00e9 formato y d\u00f3nde se almacenan los datos. Incorpora Beeline : una herramienta por l\u00ednea de comandos para realizar consultas con HiveQL . En cambio, Hive no es la mejor opci\u00f3n para consultas en tiempo real o de tipo transaccional. Adem\u00e1s, no est\u00e1 dise\u00f1ado para usarse con actualizaciones de valores al nivel de registro, y el soporte de SQL es limitado. Alternativas \u00b6 Una de las alternativas m\u00e1s populares es Cloudera Impala , el cual utiliza un demonio dedicado en cada datanode del cl\u00faster, de manera que hay un coordinador que reenv\u00eda a cada datanode la consulta a realizar y luego se encarga de unir los datos en el resultado final. Impala utiliza el metastore de Hive y soporta la mayor\u00eda de construcciones de Hive , con lo que la migraci\u00f3n de un sistema a otro es sencilla. Otras alternativas open source son : Presto de Facebook y Apache Drill , con arquitecturas muy similares a Impala . Spark SQL : utiliza Spark como motor de ejecuci\u00f3n y permite utilizar consultas SQL embebidas. La estudiaremos en el \u00faltimo bloque del curso. Pig Apache Pig es una herramienta que abstrae el acceso a MapReduce de forma similar a como lo realiza Hive , pero en vez de SQL, utiliza su propio lenguaje de scripting ( PigLatin ) para expresar los flujos de datos. Actualmente ha perdido uso en detrimento de Hive / Impala y de Spark . Ten\u00e9is una peque\u00f1a introducci\u00f3n en https://www.analyticsvidhya.com/blog/2021/08/an-introduction-to-apache-pig-for-absolute-beginners/ . Componentes \u00b6 A continuaci\u00f3n podemos ver un gr\u00e1fico que relaciona los diferentes componentes de Hive y define su arquitectura: Arquitectura de Apache Hive Hive Server \u00b6 HiveServer 2 (HS2) es la \u00faltima versi\u00f3n del servicio. Se compone de una interfaz que permite a clientes externos ejecutar consultas contra Apache Hive y obtener los resultados. Est\u00e1 basado en Thrift RPC y soporta clientes concurrentes. Para arrancar el servidor, ejecutaremos el comando hiveserver2 . A este servidor nos conectaremos mediante la herramienta Beeline (Beeline CLI) con el comando beeline . Hive Metastore \u00b6 Es el repositorio central para los metatados de Hive , y se almacena en una base de datos relacional como MySQL , PostgreSQL o Apache Derby (embebida). Mantiene los metadatos, las tablas y sus tipos mediante Hive DDL ( Data Definition Language ). Adem\u00e1s, el sistema se puede configurar para que tambi\u00e9n almacene estad\u00edsticas de las operaciones y registros de autorizaci\u00f3n para optimizar las consultas. En las \u00faltimas versiones de Hive , este componente se puede desplegar de forma remota e independiente, para no compartir la misma JVM con HiveServer . Dentro del metastore podemos encontrar el Hive Catalog ( HCatalog ), que permite acceder a sus metadatos, actuando como una API. Al poder desplegarse de forma aislada e independiente, permite que otras aplicaciones hagan uso del schema sin tener que desplegar el motor de consultas de Hive. As\u00ed pues, al Metastore podremos acceder mediante HiveCLI , o a trav\u00e9s del Hive Server mediante una conexi\u00f3n remota mediante Beeline . Beeline \u00b6 Hive incorpora Beeline , el cual act\u00faa como un cliente basado en JDBC para hacer consultas por l\u00ednea de comandos contra el Hive Server , sin necesitar las dependencias de Hive . Por otro lado, tambi\u00e9n podemos utilizar Hive CLI , un cliente basado en Apache Thrift , que usa los mismos drivers que Hive . Apache Tez Hive 3 deja de soportar MapReduce . Apache Tez lo reemplaza como el motor de ejecuci\u00f3n por defecto, de manera que mejora el rendimiento y se ejecuta sobre Hadoop Yarn , que encola y planifica los trabajos en el cl\u00faster. Adem\u00e1s de Tez , Hive tambi\u00e9n puede utilizar Apache Spark como motor de ejecuci\u00f3n. Para indicar que queremos ejecutar Tez como motor de ejecuci\u00f3n, ejecutar\u00edamos el siguiente comando: SET hive.execution.engine=tez; En nuestro caso no tenemos Tez instalado en la m\u00e1quina virtual, quedando fuera del alcance del presente curso. Tipos de datos \u00b6 Los tipos de datos que podemos emplear en Hive son muy similares a los que se utilizan en el DDL de SQL. Los tipos simples m\u00e1s comunes son STRING e INT , aunque podemos utilizar otros tipos como TINYINT , BIGINT , DOUBLE , DATE , TIMESTAMP , etc... Para realizar una conversi\u00f3n explicita de tipos, por ejemplo de un tipo texto a uno num\u00e9rico, hay que utilizar la funci\u00f3n CAST : select CAST ( '1' as INT ) from tablaPruebas ; Respecto a los tipos compuestos, tenemos tres tipos: arrays mediante el tipo ARRAY , para agrupar elementos del mismo tipo: [\"manzana\", \"pera\", \"naranja] . mapas mediante el tipo MAP , para definir parejas de clave-valor: {1: \"manzana\", 2: \"pera\"} estructuras mediante el tipo STRUCT , para definir estructuras con propiedades: {\"fruta\": \"manzana\", \"cantidad\": 1, \"tipo\": \"postre\"} . Instalaci\u00f3n y configuraci\u00f3n \u00b6 M\u00e1quina virtual Los siguientes pasos no son necesarios ya que nuestra m\u00e1quina virtual ya tiene Hive instalado y configurado correctamente. Si quieres hacer tu propia instalaci\u00f3n sigue los siguientes pasos de la documentaci\u00f3n oficial. Una vez instalado, vamos a configurarlo. Para ello, debemos crear los ficheros de configuraci\u00f3n a partir de las plantilla que ofrece Hive . Para ello, desde la carpeta $HIVE_HOME/conf , ejecutaremos los siguientes comandos: cp hive-default.xml.template hive-site.xml cp hive-env.sh.template hive-env.sh cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties cp hive-log4j2.properties.template hive-log4j2.properties Modificamos el fichero hive.env.sh para incluir dos variables de entorno con las rutas de Hadoop y la configuraci\u00f3n de Hive hive.env.sh export HADOOP_HOME = /opt/hadoop-3.3.1 export HIVE_CONF_DIR = /opt/hive-3.1.2/conf Para que funcione la ingesta de datos en Hive mediante Sqoop , necesitamos a\u00f1adir una librer\u00eda a Sqoop : cp $HIVE_HOME /lib/hive-common-3.1.2.jar $SQOOP_HOME /lib Preparamos HDFS para crear la estructura de archivos: hdfs dfs -mkdir /tmp hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /tmp hdfs dfs -chmod g+w /user/hive/warehouse Para el metastore , como en nuestra m\u00e1quina virtual tenemos un servidor de MariaDB corriendo, vamos a reutilizarlo. La mayor\u00eda de ejemplos que hay en internet y la diferente bibliograf\u00eda, utilizan DerbyDB como almac\u00e9n (ya que no requiere una instalaci\u00f3n extra). As\u00ed pues, creamos el almac\u00e9n mediante: schematool -dbType mysql -initSchema Modificamos el fichero de configuraci\u00f3n hive-site.xml y configuramos : hive-site.xml <!-- nuevas propiedades --> <property> <name> system:java.io.tmpdir </name> <value> /tmp/hive/java </value> </property> <property> <name> system:user.name </name> <value> ${user.name} </value> </property> <property> <name> datanucleus.schema.autoCreateAll </name> <value> true </value> </property> <!-- propiedades existentes a modificar --> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> com.mysql.jdbc.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> iabd </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> iabd </value> </property> Hola Mundo \u00b6 Si entramos a nuestro $HIVE_HOME podemos comprobar con tenemos las siguientes herramientas: hive : Herramienta cliente beeline : Otra herramienta cliente hiserver2 : Nos permite arrancar el servidor de Hive schematool : Nos permite trabajar contra la base de datos de metadatos (Metastore) Una vez arrancado Hadoop y YARN , vamos a arrancar Hive mediante el cliente local: hive Y una vez dentro, podemos comprobar las bases de datos existentes para ver que todo se configur\u00f3 correctamente show databases ; Si quisi\u00e9ramos ejecutar un script, podemos hacerlo desde el propio comando hive con la opci\u00f3n -f : hive -f script.sql Adem\u00e1s, tenemos la opci\u00f3n de pasar una consulta desde la propia l\u00ednea de comandos mediante la opci\u00f3n -e : hive -e ' select * from tablaEjemplo ` Acceso remoto \u00b6 HiveServer2 (desde Hive 0.11) tiene su propio cliente conocido como Beeline . En entornos reales, el cliente Hive est\u00e1 en desuso a favor de Beeline , por la falta de m\u00faltiples usuarios, seguridad y otras caracter\u00edsticas de HiveServer2. Arrancamos HiveServer2 y Beeline en dos pesta\u00f1as diferentes mediante los comandos hiveserver2 y beeline . Una vez dentro de Beeline , nos conectamos al servidor: !connect jdbc:hive2://iabd-virtualbox:10000 Al conectarnos, tras introducir iabd como usuario y contrase\u00f1a, obtendremos un interfaz similar al siguiente: Beeline version 3.1.2 by Apache Hive beeline> !connect jdbc:hive2://iabd-virtualbox:10000 Connecting to jdbc:hive2://iabd-virtualbox:10000 Enter username for jdbc:hive2://iabd-virtualbox:10000: iabd Enter password for jdbc:hive2://iabd-virtualbox:10000: **** Connected to: Apache Hive (version 3.1.2) Driver: Hive JDBC (version 3.1.2) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://iabd-virtualbox:10000> Dentro de Beeline , en cualquier momento podemos ejecutar el comando help que nos mostrar\u00e1 todos los comandos disponibles. Si nos fijamos, adem\u00e1s de las comandos del cliente hive, tenemos los comandos beeline que empiezan por el s\u00edmbolo de exclamaci\u00f3n ! : 0: jdbc:hive2://iabd-virtualbox:10000> help !addlocaldriverjar Add driver jar file in the beeline client side. !addlocaldrivername Add driver name that needs to be supported in the beeline client side. !all Execute the specified SQL against all the current connections !autocommit Set autocommit mode on or off !batch Start or execute a batch of statements ... Otra forma de trabajar, para arrancar en el mismo proceso Beeline y HiveServer2 para pruebas/desarrollo y tener una experiencia similar al cliente Hive accediendo de forma local, podemos ejecutar el siguiente comando: beeline -u jdbc:hive2:// Mediante la interfaz gr\u00e1fica de Hive Server UI a la cual podemos acceder mediante http://localhost:10002 podemos monitorizar los procesos ejecutados por HiveServer2 : Monitorizaci\u00f3n mediante Hive Server UI Caso de uso 1: Creaci\u00f3n y borrado de tablas \u00b6 Para este caso de uso, vamos a utilizar la base de datos retail_db que ya utilizamos en las actividades de la sesi\u00f3n anterior. Para empezar, vamos a cargar en HDFS los datos de los clientes que contiene la tabla customer . Mediante Sqoop , ejecutamos el siguiente comando: sqoop import --connect \"jdbc:mysql://localhost/retail_db\" \\ --username iabd --password iabd \\ --table customers --target-dir /user/iabd/hive/customer \\ --fields-terminated-by '|' --delete-target-dir \\ --columns \"customer_id,customer_fname,customer_lname,customer_city\" Una vez nos hemos conectado con el cliente hive o mediante beeline , creamos una base de datos llamada iabd : create database iabd ; Nos conectamos a la base de datos que acabamos de crear: use iabd ; default Si olvidamos el comando use , se utilizar\u00e1 la base de datos default , la cual reside en /user/hive/warehouse como ra\u00edz en HDFS. A continuaci\u00f3n, vamos a crear una tabla que almacene el identificador, nombre, apellido y ciudad de los clientes (como puedes observar, la sintaxis es similar a SQL): CREATE TABLE customers ( custId INT , fName STRING , lName STRING , city STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/user/iabd/hive/customer' ; Y ya podemos realizar algunas consultas: select * from customers limit 5 ; select count ( * ) from customers ; En ocasiones necesitamos almacenar la salida de una consulta Hive en una nueva tabla. Las definiciones de las columnas de la nueva tabla se deriva de las columnas recuperadas en la consulta. Para ello, usaremos el comando create table-as select : CREATE TABLE customers_new as SELECT * from customers ; En el caso de la consulta falle por alg\u00fan motivo, la tabla no se crear\u00eda. Otra posibilidad es crear una tabla con la misma estructura que otra ya existente (pero sin datos): CREATE TABLE customers2 LIKE customers ; En cualquier momento podemos obtener informaci\u00f3n de la tabla: describe customers_new ; describe formatted customers_new ; Si empleamos la forma larga, obtendremos mucha m\u00e1s informaci\u00f3n. Por ejemplo, si nos fijamos, vemos que la localizaci\u00f3n de la nueva tabla ya no es /user/iabd/hive/customer sino hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/customers_new . Esto se debe a que en vez de crear una tabla enlazada a un recurso de HDFS ya existente, ha creado una copia de los datos en el propio almac\u00e9n de Hive (hemos pasado de una tabla externa a una interna). Igual que las creamos, las podemos eliminar: drop table customers_new ; drop table customers2 ; Si ejecutamos el comando !tables (o show tables en el cliente hive ) veremos que ya no aparecen dichas tablas. En el caso de que queramos eliminar una base de datos, de la misma manera que en SQL, ejecutar\u00edamos el comando drop database iabd; . Caso de uso 2: Insertando datos \u00b6 Para insertar datos en las tablas de Hive lo podemos hacer de varias formas: Cargando los datos mediante sentencias LOAD DATA . Insertando los datos mediante sentencias INSERT . Cargando los datos directamente mediante Sqoop o alguna herramienta similar. Cargando datos \u00b6 Para cargar datos se utiliza la sentencia LOAD DATA . Si quisi\u00e9ramos volver a cargar los datos desde HDFS utilizaremos: LOAD DATA INPATH '/user/iabd/hive/customer' overwrite into table customers ; Si en cambio vamos a cargar los datos desde un archivo local a nuestro sistema de archivos a\u00f1adiremos LOCAL : LOAD DATA LOCAL INPATH '/home/iabd/datos' overwrite into table customers ; Insertando datos \u00b6 Aunque podemos insertar datos de forma at\u00f3mica (es decir, registro a registro mediante INSERT INTO TABLE ... VALUES ), realmente las inserciones que se realizan en Hive se hacen a partir de los datos de otras tablas mediante el comando insert-select a modo de ETL: INSERT OVERWRITE TABLE destino SELECT col1 , col2 FROM fuente ; Mediante la opci\u00f3n OVERWRITE , en cada ejecuci\u00f3n se vac\u00eda la tabla y se vuelve a rellenar. Si no lo indicamos o utilizamos INTO , los datos se a\u00f1adir\u00edan a los ya existentes. Si necesitamos insertar datos en m\u00faltiples tablas a la vez lo haremos mediante el comando from-insert : FROM fuente INSERT OVERWRITE TABLE destino1 SELECT col1 , col2 INSERT OVERWRITE TABLE destino2 SELECT col1 , col3 Por ejemplo, vamos a crear un par de tablas con la misma estructura de clientes, pero para almacenar los clientes de determinadas ciudades: CREATE TABLE customers_brooklyn LIKE customers ; CREATE TABLE customers_caguas LIKE customers ; Y a continuaci\u00f3n rellenamos ambas tablas con sus clientes; FROM customers INSERT OVERWRITE TABLE customers_brooklyn SELECT custId , fName , lName , city WHERE city = \"Brooklyn\" INSERT OVERWRITE TABLE customers_caguas SELECT custId , fName , lName , city WHERE city = \"Caguas\" ; Ingestando datos \u00b6 Tal como vimos en la sesi\u00f3n anterior , podemos ingestar los datos en Hive haciendo uso de Sqoop (tambi\u00e9n lo podemos hacer con Flume o Nifi ). Por ejemplo, vamos a ingestar los datos de la tabla orders de la base de datos MariaDB que tenemos instalada en nuestra m\u00e1quina virtual. En el comando de Sqoop le indicamos que dentro de Hive lo ingeste en la base de datos iabd y que cree una tabla llamada orders : sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = orders --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table orders Una vez realizada la ingesta, podemos comprobar que los datos est\u00e1n dentro de Hive (en una tabla interna/gestionada): hdfs dfs -ls /user/hive/warehouse/iabd.db/orders Y si entramos a hive , podemos consultar sus datos: select * from orders limit 10 ; Extrayendo datos insertados \u00b6 Combinando los comandos de HQL y HDFS podemos extraer datos a ficheros locales o remotos: # A\u00f1adiendo contenido local hive -e \"use iabd; select * from customers\" >> prueba1 # Sobreescribiendo contenido local hive -e \"use iabd; select * from customers\" > prueba2 # A\u00f1adiendo contenido HDFS hive -e \"use iabd; select * from customers\" | hdfs dfs --appendToFile /tmp/prueba3 # Sobreescribiendo contenido hive -e \"use iabd; select * from customers\" | hdfs dfs --put -f /tmp/prueba4 Si indicamos la propiedad set hive.cli.print.header=true antes de la consulta, tambi\u00e9n nos mostrar\u00e1 el encabezado de las columnas. Esto puede ser \u00fatil si queremos generar un csv con el resultado de una consulta: hive -e 'use iabd; set hive.cli.print.header=true; select * from customers' | \\ sed 's/[\\t]/,/g' > fichero.csv \u00bfY usar INSERT LOCAL ? Mediante INSERT LOCAL podemos escribir el resultado de una consulta en nuestro sistema de archivos, fuera de HDFS. El problema es que si hay muchos datos crear\u00e1 m\u00faltiples ficheros y necesitaremos concatenarlos para tener un \u00fanico resultado: insert overwrite local directory '/home/iabd/datos' row format delimited field terminated by ',' select * from customers ; Caso de uso 3: Consultas con join \u00b6 En este caso de uso vamos a trabajar con los datos de clientes que hemos cargado en los dos casos anteriores, tanto en customers como en orders . Si queremos relacionar los datos de ambas tablas, tenemos que hacer un join entre la clave ajena de orders ( order_customer_id ) y la clave primaria de customers ( custid ): hive > describe customers ; OK custid int fname string lname string city string Time taken : 0 . 426 seconds , Fetched : 4 row ( s ) hive > describe orders ; OK order_id int order_date string order_customer_id int order_status string Time taken : 0 . 276 seconds , Fetched : 4 row ( s ) Para ello, para obtener la ciudad de cada pedido, podemos ejecutar la consulta: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Outer join \u00b6 De la misma manera que en cualquier SGBD, podemos realizar un outer join , tanto left como right o full . Por ejemplo, vamos a obtener para cada cliente, cuantos pedidos ha realizado: select c . custid , count ( order_id ) from customers c join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Si queremos que salgan todos los clientes, independientemente de que tengan pedidos, deberemos realizar un left outer join : select c . custid , count ( order_id ) from customers c left outer join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Semi-joins \u00b6 Si quisi\u00e9ramos obtener las ciudades de los clientes que han realizado pedidos podr\u00edamos realizar la siguiente consulta: select distinct c . city from customers c where c . custid in ( select order_customer_id from orders ); Mediante un semi-join podemos obtener el mismo resultado: select distinct city from customers c left semi join orders o on ( c . custid = o . order_customer_id ) Hay que tener en cuenta la restricci\u00f3n que las columnas de la tabla de la derecha s\u00f3lo pueden aparecer en la clausula on , nunca en la expresi\u00f3n select . Map joins \u00b6 Consideramos la consulta inicial de join: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Si una tabla es suficientemente peque\u00f1a para caber en memoria, tal como nos ocurre con nuestros datos, Hive puede cargarla en memoria para realizar el join en cada uno de los mappers . Esto se conoce como un map join . El job que ejecuta la consulta no tiene reducers , con lo que esta consulta no funcionar\u00e1 para un right o right outer join , ya que la ausencias de coincidencias s\u00f3lo se puede detectar en los pasos de agregaci\u00f3n ( reduce ). En el caso de utilizar map joins con tablas organizadas en buckets , la sintaxis es la misma, s\u00f3lo siendo necesario activarlo mediante la propiedad hive.optimize.bucketmapjoin : SET hive.optimize.bucketmapjoin = true ; Comandos \u00b6 Mediante los casos de uso realizados hasta ahora, hemos podido observar c\u00f3mo para interactuar con Hive se utilizan comandos similares a SQL. Es conveniente consultar la siguiente cheatsheet : http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf Adem\u00e1s Hive viene con un conjunto de funciones predefinidas para tratamiento de cadenas, fechas, funciones estad\u00edsticas, condicionales, etc... las cuales puedes consultar en la documentaci\u00f3n oficial . Mediante el comando show functions podemos obtener una lista de las funciones. Si queremos m\u00e1s informaci\u00f3n sobre una determinada funci\u00f3n utilizaremos el comando describe function nombreFuncion : hive > describe function length ; length ( str | binary ) - Returns the length of str or number of bytes in binary data Caso de uso 4: Tabla interna \u00b6 Hive permite crear tablas de dos tipos: tabla interna o gestionada: Hive gestiona la estructura y el almacenamiento de los datos. Para ello, crea los datos en HDFS. Al borrar la tabla de Hive , se borra la informaci\u00f3n de HDFS. tabla externa : Hive define la estructura de los datos en el metastore , pero los datos ya residen previamente en HDFS. Al borrar la tabla de Hive , no se eliminan los datos de HDFS. Se emplea cuando compartimos datos almacenados en HDFS entre diferentes herramientas. En este caso de uso, vamos a centrarnos en una tabla interna. Supongamos el siguiente fichero con datos de empleados: empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Podemos observar como se utiliza | como separador de campos. Analizando los datos, vemos que tenemos los siguientes campos: Nombre Centros de trabajo (array con las ciudades) Sexo y edad Destreza y puntuaci\u00f3n Departamento y cargo Creamos la siguiente tabla interna en Hive mediante el siguiente comando: CREATE TABLE IF NOT EXISTS empleados_interna ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT 'Esto es una tabla interna' ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' ; La sintaxis es muy similar a SQL, destacando las siguientes opciones: ROW FORMAT DELIMITED : cada registro ocupa una l\u00ednea FIELDS TERMINATED BY '|' : define el | como separador de campos COLLECTION ITEMS TERMINATED BY ',' : define la coma como separador de los arrays / estructuras MAP KEYS TERMINATED BY ':' : define los dos puntos como separador utilizado en los mapas. Si queremos comprobar la estructura de la tabla mediante el comando show create table empleados_interna veremos las opciones que hemos indicado: + ----------------------------------------------------+ | createtab_stmt | + ----------------------------------------------------+ | CREATE TABLE ` empleados_interna ` ( | | ` name ` string , | | ` work_place ` array < string > , | | ` sex_age ` struct < sex : string , age : int > , | | ` skills_score ` map < string , int > , | | ` depart_title ` map < string , array < string >> ) | | COMMENT 'Esto es una tabla interna' | | ROW FORMAT SERDE | | 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' | | WITH SERDEPROPERTIES ( | | 'collection.delim' = ',' , | | 'field.delim' = '|' , | | 'mapkey.delim' = ':' , | | 'serialization.format' = '|' ) | | STORED AS INPUTFORMAT | | 'org.apache.hadoop.mapred.TextInputFormat' | | OUTPUTFORMAT | | 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' | | LOCATION | | 'hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/empleados_interna' | | TBLPROPERTIES ( | | 'bucketing_version' = '2' , | | 'transient_lastDdlTime' = '1647432129' ) | + ----------------------------------------------------+ A continuaci\u00f3n, vamos a cargar los datos del fichero empleados.txt , el cual colocaremos en nuestra carpeta de Descargas : LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_interna ; Comprobamos que los datos se han cargado correctamente: select * from empleados_interna ; Y obtenemos: +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | empleados_interna.name | empleados_interna.work_place | empleados_interna.sex_age | empleados_interna.skills_score | empleados_interna.depart_title | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | Michael | [\"Montreal\",\"Toronto\"] | {\"sex\":\"Male\",\"age\":30} | {\"DB\":80} | {\"Product\":[\"Developer\",\"Lead\"]} | | Will | [\"Montreal\"] | {\"sex\":\"Male\",\"age\":35} | {\"Perl\":85} | {\"Product\":[\"Lead\"],\"Test\":[\"Lead\"]} | | Shelley | [\"New York\"] | {\"sex\":\"Female\",\"age\":27} | {\"Python\":80} | {\"Test\":[\"Lead\"],\"COE\":[\"Architect\"]} | | Lucy | [\"Vancouver\"] | {\"sex\":\"Female\",\"age\":57} | {\"Sales\":89,\"HR\":94} | {\"Sales\":[\"Lead\"]} | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ Y si nos abrimos otra pesta\u00f1a, mediante HDFS, comprobamos que tenemos los datos: hdfs dfs -ls /user/hive/warehouse/curso.db/empleados_interna hdfs dfs -cat /user/hive/warehouse/curso.db/empleados_interna/empleados.txt Y obtenemos: Michael|Montreal,Toronto|Male,30|DB:80|Product:DeveloperLead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Consultando datos compuestos \u00b6 Si nos fijamos bien en la tabla, tenemos tres columnas con diferentes datos compuestos. work_place , con un ARRAY<string> sex_age con una STRUCT<sex:string,age:int> skills_score con un MAP<string,int> , depart_title con un MAP<STRING,ARRAY<STRING>> Si queremos obtener los datos del array , podemos realizar las siguientes consultas: Todos los elementos Elementos individuales Cantidad de elementos Explode array Todos los lugares de trabajo: select name , work_place from empleados_interna ; Resultado: +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [ \"Montreal\" , \"Toronto\" ] | | Will | [ \"Montreal\" ] | | Shelley | [ \"New York\" ] | | Lucy | [ \"Vancouver\" ] | +----------+-------------------------+ Los dos primeros puestos de trabajo: select work_place [ 0 ] as lugar1 , work_place [ 1 ] as lugar2 from empleados_interna ; Resultado: +------------+----------+ | lugar1 | lugar2 | +------------+----------+ | Montreal | Toronto | | Montreal | NULL | | New York | NULL | | Vancouver | NULL | +------------+----------+ Cantidad de lugares de trabajo: select size ( work_place ) as cantLugares from empleados_interna ; Resultado: +--------------+ | cantlugares | +--------------+ | 2 | | 1 | | 1 | | 1 | +--------------+ Empleados y lugares de trabajo mostrados uno por fila: select name , lugar from empleados_interna lateral view explode ( work_place ) e2 as lugar ; Para ello hemos creado una vista lateral y con la funci\u00f3n explode desenrollamos el array. Resultado: +----------+------------+ | name | lugar | +----------+------------+ | Michael | Montreal | | Michael | Toronto | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+------------+ En el caso de la estructura con el sexo y la edad podemos realizar las siguientes consultas Todos los elementos Elementos individuales Todas las estructuras de sexo/edad: select sex_age from empleados_interna ; Resultado: +----------------------------+ | sex_age | +----------------------------+ | { \"sex\" : \"Male\" , \"age\" : 30 } | | { \"sex\" : \"Male\" , \"age\" : 35 } | | { \"sex\" : \"Female\" , \"age\" : 27 } | | { \"sex\" : \"Female\" , \"age\" : 57 } | +----------------------------+ Sexo y edad por separado select sex_age . sex as sexo , sex_age . age as edad from empleados_interna ; Resultado: +---------+-------+ | sexo | edad | +---------+-------+ | Male | 30 | | Male | 35 | | Female | 27 | | Female | 57 | +---------+-------+ Respecto al mapa con las habilidades y sus puntuaciones: Todos los elementos Elementos individuales Claves y valores Todas las habilidades como un mapa: select skills_score from empleados_interna ; Resultado: +-----------------------+ | skills_score | +-----------------------+ | { \"DB\" : 80 } | | { \"Perl\" : 85 } | | { \"Python\" : 80 } | | { \"Sales\" : 89 , \"HR\" : 94 } | +-----------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , skills_score [ \"DB\" ] as db , skills_score [ \"Perl\" ] as perl , skills_score [ \"Python\" ] as python , skills_score [ \"Sales\" ] as ventas , skills_score [ \"HR\" ] as hr from empleados_interna ; Resultado: +----------+-------+-------+---------+---------+-------+ | name | db | perl | python | ventas | hr | +----------+-------+-------+---------+---------+-------+ | Michael | 80 | NULL | NULL | NULL | NULL | | Will | NULL | 85 | NULL | NULL | NULL | | Shelley | NULL | NULL | 80 | NULL | NULL | | Lucy | NULL | NULL | NULL | 89 | 94 | +----------+-------+-------+---------+---------+-------+ Claves y valores de las habilidades: select name , map_keys ( skills_score ) as claves , map_values ( skills_score ) as valores from empleados_interna ; Resultado: +----------+-----------------+----------+ | name | claves | valores | +----------+-----------------+----------+ | Michael | [ \"DB\" ] | [ 80 ] | | Will | [ \"Perl\" ] | [ 85 ] | | Shelley | [ \"Python\" ] | [ 80 ] | | Lucy | [ \"Sales\" , \"HR\" ] | [ 89 , 94 ] | +----------+-----------------+----------+ Y finalmente, con el mapa de departamentos que contiene un array: Todos los elementos Elementos individuales Primer elemento de los elementos individuales Toda la informaci\u00f3n sobre los departamentos: select depart_title from empleados_interna ; Resultado: +----------------------------------------+ | depart_title | +----------------------------------------+ | { \"Product\" : [ \"Developer\" , \"Lead\" ]} | | { \"Product\" : [ \"Lead\" ], \"Test\" : [ \"Lead\" ]} | | { \"Test\" : [ \"Lead\" ], \"COE\" : [ \"Architect\" ]} | | { \"Sales\" : [ \"Lead\" ]} | +----------------------------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , depart_title [ \"Product\" ] as product , depart_title [ \"Test\" ] as test , depart_title [ \"COE\" ] as coe , depart_title [ \"Sales\" ] as sales from empleados_interna ; Resultado: +----------+-----------------------+-----------+----------------+-----------+ | name | product | test | coe | sales | +----------+-----------------------+-----------+----------------+-----------+ | Michael | [ \"Developer\" , \"Lead\" ] | NULL | NULL | NULL | | Will | [ \"Lead\" ] | [ \"Lead\" ] | NULL | NULL | | Shelley | NULL | [ \"Lead\" ] | [ \"Architect\" ] | NULL | | Lucy | NULL | NULL | NULL | [ \"Lead\" ] | +----------+-----------------------+-----------+----------------+-----------+ Primera habilidad de producto y pruebas de cada empleado: select name , depart_title [ \"Product\" ][ 0 ] as product0 , depart_title [ \"Test\" ][ 0 ] as test0 from empleados_interna ; Resultado: +----------+-------------+--------+ | name | product0 | test0 | +----------+-------------+--------+ | Michael | Developer | NULL | | Will | Lead | Lead | | Shelley | NULL | Lead | | Lucy | NULL | NULL | +----------+-------------+--------+ Caso de uso 5: Tabla externa \u00b6 En este caso de uso vamos a repetir la misma estructura de la tabla del caso anterior, pero en esta ocasi\u00f3n en una tabla externa. De esta manera, al borrar la tabla de Hive , no se borra la informaci\u00f3n de HDFS. Para ello, \u00fanicamente hemos de a\u00f1adir la palabra EXTERNAL a la instrucci\u00f3n CREATE TABLE y la clausula LOCATION para indicar la ruta de HDFS donde se encuentran los datos: CREATE EXTERNAL TABLE IF NOT EXISTS empleados_externa ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT \"Esto es una tabla externa\" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LOCATION \"/user/iabd/hive/empleados_externa\" ; Realizamos la misma carga que en el caso anterior: LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_externa ; Si hacemos una consulta sobre la tabla para ver que est\u00e1n todos los campos obtendremos la misma informaci\u00f3n que antes: SELECT * FROM empleados_externa ; Interna o externa Como normal general, si todo nuestro procesamiento lo hacemos mediante Hive , es m\u00e1s c\u00f3modo utilizar tablas internas. Si no es as\u00ed, y otras herramientas acceden al mismo dataset, es mejor utilizar tablas externas. Un patr\u00f3n de uso muy com\u00fan es utilizar una tabla externa para acceder al dataset inicial almacenado en HDFS (creado por otro proceso), y posteriormente crear una transformaci\u00f3n en Hive para mover los datos a una tabla interna. Estructuras de datos en Hive \u00b6 Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: tablas, particiones y buckets . Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Acabamos de ver en el apartado anterior que Hive permite crear tablas externas, similares a las tablas en una base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS. Particiones \u00b6 Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento cuando utilizamos consultas que filtran los datos mediante la cl\u00e1usula where . Por ejemplo, si estamos almacenado ficheros de log (tanto la l\u00ednea del log como su timestamp ), podemos pensar en agrupar por fecha los diferentes ficheros. Podr\u00edamos a\u00f1adir otra partici\u00f3n para tambi\u00e9n dividirlos por pa\u00edses: CREATE TABLE logs ( ts BIGINT , linea STRING ) PARTITIONED BY ( fecha STRING , pais STRING ); Por ejemplo, para cargar los datos en una partici\u00f3n: LOAD DATA LOCAL INPATH 'input/hive/particiones/log1' INTO TABLE logs PARTITION ( fecha = '2022-01-01' , pais = 'ES' ); A nivel del sistema de fichero, las particiones se traducen en subdirectorios dentro de la carpeta de la tabla. Por ejemplo, tras insertar varios ficheros de logs, podr\u00edamos tener una estructura similar a: /user/hive/warehouse/logs \u251c\u2500\u2500 fecha=2022-01-01/ \u2502 \u251c\u2500\u2500 pais=ES/ \u2502 \u2502 \u251c\u2500\u2500 log1 \u2502 \u2502 \u2514\u2500\u2500 log2 \u2502 \u2514\u2500\u2500 pais=US/ \u2502 \u2514\u2500\u2500 log3 \u2514\u2500\u2500 fecha=2022-01-02/ \u251c\u2500\u2500 pais=ES/ \u2502 \u2514\u2500\u2500 log4 \u2514\u2500\u2500 pais=US/ \u251c\u2500\u2500 log5 \u2514\u2500\u2500 log6 Para averiguar las particiones en Hive , utilizaremos el comando SHOW PARTITIONS : hive> SHOW PARTITIONS logs; fecha=2022-01-01/pais=ES fecha=2022-01-01/pais=US fecha=2022-01-02/pais=ES fecha=2022-01-02/pais=US Hay que tener en cuenta que la definici\u00f3n de columnas de la clausula PARTITIONED BY forman parte de las columnas de la tabla, y se conocen como columnas de partici\u00f3n . Sin embargo, los ficheros de datos no contienen valores para esas columnas, ya que se deriva el nombre del subdirectorio. Podemos utilizar las columnas de partici\u00f3n en las consultas igual que una columna ordinaria. Hive traduce la consulta en la navegaci\u00f3n adecuada para s\u00f3lo escanear las particiones relevantes. Por ejemplo, la siguiente consulta solo escanear\u00e1 los ficheros log1 , log2 y log4 : SELECT ts , fecha , linea FROM logs WHERE pais = 'ES' ; Moviendo datos a una tabla particionada Si queremos mover datos de una tabla ordinaria a una particionada (vaciando los datos de la partici\u00f3n existente): INSERT OVERWRITE TABLE logs PARTITION ( dt = '2022-01-01' ) SELECT col1 , col2 FROM fuente ; Otra posibilidad es utilizar un particionado din\u00e1mico , de manera que las particiones se crean de forma relativa a los datos: INSERT OVERWRITE TABLE logs PARTITION ( dt ) SELECT col1 , col2 FROM fuente ; Para ello, previamente hay que habilitarlo (por defecto est\u00e1 deshabilitado para evitar la creaci\u00f3n de m\u00faltiples particiones sin querer) y configurar el modo no estricto para que no nos obligue a indicar al menos una partici\u00f3n est\u00e1tica: set hive.exec.dynamic.partition = true set hive.exec.dynamic.partition.mode = nonstrict ; Buckets \u00b6 Otro concepto importante en Hive son los buckets . Son particiones hasheadas por una columna/clave, en las que los datos se distribuyen en funci\u00f3n de su valor hash . Existen dos razones por las cuales queramos organizar las tablas (o particiones) en buckets . La primera es para conseguir consultas m\u00e1s eficientes, ya que imponen una estructura extra en las tablas. Los buckets pueden acelerar las operaciones de tipo join si las claves de bucketing y de join coinciden, ya que una clave ajena busca \u00fanicamente en el bucket adecuado de la clave primaria. Debido a los beneficios de los buckets/particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. Para indicar que nuestras tablas utilicen buckets , hemos de emplear la clausula CLUSTERED BY para indicar la columnas y el n\u00famero de buckets (se recomienda que la cantidad de buckets sea potencia de 2): CREATE TABLE usuarios_bucketed ( id INT , nombre STRING ) CLUSTERED BY ( id ) INTO 4 BUCKETS ; Bucketing est\u00e1 muy relacionado con el proceso de carga de datos. Para cargar los datos en una tabla con buckets , debemos bien indicar el n\u00famero m\u00e1ximo de reducers para que coincida con el n\u00famero de buckets , o habilitar el bucketing (esta es la recomendada): set map . reduce . tasks = 4 ; set hive . enforce . bucketing = true ; -- mejor as\u00ed Una vez creada la tabla, se rellena con los datos que tenemos en otra tabla: INSERT OVERWRITE TABLE usuarios_bucketed SELECT * FROM usuarios ; F\u00edsicamente, cada bucket es un fichero de la carpeta con la tabla (o partici\u00f3n). El nombre del fichero no es importante, pero el bucket n es el fichero n\u00famero n . Por ejemplo, si miramos el contenido de la tabla en HDFS tendremos: hive> dfs -ls /user/hive/warehouse/usuarios_bucketed ; 000000_0 000001_0 000002_0 000003_0 El segundo motivo es para obtener un sampling de forma m\u00e1s eficiente. Al trabajar con grandes datasets, normalmente obtenemos una peque\u00f1a fracci\u00f3n del dataset para comprender o refinar los datos. Por ejemplo, podemos obtener los datos de \u00fanicamente uno de los buckets : SELECT * FROM usuarios_bucketed TABLESAMPLE ( BUCKET 1 OUT OF 4 ON id ); Mediante el id del usuario determinamos el bucket (el cual se utiliza para realizar el hash del valor y ubicarlo dentro de uno de los buckets ), de manera que cada bucket contendr\u00e1 de manera eficiente un conjunto aleatorio de usuarios. Resumen \u00b6 A continuaci\u00f3n mostramos en una tabla puntos a favor y en contra de utilizar estas estructuras Particionado + Particionado - Bucketing + Bucketing - Distribuye la carga de ejecuci\u00f3n horizontalmente. Existe la posibilidad de crear demasiadas particiones que contienen muy poco datos Proporciona una respuesta de consulta m\u00e1s r\u00e1pida, al acceder a porciones. El n\u00famero de buckets se define durante la creaci\u00f3n de la tabla -> Los programadores deben cargar manualmente un volumen equilibrado de datos. En la partici\u00f3n tiene lugar la ejecuci\u00f3n m\u00e1s r\u00e1pida de consultas con el volumen de datos bajo. Por ejemplo, la poblaci\u00f3n de b\u00fasqueda de la Ciudad del Vaticano devuelve muy r\u00e1pido en lugar de buscar la poblaci\u00f3n mundial completa. La partici\u00f3n es eficaz para datos de bajo volumen. Pero algunas consultas como agrupar por un gran volumen de datos tardan mucho en ejecutarse. Por ejemplo, agrupar las consultas del mes de Enero tardar\u00e1 m\u00e1s que los viernes de Enero. Al utilizar vol\u00famenes similares de datos en cada partici\u00f3n, los map joins ser\u00e1n m\u00e1s r\u00e1pidos. Supongamos que $HDFS_HIVE contiene la ruta con la ra\u00edz de las tablas internas de Hive, en nuestro caso /user/hive/warehouse . Respecto al nivel de estructura y representaci\u00f3n en carpetas de HDFS tendr\u00edamos: ENTIDAD EJEMPLO UBICACI\u00d3N base de datos iabd $HDFS_HIVE/iabd.db tabla T $HDFS_HIVE/iabd.db/T partici\u00f3n fecha='01012022' $HDFS_HIVE/iabd.db/T/fecha=01012022 bucket columna id $HDFS_HIVE/iabd.db/T/fecha=01012022/000000_0 Caso de uso 6: Particionado y Bucketing \u00b6 A continuaci\u00f3n, vamos a coger los datos de las productos y las categor\u00edas de la base de datos retail_db , y colocarlos en una estructura de Hive particionada y que utilice bucketing . La estructura de las tablas es la siguiente: Relaci\u00f3n entre productos y categor\u00edas El primer paso es traernos los datos de MariaDB a tablas internas haciendo uso de Sqoop : Tabla categories Tabla products sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = categories --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table categories sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = products --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table products El siguiente paso que vamos a realizar es crear en Hive una tabla con el c\u00f3digo del producto, su nombre, el nombre de la categor\u00eda y el precio del producto. Estos datos los vamos a particionar por categor\u00eda y clusterizado en 8 buckets: CREATE TABLE IF NOT EXISTS productos ( id INT , nombre STRING , precio DOUBLE ) PARTITIONED BY ( categoria STRING ) CLUSTERED BY ( id ) INTO 8 BUCKETS ; Y cargamos los datos con una consulta que realice un join de las tablas categories y products con particionado din\u00e1mico (recuerda activarlo mediante set hive.exec.dynamic.partition.mode=nonstrict; ): INSERT OVERWRITE TABLE productos PARTITION ( categoria ) SELECT p . product_id as id , p . product_name as nombre , p . product_price as precio , c . category_name as categoria FROM products p join categories c on ( p . product_category_id = c . category_id ); Si queremos comprobar como se han creado las particiones y los buckets, desde un terminal podemos acceder a HDFS y mostrar su contenido: hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos/categoria = Accessories Si volvemos a Hive , ahora podemos consultar los datos: select * from productos limit 5 ; Y vemos c\u00f3mo aparecen 5 elementos que pertenecen a la primera partici\u00f3n. Si quisi\u00e9ramos, por ejemplo, 10 elementos de particiones diferentes deber\u00edamos ordenarlos de manera aleatoria: select * from productos order by rand () limit 10 ; A continuaci\u00f3n vamos a realizar diversas consultas utilizando las funciones ventana que soporta Hive . M\u00e1s informaci\u00f3n en https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics . Consultas con enteros que cuentan/ordenan \u00b6 Consultas sobre categor\u00edas Las siguientes consultas las vamos a realizar sobre s\u00f3lo dos categor\u00edas para acotar los resultados obtenidos. Adem\u00e1s, hemos recortado el nombre del producto a 20 caracteres para facilitar la legibilidad de los resultados. Las funciones rank y dense_rank permite obtener la posici\u00f3n que ocupan los datos. Se diferencia en que rank cuenta los elementos repetidos/empatados, mientras que dense_rank no. Por ejemplo, vamos a obtener la posici\u00f3n que ocupan los productos respecto al precio agrupados por su categor\u00eda: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , rank () over ( partition by categoria order by precio desc ) as rank , dense_rank () over ( partition by categoria order by precio desc ) as denseRank from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+-------+------------+ | nombre | categoria | precio | rank | denserank | +-----------------------+--------------------+----------+-------+------------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 5 | 5 | | Diamondback Adult So | Basketball | 299.98 | 7 | 6 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | 7 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | 8 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | 9 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | 10 | | Easton S1 Youth Bat | Basketball | 179.97 | 11 | 10 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | 11 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | 12 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | 13 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | 14 | | Elevation Training M | Basketball | 79.99 | 17 | 15 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | 16 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | 17 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | 18 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | 19 | | adidas Brazuca 2014 | Basketball | 29.99 | 21 | 19 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | 20 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | 21 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 2 | 2 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | 3 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | 4 | +-----------------------+--------------------+----------+-------+------------+ La funci\u00f3n row_number permite numerar los resultados: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , row_number () over ( partition by categoria order by precio desc ) as numfila from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | numfila | +-----------------------+--------------------+----------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 6 | | Diamondback Adult So | Basketball | 299.98 | 7 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | | Easton S1 Youth Bat | Basketball | 179.97 | 12 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | | Elevation Training M | Basketball | 79.99 | 17 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | | adidas Brazuca 2014 | Basketball | 29.99 | 22 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 3 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | +-----------------------+--------------------+----------+----------+ Consultas por posici\u00f3n \u00b6 A continuaci\u00f3n vamos a ver las funciones lead y lag . Estas funciones se encargan de obtener el valor posterior y anterior respecto a un valor. select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , lead ( precio ) over ( partition by categoria order by precio desc ) as sig , lag ( precio ) over ( partition by categoria order by precio desc ) as ant from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+---------+----------+ | nombre | categoria | precio | sig | ant | +-----------------------+--------------------+----------+---------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 999.99 | NULL | | SOLE E25 Elliptical | Basketball | 999.99 | 349.98 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 309.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 299.99 | 349.98 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | 309.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.98 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 249.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 209.99 | 299.98 | | Fitness Gear 300 lb | Basketball | 209.99 | 199.99 | 249.97 | | Quik Shade Summit SX | Basketball | 199.99 | 179.97 | 209.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | 199.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 159.99 | 179.97 | | adidas Brazuca 2014 | Basketball | 159.99 | 149.99 | 179.97 | | Quest 12' x 12' Dome | Basketball | 149.99 | 99.95 | 159.99 | | Fitbit Flex Wireless | Basketball | 99.95 | 99.0 | 149.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 79.99 | 99.95 | | Elevation Training M | Basketball | 79.99 | 69.99 | 99.0 | | MAC Sports Collapsib | Basketball | 69.99 | 59.98 | 79.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 39.99 | 69.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 29.99 | 59.98 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 29.99 | 39.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 28.0 | 29.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 21.99 | 29.99 | | Nike Women's Pro Vic | Basketball | 21.99 | NULL | 28.0 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 169.99 | NULL | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 119.99 | 169.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 0.0 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | NULL | 119.99 | +-----------------------+--------------------+----------+---------+----------+ Consultas de agregaci\u00f3n \u00b6 Las funciones de agregaci\u00f3n que ya conocemos como count , sum , min y max tambi\u00e9n las podemos aplicar sobre particiones de datos y as\u00ed poder mostrar los datos agregados para cada elemento: select substr ( nombre , 1 , 20 ) as nombre , categoria , count ( precio ) over ( partition by categoria ) as cantidad , min ( precio ) over ( partition by categoria ) as menor , max ( precio ) over ( partition by categoria ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+-----------+--------+----------+ | nombre | categoria | cantidad | menor | mayor | +-----------------------+--------------------+-----------+--------+----------+ | Fitbit Flex Wireless | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Re | Basketball | 24 | 21.99 | 1799.99 | | Nike+ Fuelband SE | Basketball | 24 | 21.99 | 1799.99 | | Elevation Training M | Basketball | 24 | 21.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Girls' C | Basketball | 24 | 21.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | Easton Mako Youth Ba | Basketball | 24 | 21.99 | 1799.99 | | SOLE E25 Elliptical | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 24 | 21.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 24 | 21.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 24 | 21.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Boys' In | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult So | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Cor | Basketball | 24 | 21.99 | 1799.99 | | Quik Shade Summit SX | Basketball | 24 | 21.99 | 1799.99 | | Quest 12' x 12' Dome | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Vic | Basketball | 24 | 21.99 | 1799.99 | | Cleveland Golf Class | Bike & Skate Shop | 5 | 0.0 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | +-----------------------+--------------------+-----------+--------+----------+ Las consultas que hemos visto en este caso de uso tambi\u00e9n se conocen como funciones ventana , ya que se ejecutan sobre un subconjunto de los datos. La ventana viene dada por la partici\u00f3n o por la posici\u00f3n una vez ordenados los datos. Los posibles valores son: rows between current row and unbounded following : desde la fila actual hasta el final de la ventana/partici\u00f3n. rows between current row and N following : desde la fila actual hasta los N siguientes. rows between unbounded preceding and current row : desde el inicio de la ventana hasta la fila actual. rows between unbounded preceding and N following : desde el inicio de la ventana hasta los N siguientes. rows between unbounded preceding and unbounded following : desde el inicio de la ventana hasta el final de la ventana (caso por defecto) rows between N preceding and M following : desde N filas anteriores hasta M filas siguientes. Por ejemplo, para obtener el m\u00e1ximo precio desde la fila actual hasta el resto de la partici\u00f3n: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between current row and unbounded following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 1799.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 1799.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 1799.99 | | Elevation Training M | Basketball | 79.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 1799.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 1799.99 | | Diamondback Girls' C | Basketball | 299.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 1799.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 1799.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 309.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 69.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 299.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.98 | | Nike Women's Pro Cor | Basketball | 28.0 | 199.99 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 149.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 21.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 169.99 | +-----------------------+--------------------+----------+----------+ Si queremos comparar el precio y quedarnos con el mayor respecto al anterior y el posterior podr\u00edamos realizar la siguiente consulta: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between 1 preceding and 1 following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 99.95 | | adidas Brazuca 2014 | Basketball | 39.99 | 209.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 349.98 | | Diamondback Adult Re | Basketball | 349.98 | 349.98 | | Nike+ Fuelband SE | Basketball | 99.0 | 349.98 | | Elevation Training M | Basketball | 79.99 | 179.97 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | | adidas Brazuca 2014 | Basketball | 29.99 | 299.99 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 999.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 999.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 309.99 | | MAC Sports Collapsib | Basketball | 69.99 | 159.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 1799.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 299.98 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 199.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 149.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 169.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 179.99 | +-----------------------+--------------------+----------+----------+ Referencias \u00b6 P\u00e1gina oficial de Hive Apache Hive Essentials - Second Edition de Dayong Du. Tutorial de Hive de TutorialsPoint. Actividades \u00b6 Realiza los casos de uso del 1 al 5. En la entrega debes adjuntar un una captura de pantalla donde se vea la ejecuci\u00f3n de las diferentes instrucciones. (opcional) Realiza el caso de uso 6. (opcional) A partir de la base de datos retail_db , importa las tablas orders y order_items , en las cuales puedes obtener la cantidad de productos que contiene un pedido. Utilizando todas las tablas que ya hemos importado en los casos anteriores, crea una tabla externa en Hive llamada pedidos utilizando 8 buckets con el c\u00f3digo del cliente, que contenga: C\u00f3digo y fecha del pedido. Precio del pedido (sumando las l\u00edneas de pedido). C\u00f3digo, nombre y apellidos del cliente. Adjunta scripts y capturas de: la importaci\u00f3n, creaci\u00f3n y carga de datos de las tablas que necesites. la definici\u00f3n de la tabla: describe formatted pedidos; contenido de HDFS que demuestre la creaci\u00f3n de los buckets . (opcional) Investiga la creaci\u00f3n de vistas en Hive y crea una vista con los datos de los clientes y sus pedidos siempre y cuando superen los 200$.","title":"4.- Hive"},{"location":"apuntes/bdaplicado04hive.html#hive","text":"Apache Hive ( https://hive.apache.org/ ) es una tecnolog\u00eda distribuida dise\u00f1ada y construida sobre un cl\u00faster de Hadoop . Permite leer, escribir y gestionar grandes datasets (con escala de petabytes) que residen en HDFS haciendo uso de un lenguaje dialecto de SQL, conocido como HiveSQL , lo que simplifica mucho el desarrollo y la gesti\u00f3n de Hadoop . Logo de Apache Hive El proyecto lo inici\u00f3 Facebook para conseguir que la interacci\u00f3n con Hadoop fuera similar a la que se realiza con un datawarehouse tradicional. La tecnolog\u00eda Hadoop es altamente escalable, aunque hay que destacar su dificultad de uso y que est\u00e1 orientado \u00fanicamente a operaciones batch , con lo que no soporta el acceso aleatorio ni est\u00e1 optimizado para ficheros peque\u00f1os.","title":"Hive"},{"location":"apuntes/bdaplicado04hive.html#hive-y-hadoop","text":"Si volvemos a ver como casa Hive dentro del ecosistema de Hadoop , Hive es una fachada construida sobre Hadoop que permite acceder a los datos almacenados en HDFS de forma muy sencilla sin necesidad de conocer Java , Map Reduce u otras tecnolog\u00edas. Aunque en principio estaba dise\u00f1ado para el procesamiento batch , ahora se integra con frameworks en streaming como Tez y Spark . Ecosistema Hadoop","title":"Hive y Hadoop"},{"location":"apuntes/bdaplicado04hive.html#caracteristicas","text":"Hive impone una estructura sobre los datos almacenados en HDFS. Esta estructura se conoce como Schema , y Hive la almacena en su propia base de datos ( metastore ). Gracias a ella, optimiza de forma autom\u00e1tica el plan de ejecuci\u00f3n y usa particionado de tablas en determinadas consultas. Tambi\u00e9n soporta diferentes formatos de ficheros, codificaciones y fuentes de datos como HBase . Para interactuar con Hive utilizaremos HiveQL , el cual es un dialecto de SQL (recuerda que SQL no es sensible a las may\u00fasculas, excepto en la comparaci\u00f3n de cadenas). Hive ampl\u00eda el paradigma de SQL incluyendo formatos de serializaci\u00f3n. Tambi\u00e9n podemos personalizar el procesamiento de consultas creando un esquema de tabla acorde con nuestros datos, pero sin tocar los datos. Aunque SQL solo es compatible con tipos de valor primitivos (como fechas, n\u00fameros y cadenas), los valores de las tablas de Hive son elementos estructurados, por ejemplo, objetos JSON o cualquier tipo de datos definido por el usuario o cualquier funci\u00f3n escrita en Java. Una consulta t\u00edpica en Hive se ejecuta en varios datanodes en paralelo, con varios trabajos MapReduce asociados. Estas operaciones son de tipo batch , por lo que la latencia es m\u00e1s alta que en otros tipos de bases de datos. Adem\u00e1s, hay que considerar el retardo producido por la inicializaci\u00f3n de los trabajos, sobre todo en el caso de consultar peque\u00f1os datasets.","title":"Caracter\u00edsticas"},{"location":"apuntes/bdaplicado04hive.html#componentes","text":"A continuaci\u00f3n podemos ver un gr\u00e1fico que relaciona los diferentes componentes de Hive y define su arquitectura: Arquitectura de Apache Hive","title":"Componentes"},{"location":"apuntes/bdaplicado04hive.html#tipos-de-datos","text":"Los tipos de datos que podemos emplear en Hive son muy similares a los que se utilizan en el DDL de SQL. Los tipos simples m\u00e1s comunes son STRING e INT , aunque podemos utilizar otros tipos como TINYINT , BIGINT , DOUBLE , DATE , TIMESTAMP , etc... Para realizar una conversi\u00f3n explicita de tipos, por ejemplo de un tipo texto a uno num\u00e9rico, hay que utilizar la funci\u00f3n CAST : select CAST ( '1' as INT ) from tablaPruebas ; Respecto a los tipos compuestos, tenemos tres tipos: arrays mediante el tipo ARRAY , para agrupar elementos del mismo tipo: [\"manzana\", \"pera\", \"naranja] . mapas mediante el tipo MAP , para definir parejas de clave-valor: {1: \"manzana\", 2: \"pera\"} estructuras mediante el tipo STRUCT , para definir estructuras con propiedades: {\"fruta\": \"manzana\", \"cantidad\": 1, \"tipo\": \"postre\"} .","title":"Tipos de datos"},{"location":"apuntes/bdaplicado04hive.html#instalacion-y-configuracion","text":"M\u00e1quina virtual Los siguientes pasos no son necesarios ya que nuestra m\u00e1quina virtual ya tiene Hive instalado y configurado correctamente. Si quieres hacer tu propia instalaci\u00f3n sigue los siguientes pasos de la documentaci\u00f3n oficial. Una vez instalado, vamos a configurarlo. Para ello, debemos crear los ficheros de configuraci\u00f3n a partir de las plantilla que ofrece Hive . Para ello, desde la carpeta $HIVE_HOME/conf , ejecutaremos los siguientes comandos: cp hive-default.xml.template hive-site.xml cp hive-env.sh.template hive-env.sh cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties cp hive-log4j2.properties.template hive-log4j2.properties Modificamos el fichero hive.env.sh para incluir dos variables de entorno con las rutas de Hadoop y la configuraci\u00f3n de Hive hive.env.sh export HADOOP_HOME = /opt/hadoop-3.3.1 export HIVE_CONF_DIR = /opt/hive-3.1.2/conf Para que funcione la ingesta de datos en Hive mediante Sqoop , necesitamos a\u00f1adir una librer\u00eda a Sqoop : cp $HIVE_HOME /lib/hive-common-3.1.2.jar $SQOOP_HOME /lib Preparamos HDFS para crear la estructura de archivos: hdfs dfs -mkdir /tmp hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /tmp hdfs dfs -chmod g+w /user/hive/warehouse Para el metastore , como en nuestra m\u00e1quina virtual tenemos un servidor de MariaDB corriendo, vamos a reutilizarlo. La mayor\u00eda de ejemplos que hay en internet y la diferente bibliograf\u00eda, utilizan DerbyDB como almac\u00e9n (ya que no requiere una instalaci\u00f3n extra). As\u00ed pues, creamos el almac\u00e9n mediante: schematool -dbType mysql -initSchema Modificamos el fichero de configuraci\u00f3n hive-site.xml y configuramos : hive-site.xml <!-- nuevas propiedades --> <property> <name> system:java.io.tmpdir </name> <value> /tmp/hive/java </value> </property> <property> <name> system:user.name </name> <value> ${user.name} </value> </property> <property> <name> datanucleus.schema.autoCreateAll </name> <value> true </value> </property> <!-- propiedades existentes a modificar --> <property> <name> javax.jdo.option.ConnectionURL </name> <value> jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true </value> </property> <property> <name> javax.jdo.option.ConnectionDriverName </name> <value> com.mysql.jdbc.Driver </value> </property> <property> <name> javax.jdo.option.ConnectionUserName </name> <value> iabd </value> </property> <property> <name> javax.jdo.option.ConnectionPassword </name> <value> iabd </value> </property>","title":"Instalaci\u00f3n y configuraci\u00f3n"},{"location":"apuntes/bdaplicado04hive.html#hola-mundo","text":"Si entramos a nuestro $HIVE_HOME podemos comprobar con tenemos las siguientes herramientas: hive : Herramienta cliente beeline : Otra herramienta cliente hiserver2 : Nos permite arrancar el servidor de Hive schematool : Nos permite trabajar contra la base de datos de metadatos (Metastore) Una vez arrancado Hadoop y YARN , vamos a arrancar Hive mediante el cliente local: hive Y una vez dentro, podemos comprobar las bases de datos existentes para ver que todo se configur\u00f3 correctamente show databases ; Si quisi\u00e9ramos ejecutar un script, podemos hacerlo desde el propio comando hive con la opci\u00f3n -f : hive -f script.sql Adem\u00e1s, tenemos la opci\u00f3n de pasar una consulta desde la propia l\u00ednea de comandos mediante la opci\u00f3n -e : hive -e ' select * from tablaEjemplo `","title":"Hola Mundo"},{"location":"apuntes/bdaplicado04hive.html#acceso-remoto","text":"HiveServer2 (desde Hive 0.11) tiene su propio cliente conocido como Beeline . En entornos reales, el cliente Hive est\u00e1 en desuso a favor de Beeline , por la falta de m\u00faltiples usuarios, seguridad y otras caracter\u00edsticas de HiveServer2. Arrancamos HiveServer2 y Beeline en dos pesta\u00f1as diferentes mediante los comandos hiveserver2 y beeline . Una vez dentro de Beeline , nos conectamos al servidor: !connect jdbc:hive2://iabd-virtualbox:10000 Al conectarnos, tras introducir iabd como usuario y contrase\u00f1a, obtendremos un interfaz similar al siguiente: Beeline version 3.1.2 by Apache Hive beeline> !connect jdbc:hive2://iabd-virtualbox:10000 Connecting to jdbc:hive2://iabd-virtualbox:10000 Enter username for jdbc:hive2://iabd-virtualbox:10000: iabd Enter password for jdbc:hive2://iabd-virtualbox:10000: **** Connected to: Apache Hive (version 3.1.2) Driver: Hive JDBC (version 3.1.2) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://iabd-virtualbox:10000> Dentro de Beeline , en cualquier momento podemos ejecutar el comando help que nos mostrar\u00e1 todos los comandos disponibles. Si nos fijamos, adem\u00e1s de las comandos del cliente hive, tenemos los comandos beeline que empiezan por el s\u00edmbolo de exclamaci\u00f3n ! : 0: jdbc:hive2://iabd-virtualbox:10000> help !addlocaldriverjar Add driver jar file in the beeline client side. !addlocaldrivername Add driver name that needs to be supported in the beeline client side. !all Execute the specified SQL against all the current connections !autocommit Set autocommit mode on or off !batch Start or execute a batch of statements ... Otra forma de trabajar, para arrancar en el mismo proceso Beeline y HiveServer2 para pruebas/desarrollo y tener una experiencia similar al cliente Hive accediendo de forma local, podemos ejecutar el siguiente comando: beeline -u jdbc:hive2:// Mediante la interfaz gr\u00e1fica de Hive Server UI a la cual podemos acceder mediante http://localhost:10002 podemos monitorizar los procesos ejecutados por HiveServer2 : Monitorizaci\u00f3n mediante Hive Server UI","title":"Acceso remoto"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-1-creacion-y-borrado-de-tablas","text":"Para este caso de uso, vamos a utilizar la base de datos retail_db que ya utilizamos en las actividades de la sesi\u00f3n anterior. Para empezar, vamos a cargar en HDFS los datos de los clientes que contiene la tabla customer . Mediante Sqoop , ejecutamos el siguiente comando: sqoop import --connect \"jdbc:mysql://localhost/retail_db\" \\ --username iabd --password iabd \\ --table customers --target-dir /user/iabd/hive/customer \\ --fields-terminated-by '|' --delete-target-dir \\ --columns \"customer_id,customer_fname,customer_lname,customer_city\" Una vez nos hemos conectado con el cliente hive o mediante beeline , creamos una base de datos llamada iabd : create database iabd ; Nos conectamos a la base de datos que acabamos de crear: use iabd ; default Si olvidamos el comando use , se utilizar\u00e1 la base de datos default , la cual reside en /user/hive/warehouse como ra\u00edz en HDFS. A continuaci\u00f3n, vamos a crear una tabla que almacene el identificador, nombre, apellido y ciudad de los clientes (como puedes observar, la sintaxis es similar a SQL): CREATE TABLE customers ( custId INT , fName STRING , lName STRING , city STRING ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/user/iabd/hive/customer' ; Y ya podemos realizar algunas consultas: select * from customers limit 5 ; select count ( * ) from customers ; En ocasiones necesitamos almacenar la salida de una consulta Hive en una nueva tabla. Las definiciones de las columnas de la nueva tabla se deriva de las columnas recuperadas en la consulta. Para ello, usaremos el comando create table-as select : CREATE TABLE customers_new as SELECT * from customers ; En el caso de la consulta falle por alg\u00fan motivo, la tabla no se crear\u00eda. Otra posibilidad es crear una tabla con la misma estructura que otra ya existente (pero sin datos): CREATE TABLE customers2 LIKE customers ; En cualquier momento podemos obtener informaci\u00f3n de la tabla: describe customers_new ; describe formatted customers_new ; Si empleamos la forma larga, obtendremos mucha m\u00e1s informaci\u00f3n. Por ejemplo, si nos fijamos, vemos que la localizaci\u00f3n de la nueva tabla ya no es /user/iabd/hive/customer sino hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/customers_new . Esto se debe a que en vez de crear una tabla enlazada a un recurso de HDFS ya existente, ha creado una copia de los datos en el propio almac\u00e9n de Hive (hemos pasado de una tabla externa a una interna). Igual que las creamos, las podemos eliminar: drop table customers_new ; drop table customers2 ; Si ejecutamos el comando !tables (o show tables en el cliente hive ) veremos que ya no aparecen dichas tablas. En el caso de que queramos eliminar una base de datos, de la misma manera que en SQL, ejecutar\u00edamos el comando drop database iabd; .","title":"Caso de uso 1: Creaci\u00f3n y borrado de tablas"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-2-insertando-datos","text":"Para insertar datos en las tablas de Hive lo podemos hacer de varias formas: Cargando los datos mediante sentencias LOAD DATA . Insertando los datos mediante sentencias INSERT . Cargando los datos directamente mediante Sqoop o alguna herramienta similar.","title":"Caso de uso 2: Insertando datos"},{"location":"apuntes/bdaplicado04hive.html#cargando-datos","text":"Para cargar datos se utiliza la sentencia LOAD DATA . Si quisi\u00e9ramos volver a cargar los datos desde HDFS utilizaremos: LOAD DATA INPATH '/user/iabd/hive/customer' overwrite into table customers ; Si en cambio vamos a cargar los datos desde un archivo local a nuestro sistema de archivos a\u00f1adiremos LOCAL : LOAD DATA LOCAL INPATH '/home/iabd/datos' overwrite into table customers ;","title":"Cargando datos"},{"location":"apuntes/bdaplicado04hive.html#insertando-datos","text":"Aunque podemos insertar datos de forma at\u00f3mica (es decir, registro a registro mediante INSERT INTO TABLE ... VALUES ), realmente las inserciones que se realizan en Hive se hacen a partir de los datos de otras tablas mediante el comando insert-select a modo de ETL: INSERT OVERWRITE TABLE destino SELECT col1 , col2 FROM fuente ; Mediante la opci\u00f3n OVERWRITE , en cada ejecuci\u00f3n se vac\u00eda la tabla y se vuelve a rellenar. Si no lo indicamos o utilizamos INTO , los datos se a\u00f1adir\u00edan a los ya existentes. Si necesitamos insertar datos en m\u00faltiples tablas a la vez lo haremos mediante el comando from-insert : FROM fuente INSERT OVERWRITE TABLE destino1 SELECT col1 , col2 INSERT OVERWRITE TABLE destino2 SELECT col1 , col3 Por ejemplo, vamos a crear un par de tablas con la misma estructura de clientes, pero para almacenar los clientes de determinadas ciudades: CREATE TABLE customers_brooklyn LIKE customers ; CREATE TABLE customers_caguas LIKE customers ; Y a continuaci\u00f3n rellenamos ambas tablas con sus clientes; FROM customers INSERT OVERWRITE TABLE customers_brooklyn SELECT custId , fName , lName , city WHERE city = \"Brooklyn\" INSERT OVERWRITE TABLE customers_caguas SELECT custId , fName , lName , city WHERE city = \"Caguas\" ;","title":"Insertando datos"},{"location":"apuntes/bdaplicado04hive.html#ingestando-datos","text":"Tal como vimos en la sesi\u00f3n anterior , podemos ingestar los datos en Hive haciendo uso de Sqoop (tambi\u00e9n lo podemos hacer con Flume o Nifi ). Por ejemplo, vamos a ingestar los datos de la tabla orders de la base de datos MariaDB que tenemos instalada en nuestra m\u00e1quina virtual. En el comando de Sqoop le indicamos que dentro de Hive lo ingeste en la base de datos iabd y que cree una tabla llamada orders : sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = orders --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table orders Una vez realizada la ingesta, podemos comprobar que los datos est\u00e1n dentro de Hive (en una tabla interna/gestionada): hdfs dfs -ls /user/hive/warehouse/iabd.db/orders Y si entramos a hive , podemos consultar sus datos: select * from orders limit 10 ;","title":"Ingestando datos"},{"location":"apuntes/bdaplicado04hive.html#extrayendo-datos-insertados","text":"Combinando los comandos de HQL y HDFS podemos extraer datos a ficheros locales o remotos: # A\u00f1adiendo contenido local hive -e \"use iabd; select * from customers\" >> prueba1 # Sobreescribiendo contenido local hive -e \"use iabd; select * from customers\" > prueba2 # A\u00f1adiendo contenido HDFS hive -e \"use iabd; select * from customers\" | hdfs dfs --appendToFile /tmp/prueba3 # Sobreescribiendo contenido hive -e \"use iabd; select * from customers\" | hdfs dfs --put -f /tmp/prueba4 Si indicamos la propiedad set hive.cli.print.header=true antes de la consulta, tambi\u00e9n nos mostrar\u00e1 el encabezado de las columnas. Esto puede ser \u00fatil si queremos generar un csv con el resultado de una consulta: hive -e 'use iabd; set hive.cli.print.header=true; select * from customers' | \\ sed 's/[\\t]/,/g' > fichero.csv \u00bfY usar INSERT LOCAL ? Mediante INSERT LOCAL podemos escribir el resultado de una consulta en nuestro sistema de archivos, fuera de HDFS. El problema es que si hay muchos datos crear\u00e1 m\u00faltiples ficheros y necesitaremos concatenarlos para tener un \u00fanico resultado: insert overwrite local directory '/home/iabd/datos' row format delimited field terminated by ',' select * from customers ;","title":"Extrayendo datos insertados"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-3-consultas-con-join","text":"En este caso de uso vamos a trabajar con los datos de clientes que hemos cargado en los dos casos anteriores, tanto en customers como en orders . Si queremos relacionar los datos de ambas tablas, tenemos que hacer un join entre la clave ajena de orders ( order_customer_id ) y la clave primaria de customers ( custid ): hive > describe customers ; OK custid int fname string lname string city string Time taken : 0 . 426 seconds , Fetched : 4 row ( s ) hive > describe orders ; OK order_id int order_date string order_customer_id int order_status string Time taken : 0 . 276 seconds , Fetched : 4 row ( s ) Para ello, para obtener la ciudad de cada pedido, podemos ejecutar la consulta: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid );","title":"Caso de uso 3: Consultas con join"},{"location":"apuntes/bdaplicado04hive.html#outer-join","text":"De la misma manera que en cualquier SGBD, podemos realizar un outer join , tanto left como right o full . Por ejemplo, vamos a obtener para cada cliente, cuantos pedidos ha realizado: select c . custid , count ( order_id ) from customers c join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ; Si queremos que salgan todos los clientes, independientemente de que tengan pedidos, deberemos realizar un left outer join : select c . custid , count ( order_id ) from customers c left outer join orders o on ( c . custid = o . order_customer_id ) group by c . custid order by count ( order_id ) desc ;","title":"Outer join"},{"location":"apuntes/bdaplicado04hive.html#semi-joins","text":"Si quisi\u00e9ramos obtener las ciudades de los clientes que han realizado pedidos podr\u00edamos realizar la siguiente consulta: select distinct c . city from customers c where c . custid in ( select order_customer_id from orders ); Mediante un semi-join podemos obtener el mismo resultado: select distinct city from customers c left semi join orders o on ( c . custid = o . order_customer_id ) Hay que tener en cuenta la restricci\u00f3n que las columnas de la tabla de la derecha s\u00f3lo pueden aparecer en la clausula on , nunca en la expresi\u00f3n select .","title":"Semi-joins"},{"location":"apuntes/bdaplicado04hive.html#map-joins","text":"Consideramos la consulta inicial de join: select o . order_id , o . order_date , c . city from orders o join customers c on ( o . order_customer_id = c . custid ); Si una tabla es suficientemente peque\u00f1a para caber en memoria, tal como nos ocurre con nuestros datos, Hive puede cargarla en memoria para realizar el join en cada uno de los mappers . Esto se conoce como un map join . El job que ejecuta la consulta no tiene reducers , con lo que esta consulta no funcionar\u00e1 para un right o right outer join , ya que la ausencias de coincidencias s\u00f3lo se puede detectar en los pasos de agregaci\u00f3n ( reduce ). En el caso de utilizar map joins con tablas organizadas en buckets , la sintaxis es la misma, s\u00f3lo siendo necesario activarlo mediante la propiedad hive.optimize.bucketmapjoin : SET hive.optimize.bucketmapjoin = true ;","title":"Map joins"},{"location":"apuntes/bdaplicado04hive.html#comandos","text":"Mediante los casos de uso realizados hasta ahora, hemos podido observar c\u00f3mo para interactuar con Hive se utilizan comandos similares a SQL. Es conveniente consultar la siguiente cheatsheet : http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf Adem\u00e1s Hive viene con un conjunto de funciones predefinidas para tratamiento de cadenas, fechas, funciones estad\u00edsticas, condicionales, etc... las cuales puedes consultar en la documentaci\u00f3n oficial . Mediante el comando show functions podemos obtener una lista de las funciones. Si queremos m\u00e1s informaci\u00f3n sobre una determinada funci\u00f3n utilizaremos el comando describe function nombreFuncion : hive > describe function length ; length ( str | binary ) - Returns the length of str or number of bytes in binary data","title":"Comandos"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-4-tabla-interna","text":"Hive permite crear tablas de dos tipos: tabla interna o gestionada: Hive gestiona la estructura y el almacenamiento de los datos. Para ello, crea los datos en HDFS. Al borrar la tabla de Hive , se borra la informaci\u00f3n de HDFS. tabla externa : Hive define la estructura de los datos en el metastore , pero los datos ya residen previamente en HDFS. Al borrar la tabla de Hive , no se eliminan los datos de HDFS. Se emplea cuando compartimos datos almacenados en HDFS entre diferentes herramientas. En este caso de uso, vamos a centrarnos en una tabla interna. Supongamos el siguiente fichero con datos de empleados: empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Podemos observar como se utiliza | como separador de campos. Analizando los datos, vemos que tenemos los siguientes campos: Nombre Centros de trabajo (array con las ciudades) Sexo y edad Destreza y puntuaci\u00f3n Departamento y cargo Creamos la siguiente tabla interna en Hive mediante el siguiente comando: CREATE TABLE IF NOT EXISTS empleados_interna ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT 'Esto es una tabla interna' ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' ; La sintaxis es muy similar a SQL, destacando las siguientes opciones: ROW FORMAT DELIMITED : cada registro ocupa una l\u00ednea FIELDS TERMINATED BY '|' : define el | como separador de campos COLLECTION ITEMS TERMINATED BY ',' : define la coma como separador de los arrays / estructuras MAP KEYS TERMINATED BY ':' : define los dos puntos como separador utilizado en los mapas. Si queremos comprobar la estructura de la tabla mediante el comando show create table empleados_interna veremos las opciones que hemos indicado: + ----------------------------------------------------+ | createtab_stmt | + ----------------------------------------------------+ | CREATE TABLE ` empleados_interna ` ( | | ` name ` string , | | ` work_place ` array < string > , | | ` sex_age ` struct < sex : string , age : int > , | | ` skills_score ` map < string , int > , | | ` depart_title ` map < string , array < string >> ) | | COMMENT 'Esto es una tabla interna' | | ROW FORMAT SERDE | | 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' | | WITH SERDEPROPERTIES ( | | 'collection.delim' = ',' , | | 'field.delim' = '|' , | | 'mapkey.delim' = ':' , | | 'serialization.format' = '|' ) | | STORED AS INPUTFORMAT | | 'org.apache.hadoop.mapred.TextInputFormat' | | OUTPUTFORMAT | | 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' | | LOCATION | | 'hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/empleados_interna' | | TBLPROPERTIES ( | | 'bucketing_version' = '2' , | | 'transient_lastDdlTime' = '1647432129' ) | + ----------------------------------------------------+ A continuaci\u00f3n, vamos a cargar los datos del fichero empleados.txt , el cual colocaremos en nuestra carpeta de Descargas : LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_interna ; Comprobamos que los datos se han cargado correctamente: select * from empleados_interna ; Y obtenemos: +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | empleados_interna.name | empleados_interna.work_place | empleados_interna.sex_age | empleados_interna.skills_score | empleados_interna.depart_title | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ | Michael | [\"Montreal\",\"Toronto\"] | {\"sex\":\"Male\",\"age\":30} | {\"DB\":80} | {\"Product\":[\"Developer\",\"Lead\"]} | | Will | [\"Montreal\"] | {\"sex\":\"Male\",\"age\":35} | {\"Perl\":85} | {\"Product\":[\"Lead\"],\"Test\":[\"Lead\"]} | | Shelley | [\"New York\"] | {\"sex\":\"Female\",\"age\":27} | {\"Python\":80} | {\"Test\":[\"Lead\"],\"COE\":[\"Architect\"]} | | Lucy | [\"Vancouver\"] | {\"sex\":\"Female\",\"age\":57} | {\"Sales\":89,\"HR\":94} | {\"Sales\":[\"Lead\"]} | +-------------------------+-------------------------------+----------------------------+---------------------------------+----------------------------------------+ Y si nos abrimos otra pesta\u00f1a, mediante HDFS, comprobamos que tenemos los datos: hdfs dfs -ls /user/hive/warehouse/curso.db/empleados_interna hdfs dfs -cat /user/hive/warehouse/curso.db/empleados_interna/empleados.txt Y obtenemos: Michael|Montreal,Toronto|Male,30|DB:80|Product:DeveloperLead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead","title":"Caso de uso 4: Tabla interna"},{"location":"apuntes/bdaplicado04hive.html#consultando-datos-compuestos","text":"Si nos fijamos bien en la tabla, tenemos tres columnas con diferentes datos compuestos. work_place , con un ARRAY<string> sex_age con una STRUCT<sex:string,age:int> skills_score con un MAP<string,int> , depart_title con un MAP<STRING,ARRAY<STRING>> Si queremos obtener los datos del array , podemos realizar las siguientes consultas: Todos los elementos Elementos individuales Cantidad de elementos Explode array Todos los lugares de trabajo: select name , work_place from empleados_interna ; Resultado: +----------+-------------------------+ | name | work_place | +----------+-------------------------+ | Michael | [ \"Montreal\" , \"Toronto\" ] | | Will | [ \"Montreal\" ] | | Shelley | [ \"New York\" ] | | Lucy | [ \"Vancouver\" ] | +----------+-------------------------+ Los dos primeros puestos de trabajo: select work_place [ 0 ] as lugar1 , work_place [ 1 ] as lugar2 from empleados_interna ; Resultado: +------------+----------+ | lugar1 | lugar2 | +------------+----------+ | Montreal | Toronto | | Montreal | NULL | | New York | NULL | | Vancouver | NULL | +------------+----------+ Cantidad de lugares de trabajo: select size ( work_place ) as cantLugares from empleados_interna ; Resultado: +--------------+ | cantlugares | +--------------+ | 2 | | 1 | | 1 | | 1 | +--------------+ Empleados y lugares de trabajo mostrados uno por fila: select name , lugar from empleados_interna lateral view explode ( work_place ) e2 as lugar ; Para ello hemos creado una vista lateral y con la funci\u00f3n explode desenrollamos el array. Resultado: +----------+------------+ | name | lugar | +----------+------------+ | Michael | Montreal | | Michael | Toronto | | Will | Montreal | | Shelley | New York | | Lucy | Vancouver | +----------+------------+ En el caso de la estructura con el sexo y la edad podemos realizar las siguientes consultas Todos los elementos Elementos individuales Todas las estructuras de sexo/edad: select sex_age from empleados_interna ; Resultado: +----------------------------+ | sex_age | +----------------------------+ | { \"sex\" : \"Male\" , \"age\" : 30 } | | { \"sex\" : \"Male\" , \"age\" : 35 } | | { \"sex\" : \"Female\" , \"age\" : 27 } | | { \"sex\" : \"Female\" , \"age\" : 57 } | +----------------------------+ Sexo y edad por separado select sex_age . sex as sexo , sex_age . age as edad from empleados_interna ; Resultado: +---------+-------+ | sexo | edad | +---------+-------+ | Male | 30 | | Male | 35 | | Female | 27 | | Female | 57 | +---------+-------+ Respecto al mapa con las habilidades y sus puntuaciones: Todos los elementos Elementos individuales Claves y valores Todas las habilidades como un mapa: select skills_score from empleados_interna ; Resultado: +-----------------------+ | skills_score | +-----------------------+ | { \"DB\" : 80 } | | { \"Perl\" : 85 } | | { \"Python\" : 80 } | | { \"Sales\" : 89 , \"HR\" : 94 } | +-----------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , skills_score [ \"DB\" ] as db , skills_score [ \"Perl\" ] as perl , skills_score [ \"Python\" ] as python , skills_score [ \"Sales\" ] as ventas , skills_score [ \"HR\" ] as hr from empleados_interna ; Resultado: +----------+-------+-------+---------+---------+-------+ | name | db | perl | python | ventas | hr | +----------+-------+-------+---------+---------+-------+ | Michael | 80 | NULL | NULL | NULL | NULL | | Will | NULL | 85 | NULL | NULL | NULL | | Shelley | NULL | NULL | 80 | NULL | NULL | | Lucy | NULL | NULL | NULL | 89 | 94 | +----------+-------+-------+---------+---------+-------+ Claves y valores de las habilidades: select name , map_keys ( skills_score ) as claves , map_values ( skills_score ) as valores from empleados_interna ; Resultado: +----------+-----------------+----------+ | name | claves | valores | +----------+-----------------+----------+ | Michael | [ \"DB\" ] | [ 80 ] | | Will | [ \"Perl\" ] | [ 85 ] | | Shelley | [ \"Python\" ] | [ 80 ] | | Lucy | [ \"Sales\" , \"HR\" ] | [ 89 , 94 ] | +----------+-----------------+----------+ Y finalmente, con el mapa de departamentos que contiene un array: Todos los elementos Elementos individuales Primer elemento de los elementos individuales Toda la informaci\u00f3n sobre los departamentos: select depart_title from empleados_interna ; Resultado: +----------------------------------------+ | depart_title | +----------------------------------------+ | { \"Product\" : [ \"Developer\" , \"Lead\" ]} | | { \"Product\" : [ \"Lead\" ], \"Test\" : [ \"Lead\" ]} | | { \"Test\" : [ \"Lead\" ], \"COE\" : [ \"Architect\" ]} | | { \"Sales\" : [ \"Lead\" ]} | +----------------------------------------+ Nombre y puntuaci\u00f3n de las habilidades: select name , depart_title [ \"Product\" ] as product , depart_title [ \"Test\" ] as test , depart_title [ \"COE\" ] as coe , depart_title [ \"Sales\" ] as sales from empleados_interna ; Resultado: +----------+-----------------------+-----------+----------------+-----------+ | name | product | test | coe | sales | +----------+-----------------------+-----------+----------------+-----------+ | Michael | [ \"Developer\" , \"Lead\" ] | NULL | NULL | NULL | | Will | [ \"Lead\" ] | [ \"Lead\" ] | NULL | NULL | | Shelley | NULL | [ \"Lead\" ] | [ \"Architect\" ] | NULL | | Lucy | NULL | NULL | NULL | [ \"Lead\" ] | +----------+-----------------------+-----------+----------------+-----------+ Primera habilidad de producto y pruebas de cada empleado: select name , depart_title [ \"Product\" ][ 0 ] as product0 , depart_title [ \"Test\" ][ 0 ] as test0 from empleados_interna ; Resultado: +----------+-------------+--------+ | name | product0 | test0 | +----------+-------------+--------+ | Michael | Developer | NULL | | Will | Lead | Lead | | Shelley | NULL | Lead | | Lucy | NULL | NULL | +----------+-------------+--------+","title":"Consultando datos compuestos"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-5-tabla-externa","text":"En este caso de uso vamos a repetir la misma estructura de la tabla del caso anterior, pero en esta ocasi\u00f3n en una tabla externa. De esta manera, al borrar la tabla de Hive , no se borra la informaci\u00f3n de HDFS. Para ello, \u00fanicamente hemos de a\u00f1adir la palabra EXTERNAL a la instrucci\u00f3n CREATE TABLE y la clausula LOCATION para indicar la ruta de HDFS donde se encuentran los datos: CREATE EXTERNAL TABLE IF NOT EXISTS empleados_externa ( name string , work_place ARRAY < string > , sex_age STRUCT < sex : string , age : int > , skills_score MAP < string , int > , depart_title MAP < STRING , ARRAY < STRING >> ) COMMENT \"Esto es una tabla externa\" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':' LOCATION \"/user/iabd/hive/empleados_externa\" ; Realizamos la misma carga que en el caso anterior: LOAD DATA LOCAL INPATH '/home/iabd/Descargas/empleados.txt' OVERWRITE INTO TABLE empleados_externa ; Si hacemos una consulta sobre la tabla para ver que est\u00e1n todos los campos obtendremos la misma informaci\u00f3n que antes: SELECT * FROM empleados_externa ; Interna o externa Como normal general, si todo nuestro procesamiento lo hacemos mediante Hive , es m\u00e1s c\u00f3modo utilizar tablas internas. Si no es as\u00ed, y otras herramientas acceden al mismo dataset, es mejor utilizar tablas externas. Un patr\u00f3n de uso muy com\u00fan es utilizar una tabla externa para acceder al dataset inicial almacenado en HDFS (creado por otro proceso), y posteriormente crear una transformaci\u00f3n en Hive para mover los datos a una tabla interna.","title":"Caso de uso 5: Tabla externa"},{"location":"apuntes/bdaplicado04hive.html#estructuras-de-datos-en-hive","text":"Hive proporciona una estructura basada en tablas sobre HDFS. Soporta tres tipos de estructuras: tablas, particiones y buckets . Las tablas se corresponden con directorios de HDFS, las particiones son las divisiones de las tablas y los buckets son las divisiones de las particiones. Acabamos de ver en el apartado anterior que Hive permite crear tablas externas, similares a las tablas en una base de datos, pero a la que se les proporciona una ubicaci\u00f3n. En este caso, cuando se elimina la tabla externa, los datos contin\u00faan en HDFS.","title":"Estructuras de datos en Hive"},{"location":"apuntes/bdaplicado04hive.html#particiones","text":"Las particiones en Hive consisten en dividir las tablas en varios subdirectorios. Esta estructura permite aumentar el rendimiento cuando utilizamos consultas que filtran los datos mediante la cl\u00e1usula where . Por ejemplo, si estamos almacenado ficheros de log (tanto la l\u00ednea del log como su timestamp ), podemos pensar en agrupar por fecha los diferentes ficheros. Podr\u00edamos a\u00f1adir otra partici\u00f3n para tambi\u00e9n dividirlos por pa\u00edses: CREATE TABLE logs ( ts BIGINT , linea STRING ) PARTITIONED BY ( fecha STRING , pais STRING ); Por ejemplo, para cargar los datos en una partici\u00f3n: LOAD DATA LOCAL INPATH 'input/hive/particiones/log1' INTO TABLE logs PARTITION ( fecha = '2022-01-01' , pais = 'ES' ); A nivel del sistema de fichero, las particiones se traducen en subdirectorios dentro de la carpeta de la tabla. Por ejemplo, tras insertar varios ficheros de logs, podr\u00edamos tener una estructura similar a: /user/hive/warehouse/logs \u251c\u2500\u2500 fecha=2022-01-01/ \u2502 \u251c\u2500\u2500 pais=ES/ \u2502 \u2502 \u251c\u2500\u2500 log1 \u2502 \u2502 \u2514\u2500\u2500 log2 \u2502 \u2514\u2500\u2500 pais=US/ \u2502 \u2514\u2500\u2500 log3 \u2514\u2500\u2500 fecha=2022-01-02/ \u251c\u2500\u2500 pais=ES/ \u2502 \u2514\u2500\u2500 log4 \u2514\u2500\u2500 pais=US/ \u251c\u2500\u2500 log5 \u2514\u2500\u2500 log6 Para averiguar las particiones en Hive , utilizaremos el comando SHOW PARTITIONS : hive> SHOW PARTITIONS logs; fecha=2022-01-01/pais=ES fecha=2022-01-01/pais=US fecha=2022-01-02/pais=ES fecha=2022-01-02/pais=US Hay que tener en cuenta que la definici\u00f3n de columnas de la clausula PARTITIONED BY forman parte de las columnas de la tabla, y se conocen como columnas de partici\u00f3n . Sin embargo, los ficheros de datos no contienen valores para esas columnas, ya que se deriva el nombre del subdirectorio. Podemos utilizar las columnas de partici\u00f3n en las consultas igual que una columna ordinaria. Hive traduce la consulta en la navegaci\u00f3n adecuada para s\u00f3lo escanear las particiones relevantes. Por ejemplo, la siguiente consulta solo escanear\u00e1 los ficheros log1 , log2 y log4 : SELECT ts , fecha , linea FROM logs WHERE pais = 'ES' ; Moviendo datos a una tabla particionada Si queremos mover datos de una tabla ordinaria a una particionada (vaciando los datos de la partici\u00f3n existente): INSERT OVERWRITE TABLE logs PARTITION ( dt = '2022-01-01' ) SELECT col1 , col2 FROM fuente ; Otra posibilidad es utilizar un particionado din\u00e1mico , de manera que las particiones se crean de forma relativa a los datos: INSERT OVERWRITE TABLE logs PARTITION ( dt ) SELECT col1 , col2 FROM fuente ; Para ello, previamente hay que habilitarlo (por defecto est\u00e1 deshabilitado para evitar la creaci\u00f3n de m\u00faltiples particiones sin querer) y configurar el modo no estricto para que no nos obligue a indicar al menos una partici\u00f3n est\u00e1tica: set hive.exec.dynamic.partition = true set hive.exec.dynamic.partition.mode = nonstrict ;","title":"Particiones"},{"location":"apuntes/bdaplicado04hive.html#buckets","text":"Otro concepto importante en Hive son los buckets . Son particiones hasheadas por una columna/clave, en las que los datos se distribuyen en funci\u00f3n de su valor hash . Existen dos razones por las cuales queramos organizar las tablas (o particiones) en buckets . La primera es para conseguir consultas m\u00e1s eficientes, ya que imponen una estructura extra en las tablas. Los buckets pueden acelerar las operaciones de tipo join si las claves de bucketing y de join coinciden, ya que una clave ajena busca \u00fanicamente en el bucket adecuado de la clave primaria. Debido a los beneficios de los buckets/particiones, se deben considerar siempre que puedan optimizar el rendimiento de las consultas realizadas. Para indicar que nuestras tablas utilicen buckets , hemos de emplear la clausula CLUSTERED BY para indicar la columnas y el n\u00famero de buckets (se recomienda que la cantidad de buckets sea potencia de 2): CREATE TABLE usuarios_bucketed ( id INT , nombre STRING ) CLUSTERED BY ( id ) INTO 4 BUCKETS ; Bucketing est\u00e1 muy relacionado con el proceso de carga de datos. Para cargar los datos en una tabla con buckets , debemos bien indicar el n\u00famero m\u00e1ximo de reducers para que coincida con el n\u00famero de buckets , o habilitar el bucketing (esta es la recomendada): set map . reduce . tasks = 4 ; set hive . enforce . bucketing = true ; -- mejor as\u00ed Una vez creada la tabla, se rellena con los datos que tenemos en otra tabla: INSERT OVERWRITE TABLE usuarios_bucketed SELECT * FROM usuarios ; F\u00edsicamente, cada bucket es un fichero de la carpeta con la tabla (o partici\u00f3n). El nombre del fichero no es importante, pero el bucket n es el fichero n\u00famero n . Por ejemplo, si miramos el contenido de la tabla en HDFS tendremos: hive> dfs -ls /user/hive/warehouse/usuarios_bucketed ; 000000_0 000001_0 000002_0 000003_0 El segundo motivo es para obtener un sampling de forma m\u00e1s eficiente. Al trabajar con grandes datasets, normalmente obtenemos una peque\u00f1a fracci\u00f3n del dataset para comprender o refinar los datos. Por ejemplo, podemos obtener los datos de \u00fanicamente uno de los buckets : SELECT * FROM usuarios_bucketed TABLESAMPLE ( BUCKET 1 OUT OF 4 ON id ); Mediante el id del usuario determinamos el bucket (el cual se utiliza para realizar el hash del valor y ubicarlo dentro de uno de los buckets ), de manera que cada bucket contendr\u00e1 de manera eficiente un conjunto aleatorio de usuarios.","title":"Buckets"},{"location":"apuntes/bdaplicado04hive.html#resumen","text":"A continuaci\u00f3n mostramos en una tabla puntos a favor y en contra de utilizar estas estructuras Particionado + Particionado - Bucketing + Bucketing - Distribuye la carga de ejecuci\u00f3n horizontalmente. Existe la posibilidad de crear demasiadas particiones que contienen muy poco datos Proporciona una respuesta de consulta m\u00e1s r\u00e1pida, al acceder a porciones. El n\u00famero de buckets se define durante la creaci\u00f3n de la tabla -> Los programadores deben cargar manualmente un volumen equilibrado de datos. En la partici\u00f3n tiene lugar la ejecuci\u00f3n m\u00e1s r\u00e1pida de consultas con el volumen de datos bajo. Por ejemplo, la poblaci\u00f3n de b\u00fasqueda de la Ciudad del Vaticano devuelve muy r\u00e1pido en lugar de buscar la poblaci\u00f3n mundial completa. La partici\u00f3n es eficaz para datos de bajo volumen. Pero algunas consultas como agrupar por un gran volumen de datos tardan mucho en ejecutarse. Por ejemplo, agrupar las consultas del mes de Enero tardar\u00e1 m\u00e1s que los viernes de Enero. Al utilizar vol\u00famenes similares de datos en cada partici\u00f3n, los map joins ser\u00e1n m\u00e1s r\u00e1pidos. Supongamos que $HDFS_HIVE contiene la ruta con la ra\u00edz de las tablas internas de Hive, en nuestro caso /user/hive/warehouse . Respecto al nivel de estructura y representaci\u00f3n en carpetas de HDFS tendr\u00edamos: ENTIDAD EJEMPLO UBICACI\u00d3N base de datos iabd $HDFS_HIVE/iabd.db tabla T $HDFS_HIVE/iabd.db/T partici\u00f3n fecha='01012022' $HDFS_HIVE/iabd.db/T/fecha=01012022 bucket columna id $HDFS_HIVE/iabd.db/T/fecha=01012022/000000_0","title":"Resumen"},{"location":"apuntes/bdaplicado04hive.html#caso-de-uso-6-particionado-y-bucketing","text":"A continuaci\u00f3n, vamos a coger los datos de las productos y las categor\u00edas de la base de datos retail_db , y colocarlos en una estructura de Hive particionada y que utilice bucketing . La estructura de las tablas es la siguiente: Relaci\u00f3n entre productos y categor\u00edas El primer paso es traernos los datos de MariaDB a tablas internas haciendo uso de Sqoop : Tabla categories Tabla products sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = categories --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table categories sqoop import --connect jdbc:mysql://localhost/retail_db \\ --username = iabd --password = iabd \\ --table = products --driver = com.mysql.jdbc.Driver \\ --hive-import --hive-database iabd \\ --create-hive-table --hive-table products El siguiente paso que vamos a realizar es crear en Hive una tabla con el c\u00f3digo del producto, su nombre, el nombre de la categor\u00eda y el precio del producto. Estos datos los vamos a particionar por categor\u00eda y clusterizado en 8 buckets: CREATE TABLE IF NOT EXISTS productos ( id INT , nombre STRING , precio DOUBLE ) PARTITIONED BY ( categoria STRING ) CLUSTERED BY ( id ) INTO 8 BUCKETS ; Y cargamos los datos con una consulta que realice un join de las tablas categories y products con particionado din\u00e1mico (recuerda activarlo mediante set hive.exec.dynamic.partition.mode=nonstrict; ): INSERT OVERWRITE TABLE productos PARTITION ( categoria ) SELECT p . product_id as id , p . product_name as nombre , p . product_price as precio , c . category_name as categoria FROM products p join categories c on ( p . product_category_id = c . category_id ); Si queremos comprobar como se han creado las particiones y los buckets, desde un terminal podemos acceder a HDFS y mostrar su contenido: hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos hdfs dfs -ls hdfs://iabd-virtualbox:9000/user/hive/warehouse/iabd.db/productos/categoria = Accessories Si volvemos a Hive , ahora podemos consultar los datos: select * from productos limit 5 ; Y vemos c\u00f3mo aparecen 5 elementos que pertenecen a la primera partici\u00f3n. Si quisi\u00e9ramos, por ejemplo, 10 elementos de particiones diferentes deber\u00edamos ordenarlos de manera aleatoria: select * from productos order by rand () limit 10 ; A continuaci\u00f3n vamos a realizar diversas consultas utilizando las funciones ventana que soporta Hive . M\u00e1s informaci\u00f3n en https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics .","title":"Caso de uso 6: Particionado y Bucketing"},{"location":"apuntes/bdaplicado04hive.html#consultas-con-enteros-que-cuentanordenan","text":"Consultas sobre categor\u00edas Las siguientes consultas las vamos a realizar sobre s\u00f3lo dos categor\u00edas para acotar los resultados obtenidos. Adem\u00e1s, hemos recortado el nombre del producto a 20 caracteres para facilitar la legibilidad de los resultados. Las funciones rank y dense_rank permite obtener la posici\u00f3n que ocupan los datos. Se diferencia en que rank cuenta los elementos repetidos/empatados, mientras que dense_rank no. Por ejemplo, vamos a obtener la posici\u00f3n que ocupan los productos respecto al precio agrupados por su categor\u00eda: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , rank () over ( partition by categoria order by precio desc ) as rank , dense_rank () over ( partition by categoria order by precio desc ) as denseRank from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+-------+------------+ | nombre | categoria | precio | rank | denserank | +-----------------------+--------------------+----------+-------+------------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 5 | 5 | | Diamondback Adult So | Basketball | 299.98 | 7 | 6 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | 7 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | 8 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | 9 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | 10 | | Easton S1 Youth Bat | Basketball | 179.97 | 11 | 10 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | 11 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | 12 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | 13 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | 14 | | Elevation Training M | Basketball | 79.99 | 17 | 15 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | 16 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | 17 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | 18 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | 19 | | adidas Brazuca 2014 | Basketball | 29.99 | 21 | 19 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | 20 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | 21 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 2 | 2 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | 3 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | 4 | +-----------------------+--------------------+----------+-------+------------+ La funci\u00f3n row_number permite numerar los resultados: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , row_number () over ( partition by categoria order by precio desc ) as numfila from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | numfila | +-----------------------+--------------------+----------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 1 | | SOLE E25 Elliptical | Basketball | 999.99 | 2 | | Diamondback Adult Re | Basketball | 349.98 | 3 | | Diamondback Adult Ou | Basketball | 309.99 | 4 | | Diamondback Girls' C | Basketball | 299.99 | 5 | | Diamondback Boys' In | Basketball | 299.99 | 6 | | Diamondback Adult So | Basketball | 299.98 | 7 | | Easton Mako Youth Ba | Basketball | 249.97 | 8 | | Fitness Gear 300 lb | Basketball | 209.99 | 9 | | Quik Shade Summit SX | Basketball | 199.99 | 10 | | Easton XL1 Youth Bat | Basketball | 179.97 | 11 | | Easton S1 Youth Bat | Basketball | 179.97 | 12 | | adidas Brazuca 2014 | Basketball | 159.99 | 13 | | Quest 12' x 12' Dome | Basketball | 149.99 | 14 | | Fitbit Flex Wireless | Basketball | 99.95 | 15 | | Nike+ Fuelband SE | Basketball | 99.0 | 16 | | Elevation Training M | Basketball | 79.99 | 17 | | MAC Sports Collapsib | Basketball | 69.99 | 18 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 19 | | adidas Brazuca 2014 | Basketball | 39.99 | 20 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 21 | | adidas Brazuca 2014 | Basketball | 29.99 | 22 | | Nike Women's Pro Cor | Basketball | 28.0 | 23 | | Nike Women's Pro Vic | Basketball | 21.99 | 24 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 1 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 2 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 3 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 4 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 5 | +-----------------------+--------------------+----------+----------+","title":"Consultas con enteros que cuentan/ordenan"},{"location":"apuntes/bdaplicado04hive.html#consultas-por-posicion","text":"A continuaci\u00f3n vamos a ver las funciones lead y lag . Estas funciones se encargan de obtener el valor posterior y anterior respecto a un valor. select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , lead ( precio ) over ( partition by categoria order by precio desc ) as sig , lag ( precio ) over ( partition by categoria order by precio desc ) as ant from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+---------+----------+ | nombre | categoria | precio | sig | ant | +-----------------------+--------------------+----------+---------+----------+ | SOLE F85 Treadmill | Basketball | 1799.99 | 999.99 | NULL | | SOLE E25 Elliptical | Basketball | 999.99 | 349.98 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 309.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 299.99 | 349.98 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | 309.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.98 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 249.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 209.99 | 299.98 | | Fitness Gear 300 lb | Basketball | 209.99 | 199.99 | 249.97 | | Quik Shade Summit SX | Basketball | 199.99 | 179.97 | 209.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | 199.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 159.99 | 179.97 | | adidas Brazuca 2014 | Basketball | 159.99 | 149.99 | 179.97 | | Quest 12' x 12' Dome | Basketball | 149.99 | 99.95 | 159.99 | | Fitbit Flex Wireless | Basketball | 99.95 | 99.0 | 149.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 79.99 | 99.95 | | Elevation Training M | Basketball | 79.99 | 69.99 | 99.0 | | MAC Sports Collapsib | Basketball | 69.99 | 59.98 | 79.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 39.99 | 69.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 29.99 | 59.98 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 29.99 | 39.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 28.0 | 29.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 21.99 | 29.99 | | Nike Women's Pro Vic | Basketball | 21.99 | NULL | 28.0 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 169.99 | NULL | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 119.99 | 169.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 0.0 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | NULL | 119.99 | +-----------------------+--------------------+----------+---------+----------+","title":"Consultas por posici\u00f3n"},{"location":"apuntes/bdaplicado04hive.html#consultas-de-agregacion","text":"Las funciones de agregaci\u00f3n que ya conocemos como count , sum , min y max tambi\u00e9n las podemos aplicar sobre particiones de datos y as\u00ed poder mostrar los datos agregados para cada elemento: select substr ( nombre , 1 , 20 ) as nombre , categoria , count ( precio ) over ( partition by categoria ) as cantidad , min ( precio ) over ( partition by categoria ) as menor , max ( precio ) over ( partition by categoria ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+-----------+--------+----------+ | nombre | categoria | cantidad | menor | mayor | +-----------------------+--------------------+-----------+--------+----------+ | Fitbit Flex Wireless | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Re | Basketball | 24 | 21.99 | 1799.99 | | Nike+ Fuelband SE | Basketball | 24 | 21.99 | 1799.99 | | Elevation Training M | Basketball | 24 | 21.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Girls' C | Basketball | 24 | 21.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 24 | 21.99 | 1799.99 | | Easton Mako Youth Ba | Basketball | 24 | 21.99 | 1799.99 | | SOLE E25 Elliptical | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 24 | 21.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 24 | 21.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 24 | 21.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 24 | 21.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 24 | 21.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Boys' In | Basketball | 24 | 21.99 | 1799.99 | | Diamondback Adult So | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Cor | Basketball | 24 | 21.99 | 1799.99 | | Quik Shade Summit SX | Basketball | 24 | 21.99 | 1799.99 | | Quest 12' x 12' Dome | Basketball | 24 | 21.99 | 1799.99 | | Nike Women's Pro Vic | Basketball | 24 | 21.99 | 1799.99 | | Cleveland Golf Class | Bike & Skate Shop | 5 | 0.0 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 5 | 0.0 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 5 | 0.0 | 179.99 | +-----------------------+--------------------+-----------+--------+----------+ Las consultas que hemos visto en este caso de uso tambi\u00e9n se conocen como funciones ventana , ya que se ejecutan sobre un subconjunto de los datos. La ventana viene dada por la partici\u00f3n o por la posici\u00f3n una vez ordenados los datos. Los posibles valores son: rows between current row and unbounded following : desde la fila actual hasta el final de la ventana/partici\u00f3n. rows between current row and N following : desde la fila actual hasta los N siguientes. rows between unbounded preceding and current row : desde el inicio de la ventana hasta la fila actual. rows between unbounded preceding and N following : desde el inicio de la ventana hasta los N siguientes. rows between unbounded preceding and unbounded following : desde el inicio de la ventana hasta el final de la ventana (caso por defecto) rows between N preceding and M following : desde N filas anteriores hasta M filas siguientes. Por ejemplo, para obtener el m\u00e1ximo precio desde la fila actual hasta el resto de la partici\u00f3n: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between current row and unbounded following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 1799.99 | | adidas Brazuca 2014 | Basketball | 39.99 | 1799.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 1799.99 | | Diamondback Adult Re | Basketball | 349.98 | 1799.99 | | Nike+ Fuelband SE | Basketball | 99.0 | 1799.99 | | Elevation Training M | Basketball | 79.99 | 1799.99 | | Easton XL1 Youth Bat | Basketball | 179.97 | 1799.99 | | adidas Brazuca 2014 | Basketball | 29.99 | 1799.99 | | Diamondback Girls' C | Basketball | 299.99 | 1799.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 1799.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 1799.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 1799.99 | | Diamondback Adult Ou | Basketball | 309.99 | 1799.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 1799.99 | | MAC Sports Collapsib | Basketball | 69.99 | 1799.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 299.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.98 | | Nike Women's Pro Cor | Basketball | 28.0 | 199.99 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 149.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 21.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 179.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 179.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 169.99 | +-----------------------+--------------------+----------+----------+ Si queremos comparar el precio y quedarnos con el mayor respecto al anterior y el posterior podr\u00edamos realizar la siguiente consulta: select substr ( nombre , 1 , 20 ) as nombre , categoria , precio , max ( precio ) over ( partition by categoria rows between 1 preceding and 1 following ) as mayor from productos where categoria = \"Bike & Skate Shop\" or categoria = \"Basketball\" ; Resultado: +-----------------------+--------------------+----------+----------+ | nombre | categoria | precio | mayor | +-----------------------+--------------------+----------+----------+ | Fitbit Flex Wireless | Basketball | 99.95 | 99.95 | | adidas Brazuca 2014 | Basketball | 39.99 | 209.99 | | Fitness Gear 300 lb | Basketball | 209.99 | 349.98 | | Diamondback Adult Re | Basketball | 349.98 | 349.98 | | Nike+ Fuelband SE | Basketball | 99.0 | 349.98 | | Elevation Training M | Basketball | 79.99 | 179.97 | | Easton XL1 Youth Bat | Basketball | 179.97 | 179.97 | | adidas Brazuca 2014 | Basketball | 29.99 | 299.99 | | Diamondback Girls' C | Basketball | 299.99 | 299.99 | | Easton S1 Youth Bat | Basketball | 179.97 | 299.99 | | Easton Mako Youth Ba | Basketball | 249.97 | 999.99 | | SOLE E25 Elliptical | Basketball | 999.99 | 999.99 | | Diamondback Adult Ou | Basketball | 309.99 | 999.99 | | Kijaro Dual Lock Cha | Basketball | 29.99 | 309.99 | | MAC Sports Collapsib | Basketball | 69.99 | 159.99 | | adidas Brazuca 2014 | Basketball | 159.99 | 1799.99 | | SOLE F85 Treadmill | Basketball | 1799.99 | 1799.99 | | Quest Q64 10 FT. x 1 | Basketball | 59.98 | 1799.99 | | Diamondback Boys' In | Basketball | 299.99 | 299.99 | | Diamondback Adult So | Basketball | 299.98 | 299.99 | | Nike Women's Pro Cor | Basketball | 28.0 | 299.98 | | Quik Shade Summit SX | Basketball | 199.99 | 199.99 | | Quest 12' x 12' Dome | Basketball | 149.99 | 199.99 | | Nike Women's Pro Vic | Basketball | 21.99 | 149.99 | | Cleveland Golf Class | Bike & Skate Shop | 119.99 | 169.99 | | TaylorMade RocketBal | Bike & Skate Shop | 169.99 | 169.99 | | Callaway X Hot Drive | Bike & Skate Shop | 0.0 | 179.99 | | Nike VR_S Covert Dri | Bike & Skate Shop | 179.99 | 179.99 | | Cobra AMP Cell Drive | Bike & Skate Shop | 169.99 | 179.99 | +-----------------------+--------------------+----------+----------+","title":"Consultas de agregaci\u00f3n"},{"location":"apuntes/bdaplicado04hive.html#referencias","text":"P\u00e1gina oficial de Hive Apache Hive Essentials - Second Edition de Dayong Du. Tutorial de Hive de TutorialsPoint.","title":"Referencias"},{"location":"apuntes/bdaplicado04hive.html#actividades","text":"Realiza los casos de uso del 1 al 5. En la entrega debes adjuntar un una captura de pantalla donde se vea la ejecuci\u00f3n de las diferentes instrucciones. (opcional) Realiza el caso de uso 6. (opcional) A partir de la base de datos retail_db , importa las tablas orders y order_items , en las cuales puedes obtener la cantidad de productos que contiene un pedido. Utilizando todas las tablas que ya hemos importado en los casos anteriores, crea una tabla externa en Hive llamada pedidos utilizando 8 buckets con el c\u00f3digo del cliente, que contenga: C\u00f3digo y fecha del pedido. Precio del pedido (sumando las l\u00edneas de pedido). C\u00f3digo, nombre y apellidos del cliente. Adjunta scripts y capturas de: la importaci\u00f3n, creaci\u00f3n y carga de datos de las tablas que necesites. la definici\u00f3n de la tabla: describe formatted pedidos; contenido de HDFS que demuestre la creaci\u00f3n de los buckets . (opcional) Investiga la creaci\u00f3n de vistas en Hive y crea una vista con los datos de los clientes y sus pedidos siempre y cuando superen los 200$.","title":"Actividades"},{"location":"apuntes/bdaplicado05kafka.html","text":"Kafka \u00b6 Introducci\u00f3n \u00b6 Apache Kafka es, en pocas palabras, un middleware de mensajer\u00eda entre sistemas heterog\u00e9neos, el cual, mediante un sistema de colas ( topics , para ser concreto) facilita la comunicaci\u00f3n as\u00edncrona, desacoplando los flujos de datos de los sistemas que los producen o consumen. Funciona como un broker de mensajes, encargado de enrutar los mensajes entre los clientes de un modo muy r\u00e1pido. Kafka como middleware/broker de mensajes En concreto, se trata de una plataforma open source distribuida de transmisi\u00f3n de eventos/mensajes en tiempo real con almacenamiento duradero y que proporciona de base un alto rendimiento (capaz de manejar billones de peticiones al d\u00eda, con una latencia inferior a 10ms), tolerancia a fallos, disponibilidad y escalabilidad horizontal (mediante cientos de nodos). Evento / Mensaje Dentro del vocabulario asociado a arquitectura as\u00edncronas basadas en productor/consumidor o publicador/suscriptor, se utiliza el mensaje para indicar el dato que viaja desde un punto a otro. En Kafka, adem\u00e1s de utilizar el concepto mensaje, se emplea el t\u00e9rmino evento. M\u00e1s del 80% de las 100 compa\u00f1\u00edas m\u00e1s importantes de EEUU utilizan Kafka : Uber , Twitter , Netflix , Spotify , Blizzard , LinkedIn , Spotify , y PayPal procesan cada d\u00eda sus mensajes con Kafka . Como sistema de mensajes, sigue un modelo publicador-suscriptor. Su arquitectura tiene dos directivas claras: No bloquear los productores (para poder gestionar la back pressure , la cual sucede cuando un publicador produce m\u00e1s elementos de los que un suscriptor puede consumir). Aislar los productores y los consumidores, de manera que los productores y los consumidores no se conocen. A d\u00eda de hoy, Apache Kafka se utiliza, adem\u00e1s de como un sistema de mensajer\u00eda, para ingestar datos, realizar procesado de datos en streaming y anal\u00edtica de datos en tiempo real, as\u00ed como en arquitectura de microservicios y sistemas IOT. Amazon Kinesis Amazon Kinesis es un producto similar a Apache Kafka pero dentro de la plataforma AWS, por lo que no es un producto open source como tal. Su principal ventaja es la facilidad de escalabilidad a golpe de click e integraci\u00f3n con el resto de servicios que ofrece AWS. Se trata de una herramienta muy utilizada que permite incorporar datos en tiempo real, como v\u00eddeos, audios, registros de aplicaciones, secuencias de clicks de sitios web y datos de sensores IoT para machine learning, anal\u00edtica de datos en streaming, etc... Publicador / Suscriptor \u00b6 Antes de entrar en detalle sobre Kafka, hay que conocer el modelo publicador/suscriptor. Este patr\u00f3n tambi\u00e9n se conoce como publish / subscribe o productor / consumidor . Hay tres elementos que hay que tener realmente claros: Publicador ( publisher / productor / emisor): genera un dato y lo coloca en un topic como un mensaje. topic (tema): almac\u00e9n temporal/duradero que guarda los mensajes funcionando como una cola. Suscriptor ( subscriber / consumidor / receptor): recibe el mensaje. Cabe destacar que un productor no se comunica nunca directamente con un consumidor, siempre lo hace a trav\u00e9s de un topic : Productor - Consumidor Caso 0: Hola Kafka \u00b6 Para arrancar Kafka, vamos a utilizar la instalaci\u00f3n que tenemos creada en nuestra m\u00e1quina virtual. Kafka mediante Docker Bitnami tiene una imagen para trabajar con Docker la cual permite probar todos los ejemplos de esta sesi\u00f3n. Para ello, se recomienda seguir los pasos de la p\u00e1gina oficial: https://hub.docker.com/r/bitnami/kafka/ El primer paso, una vez dentro de la carpeta de instalaci\u00f3n de Kafka (en nuestro caso /opt/kafka_2.13-2.8.1 ), es arrancar Zookeeper mediante el comando zookeeper-server-start.sh , el cual se encarga de gestionar la comunicaci\u00f3n entre los diferentes brokers: zookeeper-server-start.sh ./config/zookeeper.properties zookeeper.properties Del archivo de configuraci\u00f3n de Zookeeper conviene destacar dos propiedades: clientPort : puerto por defecto (2181) dataDir : indica donde est\u00e1 el directorio de datos de Zookeeper (por defecto es tmp/zookeeper , pero si queremos que dicha carpeta no se elimine es mejor que apunte a una ruta propia, por ejemplo /opt/zookeeper-data ) Para comprobar que Zookeeper est\u00e1 arrancado, podemos ejecutar el comando lsof -i :2181 , el cual escanea el puerto 2181 donde est\u00e1 corriendo Zookeeper . Una vez comprobado, en un nuevo terminal, arrancamos el servidor de Kafka mediante el comando kafka-server-start.sh (de manera que tenemos corriendo a la vez Zookeeper y Kafka ): kafka-server-start.sh ./config/server.properties Creando un topic \u00b6 A continuaci\u00f3n, en un tercer terminal, vamos a crear un topic mediante el comando kafka-topics.sh : kafka-topics.sh --create --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 Si queremos obtener la descripci\u00f3n del topic creado con la cantidad de particiones le pasamos el par\u00e1metro --describe : kafka-topics.sh --describe --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 Obteniendo la siguiente informaci\u00f3n: Topic: iabd-topic TopicId: ogKnRpOFS7mfOhspLcuB4A PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 Topic: iabd-topic Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Produciendo mensajes \u00b6 Para enviar un mensaje a un topic , ejecutaremos en un cuarto terminal un productor mediante el comando kafka-console-producer.sh . Por defecto, cada l\u00ednea que introduzcamos resultar\u00e1 en un evento separado que escribir\u00e1 un mensaje en el topic (podemos pulsar CTRL+C en cualquier momento para cancelar): kafka-console-producer.sh --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 As\u00ed pues, escribimos los mensajes que queramos: >Este es un mensaje >Y este es otro >Y el tercero Consumiendo mensajes \u00b6 Y finalmente, en otro terminal, vamos a consumir los mensajes: kafka-console-consumer.sh --topic iabd-topic --from-beginning --bootstrap-server iabd-virtualbox:9092 Al ejecutarlo veremos los mensajes que hab\u00edamos introducido antes (ya que hemos indicado la opci\u00f3n --from-beginning ). Si ahora volvemos a escribir en el productor, casi instant\u00e1neamente, aparecer\u00e1 en el consumidor el mismo mensaje. Tras esto, paramos todos los procesos que se est\u00e1n ejecutando mediante CTRL+C y hemos finalizado nuestro primer contacto con Kafka. Elementos \u00b6 Dentro de una arquitectura con Kafka, existen m\u00faltiples elementos que interact\u00faan entre s\u00ed. Topic y Particiones \u00b6 Un topic (\u00bftema?) es un flujo particular de datos que funciona como una cola almacenando de forma temporal o duradera los datos que se colocan en \u00e9l. Podemos crear tantos topics como queramos y cada uno de ellos tendr\u00e1 un nombre un\u00edvoco. Un topic se divide en particiones , las cuales se numeran, siendo la primera la 0. Al crear un topic hemos de indicar la cantidad de particiones inicial, la cual podemos modificar a posteriori . Cada partici\u00f3n est\u00e1 ordenada, de manera que cada mensaje dentro de una partici\u00f3n tendr\u00e1 un identificador incremental, llamado offset (desplazamiento). Cada partici\u00f3n funciona como un commit log almacenando los mensajes que recibe. Offset dentro de las particiones de un topic Como podemos observar en la imagen, cada partici\u00f3n tiene sus propios offset (el offset 3 de la partici\u00f3n 0 no representa el mismo dato que el offset 3 de la partici\u00f3n 1). Hab\u00edamos comentado que las particiones est\u00e1n ordenadas, pero el orden s\u00f3lo se garantiza dentro de una partici\u00f3n (no entre particiones), es decir, el mensaje 7 de la partici\u00f3n 0 puede haber llegado antes, a la vez, o despu\u00e9s que el mensaje 5 de la partici\u00f3n 1. Los datos de una partici\u00f3n tiene un tiempo de vida limitado ( retention period ) que indica el tiempo que se mantendr\u00e1n los mensajes antes de eliminarlos. Por defecto es de una semana. Adem\u00e1s, una vez que los datos se escriben en una partici\u00f3n, no se pueden modificar (las mensajes son immutables). Finalmente, por defecto, los datos se asignan de manera aleatoria a una partici\u00f3n. Sin embargo, existe la posibilidad de indicar una clave de particionado. Brokers \u00b6 Un cl\u00faster de Kafka est\u00e1 compuesto de m\u00faltiples nodos conocidos como Brokers , donde cada broker es un servidor de Kafka . Cada broker se identifica con un id, el cual debe ser un n\u00famero entero. Cada broker contiene un conjunto de particiones, de manera que un broker contiene parte de los datos, nunca los datos completos ya que Kafka es un sistema distribuido. Al conectarse a un broker del cl\u00faster ( bootstrap broker ), autom\u00e1ticamente nos conectaremos al cl\u00faster entero. Para comenzar se recomienda una arquitectura de 3 brokers, aunque algunos cl\u00fasters lo forman cerca de un centenar de brokers . Por ejemplo, el siguiente gr\u00e1fico muestra el topic A dividido en tres particiones, cada una de ellas residiendo en un broker diferente (no hay ninguna relaci\u00f3n entre el n\u00famero de la partici\u00f3n y el nombre del broker), y el topic B dividido en dos particiones: Ejemplo de 3 brokers En el caso de haber introducido un nuevo topic con 4 particiones, uno de los brokers contendr\u00eda dos particiones. Factor de replicaci\u00f3n \u00b6 Para soportar la tolerancia a fallos, los topics deben tener un factor de replicaci\u00f3n mayor que uno (normalmente se configura entre 2 y 3). En la siguiente imagen podemos ver como tenemos 3 brokers, y un topic A con dos particiones y una factor de replicaci\u00f3n de 2, de manera que cada partici\u00f3n crea un replica de si misma: Divisiones de un broker en particiones Si se cayera el broker 102 , Kafka podr\u00eda devolver los datos al estar disponibles en los nodos 101 y 103. R\u00e9plica l\u00edder \u00b6 Acabamos de ver que cada broker tiene m\u00faltiples particiones, y cada partici\u00f3n tiene m\u00faltiples r\u00e9plicas, de manera que si se cae un nodo/broker, Kafka puede utilizar otro broker para servir los datos. En cualquier instante, una determinada partici\u00f3n tendr\u00e1 una \u00fanica r\u00e9plica que ser\u00e1 la l\u00edder, y esta r\u00e9plica l\u00edder ser\u00e1 la \u00fanica que pueda recibir y servir los datos de una partici\u00f3n. La r\u00e9plica l\u00edder es importante porque todas las lecturas y escrituras siempre van a esta r\u00e9plica. El resto de brokers sincronizar\u00e1n sus datos. En resume, cada partici\u00f3n tendr\u00e1 un l\u00edder y m\u00faltiples ISR ( in-sync replica ). R\u00e9plicas de una partici\u00f3n Si se cayera el Broker 101 , entonces la partici\u00f3n 0 del Broker 102 se convertir\u00eda en la l\u00edder. Y cuando vuelva a funcionar el Broker 101 , intentar\u00e1 volver a ser la partici\u00f3n l\u00edder. Productores \u00b6 Los productores escriben datos en los topics , sabiendo autom\u00e1ticamente el broker y la partici\u00f3n en la cual deben escribir. En el caso de un fallo de un broker , los productores autom\u00e1ticamente se recuperan y se comunican con el broker adecuado. Si el productor env\u00eda los datos sin una clave determinada, Kafka realiza una algoritmo de Round Robin , de manera que cada mensaje se va alternando entre los diferentes brokers . La carga se balancea entre los brokers Podemos configurar los productores para que reciban un ACK de las escrituras de los datos con los siguientes valores: ack=0 : El productor no espera la confirmaci\u00f3n (posible p\u00e9rdida de datos). ack=1 : El productor espera la confirmaci\u00f3n del l\u00edder (limitaci\u00f3n de la p\u00e9rdida de datos). ack=all : El productores espera la confirmaci\u00f3n del l\u00edder y de todas las r\u00e9plicas (sin p\u00e9rdida de datos). Clave de mensaje \u00b6 Los productores pueden enviar una clave con el mensaje (de tipo cadena, num\u00e9rico, etc...). Cuando la clave no se env\u00eda, ya hemos comentado que los datos se env\u00edan mediante Round Robin (primero Broker 101 , luego el 102, el 103, etc... y vuelta al 101). Si se env\u00eda la clave, todos los mensajes con la misma clave siempre ir\u00e1n a la misma partici\u00f3n. Por lo tanto, enviaremos una clave cuando necesitemos ordenar los mensajes por un campo espec\u00edfico (por ejemplo, el identificador de una operaci\u00f3n). Consumidores \u00b6 Los consumidores obtienen los datos de los topics y las particiones, y saben de qu\u00e9 broker deben leer los datos. Igual que los productores, en el caso de un fallo de un broker , los consumidores autom\u00e1ticamente se recuperan y se comunican con el broker adecuado. Los datos se leen en orden dentro de cada partici\u00f3n, de manera que el consumidor no podr\u00e1 leer, por ejemplo, los datos del offset 6 hasta que no haya le\u00eddo los del offset 5. Adem\u00e1s, un consumidor puede leer de varias particiones (se realiza en paralelo), pero el orden s\u00f3lo se respeta dentro de cada partici\u00f3n, no entre particiones: Los consumidores leen en orden dentro de cada partici\u00f3n Grupo de consumidores \u00b6 Un consumidor puede pertenecer a un grupo de consumidores, de manera que cada uno de los consumidores del grupo obtendr\u00e1n una parte de los datos, es decir, una partici\u00f3n de un topic . Por ejemplo, tenemos una aplicaci\u00f3n compuesta de dos consumidores, formando un grupo de consumidores. El consumidor 1 lo har\u00e1 de dos particiones, y el consumidor 2 lo har\u00e1 de la tercera partici\u00f3n. Tambi\u00e9n tenemos otra aplicaci\u00f3n compuesta de tres consumidores, de manera que cada consumidor lo har\u00e1 de cada una de las particiones. Finalmente, tenemos un tercer grupo de consumidores formado por un \u00fanico consumidor que leer\u00e1 las tres particiones. En conclusi\u00f3n, cada grupo de consumidores funciona como un \u00fanico consumidor de manera que accede a todas las particiones de un topic . Grupos de consumidores Coordinando los consumidores Los consumidores, por s\u00ed solos, no saben con que partici\u00f3n se deben comunicar. Para ello, se utiliza un GroupCoordinator y un Consumer Coordinator para asignar los consumidores a cada partici\u00f3n. Esta gesti\u00f3n la realiza Kafka. Cabe destacar que los diferentes grupos de consumidores reciben el mismo dato de cada partici\u00f3n, es decir, el consumidor 1 del grupo 1 y el consumidor 1 del grupo 2 reciben la informaci\u00f3n que hab\u00eda en la partici\u00f3n 0. Este caso de uso es muy \u00fatil cuando tenemos dos aplicaciones que queremos que reciban los mismos datos (por ejemplo, uno encargado de realizar machine learning y otro anal\u00edtica de datos). En el caso de tener m\u00e1s consumidores que particiones, algunos consumidores no realizar\u00e1n nada. Este caso de uso es at\u00edpico, ya que lo recomendable es tener tantos consumidores como el mayor n\u00famero de particiones existentes. Probando los grupos de consumidores \u00b6 Vamos a simular el gr\u00e1fico anterior mediante un ejemplo con el terminal. Primero crearemos un topic que contenga tres particiones: kafka-topics.sh --create --topic iabd-topic-group \\ --bootstrap-server iabd-virtualbox:9092 --partitions 3 Si comprobamos el estado del topic mediante: kafka-topics.sh --describe --topic iabd-topic-group \\ --bootstrap-server iabd-virtualbox:9092 Obtendremos la siguiente informaci\u00f3n: Topic: iabd-topic-group TopicId: p1i3m4fMRximngLjAV5rsA PartitionCount: 3 ReplicationFactor: 1 Configs: segment.bytes=1073741824 Topic: iabd-topic-group Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: iabd-topic-group Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Topic: iabd-topic-group Partition: 2 Leader: 0 Replicas: 0 Isr: 0 A continuaci\u00f3n, en dos pesta\u00f1as diferentes, vamos a crear dos consumidores que pertenezcan al mismo grupo de consumidores: kafka-console-consumer.sh --topic iabd-topic-group \\ --group iabd-app1 \\ --bootstrap-server iabd-virtualbox:9092 Y finalmente, creamos un nuevo productor sobre el topic : kafka-console-producer.sh --topic iabd-topic-group \\ --bootstrap-server iabd-virtualbox:9092 Y si creamos varios mensajes en el productor, veremos c\u00f3mo van llegando de manera alterna a los diferentes consumidores: Ejemplo de grupo de consumidores Autoevaluaci\u00f3n \u00bfQue suceder\u00e1 se creamos un nuevo consumidor que lo haga del mismo topic pero con un grupo de consumidores diferente (por ejemplo, iabd-app2 ) y le pedimos que lea los mensajes desde el principio (mediante --from-beginning ) ? Que aparecer\u00e1n todos los mensajes desde el principio. \u00bfY si lo detenemos y volvemos a crear el mismo consumidor (tambi\u00e9n con el grupo de consumidores iabd-app2 y los vuelva a leer desde el principio tambi\u00e9n)? En esta ocasi\u00f3n, ya no recibir\u00e1 ning\u00fan mensaje, ya que el primer consumidor hace commit de la lectura y el segundo al hacerlo desde el mismo grupo de consumidores ya tiene los mensajes previos marcados como le\u00eddos. \u00bfY si detenemos todos los consumidores y seguimos creando mensajes en el productor? Los mensajes se almacenan en el topic . \u00bfY si arrancamos de nuevo un consumidor sobre el grupo de consumidores iabd-app2 ? Que consumir\u00e1 los mensajes que acabamos de crear. Mediante el comando kafka-consumer-groups.sh podemos obtener sobre los diferentes grupos de consumidores que tenemos creado, as\u00ed como eliminarlos o resetear sus offsets. Por ejemplo, si queremos listar los grupos de consumidores existentes ejecutaremos: kafka-consumer-groups.sh --list \\ --bootstrap-server iabd-virtualbox:9092 En cambio, si queremos obtener la informaci\u00f3n de un determinado grupo ejecutaremos: kafka-consumer-groups.sh --describe --group iabd-app1 \\ --bootstrap-server iabd-virtualbox:9092 Obteniendo informaci\u00f3n a destacar como: CURRENT-OFFSET : valor actual del offset LOG-END-OFFSET : offset del \u00faltimo mensaje de la partici\u00f3n LAG : cantidad de mensajes pendientes de leer GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID iabd-app1 iabd-topic-group 0 4 4 0 consumer-iabd-app1-1-405b2b39-2252-4e12-ba55-00a579441df2 /127.0.0.1 consumer-iabd-app1-1 iabd-app1 iabd-topic-group 1 2 2 0 consumer-iabd-app1-1-405b2b39-2252-4e12-ba55-00a579441df2 /127.0.0.1 consumer-iabd-app1-1 iabd-app1 iabd-topic-group 2 4 4 0 consumer-iabd-app1-1-8f09bc45-8e8c-46d2-9c9c-cf6bd3a5fdc7 /127.0.0.1 consumer-iabd-app1-1 Si por ejemplo, con todos los consumidores detenidos, mediante un productor lanzamos 5 mensajes nuevos, estos se quedar\u00e1n en el topic a la espera de ser consumidos, y se habr\u00e1n repartidos entre las diferentes particiones. Si volvemos a lanzar el comando anterior obtendr\u00edamos: Consumer group 'iabd-app1' has no active members. GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID iabd-app1 iabd-topic-group 2 4 5 1 - - - iabd-app1 iabd-topic-group 1 2 4 2 - - - iabd-app1 iabd-topic-group 0 4 6 2 - - - Offsets de Consumidor \u00b6 Kafka almacena los offsets por el que va leyendo un grupo de consumidores, a modo de checkpoint , en un topic llamado __consumer_offsets . Cuando un consumidor de un grupo ha procesado los datos que ha le\u00eddo de Kafka , realizar\u00e1 un commit de sus offsets . Si el consumidor se cae, podr\u00e1 volver a leer los mensajes desde el \u00faltimo offset sobre el que se realiz\u00f3 commit . Por ejemplo, supongamos que tenemos un consumidor el cual ha hecho un commit tras el offset 4262. Tras el commit seguimos leyendo los siguientes mensajes: 4263, 4264, 4265 y de repente el consumidor se cae sin haber hecho commit de esos mensajes. Cuando el consumidor vuelva a funcionar, volver\u00e1 a leer los mensajes desde el 4263, asegur\u00e1ndose que no se ha quedado ning\u00fan mensaje sin procesar. Offsets de consumidor El commit de los mensajes est\u00e1 muy relacionado con la sem\u00e1ntica de la entrega. Los consumidores eligen cuando realizar el commit de los offsets : As most once : se realiza el commit del mensaje tan pronto como se recibe el mensaje. Si falla su procesamiento, el mensaje se perder\u00e1 (y no se volver\u00e1 a leer). At least once (opci\u00f3n m\u00e1s equilibrada): El commit se realiza una vez procesado el mensaje. Este enfoque puede resultar en un procesado duplicado de los mensajes, por lo que hemos de asegurarnos que son idempotentes (el volver a procesar un mensaje no tendr\u00e1 un impacto en el sistema) Exactly once : s\u00f3lo se puede conseguir utilizando flujos de trabajo de Kafka con Kafka mediante el API de Kafka Streams . Si necesitamos la interacci\u00f3n de Kafka con un sistema externo, como una base de datos, se recomienda utilizar un consumidor idempotente que nos asegura que no habr\u00e1 duplicados en la base de datos. Descubrimiento de brokers \u00b6 Cada broker de Kafka es un bootstrap server , lo que significa que dicho servidor contiene un listado con todos los nodos del cl\u00faster, de manera que al conectarnos a un broker , autom\u00e1ticamente nos conectaremos al cl\u00faster entero. Mediante esta configuraci\u00f3n, cada broker conoce todos los brokers , topics y particiones (metadatos del cl\u00faster). As\u00ed pues, cuando un cliente se conecta a un broker , tambi\u00e9n realiza una petici\u00f3n de los metadatos, y obtiene un listado con todos los brokers . Tras ello, ya puede conectarse a cualquiera de los brokers que necesite: Descubrimiento de brokers Zookeeper \u00b6 En la primera sesi\u00f3n de Hadoop ya vimos que ZooKeeper es un servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas dentro del ecosistema de Apache . No s\u00f3lo se utiliza en Hadoop , pero es muy \u00fatil ya que elimina la complejidad de la gesti\u00f3n distribuida de la plataforma. En el caso de Kafka , Zookeeper : gestiona los brokers (manteniendo una lista de ellos). ayuda en la elecci\u00f3n de la partici\u00f3n l\u00edder env\u00eda notificaciones a Kafka cuando hay alg\u00fan cambio (por ejemplo, se crea un topic , se cae un broker, se recupera un broker , al eliminar un topic , etc...). Por todo ello, Kafka no puede funcionar sin Zookeeper . En un entorno real, se instalan un n\u00famero impar de servidores Zookeeper (3, 5, 7). Para su gesti\u00f3n, Zookeeper define un l\u00edder (gestiona las escrituras) y el resto de servidores funcionan como r\u00e9plicas de lectura. Kafka y Zookeeper Pese a su dependencia, los productores y consumidores no interact\u00faan nunca con Zookeeper , s\u00f3lo lo hacen con Kafka . Kafka garantiza que... Los mensajes se a\u00f1aden a una partici\u00f3n/ topic en el orden en el que se env\u00edan Los consumidores leen los mensajes en el orden en que se almacenaron en la partici\u00f3n/ topic Con un factor de replicaci\u00f3n N, los productores y consumidores pueden soportar que se caigan N-1 brokers. Por ejemplo, con un factor de replicaci\u00f3n de 3 (el cual es un valor muy apropiado), podemos tener un nodo detenido para mantenimiento y podemos permitirnos que otro de los nodos se caiga de forma inesperada. Mientras el n\u00famero de particiones de un topic permanezca constante (no se hayan creado nuevas particiones), la misma clave implicar\u00e1 que los mensajes vayan a la misma partici\u00f3n. Caso 1: Kafka y Python \u00b6 Para poder producir y consumir mensajes desde Python necesitamos instalar la librer\u00eda Kafka-python : pip install kafka-python KafkaConsumer \u00b6 Vamos a crear un consumidor, mediante un KafkaConsumer , que escuche de nuestro servidor de Kafka : consumer.py from kafka import KafkaConsumer from json import loads consumer = KafkaConsumer ( 'iabd-topic' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-grupo-1' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) for m in consumer : print ( m . value ) Al crear el consumidor, configuramos los siguientes par\u00e1metros: en el primer par\u00e1metro indicamos el topic desde el que vamos a consumir los mensajes bootstrap_servers : listado de brokers de Kafka auto_offset_reset : le indica al consumidor desde donde empezar a leer los mensaje si se cae: earliest se mover\u00e1 hasta el mensaje m\u00e1s antiguo y latest al m\u00e1s reciente. enable_auto_commit : si True , el offset del consumidor realizar\u00e1 peri\u00f3dicamente commit en segundo plano. value_deserializer : m\u00e9todo utilizado para deserializar los datos. En este caso, transforma los datos recibidos en JSON. KafkaProducer \u00b6 Y para el productor, mediante un KafkaProducer , vamos a enviar 10 mensajes en formato JSON: producer.py from kafka import KafkaProducer from json import dumps import time producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) for i in range ( 10 ): producer . send ( \"iabd-topic\" , value = { \"nombre\" : \"producer \" + str ( i )}) # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el mensaje, esperamos 1 seg time . sleep ( 1 ) # producer.flush() Tras ejecutar ambos programas en pesta\u00f1as diferentes, en la salida del consumidor recibiremos: { ' n ombre' : 'producer 0 ' } { ' n ombre' : 'producer 1 ' } { ' n ombre' : 'producer 2 ' } { ' n ombre' : 'producer 3 ' } { ' n ombre' : 'producer 4 ' } { ' n ombre' : 'producer 5 ' } { ' n ombre' : 'producer 6 ' } { ' n ombre' : 'producer 7 ' } { ' n ombre' : 'producer 8 ' } { ' n ombre' : 'producer 9 ' } Caso 2: Cl\u00faster de Kafka \u00b6 En Kafka hay tres tipos de cl\u00fasters: Un nodo con un broker Un nodo con muchos brokers Muchos nodos con m\u00faltiples brokers Para nuestro ejemplo, como s\u00f3lo disponemos de una m\u00e1quina, vamos a crear 3 brokers en un nodo. Creando brokers \u00b6 Para ello, vamos a crear diferentes archivos de configuraci\u00f3n a partir del archivo config/server.properties que utiliz\u00e1bamos para arrancar el servidor. As\u00ed pues, crearemos 3 copias del archivo modificando las propiedades broker.id (identificador del broker), listeners (URL y puerto de escucha del broker) y log.dirs (carpeta donde se guardaran los logs del broker): Broker 101 Broker 102 Broker 103 server101.properties broker.id = 101 listeners = PLAINTEXT://:9092 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_101 zookeeper.connect = localhost:2181 server102.properties broker.id = 102 listeners = PLAINTEXT://:9093 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_102 zookeeper.connect = localhost:2181 server103.properties broker.id = 103 listeners = PLAINTEXT://:9094 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_103 zookeeper.connect = localhost:2181 Una vez creados los tres archivos, ejecutaremos los siguientes comandos (cada uno en un terminal diferente) para arrancar Zookeeper y cada uno de los brokers : zookeeper-server-start.sh ./config/zookeeper.properties kafka-server-start.sh ./config/server101.properties kafka-server-start.sh ./config/server102.properties kafka-server-start.sh ./config/server103.properties Creando topics \u00b6 Con cada comando que vayamos a interactuar con Kafka , le vamos a pasar como par\u00e1metro --bootstrap-server iabd-virtualbox:9092 para indicarle donde se encuentra uno de los brokers (en versiones antiguas de Kafka se indicaba donde estaba Zookeeper mediante --zookeeper iabd-virtualbox:9092 ). A la hora de crear un topic , adem\u00e1s de indicarle donde est\u00e1 Zookeeper y el nombre del topic , indicaremos: la cantidad de particiones con el par\u00e1metro --partitions el factor de replicaci\u00f3n con el par\u00e1metro --replication-factor As\u00ed pues, vamos a crear un topic con tres particiones y factor de replicaci\u00f3n 2: kafka-topics.sh --create --topic iabd-topic-3p2r \\ --bootstrap-server iabd-virtualbox:9092 \\ --partitions 3 --replication-factor 2 Si ahora obtenemos la informaci\u00f3n del topic kafka-topics.sh --describe --topic iabd-topic-3p2r \\ --bootstrap-server iabd-virtualbox:9092 Podemos observar como cada partici\u00f3n tiene la partici\u00f3n l\u00edder en un broker distinto y en qu\u00e9 brokers se encuentran las r\u00e9plicas: Topic: iabd-topic-3p2r TopicId: lyrv4qXkS1-c09XAXnIj7w PartitionCount: 3 ReplicationFactor: 2 Configs: segment.bytes=1073741824 Topic: iabd-topic-3p2r Partition: 0 Leader: 103 Replicas: 103,102 Isr: 103,102 Topic: iabd-topic-3p2r Partition: 1 Leader: 102 Replicas: 102,101 Isr: 102,101 Topic: iabd-topic-3p2r Partition: 2 Leader: 101 Replicas: 101,103 Isr: 101,103 Produciendo y consumiendo \u00b6 Respecto al c\u00f3digo Python, va a ser el mismo que hemos visto antes pero modificando: el nombre del topic la lista de boostrap_servers (aunque podr\u00edamos haber dejado \u00fanicamente el nodo principal, ya que Kafka le comunica al cliente el resto de nodos del cl\u00faster, es una buena pr\u00e1ctica por si el nodo al que nos conectamos de manera expl\u00edcita est\u00e1 ca\u00eddo). Productor Consumidor producer-cluster.py from kafka import KafkaProducer from json import dumps import time producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' , 'iabd-virtualbox:9093' , 'iabd-virtualbox:9094' ]) for i in range ( 10 ): producer . send ( \"iabd-topic-3p2r\" , value = { \"nombre\" : \"producer \" + str ( i )}, key = b \"iabd\" ) # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el mensaje, esperamos 1 seg time . sleep ( 1 ) En el consumidor, adem\u00e1s hemos modificado la forma de mostrar los mensajes para visualizar m\u00e1s informaci\u00f3n: consumer-cluster.py from kafka import KafkaConsumer from json import loads consumer = KafkaConsumer ( 'iabd-topic-3p2r' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-grupo-1' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' , 'iabd-virtualbox:9093' , 'iabd-virtualbox:9094' ]) for m in consumer : print ( f \"P: { m . partition } O: { m . offset } K: { m . key } V: { m . value } \" ) Como ahora tenemos los datos repartidos en dos brokers (por el factor de replicaci\u00f3n) y tres particiones, los datos consumidos no tienen por qu\u00e9 llegar en orden (como es el caso), ya que los productores han enviado los datos de manera aleatoria para repartir la carga: P:1 O:0 K:None V:{'nombre': 'producer 0'} P:1 O:1 K:None V:{'nombre': 'producer 3'} P:2 O:0 K:None V:{'nombre': 'producer 1'} P:2 O:1 K:None V:{'nombre': 'producer 5'} P:2 O:2 K:None V:{'nombre': 'producer 6'} P:2 O:3 K:None V:{'nombre': 'producer 7'} P:2 O:4 K:None V:{'nombre': 'producer 8'} P:0 O:0 K:None V:{'nombre': 'producer 2'} P:0 O:1 K:None V:{'nombre': 'producer 4'} P:0 O:2 K:None V:{'nombre': 'producer 9'} Para asegurarnos el orden, debemos enviar los mensajes con una clave de partici\u00f3n con el atributo key del m\u00e9todo send : producer . send ( \"iabd-topic-3p2r\" , value = { \"nombre\" : \"producer \" + str ( i )}, key = b \"iabd\" ) Si volvemos a ejecutar el productor con esa clave, el resultado s\u00ed que sale ordenado: P:0 O:3 K:b'iabd' V:{'nombre': 'producer 0'} P:0 O:4 K:b'iabd' V:{'nombre': 'producer 1'} P:0 O:5 K:b'iabd' V:{'nombre': 'producer 2'} P:0 O:6 K:b'iabd' V:{'nombre': 'producer 3'} P:0 O:7 K:b'iabd' V:{'nombre': 'producer 4'} P:0 O:8 K:b'iabd' V:{'nombre': 'producer 5'} P:0 O:9 K:b'iabd' V:{'nombre': 'producer 6'} P:0 O:10 K:b'iabd' V:{'nombre': 'producer 7'} P:0 O:11 K:b'iabd' V:{'nombre': 'producer 8'} P:0 O:12 K:b'iabd' V:{'nombre': 'producer 9'} Decisiones de rendimiento \u00b6 Aunque podemos modificar la cantidad de particiones y el factor de replicaci\u00f3n una vez creado el cl\u00faster, es mejor hacerlo de la manera correcta durante la creaci\u00f3n ya que tienen un impacto directo en el rendimiento y durabilidad del sistema: si el n\u00famero de particiones crece con el cl\u00faster ya creado, el orden de las claves no est\u00e1 garantizado. si se incrementa el factor de replicaci\u00f3n durante el ciclo de vida de un topic , estaremos metiendo presi\u00f3n al cl\u00faster, que provocar\u00e1 un decremento inesperado del rendimiento. Cada partici\u00f3n puede manejar un rendimiento de unos pocos MB/s. Al a\u00f1adir m\u00e1s particiones, obtendremos mejor paralelizaci\u00f3n y por tanto, mejor rendimiento. Adem\u00e1s, podremos ejecutar m\u00e1s consumidores en un grupo. Pero el hecho de a\u00f1adir m\u00e1s brokers al cl\u00faster para que las particiones los aprovechen, provocar\u00e1 que Zookeeper tenga que realizar m\u00e1s elecciones y que Kafka tenga m\u00e1s ficheros abiertos. Gu\u00eda de rendimiento Una propuesta es: Si nuestro cl\u00faster es peque\u00f1o (menos de 6 brokers ), crear el doble de particiones que brokers . Si tenemos un cl\u00faster grande (m\u00e1s de 12 brokers ), crear la misma cantidad de particiones que brokers . Ajustar el n\u00famero de consumidores necesarios que necesitamos que se ejecuten en paralelo en los picos de rendimiento. Independientemente de la decisi\u00f3n que tomemos, hay que realizar pruebas de rendimiento con diferentes configuraciones. Respecto al factor de replicaci\u00f3n, deber\u00eda ser, al menos 2, siendo 3 la cantidad recomendada (es necesario tener al menos 3 brokers ) y no sobrepasar de 4. Cuanto mayor sea el factor de replicaci\u00f3n (RF): El sistema tendr\u00e1 mejor tolerancia a fallos (se pueden caer RF-1 brokers ) Pero tendremos mayor replicaci\u00f3n (lo que implicar\u00e1 una mayor latencia si acks=all ) Y tambi\u00e9n ocupar\u00e1 m\u00e1s espacio en disco (50% m\u00e1s si RF es 3 en vez de 2). Respecto al cl\u00faster, se recomienda que un broker no contenga m\u00e1s de 2000-4000 particiones (entre todos los topics de ese broker). Adem\u00e1s, un cl\u00faster de Kafka deber\u00eda tener un m\u00e1ximo de 20.000 particiones entre todos los brokers, ya que si se cayese alg\u00fan nodo, Zookeeper necesitar\u00eda realizar muchas elecciones de l\u00edder. Caso 3: De Twitter a Elasticsearch con Python \u00b6 A continuaci\u00f3n vamos a crear un ejemplo completo de flujo de datos mediante Python que nos permita recoger tweets y meterlos dentro de ElasticSearch . Vamos a suponer que ya disponemos de una cuenta de Twitter y que tenemos las credenciales de acceso, las cuales vamos a almacenar en un fichero denominado credential.py : credentials.py API_KEY = 'YOUR_API_KEY' API_SECRET_KEY = 'YOUR_API_SECRET_KEY' ACCESS_TOKEN = 'YOUR_ACCESS_TOKEN' ACCESS_TOKEN_SECRET = 'YOUR_ACCESS_TOKEN_SECRET' Tweepy \u00b6 Para acceder a Twitter desde Python, la librer\u00eda por excelencia es Tweepy , la cual instalaremos mediante: pip install tweepy A continuaci\u00f3n, vamos a realizar el proceso de autenticaci\u00f3n en Tweepy y recoger el timeline de mi usuario: timeline.py import credentials import tweepy # Nos autenticamos mediante OAuth auth = tweepy . OAuthHandler ( credentials . API_KEY , credentials . API_SECRET_KEY ) auth . set_access_token ( credentials . ACCESS_TOKEN , credentials . ACCESS_TOKEN_SECRET ) api = tweepy . API ( auth ) miTimeline = api . home_timeline () for tweet in miTimeline : print ( f ' { tweet . user . screen_name } : \\n { tweet . text } \\n { \"*\" * 60 } ' ) Productor de Tweets \u00b6 En este caso de uso vamos a buscar los tweets que contengan la palabra bigdata y meterlos en un topic de Kafka . As\u00ed pues, en vez de obtener el timeline de un usuario, realizaremos una b\u00fasqueda mediante la funci\u00f3n search y para cada elemento recuperado, lo enviaremos al productor con toda la informaci\u00f3n (seguimos un planteamiento ELT). Primero creamos el topic: kafka-topics.sh --create --topic iabd-twitter --bootstrap-server iabd-virtualbox:9092 Y a continuaci\u00f3n desarrollamos el productor: producerTwitter.py import credentials import tweepy from kafka import KafkaProducer from json import dumps import time # Creamos el productor de Kafka producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) # Nos autenticamos mediante OAuth en Twitter auth = tweepy . OAuthHandler ( credentials . API_KEY , credentials . API_SECRET_KEY ) auth . set_access_token ( credentials . ACCESS_TOKEN , credentials . ACCESS_TOKEN_SECRET ) api = tweepy . API ( auth ) # Cargamos 500 tweets id = None cantidad = 0 while cantidad <= 500 : tweets = api . search_tweets ( q = 'bigdata' , tweet_mode = 'extended' , max_id = id ) for tweet in tweets : producer . send ( \"iabd-twitter\" , value = tweet . _json ) cantidad += 1 # Al final del ciclo le asignamos el id del \u00faltimo tweet # para que en cada ciclo se consulten solo los tweets hasta el m\u00e1s antiguos del ciclo anterior id = tweet . id # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el \u00faltimo mensaje, esperamos 1 seg time . sleep ( 1 ) Elasticsearch desde Python \u00b6 Para poder acceder a Elasticsearch desde Python necesitamos descargar la librer\u00eda: pip install elasticsearch El siguiente fragmento muestra varias operaciones b\u00e1sicas y como las operaciones REST de Elasticsearch se traducen en m\u00e9todos: C\u00f3digo Python Resultado prueba-elasticsearch.py from datetime import datetime from elasticsearch import Elasticsearch es = Elasticsearch ( \"http://localhost:9200\" ) doc1 = { 'author' : 'Aitor Medrano' , 'text' : 'Prueba de texto desde Python' , 'timestamp' : datetime . now (), } doc2 = { 'author' : 'Aitor Medrano' , 'text' : 'Y otra #prueba desde @Python' , 'timestamp' : datetime . now (), } # Inserci\u00f3n resp = es . index ( index = \"prueba\" , id = 1 , document = doc1 ) resp = es . index ( index = \"prueba\" , id = 2 , document = doc2 ) print ( resp [ 'result' ]) # Recuperaci\u00f3n resp = es . get ( index = \"prueba\" , id = 1 ) print ( resp [ '_source' ]) es . indices . refresh ( index = \"prueba\" ) # B\u00fasqueda resp = es . search ( index = \"prueba\" , query = { \"match_all\" : {}}) print ( \"Encontrados %d Hits:\" % resp [ 'hits' ][ 'total' ][ 'value' ]) for hit in resp [ 'hits' ][ 'hits' ]: print ( \" %(timestamp)s %(author)s : %(text)s \" % hit [ \"_source\" ]) resp = es.index(index=\"prueba\", id=1, document=doc1) resp = es.index(index=\"prueba\", id=2, document=doc2) updated resp = es.get(index=\"prueba\", id=1) {'author': 'Aitor Medrano', 'text': 'Prueba de texto desde Python', 'timestamp': '2022-02-25T15:11:43.720820'} es.indices.refresh(index=\"prueba\") resp = es.search(index=\"prueba\", query={\"match_all\": {}}) Encontrados 2 Hits: 2022-02-25T15:11:43.720820 Aitor Medrano: Prueba de texto desde Python 2022-02-25T15:11:43.720826 Aitor Medrano: Y otra #prueba desde @Python Consumidor en Elasticsearch \u00b6 Finalmente, vamos a crear un consumidor que se conecte a Kafka para consumir los mensajes, e introduzca cada uno de los tuits en Elasticsearch : consumerTwitter.py from datetime import datetime from elasticsearch import Elasticsearch from kafka import KafkaConsumer from json import loads import ast es = Elasticsearch ( \"http://localhost:9200\" ) consumer = KafkaConsumer ( 'iabd-twitter' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-caso3' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) cantidad = 1 for m in consumer : tweet = m . value # print(type(tweet)) # tweet = ast.literal_eval(m.value) # print(tweet['user']) # doc = { # 'user': m.value.user.screen_name, # 'text': m.value.full_text, # 'created_at': m.value.created_at, # 'likes': m.value.favourite_count # } resp = es . index ( index = \"iabd-twitter-es\" , id = cantidad , document = tweet ) cantidad += 1 # Forzamos que se persistan los cambios es . indices . refresh ( index = \"iabd-twitter-es\" ) Si accedemos a http://localhost:9200/iabd-twitter-es/_search?pretty podremos ver como Elasticsearch contiene los tuits que hab\u00edamos producido previamente: Resultado de consumir tuits desde Kafka y cargar en Elasticsearch Todo en uno con Nifi \u00b6 Vamos a realizar un flujo de datos en Nifi para el mismo caso de uso que acabamos de desarrollar. Para ello, vamos a crear dos grupos de procesos para tener los flujos ordenados: Grupos de procesos para E y L En el primer grupo, que hemos denominado Extract Twitter , vamos a realizar la carga desde Twitter (filtrando los mensajes que contengan la palabra bigdata ) y los vamos a meter en el topic iabd-twitter . Para ello, conectaremos los siguientes procesadores: GetTwitter : tras introducir los valores para autenticarnos en Twitter, configuraremos como endpoint que sea de tipo Filter Endpoint y como t\u00e9rmino a filtrar bigdata (no olvides poner las claves de autenticaci\u00f3n). PublishKafka_2_6 : en este procesador, adem\u00e1s del topic iabd-twitter , indicaremos que no utilizaremos transacciones ( Use Transactions: false ), as\u00ed como que intente garantizar la entrega (*Delivery Guarantee: Best Effort): De Twitter a Kafka Del mismo modo, dentro del segundo grupo ( Load Elasticsearch ), conectaremos los siguientes procesadores: ConsumerKafka_2_6 : donde consumiremos los mensajes del topic iabd-twitter de Kafka , y por ejemplo, como grupo de consumidores le indicaremos que usamos nifi ( group id: nifi ) PutElasticsearchHttp : como ya hicimos en la sesi\u00f3n de Nifi , indicaremos la URL de Elasticsearch ( http://localhost:9200 ) y que lo almacene en un indice que hemos denominado iabd-twitter-es : De Kafka a Elasticsearch Kafka Connect \u00b6 Si hacerlo con Nifi ya es un avance respecto a tener que codificarlo con Python , \u00bfqu\u00e9 dir\u00edas si Kafka ofreciera una serie de conectores para las operaciones m\u00e1s comunes? As\u00ed pues, Kafka Connect permite importar/exportar datos desde/hacia Kafka , facilitando la integraci\u00f3n en sistemas existentes mediante alguno del centenar de conectores disponibles . Arquitectura Kafka Connect Los elementos que forman Kafka Connect son: Conectores fuente ( source ), para obtener datos desde las fuentes de datos (E en ETL) Conectores destino ( sink ) para publicar los datos en los almacenes de datos (L en ETL). Estos conectores facilitan que desarrolladores no expertos puedan trabajar con sus datos en Kafka de forma r\u00e1pida y fiable, de manera que podamos introducir Kafka dentro de nuestros procesos ETL. Hola Kafka Connect \u00b6 Vamos a realizar un ejemplo muy sencillo leyendo datos de una base de datos para meterlos en Kafka. Para ello, utilizaremos la base de datos de retail_db que ya hemos empleado en otras sesiones y vamos a cargar en Kafka los datos de la tabla categories : MariaDB [ retail_db ] > describe categories ; +------------------------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +------------------------+-------------+------+-----+---------+----------------+ | category_id | int ( 11 ) | NO | PRI | NULL | auto_increment | | category_department_id | int ( 11 ) | NO | | NULL | | | category_name | varchar ( 45 ) | NO | | NULL | | +------------------------+-------------+------+-----+---------+---------------- Configuraci\u00f3n \u00b6 Cuando ejecutemos Kafka Connect , le debemos pasar un archivo de configuraci\u00f3n. Para empezar, tenemos config/connect-standalone.properties el cual ya viene rellenado e indica los formatos que utilizar\u00e1n los conversores y otros aspectos: config/connect-standalone.properties bootstrap.servers = localhost:9092 key.converter = org.apache.kafka.connect.json.JsonConverter value.converter = org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable = true value.converter.schemas.enable = true offset.storage.file.filename = /tmp/connect.offsets Los conectores se incluyen en Kafka como plugins. Para ello, primero hemos de indicarle a Kafka donde se encuentran. Para ello, en el archivo de configuraci\u00f3n le indicaremos la siguiente ruta: plugin.path = /opt/kafka_2.13-2.8.1/plugins Extrayendo datos mediante Kafka Connect \u00b6 As\u00ed pues, el primer paso es crear el archivo de configuraci\u00f3n de Kafka Connect con los datos (en nuestro caso lo colocamos en la carpeta config de la instalaci\u00f3n de Kafka ) utilizando un conector fuente de JDBC : retaildb-mariadb-source-connector.properties name = retaildb-mariabd-source-jdbc-autoincrement connector.class = io.confluent.connect.jdbc.JdbcSourceConnector tasks.max = 1 connection.url = jdbc:mysql://localhost/retail_db connection.user = iabd connection.password = iabd table.whitelist = categories mode = incrementing incrementing.column.name = category_id topic.prefix = iabd-retail_db- Antes de ponerlo en marcha, debemos descargar el conector y colocar la carpeta descomprimida dentro de nuestra carpeta de plugins /opt/kafka_2.13-2.8.1/plugins y descargar el driver de MySQL y colocarlos en la carpeta /opt/kafka_2.13-2.8.1/lib . Y a continuaci\u00f3n ya podemos ejecutar Kafka Connect : connect-standalone.sh config/connect-standalone.properties config/retaildb-mariadb-source-connector.properties Guava Es posible que salte un error de ejecuci\u00f3n indicando que falta la librer\u00eda Guava la cual pod\u00e9is descargar desde aqu\u00ed y colocar en la carpeta /opt/kafka_2.13-2.8.1/lib Si ahora arrancamos un consumidor sobre el topic iabd-retail_db-categories : kafka-console-consumer.sh --topic iabd-retail_db-categories --from-beginning --bootstrap-server iabd-virtualbox:9092 Veremos que aparecen todos los datos que ten\u00edamos en la tabla en formato JSON (es lo que hemos indicado en el archivo de configuraci\u00f3n de Kafka Connect ): { \"schema\" :{ \"type\" : \"struct\" , \"fields\" :[ { \"type\" : \"int32\" , \"optional\" : false , \"field\" : \"category_id\" }, { \"type\" : \"int32\" , \"optional\" : false , \"field\" : \"category_department_id\" }, { \"type\" : \"string\" , \"optional\" : false , \"field\" : \"category_name\" }], \"optional\" : false , \"name\" : \"categories\" }, \"payload\" :{ \"category_id\" : 1 , \"category_department_id\" : 2 , \"category_name\" : \"Football\" }} { \"schema\" :{ \"type\" : \"struct\" , \"fields\" :[ { \"type\" : \"int32\" , \"optional\" : false , \"field\" : \"category_id\" }, { \"type\" : \"int32\" , \"optional\" : false , \"field\" : \"category_department_id\" }, { \"type\" : \"string\" , \"optional\" : false , \"field\" : \"category_name\" }], \"optional\" : false , \"name\" : \"categories\" }, \"payload\" :{ \"category_id\" : 2 , \"category_department_id\" : 2 , \"category_name\" : \"Soccer\" }} ... Autoevaluaci\u00f3n Vamos a dejar el consumidor y Kafka Connect corriendo. \u00bfQu\u00e9 suceder\u00e1 si inserto un nuevo registro en la base de datos en la tabla categories ? Que autom\u00e1ticamente aparecer\u00e1 en nuestro consumidor. REST API \u00b6 Como Kafka Connect est\u00e1 dise\u00f1ado como un servicio que deber\u00eda correr continuamente, ofrece un API REST para gestionar los conectores. Por defecto est\u00e1 a la escucha el puerto 8083, de manera que si accedemos a http://iabd-virtualbox:8083/ obtendremos informaci\u00f3n sobre la versi\u00f3n que se est\u00e1 ejecutando: { \"version\" : \"2.8.1\" , \"commit\" : \"839b886f9b732b15\" , \"kafka_cluster_id\" : \"iHa0JUnTSfm85fvFadsylA\" } Por ejemplo, si queremos obtener un listado de los conectores realizaremos una petici\u00f3n GET a /connectors mediante http://iabd-virtualbox:8083/connectors : [ \"retaildb-mariabd-source-jdbc-autoincrement\" ] M\u00e1s informaci\u00f3n en https://kafka.apache.org/documentation/#connect_rest Kafka Streams Kafka Streams es la tercera pata del ecosistema de Kafka, y permite procesar y transformar datos dentro de Kafka. Una vez que los datos se almacenan en Kafka como eventos, podemos procesar los datos en nuestras aplicaciones cliente mediante Kafka Streams y sus librer\u00edas desarrolladas en Java y/o Scala, ya que requiere una JVM. En nuestro caso, realizaremos en posteriores sesiones un procesamiento similar de los datos mediante Spark Streaming , permitiendo operaciones con estado y agregaciones, funciones ventana, joins , procesamiento de eventos basados en el tiempo, etc... Kafka y el Big Data \u00b6 El siguiente gr\u00e1fico muestra c\u00f3mo Kafka est\u00e1 enfocado principalmente para el tratamiento en streaming , aunque con los conectores de Kafka Connect da soporte para el procesamiento batch : Kafka y Big Data Actividades \u00b6 Realiza los casos de uso 0 y 1. (opcional) A partir del caso 2, crea un cl\u00faster de Kafka con 4 particiones y 3 nodos. A continuaci\u00f3n, en el productor utiliza Faker para crear 10 personas (almac\u00e9nalas como un diccionario). En el consumidor, muestra los datos de las personas (no es necesario recibirlos ordenados, s\u00f3lo necesitamos que se aproveche al m\u00e1ximo la infraestructura de Kafka). Realiza el caso de uso 3 (De Twitter a ElasticSearch ) tanto con Python como con Nifi . (opcional) Repite el caso de uso 2 \u00fanicamente mediante Kafka Connect . (opcional) Investiga en qu\u00e9 consiste el patr\u00f3n CDC ( Change Data Capture ) y c\u00f3mo se realiza CDC con Kafka / Kafka Connect y Debezium . \u00bfQu\u00e9 ventajas aportan las soluciones CDC? Referencias \u00b6 Apache Kafka Series - Learn Apache Kafka for Beginners Serie de art\u00edculos de V\u00edctor Madrid sobre Kafka en enmilocalfunciona.io . Distributed Databases: Kafka por Miguel del Tio Introduction to Kafka Connectors Kafka Cheatsheet","title":"5.- Kafka"},{"location":"apuntes/bdaplicado05kafka.html#kafka","text":"","title":"Kafka"},{"location":"apuntes/bdaplicado05kafka.html#introduccion","text":"Apache Kafka es, en pocas palabras, un middleware de mensajer\u00eda entre sistemas heterog\u00e9neos, el cual, mediante un sistema de colas ( topics , para ser concreto) facilita la comunicaci\u00f3n as\u00edncrona, desacoplando los flujos de datos de los sistemas que los producen o consumen. Funciona como un broker de mensajes, encargado de enrutar los mensajes entre los clientes de un modo muy r\u00e1pido. Kafka como middleware/broker de mensajes En concreto, se trata de una plataforma open source distribuida de transmisi\u00f3n de eventos/mensajes en tiempo real con almacenamiento duradero y que proporciona de base un alto rendimiento (capaz de manejar billones de peticiones al d\u00eda, con una latencia inferior a 10ms), tolerancia a fallos, disponibilidad y escalabilidad horizontal (mediante cientos de nodos). Evento / Mensaje Dentro del vocabulario asociado a arquitectura as\u00edncronas basadas en productor/consumidor o publicador/suscriptor, se utiliza el mensaje para indicar el dato que viaja desde un punto a otro. En Kafka, adem\u00e1s de utilizar el concepto mensaje, se emplea el t\u00e9rmino evento. M\u00e1s del 80% de las 100 compa\u00f1\u00edas m\u00e1s importantes de EEUU utilizan Kafka : Uber , Twitter , Netflix , Spotify , Blizzard , LinkedIn , Spotify , y PayPal procesan cada d\u00eda sus mensajes con Kafka . Como sistema de mensajes, sigue un modelo publicador-suscriptor. Su arquitectura tiene dos directivas claras: No bloquear los productores (para poder gestionar la back pressure , la cual sucede cuando un publicador produce m\u00e1s elementos de los que un suscriptor puede consumir). Aislar los productores y los consumidores, de manera que los productores y los consumidores no se conocen. A d\u00eda de hoy, Apache Kafka se utiliza, adem\u00e1s de como un sistema de mensajer\u00eda, para ingestar datos, realizar procesado de datos en streaming y anal\u00edtica de datos en tiempo real, as\u00ed como en arquitectura de microservicios y sistemas IOT. Amazon Kinesis Amazon Kinesis es un producto similar a Apache Kafka pero dentro de la plataforma AWS, por lo que no es un producto open source como tal. Su principal ventaja es la facilidad de escalabilidad a golpe de click e integraci\u00f3n con el resto de servicios que ofrece AWS. Se trata de una herramienta muy utilizada que permite incorporar datos en tiempo real, como v\u00eddeos, audios, registros de aplicaciones, secuencias de clicks de sitios web y datos de sensores IoT para machine learning, anal\u00edtica de datos en streaming, etc...","title":"Introducci\u00f3n"},{"location":"apuntes/bdaplicado05kafka.html#publicador-suscriptor","text":"Antes de entrar en detalle sobre Kafka, hay que conocer el modelo publicador/suscriptor. Este patr\u00f3n tambi\u00e9n se conoce como publish / subscribe o productor / consumidor . Hay tres elementos que hay que tener realmente claros: Publicador ( publisher / productor / emisor): genera un dato y lo coloca en un topic como un mensaje. topic (tema): almac\u00e9n temporal/duradero que guarda los mensajes funcionando como una cola. Suscriptor ( subscriber / consumidor / receptor): recibe el mensaje. Cabe destacar que un productor no se comunica nunca directamente con un consumidor, siempre lo hace a trav\u00e9s de un topic : Productor - Consumidor","title":"Publicador / Suscriptor"},{"location":"apuntes/bdaplicado05kafka.html#caso-0-hola-kafka","text":"Para arrancar Kafka, vamos a utilizar la instalaci\u00f3n que tenemos creada en nuestra m\u00e1quina virtual. Kafka mediante Docker Bitnami tiene una imagen para trabajar con Docker la cual permite probar todos los ejemplos de esta sesi\u00f3n. Para ello, se recomienda seguir los pasos de la p\u00e1gina oficial: https://hub.docker.com/r/bitnami/kafka/ El primer paso, una vez dentro de la carpeta de instalaci\u00f3n de Kafka (en nuestro caso /opt/kafka_2.13-2.8.1 ), es arrancar Zookeeper mediante el comando zookeeper-server-start.sh , el cual se encarga de gestionar la comunicaci\u00f3n entre los diferentes brokers: zookeeper-server-start.sh ./config/zookeeper.properties zookeeper.properties Del archivo de configuraci\u00f3n de Zookeeper conviene destacar dos propiedades: clientPort : puerto por defecto (2181) dataDir : indica donde est\u00e1 el directorio de datos de Zookeeper (por defecto es tmp/zookeeper , pero si queremos que dicha carpeta no se elimine es mejor que apunte a una ruta propia, por ejemplo /opt/zookeeper-data ) Para comprobar que Zookeeper est\u00e1 arrancado, podemos ejecutar el comando lsof -i :2181 , el cual escanea el puerto 2181 donde est\u00e1 corriendo Zookeeper . Una vez comprobado, en un nuevo terminal, arrancamos el servidor de Kafka mediante el comando kafka-server-start.sh (de manera que tenemos corriendo a la vez Zookeeper y Kafka ): kafka-server-start.sh ./config/server.properties","title":"Caso 0: Hola Kafka"},{"location":"apuntes/bdaplicado05kafka.html#creando-un-topic","text":"A continuaci\u00f3n, en un tercer terminal, vamos a crear un topic mediante el comando kafka-topics.sh : kafka-topics.sh --create --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 Si queremos obtener la descripci\u00f3n del topic creado con la cantidad de particiones le pasamos el par\u00e1metro --describe : kafka-topics.sh --describe --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 Obteniendo la siguiente informaci\u00f3n: Topic: iabd-topic TopicId: ogKnRpOFS7mfOhspLcuB4A PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 Topic: iabd-topic Partition: 0 Leader: 0 Replicas: 0 Isr: 0","title":"Creando un topic"},{"location":"apuntes/bdaplicado05kafka.html#produciendo-mensajes","text":"Para enviar un mensaje a un topic , ejecutaremos en un cuarto terminal un productor mediante el comando kafka-console-producer.sh . Por defecto, cada l\u00ednea que introduzcamos resultar\u00e1 en un evento separado que escribir\u00e1 un mensaje en el topic (podemos pulsar CTRL+C en cualquier momento para cancelar): kafka-console-producer.sh --topic iabd-topic --bootstrap-server iabd-virtualbox:9092 As\u00ed pues, escribimos los mensajes que queramos: >Este es un mensaje >Y este es otro >Y el tercero","title":"Produciendo mensajes"},{"location":"apuntes/bdaplicado05kafka.html#consumiendo-mensajes","text":"Y finalmente, en otro terminal, vamos a consumir los mensajes: kafka-console-consumer.sh --topic iabd-topic --from-beginning --bootstrap-server iabd-virtualbox:9092 Al ejecutarlo veremos los mensajes que hab\u00edamos introducido antes (ya que hemos indicado la opci\u00f3n --from-beginning ). Si ahora volvemos a escribir en el productor, casi instant\u00e1neamente, aparecer\u00e1 en el consumidor el mismo mensaje. Tras esto, paramos todos los procesos que se est\u00e1n ejecutando mediante CTRL+C y hemos finalizado nuestro primer contacto con Kafka.","title":"Consumiendo mensajes"},{"location":"apuntes/bdaplicado05kafka.html#elementos","text":"Dentro de una arquitectura con Kafka, existen m\u00faltiples elementos que interact\u00faan entre s\u00ed.","title":"Elementos"},{"location":"apuntes/bdaplicado05kafka.html#topic-y-particiones","text":"Un topic (\u00bftema?) es un flujo particular de datos que funciona como una cola almacenando de forma temporal o duradera los datos que se colocan en \u00e9l. Podemos crear tantos topics como queramos y cada uno de ellos tendr\u00e1 un nombre un\u00edvoco. Un topic se divide en particiones , las cuales se numeran, siendo la primera la 0. Al crear un topic hemos de indicar la cantidad de particiones inicial, la cual podemos modificar a posteriori . Cada partici\u00f3n est\u00e1 ordenada, de manera que cada mensaje dentro de una partici\u00f3n tendr\u00e1 un identificador incremental, llamado offset (desplazamiento). Cada partici\u00f3n funciona como un commit log almacenando los mensajes que recibe. Offset dentro de las particiones de un topic Como podemos observar en la imagen, cada partici\u00f3n tiene sus propios offset (el offset 3 de la partici\u00f3n 0 no representa el mismo dato que el offset 3 de la partici\u00f3n 1). Hab\u00edamos comentado que las particiones est\u00e1n ordenadas, pero el orden s\u00f3lo se garantiza dentro de una partici\u00f3n (no entre particiones), es decir, el mensaje 7 de la partici\u00f3n 0 puede haber llegado antes, a la vez, o despu\u00e9s que el mensaje 5 de la partici\u00f3n 1. Los datos de una partici\u00f3n tiene un tiempo de vida limitado ( retention period ) que indica el tiempo que se mantendr\u00e1n los mensajes antes de eliminarlos. Por defecto es de una semana. Adem\u00e1s, una vez que los datos se escriben en una partici\u00f3n, no se pueden modificar (las mensajes son immutables). Finalmente, por defecto, los datos se asignan de manera aleatoria a una partici\u00f3n. Sin embargo, existe la posibilidad de indicar una clave de particionado.","title":"Topic y Particiones"},{"location":"apuntes/bdaplicado05kafka.html#brokers","text":"Un cl\u00faster de Kafka est\u00e1 compuesto de m\u00faltiples nodos conocidos como Brokers , donde cada broker es un servidor de Kafka . Cada broker se identifica con un id, el cual debe ser un n\u00famero entero. Cada broker contiene un conjunto de particiones, de manera que un broker contiene parte de los datos, nunca los datos completos ya que Kafka es un sistema distribuido. Al conectarse a un broker del cl\u00faster ( bootstrap broker ), autom\u00e1ticamente nos conectaremos al cl\u00faster entero. Para comenzar se recomienda una arquitectura de 3 brokers, aunque algunos cl\u00fasters lo forman cerca de un centenar de brokers . Por ejemplo, el siguiente gr\u00e1fico muestra el topic A dividido en tres particiones, cada una de ellas residiendo en un broker diferente (no hay ninguna relaci\u00f3n entre el n\u00famero de la partici\u00f3n y el nombre del broker), y el topic B dividido en dos particiones: Ejemplo de 3 brokers En el caso de haber introducido un nuevo topic con 4 particiones, uno de los brokers contendr\u00eda dos particiones.","title":"Brokers"},{"location":"apuntes/bdaplicado05kafka.html#factor-de-replicacion","text":"Para soportar la tolerancia a fallos, los topics deben tener un factor de replicaci\u00f3n mayor que uno (normalmente se configura entre 2 y 3). En la siguiente imagen podemos ver como tenemos 3 brokers, y un topic A con dos particiones y una factor de replicaci\u00f3n de 2, de manera que cada partici\u00f3n crea un replica de si misma: Divisiones de un broker en particiones Si se cayera el broker 102 , Kafka podr\u00eda devolver los datos al estar disponibles en los nodos 101 y 103.","title":"Factor de replicaci\u00f3n"},{"location":"apuntes/bdaplicado05kafka.html#productores","text":"Los productores escriben datos en los topics , sabiendo autom\u00e1ticamente el broker y la partici\u00f3n en la cual deben escribir. En el caso de un fallo de un broker , los productores autom\u00e1ticamente se recuperan y se comunican con el broker adecuado. Si el productor env\u00eda los datos sin una clave determinada, Kafka realiza una algoritmo de Round Robin , de manera que cada mensaje se va alternando entre los diferentes brokers . La carga se balancea entre los brokers Podemos configurar los productores para que reciban un ACK de las escrituras de los datos con los siguientes valores: ack=0 : El productor no espera la confirmaci\u00f3n (posible p\u00e9rdida de datos). ack=1 : El productor espera la confirmaci\u00f3n del l\u00edder (limitaci\u00f3n de la p\u00e9rdida de datos). ack=all : El productores espera la confirmaci\u00f3n del l\u00edder y de todas las r\u00e9plicas (sin p\u00e9rdida de datos).","title":"Productores"},{"location":"apuntes/bdaplicado05kafka.html#consumidores","text":"Los consumidores obtienen los datos de los topics y las particiones, y saben de qu\u00e9 broker deben leer los datos. Igual que los productores, en el caso de un fallo de un broker , los consumidores autom\u00e1ticamente se recuperan y se comunican con el broker adecuado. Los datos se leen en orden dentro de cada partici\u00f3n, de manera que el consumidor no podr\u00e1 leer, por ejemplo, los datos del offset 6 hasta que no haya le\u00eddo los del offset 5. Adem\u00e1s, un consumidor puede leer de varias particiones (se realiza en paralelo), pero el orden s\u00f3lo se respeta dentro de cada partici\u00f3n, no entre particiones: Los consumidores leen en orden dentro de cada partici\u00f3n","title":"Consumidores"},{"location":"apuntes/bdaplicado05kafka.html#descubrimiento-de-brokers","text":"Cada broker de Kafka es un bootstrap server , lo que significa que dicho servidor contiene un listado con todos los nodos del cl\u00faster, de manera que al conectarnos a un broker , autom\u00e1ticamente nos conectaremos al cl\u00faster entero. Mediante esta configuraci\u00f3n, cada broker conoce todos los brokers , topics y particiones (metadatos del cl\u00faster). As\u00ed pues, cuando un cliente se conecta a un broker , tambi\u00e9n realiza una petici\u00f3n de los metadatos, y obtiene un listado con todos los brokers . Tras ello, ya puede conectarse a cualquiera de los brokers que necesite: Descubrimiento de brokers","title":"Descubrimiento de brokers"},{"location":"apuntes/bdaplicado05kafka.html#zookeeper","text":"En la primera sesi\u00f3n de Hadoop ya vimos que ZooKeeper es un servicio para mantener la configuraci\u00f3n, coordinaci\u00f3n y aprovisionamiento de aplicaciones distribuidas dentro del ecosistema de Apache . No s\u00f3lo se utiliza en Hadoop , pero es muy \u00fatil ya que elimina la complejidad de la gesti\u00f3n distribuida de la plataforma. En el caso de Kafka , Zookeeper : gestiona los brokers (manteniendo una lista de ellos). ayuda en la elecci\u00f3n de la partici\u00f3n l\u00edder env\u00eda notificaciones a Kafka cuando hay alg\u00fan cambio (por ejemplo, se crea un topic , se cae un broker, se recupera un broker , al eliminar un topic , etc...). Por todo ello, Kafka no puede funcionar sin Zookeeper . En un entorno real, se instalan un n\u00famero impar de servidores Zookeeper (3, 5, 7). Para su gesti\u00f3n, Zookeeper define un l\u00edder (gestiona las escrituras) y el resto de servidores funcionan como r\u00e9plicas de lectura. Kafka y Zookeeper Pese a su dependencia, los productores y consumidores no interact\u00faan nunca con Zookeeper , s\u00f3lo lo hacen con Kafka . Kafka garantiza que... Los mensajes se a\u00f1aden a una partici\u00f3n/ topic en el orden en el que se env\u00edan Los consumidores leen los mensajes en el orden en que se almacenaron en la partici\u00f3n/ topic Con un factor de replicaci\u00f3n N, los productores y consumidores pueden soportar que se caigan N-1 brokers. Por ejemplo, con un factor de replicaci\u00f3n de 3 (el cual es un valor muy apropiado), podemos tener un nodo detenido para mantenimiento y podemos permitirnos que otro de los nodos se caiga de forma inesperada. Mientras el n\u00famero de particiones de un topic permanezca constante (no se hayan creado nuevas particiones), la misma clave implicar\u00e1 que los mensajes vayan a la misma partici\u00f3n.","title":"Zookeeper"},{"location":"apuntes/bdaplicado05kafka.html#caso-1-kafka-y-python","text":"Para poder producir y consumir mensajes desde Python necesitamos instalar la librer\u00eda Kafka-python : pip install kafka-python","title":"Caso 1: Kafka y Python"},{"location":"apuntes/bdaplicado05kafka.html#kafkaconsumer","text":"Vamos a crear un consumidor, mediante un KafkaConsumer , que escuche de nuestro servidor de Kafka : consumer.py from kafka import KafkaConsumer from json import loads consumer = KafkaConsumer ( 'iabd-topic' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-grupo-1' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) for m in consumer : print ( m . value ) Al crear el consumidor, configuramos los siguientes par\u00e1metros: en el primer par\u00e1metro indicamos el topic desde el que vamos a consumir los mensajes bootstrap_servers : listado de brokers de Kafka auto_offset_reset : le indica al consumidor desde donde empezar a leer los mensaje si se cae: earliest se mover\u00e1 hasta el mensaje m\u00e1s antiguo y latest al m\u00e1s reciente. enable_auto_commit : si True , el offset del consumidor realizar\u00e1 peri\u00f3dicamente commit en segundo plano. value_deserializer : m\u00e9todo utilizado para deserializar los datos. En este caso, transforma los datos recibidos en JSON.","title":"KafkaConsumer"},{"location":"apuntes/bdaplicado05kafka.html#kafkaproducer","text":"Y para el productor, mediante un KafkaProducer , vamos a enviar 10 mensajes en formato JSON: producer.py from kafka import KafkaProducer from json import dumps import time producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) for i in range ( 10 ): producer . send ( \"iabd-topic\" , value = { \"nombre\" : \"producer \" + str ( i )}) # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el mensaje, esperamos 1 seg time . sleep ( 1 ) # producer.flush() Tras ejecutar ambos programas en pesta\u00f1as diferentes, en la salida del consumidor recibiremos: { ' n ombre' : 'producer 0 ' } { ' n ombre' : 'producer 1 ' } { ' n ombre' : 'producer 2 ' } { ' n ombre' : 'producer 3 ' } { ' n ombre' : 'producer 4 ' } { ' n ombre' : 'producer 5 ' } { ' n ombre' : 'producer 6 ' } { ' n ombre' : 'producer 7 ' } { ' n ombre' : 'producer 8 ' } { ' n ombre' : 'producer 9 ' }","title":"KafkaProducer"},{"location":"apuntes/bdaplicado05kafka.html#caso-2-cluster-de-kafka","text":"En Kafka hay tres tipos de cl\u00fasters: Un nodo con un broker Un nodo con muchos brokers Muchos nodos con m\u00faltiples brokers Para nuestro ejemplo, como s\u00f3lo disponemos de una m\u00e1quina, vamos a crear 3 brokers en un nodo.","title":"Caso 2: Cl\u00faster de Kafka"},{"location":"apuntes/bdaplicado05kafka.html#creando-brokers","text":"Para ello, vamos a crear diferentes archivos de configuraci\u00f3n a partir del archivo config/server.properties que utiliz\u00e1bamos para arrancar el servidor. As\u00ed pues, crearemos 3 copias del archivo modificando las propiedades broker.id (identificador del broker), listeners (URL y puerto de escucha del broker) y log.dirs (carpeta donde se guardaran los logs del broker): Broker 101 Broker 102 Broker 103 server101.properties broker.id = 101 listeners = PLAINTEXT://:9092 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_101 zookeeper.connect = localhost:2181 server102.properties broker.id = 102 listeners = PLAINTEXT://:9093 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_102 zookeeper.connect = localhost:2181 server103.properties broker.id = 103 listeners = PLAINTEXT://:9094 log.dirs = /opt/kafka_2.13-2.8.1/logs/broker_103 zookeeper.connect = localhost:2181 Una vez creados los tres archivos, ejecutaremos los siguientes comandos (cada uno en un terminal diferente) para arrancar Zookeeper y cada uno de los brokers : zookeeper-server-start.sh ./config/zookeeper.properties kafka-server-start.sh ./config/server101.properties kafka-server-start.sh ./config/server102.properties kafka-server-start.sh ./config/server103.properties","title":"Creando brokers"},{"location":"apuntes/bdaplicado05kafka.html#creando-topics","text":"Con cada comando que vayamos a interactuar con Kafka , le vamos a pasar como par\u00e1metro --bootstrap-server iabd-virtualbox:9092 para indicarle donde se encuentra uno de los brokers (en versiones antiguas de Kafka se indicaba donde estaba Zookeeper mediante --zookeeper iabd-virtualbox:9092 ). A la hora de crear un topic , adem\u00e1s de indicarle donde est\u00e1 Zookeeper y el nombre del topic , indicaremos: la cantidad de particiones con el par\u00e1metro --partitions el factor de replicaci\u00f3n con el par\u00e1metro --replication-factor As\u00ed pues, vamos a crear un topic con tres particiones y factor de replicaci\u00f3n 2: kafka-topics.sh --create --topic iabd-topic-3p2r \\ --bootstrap-server iabd-virtualbox:9092 \\ --partitions 3 --replication-factor 2 Si ahora obtenemos la informaci\u00f3n del topic kafka-topics.sh --describe --topic iabd-topic-3p2r \\ --bootstrap-server iabd-virtualbox:9092 Podemos observar como cada partici\u00f3n tiene la partici\u00f3n l\u00edder en un broker distinto y en qu\u00e9 brokers se encuentran las r\u00e9plicas: Topic: iabd-topic-3p2r TopicId: lyrv4qXkS1-c09XAXnIj7w PartitionCount: 3 ReplicationFactor: 2 Configs: segment.bytes=1073741824 Topic: iabd-topic-3p2r Partition: 0 Leader: 103 Replicas: 103,102 Isr: 103,102 Topic: iabd-topic-3p2r Partition: 1 Leader: 102 Replicas: 102,101 Isr: 102,101 Topic: iabd-topic-3p2r Partition: 2 Leader: 101 Replicas: 101,103 Isr: 101,103","title":"Creando topics"},{"location":"apuntes/bdaplicado05kafka.html#produciendo-y-consumiendo","text":"Respecto al c\u00f3digo Python, va a ser el mismo que hemos visto antes pero modificando: el nombre del topic la lista de boostrap_servers (aunque podr\u00edamos haber dejado \u00fanicamente el nodo principal, ya que Kafka le comunica al cliente el resto de nodos del cl\u00faster, es una buena pr\u00e1ctica por si el nodo al que nos conectamos de manera expl\u00edcita est\u00e1 ca\u00eddo). Productor Consumidor producer-cluster.py from kafka import KafkaProducer from json import dumps import time producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' , 'iabd-virtualbox:9093' , 'iabd-virtualbox:9094' ]) for i in range ( 10 ): producer . send ( \"iabd-topic-3p2r\" , value = { \"nombre\" : \"producer \" + str ( i )}, key = b \"iabd\" ) # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el mensaje, esperamos 1 seg time . sleep ( 1 ) En el consumidor, adem\u00e1s hemos modificado la forma de mostrar los mensajes para visualizar m\u00e1s informaci\u00f3n: consumer-cluster.py from kafka import KafkaConsumer from json import loads consumer = KafkaConsumer ( 'iabd-topic-3p2r' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-grupo-1' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' , 'iabd-virtualbox:9093' , 'iabd-virtualbox:9094' ]) for m in consumer : print ( f \"P: { m . partition } O: { m . offset } K: { m . key } V: { m . value } \" ) Como ahora tenemos los datos repartidos en dos brokers (por el factor de replicaci\u00f3n) y tres particiones, los datos consumidos no tienen por qu\u00e9 llegar en orden (como es el caso), ya que los productores han enviado los datos de manera aleatoria para repartir la carga: P:1 O:0 K:None V:{'nombre': 'producer 0'} P:1 O:1 K:None V:{'nombre': 'producer 3'} P:2 O:0 K:None V:{'nombre': 'producer 1'} P:2 O:1 K:None V:{'nombre': 'producer 5'} P:2 O:2 K:None V:{'nombre': 'producer 6'} P:2 O:3 K:None V:{'nombre': 'producer 7'} P:2 O:4 K:None V:{'nombre': 'producer 8'} P:0 O:0 K:None V:{'nombre': 'producer 2'} P:0 O:1 K:None V:{'nombre': 'producer 4'} P:0 O:2 K:None V:{'nombre': 'producer 9'} Para asegurarnos el orden, debemos enviar los mensajes con una clave de partici\u00f3n con el atributo key del m\u00e9todo send : producer . send ( \"iabd-topic-3p2r\" , value = { \"nombre\" : \"producer \" + str ( i )}, key = b \"iabd\" ) Si volvemos a ejecutar el productor con esa clave, el resultado s\u00ed que sale ordenado: P:0 O:3 K:b'iabd' V:{'nombre': 'producer 0'} P:0 O:4 K:b'iabd' V:{'nombre': 'producer 1'} P:0 O:5 K:b'iabd' V:{'nombre': 'producer 2'} P:0 O:6 K:b'iabd' V:{'nombre': 'producer 3'} P:0 O:7 K:b'iabd' V:{'nombre': 'producer 4'} P:0 O:8 K:b'iabd' V:{'nombre': 'producer 5'} P:0 O:9 K:b'iabd' V:{'nombre': 'producer 6'} P:0 O:10 K:b'iabd' V:{'nombre': 'producer 7'} P:0 O:11 K:b'iabd' V:{'nombre': 'producer 8'} P:0 O:12 K:b'iabd' V:{'nombre': 'producer 9'}","title":"Produciendo y consumiendo"},{"location":"apuntes/bdaplicado05kafka.html#decisiones-de-rendimiento","text":"Aunque podemos modificar la cantidad de particiones y el factor de replicaci\u00f3n una vez creado el cl\u00faster, es mejor hacerlo de la manera correcta durante la creaci\u00f3n ya que tienen un impacto directo en el rendimiento y durabilidad del sistema: si el n\u00famero de particiones crece con el cl\u00faster ya creado, el orden de las claves no est\u00e1 garantizado. si se incrementa el factor de replicaci\u00f3n durante el ciclo de vida de un topic , estaremos metiendo presi\u00f3n al cl\u00faster, que provocar\u00e1 un decremento inesperado del rendimiento. Cada partici\u00f3n puede manejar un rendimiento de unos pocos MB/s. Al a\u00f1adir m\u00e1s particiones, obtendremos mejor paralelizaci\u00f3n y por tanto, mejor rendimiento. Adem\u00e1s, podremos ejecutar m\u00e1s consumidores en un grupo. Pero el hecho de a\u00f1adir m\u00e1s brokers al cl\u00faster para que las particiones los aprovechen, provocar\u00e1 que Zookeeper tenga que realizar m\u00e1s elecciones y que Kafka tenga m\u00e1s ficheros abiertos. Gu\u00eda de rendimiento Una propuesta es: Si nuestro cl\u00faster es peque\u00f1o (menos de 6 brokers ), crear el doble de particiones que brokers . Si tenemos un cl\u00faster grande (m\u00e1s de 12 brokers ), crear la misma cantidad de particiones que brokers . Ajustar el n\u00famero de consumidores necesarios que necesitamos que se ejecuten en paralelo en los picos de rendimiento. Independientemente de la decisi\u00f3n que tomemos, hay que realizar pruebas de rendimiento con diferentes configuraciones. Respecto al factor de replicaci\u00f3n, deber\u00eda ser, al menos 2, siendo 3 la cantidad recomendada (es necesario tener al menos 3 brokers ) y no sobrepasar de 4. Cuanto mayor sea el factor de replicaci\u00f3n (RF): El sistema tendr\u00e1 mejor tolerancia a fallos (se pueden caer RF-1 brokers ) Pero tendremos mayor replicaci\u00f3n (lo que implicar\u00e1 una mayor latencia si acks=all ) Y tambi\u00e9n ocupar\u00e1 m\u00e1s espacio en disco (50% m\u00e1s si RF es 3 en vez de 2). Respecto al cl\u00faster, se recomienda que un broker no contenga m\u00e1s de 2000-4000 particiones (entre todos los topics de ese broker). Adem\u00e1s, un cl\u00faster de Kafka deber\u00eda tener un m\u00e1ximo de 20.000 particiones entre todos los brokers, ya que si se cayese alg\u00fan nodo, Zookeeper necesitar\u00eda realizar muchas elecciones de l\u00edder.","title":"Decisiones de rendimiento"},{"location":"apuntes/bdaplicado05kafka.html#caso-3-de-twitter-a-elasticsearch-con-python","text":"A continuaci\u00f3n vamos a crear un ejemplo completo de flujo de datos mediante Python que nos permita recoger tweets y meterlos dentro de ElasticSearch . Vamos a suponer que ya disponemos de una cuenta de Twitter y que tenemos las credenciales de acceso, las cuales vamos a almacenar en un fichero denominado credential.py : credentials.py API_KEY = 'YOUR_API_KEY' API_SECRET_KEY = 'YOUR_API_SECRET_KEY' ACCESS_TOKEN = 'YOUR_ACCESS_TOKEN' ACCESS_TOKEN_SECRET = 'YOUR_ACCESS_TOKEN_SECRET'","title":"Caso 3: De Twitter a Elasticsearch con Python"},{"location":"apuntes/bdaplicado05kafka.html#tweepy","text":"Para acceder a Twitter desde Python, la librer\u00eda por excelencia es Tweepy , la cual instalaremos mediante: pip install tweepy A continuaci\u00f3n, vamos a realizar el proceso de autenticaci\u00f3n en Tweepy y recoger el timeline de mi usuario: timeline.py import credentials import tweepy # Nos autenticamos mediante OAuth auth = tweepy . OAuthHandler ( credentials . API_KEY , credentials . API_SECRET_KEY ) auth . set_access_token ( credentials . ACCESS_TOKEN , credentials . ACCESS_TOKEN_SECRET ) api = tweepy . API ( auth ) miTimeline = api . home_timeline () for tweet in miTimeline : print ( f ' { tweet . user . screen_name } : \\n { tweet . text } \\n { \"*\" * 60 } ' )","title":"Tweepy"},{"location":"apuntes/bdaplicado05kafka.html#productor-de-tweets","text":"En este caso de uso vamos a buscar los tweets que contengan la palabra bigdata y meterlos en un topic de Kafka . As\u00ed pues, en vez de obtener el timeline de un usuario, realizaremos una b\u00fasqueda mediante la funci\u00f3n search y para cada elemento recuperado, lo enviaremos al productor con toda la informaci\u00f3n (seguimos un planteamiento ELT). Primero creamos el topic: kafka-topics.sh --create --topic iabd-twitter --bootstrap-server iabd-virtualbox:9092 Y a continuaci\u00f3n desarrollamos el productor: producerTwitter.py import credentials import tweepy from kafka import KafkaProducer from json import dumps import time # Creamos el productor de Kafka producer = KafkaProducer ( value_serializer = lambda m : dumps ( m ) . encode ( 'utf-8' ), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) # Nos autenticamos mediante OAuth en Twitter auth = tweepy . OAuthHandler ( credentials . API_KEY , credentials . API_SECRET_KEY ) auth . set_access_token ( credentials . ACCESS_TOKEN , credentials . ACCESS_TOKEN_SECRET ) api = tweepy . API ( auth ) # Cargamos 500 tweets id = None cantidad = 0 while cantidad <= 500 : tweets = api . search_tweets ( q = 'bigdata' , tweet_mode = 'extended' , max_id = id ) for tweet in tweets : producer . send ( \"iabd-twitter\" , value = tweet . _json ) cantidad += 1 # Al final del ciclo le asignamos el id del \u00faltimo tweet # para que en cada ciclo se consulten solo los tweets hasta el m\u00e1s antiguos del ciclo anterior id = tweet . id # Como el env\u00edo es as\u00edncrono, para que no se salga del programa antes de enviar el \u00faltimo mensaje, esperamos 1 seg time . sleep ( 1 )","title":"Productor de Tweets"},{"location":"apuntes/bdaplicado05kafka.html#elasticsearch-desde-python","text":"Para poder acceder a Elasticsearch desde Python necesitamos descargar la librer\u00eda: pip install elasticsearch El siguiente fragmento muestra varias operaciones b\u00e1sicas y como las operaciones REST de Elasticsearch se traducen en m\u00e9todos: C\u00f3digo Python Resultado prueba-elasticsearch.py from datetime import datetime from elasticsearch import Elasticsearch es = Elasticsearch ( \"http://localhost:9200\" ) doc1 = { 'author' : 'Aitor Medrano' , 'text' : 'Prueba de texto desde Python' , 'timestamp' : datetime . now (), } doc2 = { 'author' : 'Aitor Medrano' , 'text' : 'Y otra #prueba desde @Python' , 'timestamp' : datetime . now (), } # Inserci\u00f3n resp = es . index ( index = \"prueba\" , id = 1 , document = doc1 ) resp = es . index ( index = \"prueba\" , id = 2 , document = doc2 ) print ( resp [ 'result' ]) # Recuperaci\u00f3n resp = es . get ( index = \"prueba\" , id = 1 ) print ( resp [ '_source' ]) es . indices . refresh ( index = \"prueba\" ) # B\u00fasqueda resp = es . search ( index = \"prueba\" , query = { \"match_all\" : {}}) print ( \"Encontrados %d Hits:\" % resp [ 'hits' ][ 'total' ][ 'value' ]) for hit in resp [ 'hits' ][ 'hits' ]: print ( \" %(timestamp)s %(author)s : %(text)s \" % hit [ \"_source\" ]) resp = es.index(index=\"prueba\", id=1, document=doc1) resp = es.index(index=\"prueba\", id=2, document=doc2) updated resp = es.get(index=\"prueba\", id=1) {'author': 'Aitor Medrano', 'text': 'Prueba de texto desde Python', 'timestamp': '2022-02-25T15:11:43.720820'} es.indices.refresh(index=\"prueba\") resp = es.search(index=\"prueba\", query={\"match_all\": {}}) Encontrados 2 Hits: 2022-02-25T15:11:43.720820 Aitor Medrano: Prueba de texto desde Python 2022-02-25T15:11:43.720826 Aitor Medrano: Y otra #prueba desde @Python","title":"Elasticsearch desde Python"},{"location":"apuntes/bdaplicado05kafka.html#consumidor-en-elasticsearch","text":"Finalmente, vamos a crear un consumidor que se conecte a Kafka para consumir los mensajes, e introduzca cada uno de los tuits en Elasticsearch : consumerTwitter.py from datetime import datetime from elasticsearch import Elasticsearch from kafka import KafkaConsumer from json import loads import ast es = Elasticsearch ( \"http://localhost:9200\" ) consumer = KafkaConsumer ( 'iabd-twitter' , auto_offset_reset = 'earliest' , enable_auto_commit = True , group_id = 'iabd-caso3' , value_deserializer = lambda m : loads ( m . decode ( 'utf-8' )), bootstrap_servers = [ 'iabd-virtualbox:9092' ]) cantidad = 1 for m in consumer : tweet = m . value # print(type(tweet)) # tweet = ast.literal_eval(m.value) # print(tweet['user']) # doc = { # 'user': m.value.user.screen_name, # 'text': m.value.full_text, # 'created_at': m.value.created_at, # 'likes': m.value.favourite_count # } resp = es . index ( index = \"iabd-twitter-es\" , id = cantidad , document = tweet ) cantidad += 1 # Forzamos que se persistan los cambios es . indices . refresh ( index = \"iabd-twitter-es\" ) Si accedemos a http://localhost:9200/iabd-twitter-es/_search?pretty podremos ver como Elasticsearch contiene los tuits que hab\u00edamos producido previamente: Resultado de consumir tuits desde Kafka y cargar en Elasticsearch","title":"Consumidor en Elasticsearch"},{"location":"apuntes/bdaplicado05kafka.html#todo-en-uno-con-nifi","text":"Vamos a realizar un flujo de datos en Nifi para el mismo caso de uso que acabamos de desarrollar. Para ello, vamos a crear dos grupos de procesos para tener los flujos ordenados: Grupos de procesos para E y L En el primer grupo, que hemos denominado Extract Twitter , vamos a realizar la carga desde Twitter (filtrando los mensajes que contengan la palabra bigdata ) y los vamos a meter en el topic iabd-twitter . Para ello, conectaremos los siguientes procesadores: GetTwitter : tras introducir los valores para autenticarnos en Twitter, configuraremos como endpoint que sea de tipo Filter Endpoint y como t\u00e9rmino a filtrar bigdata (no olvides poner las claves de autenticaci\u00f3n). PublishKafka_2_6 : en este procesador, adem\u00e1s del topic iabd-twitter , indicaremos que no utilizaremos transacciones ( Use Transactions: false ), as\u00ed como que intente garantizar la entrega (*Delivery Guarantee: Best Effort): De Twitter a Kafka Del mismo modo, dentro del segundo grupo ( Load Elasticsearch ), conectaremos los siguientes procesadores: ConsumerKafka_2_6 : donde consumiremos los mensajes del topic iabd-twitter de Kafka , y por ejemplo, como grupo de consumidores le indicaremos que usamos nifi ( group id: nifi ) PutElasticsearchHttp : como ya hicimos en la sesi\u00f3n de Nifi , indicaremos la URL de Elasticsearch ( http://localhost:9200 ) y que lo almacene en un indice que hemos denominado iabd-twitter-es : De Kafka a Elasticsearch","title":"Todo en uno con Nifi"},{"location":"apuntes/bdaplicado05kafka.html#kafka-connect","text":"Si hacerlo con Nifi ya es un avance respecto a tener que codificarlo con Python , \u00bfqu\u00e9 dir\u00edas si Kafka ofreciera una serie de conectores para las operaciones m\u00e1s comunes? As\u00ed pues, Kafka Connect permite importar/exportar datos desde/hacia Kafka , facilitando la integraci\u00f3n en sistemas existentes mediante alguno del centenar de conectores disponibles . Arquitectura Kafka Connect Los elementos que forman Kafka Connect son: Conectores fuente ( source ), para obtener datos desde las fuentes de datos (E en ETL) Conectores destino ( sink ) para publicar los datos en los almacenes de datos (L en ETL). Estos conectores facilitan que desarrolladores no expertos puedan trabajar con sus datos en Kafka de forma r\u00e1pida y fiable, de manera que podamos introducir Kafka dentro de nuestros procesos ETL.","title":"Kafka Connect"},{"location":"apuntes/bdaplicado05kafka.html#hola-kafka-connect","text":"Vamos a realizar un ejemplo muy sencillo leyendo datos de una base de datos para meterlos en Kafka. Para ello, utilizaremos la base de datos de retail_db que ya hemos empleado en otras sesiones y vamos a cargar en Kafka los datos de la tabla categories : MariaDB [ retail_db ] > describe categories ; +------------------------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +------------------------+-------------+------+-----+---------+----------------+ | category_id | int ( 11 ) | NO | PRI | NULL | auto_increment | | category_department_id | int ( 11 ) | NO | | NULL | | | category_name | varchar ( 45 ) | NO | | NULL | | +------------------------+-------------+------+-----+---------+----------------","title":"Hola Kafka Connect"},{"location":"apuntes/bdaplicado05kafka.html#rest-api","text":"Como Kafka Connect est\u00e1 dise\u00f1ado como un servicio que deber\u00eda correr continuamente, ofrece un API REST para gestionar los conectores. Por defecto est\u00e1 a la escucha el puerto 8083, de manera que si accedemos a http://iabd-virtualbox:8083/ obtendremos informaci\u00f3n sobre la versi\u00f3n que se est\u00e1 ejecutando: { \"version\" : \"2.8.1\" , \"commit\" : \"839b886f9b732b15\" , \"kafka_cluster_id\" : \"iHa0JUnTSfm85fvFadsylA\" } Por ejemplo, si queremos obtener un listado de los conectores realizaremos una petici\u00f3n GET a /connectors mediante http://iabd-virtualbox:8083/connectors : [ \"retaildb-mariabd-source-jdbc-autoincrement\" ] M\u00e1s informaci\u00f3n en https://kafka.apache.org/documentation/#connect_rest Kafka Streams Kafka Streams es la tercera pata del ecosistema de Kafka, y permite procesar y transformar datos dentro de Kafka. Una vez que los datos se almacenan en Kafka como eventos, podemos procesar los datos en nuestras aplicaciones cliente mediante Kafka Streams y sus librer\u00edas desarrolladas en Java y/o Scala, ya que requiere una JVM. En nuestro caso, realizaremos en posteriores sesiones un procesamiento similar de los datos mediante Spark Streaming , permitiendo operaciones con estado y agregaciones, funciones ventana, joins , procesamiento de eventos basados en el tiempo, etc...","title":"REST API"},{"location":"apuntes/bdaplicado05kafka.html#kafka-y-el-big-data","text":"El siguiente gr\u00e1fico muestra c\u00f3mo Kafka est\u00e1 enfocado principalmente para el tratamiento en streaming , aunque con los conectores de Kafka Connect da soporte para el procesamiento batch : Kafka y Big Data","title":"Kafka y el Big Data"},{"location":"apuntes/bdaplicado05kafka.html#actividades","text":"Realiza los casos de uso 0 y 1. (opcional) A partir del caso 2, crea un cl\u00faster de Kafka con 4 particiones y 3 nodos. A continuaci\u00f3n, en el productor utiliza Faker para crear 10 personas (almac\u00e9nalas como un diccionario). En el consumidor, muestra los datos de las personas (no es necesario recibirlos ordenados, s\u00f3lo necesitamos que se aproveche al m\u00e1ximo la infraestructura de Kafka). Realiza el caso de uso 3 (De Twitter a ElasticSearch ) tanto con Python como con Nifi . (opcional) Repite el caso de uso 2 \u00fanicamente mediante Kafka Connect . (opcional) Investiga en qu\u00e9 consiste el patr\u00f3n CDC ( Change Data Capture ) y c\u00f3mo se realiza CDC con Kafka / Kafka Connect y Debezium . \u00bfQu\u00e9 ventajas aportan las soluciones CDC?","title":"Actividades"},{"location":"apuntes/bdaplicado05kafka.html#referencias","text":"Apache Kafka Series - Learn Apache Kafka for Beginners Serie de art\u00edculos de V\u00edctor Madrid sobre Kafka en enmilocalfunciona.io . Distributed Databases: Kafka por Miguel del Tio Introduction to Kafka Connectors Kafka Cheatsheet","title":"Referencias"},{"location":"apuntes/ingesta01.html","text":"Ingesta de Datos \u00b6 Introducci\u00f3n \u00b6 Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, desde diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Ingesta de datos La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de datos puedan construir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Entendemos como pipeline de datos un proceso que consume datos desde un punto de origen, los limpia y los escribe en un nuevo destino. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data, para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y qu\u00e9 se pretende conseguir con ellos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake asegur\u00e1ndose que los datos son compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n. Pipeline de datos \u00b6 Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean (principalmente para evitar problemas de rendimiento). El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (sistemas de procesamiento transaccional online, como un CRM), encargados de capturar y crear datos, y de forma separada, sistemas OLAP (sistemas de procesamiento anal\u00edtico, como un Data Warehouse ), encargados de analizar los datos. Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Recogemos los datos y los enviamos a un topic de Apache Kafka. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Ejemplo de pipeline - aprenderbigdata.com Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch, leemos los datos del buffer. Por ejemplo, mediante Spark realizamos la anal\u00edtica sobre estos datos (haciendo c\u00e1lculos, filtrados, agrupaciones de datos, etc...). Almacenamos el resultado en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuido como Amazon S3 . Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL, no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas. ETL \u00b6 Una ETL, entendida como un proceso que lleva la informaci\u00f3n de un punto A a un punto B, puede realizarse mediante diversas herramientas, scripts, Python, etc... Pero cuando nos metemos con Big Data no servir\u00e1 cualquier tipo de herramienta, ya que necesitamos que sean: Flexibles y soporten formatos variados (JSON, CSV, etc...) Escalables y tolerante a fallos. Dispongan de conectores a m\u00faltiples fuentes y destinos de datos. Los procesos ETL, siglas de e xtracci\u00f3n, t ransformaci\u00f3n y carga ( l oad ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Extracci\u00f3n, Transformaci\u00f3n y Carga (load) Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto. Extracci\u00f3n \u00b6 Encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales, pasando por mensajes de redes sociales como Twitter o Redddit . Un paso que forma parte de la extracci\u00f3n es la de analizar que los datos sean veraces, que contiene la informaci\u00f3n que se espera, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser transparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se genera en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n. Transformaci\u00f3n \u00b6 En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n. Eliminar datos duplicados. Cruzar diferentes fuentes de datos para obtener una fuente diferente. Agregar informaci\u00f3n en funci\u00f3n de alguna variable. Tomar parte de los datos para cargarlos. Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n. Estructurar mejor la informaci\u00f3n. Generar indicadores que faciliten el procesamiento y entendimiento. Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados, los datos ya estar\u00e1n listos para su carga. Carga \u00b6 Fase encargada de almacenar los datos en el destino, un data warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga debe realizarse buscando minimizar el tiempo de la transacci\u00f3n. Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift, ...) Cargadores Propios (HDFS, S3, ...) Para mejorar la carga debemos tener en cuenta la: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s ELT \u00b6 ELT cambia el orden de las siglas y se basa en extraer, cargar y transformar. Es un t\u00e9cnica de ingesti\u00f3n de datos donde los datos que se obtienen desde m\u00faltiples fuentes se colocan sin transformar directamente en un data lake o almacenamiento de objetos en la nube. Desde ah\u00ed, los datos se pueden transformar dependiendo de los diferentes objetivos de negocio. En principio un proceso ELT necesita menos ingenieros de datos necesarios. Con la separaci\u00f3n de la extracci\u00f3n y la transformaci\u00f3n, ELT permite que los analistas y cient\u00edficos de datos realicen las transformaciones, ya sea con SQL o mediante Python. De esta manera, m\u00e1s departamentos se involucran en obtener y mejorar los datos. Una de las principales razones de que ELT cueste menos de implementar es que permite una mayor generalizaci\u00f3n de la informaci\u00f3n que se almacena. Los ingenieros de datos generan un data lake con los datos obtenidos de las fuentes de datos m\u00e1s populares, dejando que la transformaci\u00f3n la realicen los expertos en el negocio. Esto tambi\u00e9n implica que los datos est\u00e9n disponibles antes, ya que mediante un proceso ETL los datos no est\u00e1n disponibles para los usuarios hasta que se han transformado, lo que suele implicar un largo proceso de trabajo. En resumen, el mercado se est\u00e1 moviendo desde un desarrollo centralizado mediante ETL a uno m\u00e1s orientado a servicios como ELT, que permite automatizar la carga del data lake y la posterior codificaci\u00f3n de los flujos de datos. Herramientas ETL \u00b6 Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD transaccionales, XML, ficheros CSV / JSON, Teradata, HDFS, Hive, S3, ... Peticiones HTTP, servicios REST... APIs de aplicaciones de terceros, logs\u2026 Permiten la planificaci\u00f3n mediante batch , eventos o en streaming . Capacidad para transformar los datos: Transformaciones simples: tipos de datos, cadenas, codificaciones, c\u00e1lculos simples. Transformaciones intermedias: agregaciones, lookups. Transformaciones complejas: algoritmos de IA, segmentaci\u00f3n, integraci\u00f3n de c\u00f3digo de terceros, integraci\u00f3n con otros lenguajes. Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL La ingesta por dentro \u00b6 La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que tenga que aplicar uno m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almac\u00e9n de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregaciones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. La ingesta de datos - StreamSets Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: servicios de mensajer\u00eda como Apache Kafka, los cuales han obtenido datos desde fuentes externas, como pueden ser dispositivos IOT o contenido obtenido directamente de las redes sociales. bases de datos relacionales, las cuales se acceden, por ejemplo, mediante JDBC. servicios REST que devuelven los datos en formato JSON. servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: servicios de mensajer\u00eda como Apache Kafka . bases de datos relacionales. bases de datos NoSQL. servicios de almacenamiento distribuido como HDFS o S3. plataformas de datos como Snowflake o Databricks . Batch vs Streaming \u00b6 El movimiento de datos entre los or\u00edgenes y los destinos se puede hacer, tal como vimos en la sesi\u00f3n de Arquitecturas de Big Data , mediante un proceso: Batch : el proceso se ejecuta de forma peri\u00f3dica (normalmente en intervalos fijos) a partir de unos datos est\u00e1ticos . Muy eficiente para grandes vol\u00famenes de datos, y donde la latencia (del orden de minutos) no es el factor m\u00e1s importante. Algunas de las herramientas utilizadas son Apache Sqoop , trabajos en MapReduce o de Spark jobs , etc... Streaming : tambi\u00e9n conocido como en tiempo real, donde los datos se leen, modifican y cargan tan pronto como llegan a la capa de ingesta (la latencia es cr\u00edtica). Algunas de las herramientas utilizadas son Apache Storm , Spark Streaming , Apache Nifi , Apache Kafka , etc... Arquitectura \u00b6 Si nos basamos en la arquitectura por capas , podemos ver como la capa de ingesta es la primera de la arquitectura por capas, la cual recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas: Arquitectura por capas (xenonstack.com) En el primer paso de la ingesta es el paso m\u00e1s pesado, por tiempo y cantidad de recursos necesarios. Es normal realizar la ingesta de flujos de datos desde cientos a miles de fuentes de datos, los cuales se obtiene a velocidades variables y en diferentes formatos. Para ello, es necesario: Priorizar las fuentes de datos Validar de forma individual cada fichero Enrutar cada elemento a su destino correcto. Resumiendo, los cuatro par\u00e1metros en los que debemos centrar nuestros esfuerzos son: Velocidad de los datos: c\u00f3mo fluyen los datos entre m\u00e1quinas, interacci\u00f3n con usuario y redes sociales, si el flujo es continuo o masivo. Tama\u00f1o de los datos: la ingesta de m\u00faltiples fuentes puede incrementarse con el tiempo. Frecuencia de los datos: \u00bfBatch o en Streaming? Formato de los datos: estructurado (tablas), desestructurado (im\u00e1genes, audios, v\u00eddeos, ...), o semi-estructurado (JSON). Herramientas de Ingesta de datos \u00b6 Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Herramientas de ingesta de datos Apache Sqoop : permite la transferencia bidireccional de datos entre Hadoop/Hive/HBase y una bases de datos SQL (datos estructurados). Aunque principalmente se interactura mediante comandos, proporciona una API Java. Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados sobre HDFS o HBase mediante una arquitectura basada en flujos de datos en streaming. Apache Nifi : herramienta que facilita un interfaz gr\u00e1fico que permite cargar datos de diferentes fuentes (tanto batch como streaming ), los pasa por un flujo de procesos (mediante grafos dirigidos) para su tratamiento y transformaci\u00f3n, y los vuelca en otra fuente. Elastic Logstash : Pensada inicialmente para la ingesta de logs en Elasticsearch , admite entradas y salidas de diferentes tipos (incluso AWS). AWS Glue : servicios gestionado para realizar tareas ETL desde la consola de AWS. Facilita el descubrimiento de datos y esquemas. Tambi\u00e9n se utiliza como almacenamiento de servicios como Amazon Athena o AWS Data Pipeline . Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema de colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services. Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure. Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud. Consideraciones \u00b6 A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data, hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCual va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos? \u00bfSer\u00e1n datos estructurados o datos sin estructura? \u00bfCu\u00e1l es el volumen de los datos? Volumen diario, y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/d\u00edas (mediante procesos batch ) o ser real-time (mediante streaming ) Actualizaciones \u00bfLas fuentes origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes + insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y DynamoDB? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / DynamoDB ? \u00bfQu\u00e9 procesos de transformaci\u00f3n se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Seguridad de los datos. Si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta? Actividades \u00b6 Completa el cuestionario disponible en Aules (dentro del apartado Big Data Aplicado ). Referencias \u00b6 Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility","title":"1.- ETL"},{"location":"apuntes/ingesta01.html#ingesta-de-datos","text":"","title":"Ingesta de Datos"},{"location":"apuntes/ingesta01.html#introduccion","text":"Formalmente, la ingesta de datos es el proceso mediante el cual se introducen datos, desde diferentes fuentes, estructura y/o caracter\u00edsticas dentro de otro sistema de almacenamiento o procesamiento de datos. Ingesta de datos La ingesta de datos es un proceso muy importante porque la productividad de un equipo va directamente ligada a la calidad del proceso de ingesta de datos. Estos procesos deben ser flexibles y \u00e1giles, ya que una vez puesta en marcha, los analistas y cient\u00edficos de datos puedan construir un pipeline de datos para mover los datos a la herramienta con la que trabajen. Entendemos como pipeline de datos un proceso que consume datos desde un punto de origen, los limpia y los escribe en un nuevo destino. Es sin duda, el primer paso que ha de tenerse en cuenta a la hora de dise\u00f1ar una arquitectura Big Data, para lo cual, hay que tener muy claro, no solamente el tipo y fuente de datos, sino cual es el objetivo final y qu\u00e9 se pretende conseguir con ellos. Por lo tanto, en este punto, hay que realizar un an\u00e1lisis detallado, porque es la base para determinar las tecnolog\u00edas que compondr\u00e1n nuestra arquitectura Big Data. Dada la gran cantidad de datos que disponen las empresas, toda la informaci\u00f3n que generan desde diferentes fuentes se deben integrar en un \u00fanico lugar, al que actualmente se le conoce como data lake asegur\u00e1ndose que los datos son compatibles entre s\u00ed. Gestionar tal volumen de datos puede llegar a ser un procedimiento complejo, normalmente dividido en procesos distintos y de relativamente larga duraci\u00f3n.","title":"Introducci\u00f3n"},{"location":"apuntes/ingesta01.html#pipeline-de-datos","text":"Un pipeline es una construcci\u00f3n l\u00f3gica que representa un proceso dividido en fases. Los pipelines de datos se caracterizan por definir el conjunto de pasos o fases y las tecnolog\u00edas involucradas en un proceso de movimiento o procesamiento de datos. Las pipelines de datos son necesarios ya que no debemos analizar los datos en los mismos sistemas donde se crean (principalmente para evitar problemas de rendimiento). El proceso de anal\u00edtica es costoso computacionalmente, por lo que se separa para evitar perjudicar el rendimiento del servicio. De esta forma, tenemos sistemas OLTP (sistemas de procesamiento transaccional online, como un CRM), encargados de capturar y crear datos, y de forma separada, sistemas OLAP (sistemas de procesamiento anal\u00edtico, como un Data Warehouse ), encargados de analizar los datos. Los movimientos de datos entre estos sistemas involucran varias fases. Por ejemplo: Recogemos los datos y los enviamos a un topic de Apache Kafka. Kafka act\u00faa aqu\u00ed como un buffer para el siguiente paso. Ejemplo de pipeline - aprenderbigdata.com Mediante una tecnolog\u00eda de procesamiento, que puede ser streaming o batch, leemos los datos del buffer. Por ejemplo, mediante Spark realizamos la anal\u00edtica sobre estos datos (haciendo c\u00e1lculos, filtrados, agrupaciones de datos, etc...). Almacenamos el resultado en una base de datos NoSQL como Amazon DynamoDB o un sistema de almacenamiento distribuido como Amazon S3 . Aunque a menudo se intercambian los t\u00e9rminos de pipeline de datos y ETL, no significan lo mismo. Las ETLs son un caso particular de pipeline de datos que involucran las fases de extracci\u00f3n, transformaci\u00f3n y carga de datos. Las pipelines de datos son cualquier proceso que involucre el movimiento de datos entre sistemas.","title":"Pipeline de datos"},{"location":"apuntes/ingesta01.html#etl","text":"Una ETL, entendida como un proceso que lleva la informaci\u00f3n de un punto A a un punto B, puede realizarse mediante diversas herramientas, scripts, Python, etc... Pero cuando nos metemos con Big Data no servir\u00e1 cualquier tipo de herramienta, ya que necesitamos que sean: Flexibles y soporten formatos variados (JSON, CSV, etc...) Escalables y tolerante a fallos. Dispongan de conectores a m\u00faltiples fuentes y destinos de datos. Los procesos ETL, siglas de e xtracci\u00f3n, t ransformaci\u00f3n y carga ( l oad ), permiten a las organizaciones recopilar en un \u00fanico lugar todos los datos de los que pueden disponer. Ya hemos comentado que estos datos provienen de diversas fuentes, por lo que es necesario acceder a ellos, y formatearlos para poder ser capaces de integrarlos. Adem\u00e1s, es muy recomendable asegurar la calidad de los datos y su veracidad, para as\u00ed evitar la creaci\u00f3n de errores en los datos. Extracci\u00f3n, Transformaci\u00f3n y Carga (load) Una vez los datos est\u00e1n unificados en un data lake , otro tipo de herramientas de an\u00e1lisis permitir\u00e1n su estudio para apoyar procesos de negocio. Dada la gran variedad de posibilidades existentes para representar la realidad en un dato, junto con la gran cantidad de datos almacenados en las diferentes fuentes de origen, los procesos ETL consumen una gran cantidad de los recursos asignados a un proyecto.","title":"ETL"},{"location":"apuntes/ingesta01.html#extraccion","text":"Encargada de recopilar los datos de los sistemas originales y transportarlos al sistema donde se almacenar\u00e1n, de manera general suele tratarse de un entorno de Data Warehouse o almac\u00e9n de datos. Las fuentes de datos pueden encontrarse en diferentes formatos, desde ficheros planos hasta bases de datos relacionales, pasando por mensajes de redes sociales como Twitter o Redddit . Un paso que forma parte de la extracci\u00f3n es la de analizar que los datos sean veraces, que contiene la informaci\u00f3n que se espera, verificando que siguen el formato que se esperaba. En caso contrario, esos datos se rechazan. La primera caracter\u00edstica deseable de un proceso de extracci\u00f3n es que debe ser un proceso r\u00e1pido, ligero, causar el menor impacto posible, ser transparente para los sistemas operacionales e independiente de las infraestructuras. La segunda caracter\u00edstica es que debe reducir al m\u00ednimo el impacto que se genera en el sistema origen de la informaci\u00f3n. No se puede poner en riesgo el sistema original, generalmente operacional, ni perder ni modificar sus datos; ya que si colapsase esto podr\u00eda afectar el uso normal del sistema y generar p\u00e9rdidas a nivel operacional. As\u00ed pues, la extracci\u00f3n convierte los datos a un formato preparado para iniciar el proceso de transformaci\u00f3n.","title":"Extracci\u00f3n"},{"location":"apuntes/ingesta01.html#transformacion","text":"En esta fase se espera realizar los cambios necesarios en los datos de manera que estos tengan el formato y contenido esperado. En concreto, la transformaci\u00f3n puede comprender: Cambios de codificaci\u00f3n. Eliminar datos duplicados. Cruzar diferentes fuentes de datos para obtener una fuente diferente. Agregar informaci\u00f3n en funci\u00f3n de alguna variable. Tomar parte de los datos para cargarlos. Transformar informaci\u00f3n para generar c\u00f3digos, claves, identificadores\u2026 Generar informaci\u00f3n. Estructurar mejor la informaci\u00f3n. Generar indicadores que faciliten el procesamiento y entendimiento. Respecto a sus caracter\u00edsticas, debe transformar los datos para mejorarlos, incrementar su calidad, integrarlos con otros sistemas, normalizarlos, eliminar duplicidades o ambig\u00fcedades. Adem\u00e1s, no debe crear informaci\u00f3n, duplicar, eliminar informaci\u00f3n relevante, ser err\u00f3nea o impredecible. Una vez transformados, los datos ya estar\u00e1n listos para su carga.","title":"Transformaci\u00f3n"},{"location":"apuntes/ingesta01.html#carga","text":"Fase encargada de almacenar los datos en el destino, un data warehouse o en cualquier tipo de base de datos. Por tanto la fase de carga interact\u00faa de manera directa con el sistema destino, y debe adaptarse al mismo con el fin de cargar los datos de manera satisfactoria. La carga debe realizarse buscando minimizar el tiempo de la transacci\u00f3n. Cada BBDD puede tener un sistema ideal de carga basado en: SQL (Oracle, SQL Server, Redshift, Postgres, Teradata, Greenplum, \u2026) Ficheros (Postgres, Redshift, ...) Cargadores Propios (HDFS, S3, ...) Para mejorar la carga debemos tener en cuenta la: Gestiones de \u00edndices Gesti\u00f3n de claves de distribuci\u00f3n y particionado Tama\u00f1o de las transacciones y commit\u2019s","title":"Carga"},{"location":"apuntes/ingesta01.html#elt","text":"ELT cambia el orden de las siglas y se basa en extraer, cargar y transformar. Es un t\u00e9cnica de ingesti\u00f3n de datos donde los datos que se obtienen desde m\u00faltiples fuentes se colocan sin transformar directamente en un data lake o almacenamiento de objetos en la nube. Desde ah\u00ed, los datos se pueden transformar dependiendo de los diferentes objetivos de negocio. En principio un proceso ELT necesita menos ingenieros de datos necesarios. Con la separaci\u00f3n de la extracci\u00f3n y la transformaci\u00f3n, ELT permite que los analistas y cient\u00edficos de datos realicen las transformaciones, ya sea con SQL o mediante Python. De esta manera, m\u00e1s departamentos se involucran en obtener y mejorar los datos. Una de las principales razones de que ELT cueste menos de implementar es que permite una mayor generalizaci\u00f3n de la informaci\u00f3n que se almacena. Los ingenieros de datos generan un data lake con los datos obtenidos de las fuentes de datos m\u00e1s populares, dejando que la transformaci\u00f3n la realicen los expertos en el negocio. Esto tambi\u00e9n implica que los datos est\u00e9n disponibles antes, ya que mediante un proceso ETL los datos no est\u00e1n disponibles para los usuarios hasta que se han transformado, lo que suele implicar un largo proceso de trabajo. En resumen, el mercado se est\u00e1 moviendo desde un desarrollo centralizado mediante ETL a uno m\u00e1s orientado a servicios como ELT, que permite automatizar la carga del data lake y la posterior codificaci\u00f3n de los flujos de datos.","title":"ELT"},{"location":"apuntes/ingesta01.html#herramientas-etl","text":"Las caracteristicas de las herramientas ETL son: Permiten conectividad con diferentes sistemas y tipos de datos Excel, BBDD transaccionales, XML, ficheros CSV / JSON, Teradata, HDFS, Hive, S3, ... Peticiones HTTP, servicios REST... APIs de aplicaciones de terceros, logs\u2026 Permiten la planificaci\u00f3n mediante batch , eventos o en streaming . Capacidad para transformar los datos: Transformaciones simples: tipos de datos, cadenas, codificaciones, c\u00e1lculos simples. Transformaciones intermedias: agregaciones, lookups. Transformaciones complejas: algoritmos de IA, segmentaci\u00f3n, integraci\u00f3n de c\u00f3digo de terceros, integraci\u00f3n con otros lenguajes. Metadatos y gesti\u00f3n de errores Permiten tener informaci\u00f3n del funcionamiento de todo el proceso Permiten el control de errores y establecer politicas al respecto Las soluciones m\u00e1s empleadas son: Pentaho Data Integration (PDI) Oracle Data Integrator Talend Open Studio Mulesoft Informatica Data Integration Herramientas ETL","title":"Herramientas ETL"},{"location":"apuntes/ingesta01.html#la-ingesta-por-dentro","text":"La ingesta extrae los datos desde la fuente donde se crean o almacenan originalmente y los carga en un destino o zona temporal. Un pipeline de datos sencillo puede que tenga que aplicar uno m\u00e1s transformaciones ligeras para enriquecer o filtrar los datos antes de escribirlos en un destino, almac\u00e9n de datos o cola de mensajer\u00eda. Se pueden a\u00f1adir nuevos pipelines para transformaciones m\u00e1s complejas como joins , agregaciones u ordenaciones para anal\u00edtica de datos, aplicaciones o sistema de informes. La ingesta de datos - StreamSets Las fuentes m\u00e1s comunes desde las que se obtienen los datos son: servicios de mensajer\u00eda como Apache Kafka, los cuales han obtenido datos desde fuentes externas, como pueden ser dispositivos IOT o contenido obtenido directamente de las redes sociales. bases de datos relacionales, las cuales se acceden, por ejemplo, mediante JDBC. servicios REST que devuelven los datos en formato JSON. servicios de almacenamiento distribuido como HDFS o S3. Los destinos donde se almacenan los datos son: servicios de mensajer\u00eda como Apache Kafka . bases de datos relacionales. bases de datos NoSQL. servicios de almacenamiento distribuido como HDFS o S3. plataformas de datos como Snowflake o Databricks .","title":"La ingesta por dentro"},{"location":"apuntes/ingesta01.html#batch-vs-streaming","text":"El movimiento de datos entre los or\u00edgenes y los destinos se puede hacer, tal como vimos en la sesi\u00f3n de Arquitecturas de Big Data , mediante un proceso: Batch : el proceso se ejecuta de forma peri\u00f3dica (normalmente en intervalos fijos) a partir de unos datos est\u00e1ticos . Muy eficiente para grandes vol\u00famenes de datos, y donde la latencia (del orden de minutos) no es el factor m\u00e1s importante. Algunas de las herramientas utilizadas son Apache Sqoop , trabajos en MapReduce o de Spark jobs , etc... Streaming : tambi\u00e9n conocido como en tiempo real, donde los datos se leen, modifican y cargan tan pronto como llegan a la capa de ingesta (la latencia es cr\u00edtica). Algunas de las herramientas utilizadas son Apache Storm , Spark Streaming , Apache Nifi , Apache Kafka , etc...","title":"Batch vs Streaming"},{"location":"apuntes/ingesta01.html#arquitectura","text":"Si nos basamos en la arquitectura por capas , podemos ver como la capa de ingesta es la primera de la arquitectura por capas, la cual recoge los datos que provienen de fuentes diversas. Los datos se categorizan y priorizan, facilitando el flujo de \u00e9stos en posteriores capas: Arquitectura por capas (xenonstack.com) En el primer paso de la ingesta es el paso m\u00e1s pesado, por tiempo y cantidad de recursos necesarios. Es normal realizar la ingesta de flujos de datos desde cientos a miles de fuentes de datos, los cuales se obtiene a velocidades variables y en diferentes formatos. Para ello, es necesario: Priorizar las fuentes de datos Validar de forma individual cada fichero Enrutar cada elemento a su destino correcto. Resumiendo, los cuatro par\u00e1metros en los que debemos centrar nuestros esfuerzos son: Velocidad de los datos: c\u00f3mo fluyen los datos entre m\u00e1quinas, interacci\u00f3n con usuario y redes sociales, si el flujo es continuo o masivo. Tama\u00f1o de los datos: la ingesta de m\u00faltiples fuentes puede incrementarse con el tiempo. Frecuencia de los datos: \u00bfBatch o en Streaming? Formato de los datos: estructurado (tablas), desestructurado (im\u00e1genes, audios, v\u00eddeos, ...), o semi-estructurado (JSON).","title":"Arquitectura"},{"location":"apuntes/ingesta01.html#herramientas-de-ingesta-de-datos","text":"Las herramientas de ingesta de datos para ecosistemas Big Data se clasifican en los siguientes bloques: Herramientas de ingesta de datos Apache Sqoop : permite la transferencia bidireccional de datos entre Hadoop/Hive/HBase y una bases de datos SQL (datos estructurados). Aunque principalmente se interactura mediante comandos, proporciona una API Java. Apache Flume : sistema de ingesta de datos semiestructurados o no estructurados sobre HDFS o HBase mediante una arquitectura basada en flujos de datos en streaming. Apache Nifi : herramienta que facilita un interfaz gr\u00e1fico que permite cargar datos de diferentes fuentes (tanto batch como streaming ), los pasa por un flujo de procesos (mediante grafos dirigidos) para su tratamiento y transformaci\u00f3n, y los vuelca en otra fuente. Elastic Logstash : Pensada inicialmente para la ingesta de logs en Elasticsearch , admite entradas y salidas de diferentes tipos (incluso AWS). AWS Glue : servicios gestionado para realizar tareas ETL desde la consola de AWS. Facilita el descubrimiento de datos y esquemas. Tambi\u00e9n se utiliza como almacenamiento de servicios como Amazon Athena o AWS Data Pipeline . Por otro lado existen sistemas de mensajer\u00eda con funciones propias de ingesta, tales como: Apache Kafka : sistema de intermediaci\u00f3n de mensajes basado en el modelo publicador/suscriptor. RabbitMQ : sistema de colas de mensajes (MQ) que act\u00faa de middleware entre productores y consumidores. Amazon Kinesis : hom\u00f3logo de Kafka para la infraestructura Amazon Web Services. Microsoft Azure Event Hubs : hom\u00f3logo de Kafka para la infraestructura Microsoft Azure. Google Pub/Sub : hom\u00f3logo de Kafka para la infraestructura Google Cloud.","title":"Herramientas de Ingesta de datos"},{"location":"apuntes/ingesta01.html#consideraciones","text":"A la hora de analizar cual ser\u00eda la tecnolog\u00eda y arquitectura adecuada para realizar la ingesta de datos en un sistema Big Data, hemos de tener en cuenta los siguientes factores: Origen y formato de los datos \u00bfCual va a ser el origen u or\u00edgenes de los datos? \u00bfProvienen de sistemas externos o internos? \u00bfSer\u00e1n datos estructurados o datos sin estructura? \u00bfCu\u00e1l es el volumen de los datos? Volumen diario, y plantear como ser\u00eda la primera carga de datos. \u00bfExiste la posibilidad de que m\u00e1s adelante se incorporen nuevas fuentes de datos? Latencia/Disponibilidad Ventana temporal que debe pasar desde que los datos se ingestan hasta que puedan ser utilizables, desde horas/d\u00edas (mediante procesos batch ) o ser real-time (mediante streaming ) Actualizaciones \u00bfLas fuentes origen se modifican habitualmente? \u00bfPodemos almacenar toda la informaci\u00f3n y guardar un hist\u00f3rico de cambios? \u00bfModificamos la informaci\u00f3n que tenemos? \u00bfmediante updates , o deletes + insert ? Transformaciones \u00bfSon necesarias durante la ingesta? \u00bfAportan latencia al sistema? \u00bfAfecta al rendimiento? \u00bfTiene consecuencias que la informaci\u00f3n sea transformada y no sea la original? Destino de los datos \u00bfSer\u00e1 necesario enviar los datos a m\u00e1s de un destino, por ejemplo, S3 y DynamoDB? \u00bfC\u00f3mo se van a utilizar los datos en el destino? \u00bfc\u00f3mo ser\u00e1n las consultas? \u00bfes necesario particionar los datos? \u00bfser\u00e1n b\u00fasquedas aleatorias o no? \u00bfUtilizaremos Hive / Pig / DynamoDB ? \u00bfQu\u00e9 procesos de transformaci\u00f3n se van a realizar una vez ingestados los datos? \u00bfCual es la frecuencia y actualizaci\u00f3n de los datos origen? Estudio de los datos Calidad de los datos \u00bfson fiables? \u00bfexisten duplicados? Seguridad de los datos. Si tenemos datos sensibles o confidenciales, \u00bflos enmascaramos o decidimos no realizar su ingesta?","title":"Consideraciones"},{"location":"apuntes/ingesta01.html#actividades","text":"Completa el cuestionario disponible en Aules (dentro del apartado Big Data Aplicado ).","title":"Actividades"},{"location":"apuntes/ingesta01.html#referencias","text":"Ingesta, es m\u00e1s que una mudanza de datos \u00bfQu\u00e9 es ETL? Building Big Data Storage Solutions (Data Lakes) for Maximum Flexibility","title":"Referencias"},{"location":"apuntes/ingesta02pentaho.html","text":"Pentaho Data Integration \u00b6 Kettle es un componente de Pentaho Data Integration (PDI - https://www.hitachivantara.com/en-us/products/data-management-analytics/pentaho/download-pentaho.html ) que a su vez contiene a Spoon . Mediante Spoon se pueden realizar procesos ETL de manera visual, de forma muy f\u00e1cil y r\u00e1pida, como por ejemplo: Conexiones a los datos. Transformaciones (filtrado, limpieza, formateado, ... y posterior almacenamiento en diferentes formatos y destinos). Inserci\u00f3n de f\u00f3rmulas. Desarrollo de data warehouses con estructura en estrella (Hechos/Dimensiones) Y todo esto sin necesidad de programar directamente con c\u00f3digo y sin necesidad de instalar o configurar nada para poder empezar a usarla. Por esto, este tipo de herramientas se conocen como herramientas de metadatos , ya que trabajan a nivel de definici\u00f3n diciendo qu\u00e9 hay que hacer, pero no el detalle del c\u00f3mo se hace, \u00e9ste queda oculto, lo cual resulta muy interesante en la mayor\u00eda de los casos. Pentaho - Ejemplo de flujo ETL Se trata de una herramienta open source multiplataforma que tambi\u00e9n tiene su soporte comercial. La versi\u00f3n open source se puede descargar desde https://sourceforge.net/projects/pentaho/ . En nuestro caso, vamos a trabajar con la versi\u00f3n 9.2 que data de agosto de 2021 (para los apuntes he trabajado indistintamente con la versi\u00f3n 9.1 y la 9.2, tanto en Windows como en Ubuntu). Es importante destacar como requisito que necesitamos tener instalado en el sistema la versi\u00f3n 8 de Java. Instalaci\u00f3n en Ubuntu Si trabajamos con Ubuntu, ser\u00e1 necesario instalar Java 8 ejecutando el comando sudo apt install openjdk-8-jdk , y tras ello sudo update-alternatives --config java para elegir la versi\u00f3n 8. A continuaci\u00f3n, para instalar el paquete libwebkitgtk , primero tenemos que a\u00f1adir su repositorio: sudo nano /etc/apt/sources.list Y a\u00f1adimos al final la siguiente entrada: deb http://cz.archive.ubuntu.com/ubuntu bionic main universe Y tras actualizar los repositorios con sudo apt-get update , instalaremos el paquete con el comando: sudo apt-get install libwebkitgtk-1.0.0 Si ha fallado la actualizaci\u00f3n del repositorio por un problema de certificados, una posible soluci\u00f3n es instalar Y PPA Manager , tal como explican aqu\u00ed . Una vez descargado el archivo y tras descomprimirlo, mediante el archivo spoon.bat (o spoon.sh ) lanzaremos la aplicaci\u00f3n. Pantalla de inicio de Pentaho/Spoon Dentro de Spoon Spoon permite dise\u00f1ar mediante un interfaz gr\u00e1ficos las transformaciones y trabajos que se ejecutan con las siguientes herramientas: Pan es un motor de transformaci\u00f3n de datos que facilita la lectura, manipulaci\u00f3n, y escritura de datos hacia y desde varias fuentes de datos. Ejecuta archivos ktr . Kitchen es un programa que ejecuta los trabajos ( jobs ) dise\u00f1ados por Spoon en XML o en una base de datos. Ejecuta archivos kjb . Para esta sesi\u00f3n, hemos planteado varios casos de uso para ir aprendiendo la herramienta mediante su uso. Elementos \u00b6 En PDI hay dos tipos de elementos: Transformations y Jobs . Se definen Transformations para transformar los datos, describiendo flujos de datos para ETL como leer desde una fuente, transformar los datos y cargarlos en un nuevo destino. Se definen Jobs para organizar tareas y transformaciones estableciendo su orden y condiciones de ejecuci\u00f3n (del tipo \u00bfexiste el fichero X en origen? \u00bfexiste la tabla X en mi base de datos?). Tanto las transformaciones como las tareas, cuando se definen, se almacenan como archivos. Los elementos del interfaz son: Interfaz de Spoon Caso de Uso 0 \u00b6 Para familiarizarnos con el entorno, vamos a crear una transformaci\u00f3n muy b\u00e1sica. Tras seleccionar File -> New Transformation , el primer elemento que vamos a utilizar est\u00e1 dentro de la categor\u00eda Input (Entrada). Men\u00fa emergente En concreto seleccionamos la transformaci\u00f3n Get system info (Informaci\u00f3n del sistema), la cual nos permite obtener informaci\u00f3n sobre el sistema. La vamos a utilizar para averiguar la versi\u00f3n de PDI que estamos utilizando. As\u00ed pues, la seleccionamos desde el \u00e1rbol de pasos y lo arrastramos a la zona de trabajo. Si dejamos el rat\u00f3n sobre el elemento, nos aparecer\u00e1 un men\u00fa emergente donde podremos conectar una entrada, editar las propiedades, ver el men\u00fa contextual del paso, conectar una salida e inyectar metadatos. Sobre este paso, vamos a editar la informaci\u00f3n que queremos obtener. Para ello, vamos a crear una propiedad con nombre Versi\u00f3n Pentaho y seleccionaremos del desplegable la opci\u00f3n Kettle Version . A continuaci\u00f3n, en la categor\u00eda Utility seleccionamos el icono Write to Log , y lo arrastramos al \u00e1rea de trabajo. Ahora conectamos la salida de Get system info con Write to log , mediante la 4\u00aa opci\u00f3n del menu emergente, quedando una transformaci\u00f3n tal como se ve en la imagen: Caso de Uso 0 - Versi\u00f3n de PDI Finalmente, s\u00f3lo nos queda ejecutar la transformaci\u00f3n mediante el icono del tri\u00e1ngulo ( Run o F9), y ver el resultado en el panel inferior. Caso de Uso 1 - Filtrando datos \u00b6 En este caso de uso, vamos a leer un archivo CSV y filtrar los datos para quedarnos con un subconjunto de los mismos. Adem\u00e1s, aprenderemos a gestionar los errores y ejecutar la transformaci\u00f3n desde el terminal. Lectura CSV \u00b6 Tras crear la nueva transformaci\u00f3n (CTRL + N), desde Input arrastraremos el paso de CSV input file para seleccionar el archivo samples\\transformations\\files\\Zipssortedbycitystate.csv dentro de nuestra instalaci\u00f3n de Pentaho. Tras seleccionar el archivo, mediante el bot\u00f3n Get Fields cargaremos y comprobaremos que los campos que vamos a leer son correctos (nombre y tipo de los datos). Caso de Uso 1 - Tras pulsar sobre Get Fields Tras ello, mediante el bot\u00f3n Preview comprobaremos que los datos se leen correctamente. Caso de Uso 1 - Resultado de la opci\u00f3n Preview sobre Ciudades Filtrado de datos \u00b6 Una vez le\u00eddo, el siguiente paso es filtrar las filas. Para ello, desde la categor\u00eda de Flow , arrastramos el paso Filter (Filtrar filas), y las conectamos tal como hemos realizado en el caso anterior. Al soltar la flecha, nos mostrar\u00e1 dos opciones: Main output of step : define los pasos con un flujo principal, donde todo funciona bien Error handling of step : define los pasos a seguir en caso de encontrar un error De momento elegimos la primera y configuramos el filtro para solo seleccionar aquellos datos cuyo estado sea NY ( STATE = NY ) Caso de Uso 1 - Configuraci\u00f3n del filtro Para configurar el resultado, seleccionamos el paso del filtro, y bien pulsamos sobre el icono del ojo de la barra de herramientas, o sobre el paso, tras pulsar con el bot\u00f3n derecho, seleccionamos la opci\u00f3n Preview . Caso de Uso 1 - Resultado de hacer Preview sobre Filtro NY Por defecto se precargan 1000 filas. Tras comprobar el resultado, pulsamos sobre Stop para detener el proceso de previsualizaci\u00f3n. Las m\u00e9tricas que aparecen nos informaci\u00f3n del proceso y su rendimiento. Ordenaci\u00f3n \u00b6 El siguiente paso que vamos a realizar es ordenar los datos por su c\u00f3digo Postal Code . Para ello, desde la categor\u00eda de Transform , arrastramos el paso de Sort rows (Ordenar filas), y conectamos la salida del filtrado con la ordenaci\u00f3n eligiendo la salida principal ( main output of step ). Forzando un error Vamos a forzar un error para comprobar c\u00f3mo lo indica Spoon . Si al elegir el nombre del campo, en vez de POSTAL CODE escribimos CP , cuando previsualizamos el resultado, podremos ver como aparece la marca de prohibido en la esquina superior derecha del paso, y si visualizamos el log y las m\u00e9tricas de los pasos, veremos el error: Caso de Uso 1 - Forzando un error Volvemos a editar el paso, corregimos el nombre del campo (escribimos POSTAL CODE ) y comprobamos que ahora s\u00ed que funciona correctamente Caso de Uso 1 - Ordenando Escritura del resultado \u00b6 Una vez realizados todos los pasos, s\u00f3lo nos queda enviar el resultado a un fichero para persistir la transformaci\u00f3n. Para ello, desde la categor\u00eda de Output arrastramos el paso Text file output , y lo conectamos desde la salida del paso de ordenaci\u00f3n. Tras ello, editar este paso para indicar el archivo donde almacenar el resultado. Caso de Uso 1 - Guardando el resultado Tras ello, podremos ejecutar la transformaci\u00f3n (icono del tri\u00e1ngulo, men\u00fa Action -> Run o F9) y comprobar el resultado en el fichero: CITY;STATE;POSTALCODE HOLTSVILLE ;NY;501 FISHERS ISLAND ;NY;6390 NEW YORK ;NY;10001 NEW YORK ;NY;10003 NEW YORK ;NY;10005 NEW YORK ;NY;10007 NEW YORK ;NY;10009 Al comprobar el fichero, vemos que se han quedado espacios en blanco a la derecha del nombre de las ciudades, ya que la columna ten\u00eda un tama\u00f1o configurado. Si volvemos a editar el \u00faltimo paso, en la pesta\u00f1a de Fields (Campos) podemos indicar mediante el bot\u00f3n de Minimal width que reduzca su anchura al m\u00ednimo: Caso de Uso 1 - Anchura m\u00ednima de los campos Y tras volver a ejecutar la transformaci\u00f3n, veremos que ahora s\u00ed que obtenemos los datos que esper\u00e1bamos: CITY;STATE;POSTALCODE HOLTSVILLE;NY;501 FISHERS ISLAND;NY;6390 NEW YORK;NY;10001 NEW YORK;NY;10003 NEW YORK;NY;10005 NEW YORK;NY;10007 NEW YORK;NY;10009 Uso de Pan \u00b6 Mediante la utilidad Pan , podemos ejecutar las transformaciones sin necesidad de arrancar Spoon . Para indicarle el archivo que contiene la transformaci\u00f3n, al comando pan.bat (o pan.sh en el caso de Ubuntu) le pasamos el par\u00e1metro /file=rutaArchivo.ktr . Para comprobar su funcionamiento, vamos a eliminar el fichero generado. A continuaci\u00f3n, ejecutamos pan : pan.bat /file = c:/IABD/caso1filtradoNY.ktr Tras algunos segundos y varias l\u00edneas de debug del arranque de pan, tendremos un mensaje similar al siguiente: 2021/10/24 18:01:42 - Start of run. 2021/10/24 18:01:42 - caso1filtradoNY - Dispatching started for transformation [caso1filtradoNY] 2021/10/24 18:01:42 - Ciudades.0 - Header row skipped in file 'C:\\data-integration\\samples\\transformations\\files\\Zipssortedbycitystate.csv' 2021/10/24 18:01:42 - Ciudades.0 - Finished processing (I=21380, O=0, R=0, W=21379, U=0, E=0) 2021/10/24 18:01:42 - Filtro NY.0 - Finished processing (I=0, O=0, R=21379, W=1146, U=0, E=0) 2021/10/24 18:01:42 - Orden por Codigo Postal.0 - Finished processing (I=0, O=0, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:42 - CiudadesNY.0 - Finished processing (I=0, O=1147, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:43 - Carte - Installing timer to purge stale objects after 1440 minutes. 2021/10/24 18:01:43 - Finished! 2021/10/24 18:01:43 - Start=2021/10/24 18:01:42.424, Stop=2021/10/24 18:01:43.041 2021/10/24 18:01:43 - Processing ended after 0 seconds. 2021/10/24 18:01:43 - caso1filtradoNY - 2021/10/24 18:01:43 - caso1filtradoNY - Step Ciudades.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Filtro NY.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Orden por Codigo Postal.0 ended successfully, processed 1146 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step CiudadesNY.0 ended successfully, processed 1146 lines. ( - lines/s) Y si comprobamos el fichero, veremos que ha vuelto a aparecer. Caso de Uso 2 - Uniendo datos \u00b6 Caso de Uso 2 - Lecturas CSV En este caso, vamos a leer datos de ventas y de productos , y vamos a unirlos de forma similar a un join , para posteriormente, tras agrupar los datos, crear un informe. Merge join \u00b6 Para ello, primero vamos leer de forma separada cada archivo mediante un paso de tipo CSV file input , utilizando el el separador necesarios en cada caso ( ; o , ). As\u00ed pues, tendremos dos pasos, tal como se puede observar en la imagen de la derecha. En el caso del CSV de ventas , al tener registros de diferentes pa\u00edses, deberemos cambiar el tipo del ZIP (el c\u00f3digo postal) a String . A continuaci\u00f3n, para unir los datos, dentro de la categor\u00eda Join / Uni\u00f3n utilizamos el paso Merge join / Uni\u00f3n por clave . En este caso tras arrastrar el paso al \u00e1rea de trabajo, vamos a enlazar desde el merge hacia los dos or\u00edgenes, en un caso como left hand side stream of the join y el otro como right hand side stream of the join : Caso de Uso 2 - Entradas de Merge A continuaci\u00f3n, editamos el Merge y le decimos que el campo de uni\u00f3n es ProductID . Cuando le damos a aceptar recibimos un mensaje de advertencia avis\u00e1ndonos que si los datos no est\u00e1n ordenados podemos obtener resultados incorrectos. Caso de Uso 2 - Aviso Merge As\u00ed pues, vamos a a\u00f1adir previo al merge un paso de ordenaci\u00f3n a cada entrada, ordenando los datos por ProductID de manera ascendente. Para comprobar su funcionamiento, podemos hacer un preview del merge . Agrupando datos \u00b6 Como el resultado final es un informe de ventas con el total de unidades vendidas y la cantidad recaudada agrupada por pa\u00eds y categor\u00eda del producto, necesitamos agrupar los datos. Para ello, dentro de la categor\u00eda Statistics , utilizaremos el paso Group by / Agrupar por . En nuestro caso queremos agrupar por pa\u00eds ( Country ) y categor\u00eda ( Category ), y los datos que vamos a agregar son la suma de unidades ( Units ) y la suma recaudada ( Revenue ). As\u00ed pues, nuestra agrupaci\u00f3n quedar\u00eda as\u00ed: Caso de Uso 2 - Agrupamos por pa\u00eds y categor\u00eda En este caso, sucede lo mismo que antes, que este paso necesita los datos ordenados. As\u00ed pues, vamos a ordenar las salida del merge por pa\u00eds y categor\u00eda. Caso de Uso 2 - Ordenamos antes de agrupar El \u00faltimo paso que nos queda es exportar los datos a un fichero de texto mediante el paso Text file output . Caso de Uso 3 - Cuestionarios Airbnb \u00b6 Para el siguiente caso de uso, vamos a utilizar datos de los cuestionarios de AirBnb que se pueden descargar desde http://tomslee.net/airbnb-data-collection-get-the-data . En concreto, nos vamos a centrar en los datos de Madrid que podemos descargar desde https://s3.amazonaws.com/tomslee-airbnb-data-2/madrid.zip . Uniendo datos \u00b6 Una vez descargados los datos y descomprimidos, vamos a cargar los tres ficheros en el mismo paso, utilizando dentro de Input la opci\u00f3n de Text File Input / Entrada Fichero de Texto : Caso de Uso 3 - Filtrado compuesto Recordad que antes, en la pesta\u00f1a Content / Contenido elegiremos la , como separador de campos y en Fields / Campos , tenemos que obtener los campos a leer: Caso de Uso 3 - Campos Nos vamos a quedar con un subconjunto de las columnas y las vamos a renombrar. Para ello, dentro de la categor\u00eda Transform elegimos el paso Select values / Selecciona/Renombra valores y elegimos y renombramos los siguentes campos: room_id , room_type , neighborhood , bedrooms , overall_satisfaction , accommodates , y price pasar\u00e1n a ser habitacion_id , habitacion_tipo , barrio , dormitorios , puntuacion , huespedes y precio . Caso de Uso 3 - Selecci\u00f3n y nombrado de campos Filtrado compuesto \u00b6 El siguiente paso que vamos a hacer es quedarnos con aquellos cuestionarios con m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes. As\u00ed pues, con el paso Filter Rows / Filtrar filas realizaremos: Caso de Uso 3 - Filtrado compuesto Si el filtrado fuese con condiciones m\u00e1s complejas, en ocasiones es m\u00e1s sencillo utilizar el paso Java filter (de la categor\u00eda Flow ), el cual utilizando la notaci\u00f3n de Java, podemos indicar la condici\u00f3n a cumplir. Por ejemplo, vamos filtrar los de m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes, y que su precio sea inferior a 200$ \u2192 (dormitorios>3 || huespedes>=4) && precio <200 : Caso de Uso 3 - Filtrado Java Para comprobar su funcionamiento, vamos a a\u00f1adir un par de pasos dummy / transformaci\u00f3n simulada (no realizan nada, pero sirven para finalizar tareas). Al ejecutarlo, veremos que nos da un error. Si alg\u00fan dato es nulo, el filtrado Java provocar\u00e1 un error de transformaci\u00f3n. Una posibilidad es que introduzcamos un paso de la categor\u00eda Utility denominado If value is null . Con este paso, podemos indicar el valor a tomar a todos los campos o hacerlo de forma concreta en los campos que queramos. En nuestro caso, vamos indicar que cambie todos los nulos por -1 . Caso de Uso 3 - Cambiando nulos por -1 Debemos tener en cuenta que como ahora podemos tener precios con -1 , para evitar recogerlos en el filtrado Java, deber\u00edamos modificarlo por (dormitorios > 3 || huespedes >=4) && ( precio >= 0 && precio < 200) . Generando JSON \u00b6 Caso de Uso 3 - Configuraci\u00f3n JSON Finalmente queremos almacenar los datos que cumplen el filtro en un fichero JSON. Para ello, sustituimos el dummy del camino exitoso por un paso JSON output . Tras seleccionar los campos que nos interesan, configuraremos: Filename : La ruta y el nombre del archivo Json bloc name : nombre de la propiedad que contendr\u00e1 un objeto o un array de objetos con los datos. Nr rows in a bloc : Cantidad de datos del archivo. Si ponemos 0, coloca todos los datos en el mismo fichero. Si ponemos 1, generar\u00e1 un fichero por cada registro. En la imagen que tenemos a la derecha puedes comprobar los valores introducidos. Resultado final \u00b6 En la siguiente imagen puedes visualizar la transformaci\u00f3n completa: Caso de Uso 3 - Transformaci\u00f3n final La cual, al ejecutarla, genera los siguientes datos: { \"datos\" :[ { \"dormitorios\" : 4.0 , \"huespedes\" : 10 , \"barrio\" : \"Arg\u00fcelles\" , \"precio\" : 133.0 , \"habitacion_tipo\" : \"Entire home\\/apt\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 23021 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Justicia\" , \"precio\" : 147.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 5.0 , \"habitacion_id\" : 24836 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Bellas Vistas\" , \"precio\" : 44.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 34801 }, ... ] } Caso de Uso 4 - Informe fabricantes en S3 \u00b6 A partir del caso 2 que hemos realizado previamente, vamos a generar un informe sobre ventas de los fabricantes, y a persistir el informe en S3. Unir fabricantes \u00b6 As\u00ed pues, vamos a abrir el caso de uso 2, y a partir de ah\u00ed, vamos a fusionar los datos con los de fabricantes para obtener el nombre de \u00e9stos. Para ello, creamos una nueva lectura de CSV, ordenaci\u00f3n y posterior Merge . Cabe destacar que antes de hacer el segundo join , debemos ordenar ambas entradas por la clave de uni\u00f3n ( ManufacturerID ): Caso de Uso 4 - Merge fabricantes Aplicar f\u00f3rmulas \u00b6 Tras unir los datos de las ventas con los fabricantes, para poder preparar el informe mediante group-by, del mismo modo que antes, necesitamos ordenar los datos (en este caso por ManufacturerID ). En el informe queremos mostrar para cada fabricante, adem\u00e1s de su c\u00f3digo y nombre (campos de agrupaci\u00f3n), queremos obtener el total de unidades vendidad y la recaudaci\u00f3n total de dicho fabricante. Para ello creamos una nueva agrupaci\u00f3n. Caso de Uso 4 - Agrupamos por fabricante Adem\u00e1s, queremos que nos muestre un nuevo campo que muestre el precio medio obtenido de dividir la recaudaci\u00f3n obtenida entre el total de unidades. Para ello, dentro de la categor\u00eda Transform , elegimos el paso Calculator . Una vez abierto el di\u00e1logo para editar el paso, si desplegamos la columna Calculation puedes observar todas las posibles operaciones y transformaciones (tanto n\u00famericas, como de campos de texto e incluso fecha) que podemos realizar. En nuestro caso, s\u00f3lo necesitamos la divisi\u00f3n entre A y B : Caso de Uso 4 - F\u00f3rmula entre dos campos Guardar en S3 \u00b6 Para almacenar los datos en S3, primero crearemos un bucket p\u00fablico (en mi caso lo he denominado severo2122pdi ) y para facilitar el trabajo, vamos a crear una pol\u00edtica que permita todas las operaciones: { \"Version\" : \"2012-10-17\" , \"Id\" : \"Policy1635323698048\" , \"Statement\" : [ { \"Sid\" : \"Stmt1635323796449\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : \"arn:aws:s3:::severo2122pdi\" } ] } El siguente paso es configurar las credenciales de acceso en nuestro sistema. Recuerda que lo haremos mediante las variables de entorno ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY y AWS_SESSION_TOKEN ) o almacenando las credenciales en el archivo ~/.aws/credentials . Finalmente, a\u00f1adimos el paso S3 file output indicando: La ruta del archivo, la cual se indica de forma similar a s3n://s3n/bucket/nombreArchivo . En nuestro caso, hemos utilizado el nombre s3n://s3n/severo2122pdi/informeFabricantes . La extensi\u00f3n: nosotros hemos elegido el formato csv Caso de Uso 4 - Salida a S3 Una vez todo unido, tras ejecutarlo podremos acceder a nuestra consola de AWS y comprobar c\u00f3mo ha aparecido el fichero. En resumen, en este caso de uso hemos generado la siguiente transformaci\u00f3n: Caso de Uso 4 - Resultado final Caso de Uso 5 - Jobs \u00b6 En este caso de uso vamos a crear un Job / Trabajo que nos permita coordinar los casos de uso 2 y 4 mediante un solo proceso. Para ello, primero creamos el Job desde File -> New -> Job/Trabajo . Si vamos a la pesta\u00f1a Design podemos observar c\u00f3mo aparecen nuevos elementos con los que dise\u00f1ar los trabajos. Comenzando un Job \u00b6 Caso de Uso 5 - Inicio El primer componente a incorporar ser\u00e1 Start (se encuentra dentro de la categor\u00eda General ) que dar\u00e1 origen a nuestro flujo de datos. El siguiente componente ser\u00e1 File exists (categor\u00eda Conditions ) que buscar\u00e1 en el directorio donde guardamos el resultado del caso de uso 2 con el informe de ventas. A continuaci\u00f3n, a\u00f1adimos un componente Delete Files , para que en el caso de que el fichero exista lo borremos. Adem\u00e1s, a\u00f1adimos un elemento Display msgbox info para avisarnos de que ha borrado el fichero de ventas que exist\u00eda anteriormente. A continuaci\u00f3n, a\u00f1adimos nuestra transformaci\u00f3n del caso de uso 2 a trav\u00e9s del elemento Transformation . Finalmente, unimos tanto la salida del Display msgbox info como la de File exists con nuestra transformaci\u00f3n. Caso de Uso 5 - ejecuci\u00f3n transformaci\u00f3n caso de uso 2 Abortando un Job \u00b6 En este momento, ya tenemos el informe de ventas y queremos generar el informe de fabricantes (caso de uso 4). Antes de comenzar con el caso 4, vamos a comprobar que la tranformaci\u00f3n anterior ha generado el fichero que esper\u00e1bamos mediante el componente File exists . En el caso de que no lo haga, vamos a abortar el job mediante el componente Abort job . Caso de Uso 5 - Detenci\u00f3n de un job Comprobando S3 \u00b6 A continuaci\u00f3n repetimos los pasos que hemos hecho al principio de este job pero en este caso comprobando si existe el fichero en S3, y si lo est\u00e1 operamos igual, lo borramos y volvemos a ejecutar la transformaci\u00f3n. Para comprobar que el fichero existe en S3, con el mismo componente, en la parte superior elegimos S3, y a partir de ah\u00ed ponemos la URL de nuestro bucket donde estar\u00e1n nuestros archivos CSV: Caso de Uso 5 - Comprobando S3 El resultado final deber\u00eda ser similar al siguiente gr\u00e1fico: Caso de Uso 5 - Resultado final Uso de Kitchen \u00b6 Igual que Pan nos permite ejecutar transformaciones desde un terminal, mediante Kitchen podemos ejecutar jobs . As\u00ed pues, si nuestro caso de uso lo hemos almacenado en el archivo caso5Job.kjb , lo ejecutar\u00edamos as\u00ed: kitchen.sh /file = caso5Job.kjb Tanto a Pan como a Kitchen les podemos pasar m\u00e1s par\u00e1metros: level : Indica el nivel del log, utilizando la sintaxis /level:<nivel> , siendo los posibles niveles Nothing , Minimal , Error , Basic , Detailed , Debug , y Rowlevel . param : permite pasar par\u00e1metros al job . Se indica uno por cada par\u00e1metro a pasar, y la sintaxis es /param:\"<nombre>=<valor>\" . Caso de Uso 6 - Interacci\u00f3n con bases de datos \u00b6 En este ejemplo vamos a interactuar con una base de datos relacional. En nuestro caso lo hemos planteado con Amazon RDS (si la quieres realizar en local, funcionar\u00e1 igualmente). Para ello, en RDS creamos una base de datos que vamos a llamar sports (pod\u00e9is seguir los pasos del ejemplo de RDS para crear la base de datos). Los datos est\u00e1n disponibles en http://www.sportsdb.org/sd/samples o los pod\u00e9is descargar directamente desde aqu\u00ed . Conectando con la base de datos \u00b6 Antes de empezar a crear nuestra transformaci\u00f3n, hemos de configurar la conexi\u00f3n a la base de datos reci\u00e9n creada. Antes de nada hemos de instalar el driver JDBC para conectar con MySQL (por defecto, \u00fanicamente est\u00e1 instalado el driver de PostgreSQL ). Para ello, una vez descargado (cuidado que no funciona con la \u00faltima versi\u00f3n del driver JDBC), copiar el jar dentro de la carpeta lib de nuestra instalaci\u00f3n de Pentaho . Finalmente, mediante File -> New -> Database connection , usaremos el asistente para crear la conexi\u00f3n con la base de datos que hemos importado en RDS. Caso de Uso 6 - Conexi\u00f3n con RDS Para cargar los datos tenemos varias opciones (nosotros hemos utilizado el usuario admin / adminadmin ): Desde una herramienta visual como DBeaver . Desde el terminal: mysql -h sports.cm4za4bbxb45.us-east-1.rds.amazonaws.com -u admin -p sports < sportsdb_mysql.sql Creando una instancia EC2, subiendo los datos a la instancia y luego conectarnos desde el terminal (este es el proceso que realiza la carga de datos m\u00e1s eficiente, ya que colocamos la instancia de EC2 en la misma AZ que la de RDS). M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html Consultando datos \u00b6 Vamos a partir de un archivo injuries.txt con formato CSV que contiene informaci\u00f3n sobre las lesiones que tienen los diferentes deportistas. Podemos ver extracto del archivo a continuaci\u00f3n: injuries.txt person_id;injury_type;injury_date 153;elbow;2007-07-09 186;fingers;2007-07-15 198;shoulder;2007-07-15 213;elbow;2007-07-16 As\u00ed pues, tenemos las claves primarias de los deportistas y las lesiones. Por lo tanto, vamos a comenzar nuestra transformaci\u00f3n leyendo dicho archivo mediante un CSV input file . Si comprobamos en la base de datos, la tabla display_names , adem\u00e1s de los nombres de las personas, tiene un campo entity_type con el valor persons . Caso de Uso 6 - Datos de la tabla display_names Para que al buscar los valores en la tabla obtengamos los nombres de las personas, necesitamos a\u00f1adirle dicha constante a nuestro flujo de datos. Para ello, dentro de la categor\u00eda Transform , a\u00f1adimos un paso de tipo Constant/A\u00f1adir Constante y a\u00f1adimos la propiedad type_persons con el valor persons : Caso de Uso 6 - A\u00f1adimos constante persons Para obtener el nombre de y resto de datos, vamos a acceder a la base de datos utilizando el componente Database lookup/B\u00fasqueda en base de datos el cual configuraremos tal como podemos ver en la imagen: Caso de Uso 6 - Lookup del nombre Y si hacemos preview de los datos, veremos que ya tenemos los nombre de los deportistas que han sufrido alguna lesi\u00f3n. Usuario no encontrado Si en nuestro archivo de texto tenemos un person_id que no aparece en la base de datos, los campos que recuperar\u00e1n de la base de datos aparecer\u00e1n como nulos. Si no queremos este comportamiento, hemos de marcar la casilla Do not pass the row if the lookup fails . Insertando datos \u00b6 Otro caso muy com\u00fan al trabajar con bases de datos, es tener que insertar datos. As\u00ed pues, vamos a partir del mismo fichero de texto con las lesiones, y vamos a insertar los datos en la tabla injury_phases . Vamos a suponer que las lesiones que tenemos en el fichero de texto finalizan cuando ejecutamos la transformaci\u00f3n. As\u00ed pues, vamos a tener que a\u00f1adir al flujo de datos la fecha del sistema. Para ello, mediante el paso Get System Info/Informaci\u00f3n de sistema creamos un campo que hemos llamado sysdate con la fecha del sistema. Caso de Uso 6 - Obteniendo la fecha del sistema A continuaci\u00f3n, a este paso, le a\u00f1adimos desde la categor\u00eda Output un paso de tipo Table output/Salida Tabla donde adem\u00e1s de indicar la tabla, mapeamos los campos (con ayuda del bot\u00f3n Get fields ) de nuestro flujo de datos con las columnas de la tabla injury_phases de la base de datos: Caso de Uso 6 - Insertando datos Una vez ejecutada la transformaci\u00f3n, podemos comprobar que se han insertado, por ejemplo, utilizando la siguiente consulta: SELECT * FROM injury_phases WHERE end_date_time > '2021-01-01' Modificando datos \u00b6 A continuaci\u00f3n, vamos a modificar los datos almacenados en la tabla de lesiones a partir de otro archivo injuries2.txt el cual, adem\u00e1s de los datos anteriores, nos indica el lugar de la lesi\u00f3n y un campo de comentarios: injuries2.txt person_id;injury_type;injury_side;injury_date;comment 153;elbow;left;2007-07-09;temporada 2018 198;shoulder;left;2007-07-15;jugando al futbol 2523;knee;;2007-07-31;ligamento cruzado anterior 9999;other-excused;;2021-01-06;jugando a la consola En aquellos casos que la lesi\u00f3n que leamos del fichero ya exista, queremos actualizar la informaci\u00f3n de la base de datos con los datos del fichero. Para ello, creamos una nueva transformaci\u00f3n y leemos el fichero mediante un CSV input file . A continuaci\u00f3n, a\u00f1adimos un paso Update de la categor\u00eda Output (en la imagen pod\u00e9is observar como hemos mapeado los campos de b\u00fasqueda y los de actualizaci\u00f3n) y finalizamos con un paso de Write to Log enlazando los posibles errores que podamos encontrar: Caso de Uso 6 - Modificando datos Una vez ejecutada, si comprob\u00e1is el log, podemos observar como el \u00faltimo registro no se ha procesado correctamente(En el mensaje Finished processing (I=4, O=0, R=4, W=3, U=0, E=1) el valor de E=1 indica la cantidad de errores). Para mejorar el mensaje de error, vamos a hacer bot\u00f3n derecho sobre el paso Actualiza lesiones y seleccionamos la opci\u00f3n de Error Handling.... y a\u00f1adimos los valores a los campos Error description fieldname y Error fields fieldname : Caso de Uso 6 - Configurando mensajes de error Lo que provocar\u00eda que al volver a ejectuar la transformaci\u00f3n, ya podamos obtener informaci\u00f3n del motivo del error: 2021/11/11 12:44:40 - Write to log.0 - 2021/11/11 12:44:40 - Write to log.0 - ------------> Linenr 1------------------------------ 2021/11/11 12:44:40 - Write to log.0 - person_id = 9999 2021/11/11 12:44:40 - Write to log.0 - injury_type = other-excused 2021/11/11 12:44:40 - Write to log.0 - injury_side = null 2021/11/11 12:44:40 - Write to log.0 - injury_date = 2021-01-06 2021/11/11 12:44:40 - Write to log.0 - comment = jugando a la consola 2021/11/11 12:44:40 - Write to log.0 - cantidad_errores = 1 2021/11/11 12:44:40 - Write to log.0 - msj_error = Entry to update with following key could not be found: [9999], [2021-01-06] 2021/11/11 12:44:40 - Write to log.0 - campos_error = person_id, injury_date 2021/11/11 12:44:40 - Write to log.0 - 2021/11/11 12:44:40 - Write to log.0 - ==================== 2021/11/11 12:44:40 - Write to log.0 - Finished processing (I=0, O=0, R=1, W=1, U=0, E=0) 2021/11/11 12:44:40 - Actualiza lesiones.0 - Finished processing (I=4, O=0, R=4, W=3, U=0, E=1) Upsert Si lo que queremos es que en vez de dar un mensaje de error al no encontrar el registro lo inserte, es necesario utilizar el paso Insert/Update . Actividades \u00b6 Realiza los casos pr\u00e1cticos de uso del 1 al 5. En la entrega debes adjuntar tanto el archivo .ktr como capturas de pantallas de los flujos de datos. Antes de realizar cada captura, a\u00f1ade una nota donde aparezca vuestro nombre completo (bot\u00f3n derecho -> New Note ). (opcional) Realiza el caso de uso 6, utilizando Amazon RDS o una base de datos en local (puedes elegir PostgreSQL, MariaDB o MySQL). (opcional) Realiza el tutorial oficial sobre PDI, el cual genera una lista de mailing almacenada en una base de datos a partir de un CSV, sobre el cual realiza un proceso de limpieza, formateo, estandarizando y categorizaci\u00f3n sobre los datos. Referencias \u00b6 Pentaho Data Integration Quick Start Guide de Mar\u00eda Carina Rold\u00e1n. Apuntes de Pentaho , dentro de la asignatura Sistemas Multidimensionales , impartida por Jos\u00e9 Samos Jim\u00e9nez en la Universidad de Granada. Taller sobre integraci\u00f3n de datos (abiertos) / Uso de Pentaho Data Integration , por Jose Norberto Maz\u00f3n (Universidad de Alicante) Tutorial oficial sobre PDI Curso de Spoon en youtube del usuario Learning-BI .","title":"2.- Pentaho DI"},{"location":"apuntes/ingesta02pentaho.html#pentaho-data-integration","text":"Kettle es un componente de Pentaho Data Integration (PDI - https://www.hitachivantara.com/en-us/products/data-management-analytics/pentaho/download-pentaho.html ) que a su vez contiene a Spoon . Mediante Spoon se pueden realizar procesos ETL de manera visual, de forma muy f\u00e1cil y r\u00e1pida, como por ejemplo: Conexiones a los datos. Transformaciones (filtrado, limpieza, formateado, ... y posterior almacenamiento en diferentes formatos y destinos). Inserci\u00f3n de f\u00f3rmulas. Desarrollo de data warehouses con estructura en estrella (Hechos/Dimensiones) Y todo esto sin necesidad de programar directamente con c\u00f3digo y sin necesidad de instalar o configurar nada para poder empezar a usarla. Por esto, este tipo de herramientas se conocen como herramientas de metadatos , ya que trabajan a nivel de definici\u00f3n diciendo qu\u00e9 hay que hacer, pero no el detalle del c\u00f3mo se hace, \u00e9ste queda oculto, lo cual resulta muy interesante en la mayor\u00eda de los casos. Pentaho - Ejemplo de flujo ETL Se trata de una herramienta open source multiplataforma que tambi\u00e9n tiene su soporte comercial. La versi\u00f3n open source se puede descargar desde https://sourceforge.net/projects/pentaho/ . En nuestro caso, vamos a trabajar con la versi\u00f3n 9.2 que data de agosto de 2021 (para los apuntes he trabajado indistintamente con la versi\u00f3n 9.1 y la 9.2, tanto en Windows como en Ubuntu). Es importante destacar como requisito que necesitamos tener instalado en el sistema la versi\u00f3n 8 de Java. Instalaci\u00f3n en Ubuntu Si trabajamos con Ubuntu, ser\u00e1 necesario instalar Java 8 ejecutando el comando sudo apt install openjdk-8-jdk , y tras ello sudo update-alternatives --config java para elegir la versi\u00f3n 8. A continuaci\u00f3n, para instalar el paquete libwebkitgtk , primero tenemos que a\u00f1adir su repositorio: sudo nano /etc/apt/sources.list Y a\u00f1adimos al final la siguiente entrada: deb http://cz.archive.ubuntu.com/ubuntu bionic main universe Y tras actualizar los repositorios con sudo apt-get update , instalaremos el paquete con el comando: sudo apt-get install libwebkitgtk-1.0.0 Si ha fallado la actualizaci\u00f3n del repositorio por un problema de certificados, una posible soluci\u00f3n es instalar Y PPA Manager , tal como explican aqu\u00ed . Una vez descargado el archivo y tras descomprimirlo, mediante el archivo spoon.bat (o spoon.sh ) lanzaremos la aplicaci\u00f3n. Pantalla de inicio de Pentaho/Spoon Dentro de Spoon Spoon permite dise\u00f1ar mediante un interfaz gr\u00e1ficos las transformaciones y trabajos que se ejecutan con las siguientes herramientas: Pan es un motor de transformaci\u00f3n de datos que facilita la lectura, manipulaci\u00f3n, y escritura de datos hacia y desde varias fuentes de datos. Ejecuta archivos ktr . Kitchen es un programa que ejecuta los trabajos ( jobs ) dise\u00f1ados por Spoon en XML o en una base de datos. Ejecuta archivos kjb . Para esta sesi\u00f3n, hemos planteado varios casos de uso para ir aprendiendo la herramienta mediante su uso.","title":"Pentaho Data Integration"},{"location":"apuntes/ingesta02pentaho.html#elementos","text":"En PDI hay dos tipos de elementos: Transformations y Jobs . Se definen Transformations para transformar los datos, describiendo flujos de datos para ETL como leer desde una fuente, transformar los datos y cargarlos en un nuevo destino. Se definen Jobs para organizar tareas y transformaciones estableciendo su orden y condiciones de ejecuci\u00f3n (del tipo \u00bfexiste el fichero X en origen? \u00bfexiste la tabla X en mi base de datos?). Tanto las transformaciones como las tareas, cuando se definen, se almacenan como archivos. Los elementos del interfaz son: Interfaz de Spoon","title":"Elementos"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-0","text":"Para familiarizarnos con el entorno, vamos a crear una transformaci\u00f3n muy b\u00e1sica. Tras seleccionar File -> New Transformation , el primer elemento que vamos a utilizar est\u00e1 dentro de la categor\u00eda Input (Entrada). Men\u00fa emergente En concreto seleccionamos la transformaci\u00f3n Get system info (Informaci\u00f3n del sistema), la cual nos permite obtener informaci\u00f3n sobre el sistema. La vamos a utilizar para averiguar la versi\u00f3n de PDI que estamos utilizando. As\u00ed pues, la seleccionamos desde el \u00e1rbol de pasos y lo arrastramos a la zona de trabajo. Si dejamos el rat\u00f3n sobre el elemento, nos aparecer\u00e1 un men\u00fa emergente donde podremos conectar una entrada, editar las propiedades, ver el men\u00fa contextual del paso, conectar una salida e inyectar metadatos. Sobre este paso, vamos a editar la informaci\u00f3n que queremos obtener. Para ello, vamos a crear una propiedad con nombre Versi\u00f3n Pentaho y seleccionaremos del desplegable la opci\u00f3n Kettle Version . A continuaci\u00f3n, en la categor\u00eda Utility seleccionamos el icono Write to Log , y lo arrastramos al \u00e1rea de trabajo. Ahora conectamos la salida de Get system info con Write to log , mediante la 4\u00aa opci\u00f3n del menu emergente, quedando una transformaci\u00f3n tal como se ve en la imagen: Caso de Uso 0 - Versi\u00f3n de PDI Finalmente, s\u00f3lo nos queda ejecutar la transformaci\u00f3n mediante el icono del tri\u00e1ngulo ( Run o F9), y ver el resultado en el panel inferior.","title":"Caso de Uso 0"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-1-filtrando-datos","text":"En este caso de uso, vamos a leer un archivo CSV y filtrar los datos para quedarnos con un subconjunto de los mismos. Adem\u00e1s, aprenderemos a gestionar los errores y ejecutar la transformaci\u00f3n desde el terminal.","title":"Caso de Uso 1 - Filtrando datos"},{"location":"apuntes/ingesta02pentaho.html#lectura-csv","text":"Tras crear la nueva transformaci\u00f3n (CTRL + N), desde Input arrastraremos el paso de CSV input file para seleccionar el archivo samples\\transformations\\files\\Zipssortedbycitystate.csv dentro de nuestra instalaci\u00f3n de Pentaho. Tras seleccionar el archivo, mediante el bot\u00f3n Get Fields cargaremos y comprobaremos que los campos que vamos a leer son correctos (nombre y tipo de los datos). Caso de Uso 1 - Tras pulsar sobre Get Fields Tras ello, mediante el bot\u00f3n Preview comprobaremos que los datos se leen correctamente. Caso de Uso 1 - Resultado de la opci\u00f3n Preview sobre Ciudades","title":"Lectura CSV"},{"location":"apuntes/ingesta02pentaho.html#filtrado-de-datos","text":"Una vez le\u00eddo, el siguiente paso es filtrar las filas. Para ello, desde la categor\u00eda de Flow , arrastramos el paso Filter (Filtrar filas), y las conectamos tal como hemos realizado en el caso anterior. Al soltar la flecha, nos mostrar\u00e1 dos opciones: Main output of step : define los pasos con un flujo principal, donde todo funciona bien Error handling of step : define los pasos a seguir en caso de encontrar un error De momento elegimos la primera y configuramos el filtro para solo seleccionar aquellos datos cuyo estado sea NY ( STATE = NY ) Caso de Uso 1 - Configuraci\u00f3n del filtro Para configurar el resultado, seleccionamos el paso del filtro, y bien pulsamos sobre el icono del ojo de la barra de herramientas, o sobre el paso, tras pulsar con el bot\u00f3n derecho, seleccionamos la opci\u00f3n Preview . Caso de Uso 1 - Resultado de hacer Preview sobre Filtro NY Por defecto se precargan 1000 filas. Tras comprobar el resultado, pulsamos sobre Stop para detener el proceso de previsualizaci\u00f3n. Las m\u00e9tricas que aparecen nos informaci\u00f3n del proceso y su rendimiento.","title":"Filtrado de datos"},{"location":"apuntes/ingesta02pentaho.html#ordenacion","text":"El siguiente paso que vamos a realizar es ordenar los datos por su c\u00f3digo Postal Code . Para ello, desde la categor\u00eda de Transform , arrastramos el paso de Sort rows (Ordenar filas), y conectamos la salida del filtrado con la ordenaci\u00f3n eligiendo la salida principal ( main output of step ). Forzando un error Vamos a forzar un error para comprobar c\u00f3mo lo indica Spoon . Si al elegir el nombre del campo, en vez de POSTAL CODE escribimos CP , cuando previsualizamos el resultado, podremos ver como aparece la marca de prohibido en la esquina superior derecha del paso, y si visualizamos el log y las m\u00e9tricas de los pasos, veremos el error: Caso de Uso 1 - Forzando un error Volvemos a editar el paso, corregimos el nombre del campo (escribimos POSTAL CODE ) y comprobamos que ahora s\u00ed que funciona correctamente Caso de Uso 1 - Ordenando","title":"Ordenaci\u00f3n"},{"location":"apuntes/ingesta02pentaho.html#escritura-del-resultado","text":"Una vez realizados todos los pasos, s\u00f3lo nos queda enviar el resultado a un fichero para persistir la transformaci\u00f3n. Para ello, desde la categor\u00eda de Output arrastramos el paso Text file output , y lo conectamos desde la salida del paso de ordenaci\u00f3n. Tras ello, editar este paso para indicar el archivo donde almacenar el resultado. Caso de Uso 1 - Guardando el resultado Tras ello, podremos ejecutar la transformaci\u00f3n (icono del tri\u00e1ngulo, men\u00fa Action -> Run o F9) y comprobar el resultado en el fichero: CITY;STATE;POSTALCODE HOLTSVILLE ;NY;501 FISHERS ISLAND ;NY;6390 NEW YORK ;NY;10001 NEW YORK ;NY;10003 NEW YORK ;NY;10005 NEW YORK ;NY;10007 NEW YORK ;NY;10009 Al comprobar el fichero, vemos que se han quedado espacios en blanco a la derecha del nombre de las ciudades, ya que la columna ten\u00eda un tama\u00f1o configurado. Si volvemos a editar el \u00faltimo paso, en la pesta\u00f1a de Fields (Campos) podemos indicar mediante el bot\u00f3n de Minimal width que reduzca su anchura al m\u00ednimo: Caso de Uso 1 - Anchura m\u00ednima de los campos Y tras volver a ejecutar la transformaci\u00f3n, veremos que ahora s\u00ed que obtenemos los datos que esper\u00e1bamos: CITY;STATE;POSTALCODE HOLTSVILLE;NY;501 FISHERS ISLAND;NY;6390 NEW YORK;NY;10001 NEW YORK;NY;10003 NEW YORK;NY;10005 NEW YORK;NY;10007 NEW YORK;NY;10009","title":"Escritura del resultado"},{"location":"apuntes/ingesta02pentaho.html#uso-de-pan","text":"Mediante la utilidad Pan , podemos ejecutar las transformaciones sin necesidad de arrancar Spoon . Para indicarle el archivo que contiene la transformaci\u00f3n, al comando pan.bat (o pan.sh en el caso de Ubuntu) le pasamos el par\u00e1metro /file=rutaArchivo.ktr . Para comprobar su funcionamiento, vamos a eliminar el fichero generado. A continuaci\u00f3n, ejecutamos pan : pan.bat /file = c:/IABD/caso1filtradoNY.ktr Tras algunos segundos y varias l\u00edneas de debug del arranque de pan, tendremos un mensaje similar al siguiente: 2021/10/24 18:01:42 - Start of run. 2021/10/24 18:01:42 - caso1filtradoNY - Dispatching started for transformation [caso1filtradoNY] 2021/10/24 18:01:42 - Ciudades.0 - Header row skipped in file 'C:\\data-integration\\samples\\transformations\\files\\Zipssortedbycitystate.csv' 2021/10/24 18:01:42 - Ciudades.0 - Finished processing (I=21380, O=0, R=0, W=21379, U=0, E=0) 2021/10/24 18:01:42 - Filtro NY.0 - Finished processing (I=0, O=0, R=21379, W=1146, U=0, E=0) 2021/10/24 18:01:42 - Orden por Codigo Postal.0 - Finished processing (I=0, O=0, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:42 - CiudadesNY.0 - Finished processing (I=0, O=1147, R=1146, W=1146, U=0, E=0) 2021/10/24 18:01:43 - Carte - Installing timer to purge stale objects after 1440 minutes. 2021/10/24 18:01:43 - Finished! 2021/10/24 18:01:43 - Start=2021/10/24 18:01:42.424, Stop=2021/10/24 18:01:43.041 2021/10/24 18:01:43 - Processing ended after 0 seconds. 2021/10/24 18:01:43 - caso1filtradoNY - 2021/10/24 18:01:43 - caso1filtradoNY - Step Ciudades.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Filtro NY.0 ended successfully, processed 21379 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step Orden por Codigo Postal.0 ended successfully, processed 1146 lines. ( - lines/s) 2021/10/24 18:01:43 - caso1filtradoNY - Step CiudadesNY.0 ended successfully, processed 1146 lines. ( - lines/s) Y si comprobamos el fichero, veremos que ha vuelto a aparecer.","title":"Uso de Pan"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-2-uniendo-datos","text":"Caso de Uso 2 - Lecturas CSV En este caso, vamos a leer datos de ventas y de productos , y vamos a unirlos de forma similar a un join , para posteriormente, tras agrupar los datos, crear un informe.","title":"Caso de Uso 2 - Uniendo datos"},{"location":"apuntes/ingesta02pentaho.html#merge-join","text":"Para ello, primero vamos leer de forma separada cada archivo mediante un paso de tipo CSV file input , utilizando el el separador necesarios en cada caso ( ; o , ). As\u00ed pues, tendremos dos pasos, tal como se puede observar en la imagen de la derecha. En el caso del CSV de ventas , al tener registros de diferentes pa\u00edses, deberemos cambiar el tipo del ZIP (el c\u00f3digo postal) a String . A continuaci\u00f3n, para unir los datos, dentro de la categor\u00eda Join / Uni\u00f3n utilizamos el paso Merge join / Uni\u00f3n por clave . En este caso tras arrastrar el paso al \u00e1rea de trabajo, vamos a enlazar desde el merge hacia los dos or\u00edgenes, en un caso como left hand side stream of the join y el otro como right hand side stream of the join : Caso de Uso 2 - Entradas de Merge A continuaci\u00f3n, editamos el Merge y le decimos que el campo de uni\u00f3n es ProductID . Cuando le damos a aceptar recibimos un mensaje de advertencia avis\u00e1ndonos que si los datos no est\u00e1n ordenados podemos obtener resultados incorrectos. Caso de Uso 2 - Aviso Merge As\u00ed pues, vamos a a\u00f1adir previo al merge un paso de ordenaci\u00f3n a cada entrada, ordenando los datos por ProductID de manera ascendente. Para comprobar su funcionamiento, podemos hacer un preview del merge .","title":"Merge join"},{"location":"apuntes/ingesta02pentaho.html#agrupando-datos","text":"Como el resultado final es un informe de ventas con el total de unidades vendidas y la cantidad recaudada agrupada por pa\u00eds y categor\u00eda del producto, necesitamos agrupar los datos. Para ello, dentro de la categor\u00eda Statistics , utilizaremos el paso Group by / Agrupar por . En nuestro caso queremos agrupar por pa\u00eds ( Country ) y categor\u00eda ( Category ), y los datos que vamos a agregar son la suma de unidades ( Units ) y la suma recaudada ( Revenue ). As\u00ed pues, nuestra agrupaci\u00f3n quedar\u00eda as\u00ed: Caso de Uso 2 - Agrupamos por pa\u00eds y categor\u00eda En este caso, sucede lo mismo que antes, que este paso necesita los datos ordenados. As\u00ed pues, vamos a ordenar las salida del merge por pa\u00eds y categor\u00eda. Caso de Uso 2 - Ordenamos antes de agrupar El \u00faltimo paso que nos queda es exportar los datos a un fichero de texto mediante el paso Text file output .","title":"Agrupando datos"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-3-cuestionarios-airbnb","text":"Para el siguiente caso de uso, vamos a utilizar datos de los cuestionarios de AirBnb que se pueden descargar desde http://tomslee.net/airbnb-data-collection-get-the-data . En concreto, nos vamos a centrar en los datos de Madrid que podemos descargar desde https://s3.amazonaws.com/tomslee-airbnb-data-2/madrid.zip .","title":"Caso de Uso 3 - Cuestionarios Airbnb"},{"location":"apuntes/ingesta02pentaho.html#uniendo-datos","text":"Una vez descargados los datos y descomprimidos, vamos a cargar los tres ficheros en el mismo paso, utilizando dentro de Input la opci\u00f3n de Text File Input / Entrada Fichero de Texto : Caso de Uso 3 - Filtrado compuesto Recordad que antes, en la pesta\u00f1a Content / Contenido elegiremos la , como separador de campos y en Fields / Campos , tenemos que obtener los campos a leer: Caso de Uso 3 - Campos Nos vamos a quedar con un subconjunto de las columnas y las vamos a renombrar. Para ello, dentro de la categor\u00eda Transform elegimos el paso Select values / Selecciona/Renombra valores y elegimos y renombramos los siguentes campos: room_id , room_type , neighborhood , bedrooms , overall_satisfaction , accommodates , y price pasar\u00e1n a ser habitacion_id , habitacion_tipo , barrio , dormitorios , puntuacion , huespedes y precio . Caso de Uso 3 - Selecci\u00f3n y nombrado de campos","title":"Uniendo datos"},{"location":"apuntes/ingesta02pentaho.html#filtrado-compuesto","text":"El siguiente paso que vamos a hacer es quedarnos con aquellos cuestionarios con m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes. As\u00ed pues, con el paso Filter Rows / Filtrar filas realizaremos: Caso de Uso 3 - Filtrado compuesto Si el filtrado fuese con condiciones m\u00e1s complejas, en ocasiones es m\u00e1s sencillo utilizar el paso Java filter (de la categor\u00eda Flow ), el cual utilizando la notaci\u00f3n de Java, podemos indicar la condici\u00f3n a cumplir. Por ejemplo, vamos filtrar los de m\u00e1s de 3 dormitorios o al menos 4 hu\u00e9spedes, y que su precio sea inferior a 200$ \u2192 (dormitorios>3 || huespedes>=4) && precio <200 : Caso de Uso 3 - Filtrado Java Para comprobar su funcionamiento, vamos a a\u00f1adir un par de pasos dummy / transformaci\u00f3n simulada (no realizan nada, pero sirven para finalizar tareas). Al ejecutarlo, veremos que nos da un error. Si alg\u00fan dato es nulo, el filtrado Java provocar\u00e1 un error de transformaci\u00f3n. Una posibilidad es que introduzcamos un paso de la categor\u00eda Utility denominado If value is null . Con este paso, podemos indicar el valor a tomar a todos los campos o hacerlo de forma concreta en los campos que queramos. En nuestro caso, vamos indicar que cambie todos los nulos por -1 . Caso de Uso 3 - Cambiando nulos por -1 Debemos tener en cuenta que como ahora podemos tener precios con -1 , para evitar recogerlos en el filtrado Java, deber\u00edamos modificarlo por (dormitorios > 3 || huespedes >=4) && ( precio >= 0 && precio < 200) .","title":"Filtrado compuesto"},{"location":"apuntes/ingesta02pentaho.html#generando-json","text":"Caso de Uso 3 - Configuraci\u00f3n JSON Finalmente queremos almacenar los datos que cumplen el filtro en un fichero JSON. Para ello, sustituimos el dummy del camino exitoso por un paso JSON output . Tras seleccionar los campos que nos interesan, configuraremos: Filename : La ruta y el nombre del archivo Json bloc name : nombre de la propiedad que contendr\u00e1 un objeto o un array de objetos con los datos. Nr rows in a bloc : Cantidad de datos del archivo. Si ponemos 0, coloca todos los datos en el mismo fichero. Si ponemos 1, generar\u00e1 un fichero por cada registro. En la imagen que tenemos a la derecha puedes comprobar los valores introducidos.","title":"Generando JSON"},{"location":"apuntes/ingesta02pentaho.html#resultado-final","text":"En la siguiente imagen puedes visualizar la transformaci\u00f3n completa: Caso de Uso 3 - Transformaci\u00f3n final La cual, al ejecutarla, genera los siguientes datos: { \"datos\" :[ { \"dormitorios\" : 4.0 , \"huespedes\" : 10 , \"barrio\" : \"Arg\u00fcelles\" , \"precio\" : 133.0 , \"habitacion_tipo\" : \"Entire home\\/apt\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 23021 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Justicia\" , \"precio\" : 147.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 5.0 , \"habitacion_id\" : 24836 }, { \"dormitorios\" : 1.0 , \"huespedes\" : 4 , \"barrio\" : \"Bellas Vistas\" , \"precio\" : 44.0 , \"habitacion_tipo\" : \"Private room\" , \"puntuacion\" : 4.0 , \"habitacion_id\" : 34801 }, ... ] }","title":"Resultado final"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-4-informe-fabricantes-en-s3","text":"A partir del caso 2 que hemos realizado previamente, vamos a generar un informe sobre ventas de los fabricantes, y a persistir el informe en S3.","title":"Caso de Uso 4 - Informe fabricantes en S3"},{"location":"apuntes/ingesta02pentaho.html#unir-fabricantes","text":"As\u00ed pues, vamos a abrir el caso de uso 2, y a partir de ah\u00ed, vamos a fusionar los datos con los de fabricantes para obtener el nombre de \u00e9stos. Para ello, creamos una nueva lectura de CSV, ordenaci\u00f3n y posterior Merge . Cabe destacar que antes de hacer el segundo join , debemos ordenar ambas entradas por la clave de uni\u00f3n ( ManufacturerID ): Caso de Uso 4 - Merge fabricantes","title":"Unir fabricantes"},{"location":"apuntes/ingesta02pentaho.html#aplicar-formulas","text":"Tras unir los datos de las ventas con los fabricantes, para poder preparar el informe mediante group-by, del mismo modo que antes, necesitamos ordenar los datos (en este caso por ManufacturerID ). En el informe queremos mostrar para cada fabricante, adem\u00e1s de su c\u00f3digo y nombre (campos de agrupaci\u00f3n), queremos obtener el total de unidades vendidad y la recaudaci\u00f3n total de dicho fabricante. Para ello creamos una nueva agrupaci\u00f3n. Caso de Uso 4 - Agrupamos por fabricante Adem\u00e1s, queremos que nos muestre un nuevo campo que muestre el precio medio obtenido de dividir la recaudaci\u00f3n obtenida entre el total de unidades. Para ello, dentro de la categor\u00eda Transform , elegimos el paso Calculator . Una vez abierto el di\u00e1logo para editar el paso, si desplegamos la columna Calculation puedes observar todas las posibles operaciones y transformaciones (tanto n\u00famericas, como de campos de texto e incluso fecha) que podemos realizar. En nuestro caso, s\u00f3lo necesitamos la divisi\u00f3n entre A y B : Caso de Uso 4 - F\u00f3rmula entre dos campos","title":"Aplicar f\u00f3rmulas"},{"location":"apuntes/ingesta02pentaho.html#guardar-en-s3","text":"Para almacenar los datos en S3, primero crearemos un bucket p\u00fablico (en mi caso lo he denominado severo2122pdi ) y para facilitar el trabajo, vamos a crear una pol\u00edtica que permita todas las operaciones: { \"Version\" : \"2012-10-17\" , \"Id\" : \"Policy1635323698048\" , \"Statement\" : [ { \"Sid\" : \"Stmt1635323796449\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:*\" , \"Resource\" : \"arn:aws:s3:::severo2122pdi\" } ] } El siguente paso es configurar las credenciales de acceso en nuestro sistema. Recuerda que lo haremos mediante las variables de entorno ( AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY y AWS_SESSION_TOKEN ) o almacenando las credenciales en el archivo ~/.aws/credentials . Finalmente, a\u00f1adimos el paso S3 file output indicando: La ruta del archivo, la cual se indica de forma similar a s3n://s3n/bucket/nombreArchivo . En nuestro caso, hemos utilizado el nombre s3n://s3n/severo2122pdi/informeFabricantes . La extensi\u00f3n: nosotros hemos elegido el formato csv Caso de Uso 4 - Salida a S3 Una vez todo unido, tras ejecutarlo podremos acceder a nuestra consola de AWS y comprobar c\u00f3mo ha aparecido el fichero. En resumen, en este caso de uso hemos generado la siguiente transformaci\u00f3n: Caso de Uso 4 - Resultado final","title":"Guardar en S3"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-5-jobs","text":"En este caso de uso vamos a crear un Job / Trabajo que nos permita coordinar los casos de uso 2 y 4 mediante un solo proceso. Para ello, primero creamos el Job desde File -> New -> Job/Trabajo . Si vamos a la pesta\u00f1a Design podemos observar c\u00f3mo aparecen nuevos elementos con los que dise\u00f1ar los trabajos.","title":"Caso de Uso 5 - Jobs"},{"location":"apuntes/ingesta02pentaho.html#comenzando-un-job","text":"Caso de Uso 5 - Inicio El primer componente a incorporar ser\u00e1 Start (se encuentra dentro de la categor\u00eda General ) que dar\u00e1 origen a nuestro flujo de datos. El siguiente componente ser\u00e1 File exists (categor\u00eda Conditions ) que buscar\u00e1 en el directorio donde guardamos el resultado del caso de uso 2 con el informe de ventas. A continuaci\u00f3n, a\u00f1adimos un componente Delete Files , para que en el caso de que el fichero exista lo borremos. Adem\u00e1s, a\u00f1adimos un elemento Display msgbox info para avisarnos de que ha borrado el fichero de ventas que exist\u00eda anteriormente. A continuaci\u00f3n, a\u00f1adimos nuestra transformaci\u00f3n del caso de uso 2 a trav\u00e9s del elemento Transformation . Finalmente, unimos tanto la salida del Display msgbox info como la de File exists con nuestra transformaci\u00f3n. Caso de Uso 5 - ejecuci\u00f3n transformaci\u00f3n caso de uso 2","title":"Comenzando un Job"},{"location":"apuntes/ingesta02pentaho.html#abortando-un-job","text":"En este momento, ya tenemos el informe de ventas y queremos generar el informe de fabricantes (caso de uso 4). Antes de comenzar con el caso 4, vamos a comprobar que la tranformaci\u00f3n anterior ha generado el fichero que esper\u00e1bamos mediante el componente File exists . En el caso de que no lo haga, vamos a abortar el job mediante el componente Abort job . Caso de Uso 5 - Detenci\u00f3n de un job","title":"Abortando un Job"},{"location":"apuntes/ingesta02pentaho.html#comprobando-s3","text":"A continuaci\u00f3n repetimos los pasos que hemos hecho al principio de este job pero en este caso comprobando si existe el fichero en S3, y si lo est\u00e1 operamos igual, lo borramos y volvemos a ejecutar la transformaci\u00f3n. Para comprobar que el fichero existe en S3, con el mismo componente, en la parte superior elegimos S3, y a partir de ah\u00ed ponemos la URL de nuestro bucket donde estar\u00e1n nuestros archivos CSV: Caso de Uso 5 - Comprobando S3 El resultado final deber\u00eda ser similar al siguiente gr\u00e1fico: Caso de Uso 5 - Resultado final","title":"Comprobando S3"},{"location":"apuntes/ingesta02pentaho.html#uso-de-kitchen","text":"Igual que Pan nos permite ejecutar transformaciones desde un terminal, mediante Kitchen podemos ejecutar jobs . As\u00ed pues, si nuestro caso de uso lo hemos almacenado en el archivo caso5Job.kjb , lo ejecutar\u00edamos as\u00ed: kitchen.sh /file = caso5Job.kjb Tanto a Pan como a Kitchen les podemos pasar m\u00e1s par\u00e1metros: level : Indica el nivel del log, utilizando la sintaxis /level:<nivel> , siendo los posibles niveles Nothing , Minimal , Error , Basic , Detailed , Debug , y Rowlevel . param : permite pasar par\u00e1metros al job . Se indica uno por cada par\u00e1metro a pasar, y la sintaxis es /param:\"<nombre>=<valor>\" .","title":"Uso de Kitchen"},{"location":"apuntes/ingesta02pentaho.html#caso-de-uso-6-interaccion-con-bases-de-datos","text":"En este ejemplo vamos a interactuar con una base de datos relacional. En nuestro caso lo hemos planteado con Amazon RDS (si la quieres realizar en local, funcionar\u00e1 igualmente). Para ello, en RDS creamos una base de datos que vamos a llamar sports (pod\u00e9is seguir los pasos del ejemplo de RDS para crear la base de datos). Los datos est\u00e1n disponibles en http://www.sportsdb.org/sd/samples o los pod\u00e9is descargar directamente desde aqu\u00ed .","title":"Caso de Uso 6 - Interacci\u00f3n con bases de datos"},{"location":"apuntes/ingesta02pentaho.html#conectando-con-la-base-de-datos","text":"Antes de empezar a crear nuestra transformaci\u00f3n, hemos de configurar la conexi\u00f3n a la base de datos reci\u00e9n creada. Antes de nada hemos de instalar el driver JDBC para conectar con MySQL (por defecto, \u00fanicamente est\u00e1 instalado el driver de PostgreSQL ). Para ello, una vez descargado (cuidado que no funciona con la \u00faltima versi\u00f3n del driver JDBC), copiar el jar dentro de la carpeta lib de nuestra instalaci\u00f3n de Pentaho . Finalmente, mediante File -> New -> Database connection , usaremos el asistente para crear la conexi\u00f3n con la base de datos que hemos importado en RDS. Caso de Uso 6 - Conexi\u00f3n con RDS Para cargar los datos tenemos varias opciones (nosotros hemos utilizado el usuario admin / adminadmin ): Desde una herramienta visual como DBeaver . Desde el terminal: mysql -h sports.cm4za4bbxb45.us-east-1.rds.amazonaws.com -u admin -p sports < sportsdb_mysql.sql Creando una instancia EC2, subiendo los datos a la instancia y luego conectarnos desde el terminal (este es el proceso que realiza la carga de datos m\u00e1s eficiente, ya que colocamos la instancia de EC2 en la misma AZ que la de RDS). M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Importing.NonRDSRepl.html","title":"Conectando con la base de datos"},{"location":"apuntes/ingesta02pentaho.html#consultando-datos","text":"Vamos a partir de un archivo injuries.txt con formato CSV que contiene informaci\u00f3n sobre las lesiones que tienen los diferentes deportistas. Podemos ver extracto del archivo a continuaci\u00f3n: injuries.txt person_id;injury_type;injury_date 153;elbow;2007-07-09 186;fingers;2007-07-15 198;shoulder;2007-07-15 213;elbow;2007-07-16 As\u00ed pues, tenemos las claves primarias de los deportistas y las lesiones. Por lo tanto, vamos a comenzar nuestra transformaci\u00f3n leyendo dicho archivo mediante un CSV input file . Si comprobamos en la base de datos, la tabla display_names , adem\u00e1s de los nombres de las personas, tiene un campo entity_type con el valor persons . Caso de Uso 6 - Datos de la tabla display_names Para que al buscar los valores en la tabla obtengamos los nombres de las personas, necesitamos a\u00f1adirle dicha constante a nuestro flujo de datos. Para ello, dentro de la categor\u00eda Transform , a\u00f1adimos un paso de tipo Constant/A\u00f1adir Constante y a\u00f1adimos la propiedad type_persons con el valor persons : Caso de Uso 6 - A\u00f1adimos constante persons Para obtener el nombre de y resto de datos, vamos a acceder a la base de datos utilizando el componente Database lookup/B\u00fasqueda en base de datos el cual configuraremos tal como podemos ver en la imagen: Caso de Uso 6 - Lookup del nombre Y si hacemos preview de los datos, veremos que ya tenemos los nombre de los deportistas que han sufrido alguna lesi\u00f3n. Usuario no encontrado Si en nuestro archivo de texto tenemos un person_id que no aparece en la base de datos, los campos que recuperar\u00e1n de la base de datos aparecer\u00e1n como nulos. Si no queremos este comportamiento, hemos de marcar la casilla Do not pass the row if the lookup fails .","title":"Consultando datos"},{"location":"apuntes/ingesta02pentaho.html#insertando-datos","text":"Otro caso muy com\u00fan al trabajar con bases de datos, es tener que insertar datos. As\u00ed pues, vamos a partir del mismo fichero de texto con las lesiones, y vamos a insertar los datos en la tabla injury_phases . Vamos a suponer que las lesiones que tenemos en el fichero de texto finalizan cuando ejecutamos la transformaci\u00f3n. As\u00ed pues, vamos a tener que a\u00f1adir al flujo de datos la fecha del sistema. Para ello, mediante el paso Get System Info/Informaci\u00f3n de sistema creamos un campo que hemos llamado sysdate con la fecha del sistema. Caso de Uso 6 - Obteniendo la fecha del sistema A continuaci\u00f3n, a este paso, le a\u00f1adimos desde la categor\u00eda Output un paso de tipo Table output/Salida Tabla donde adem\u00e1s de indicar la tabla, mapeamos los campos (con ayuda del bot\u00f3n Get fields ) de nuestro flujo de datos con las columnas de la tabla injury_phases de la base de datos: Caso de Uso 6 - Insertando datos Una vez ejecutada la transformaci\u00f3n, podemos comprobar que se han insertado, por ejemplo, utilizando la siguiente consulta: SELECT * FROM injury_phases WHERE end_date_time > '2021-01-01'","title":"Insertando datos"},{"location":"apuntes/ingesta02pentaho.html#modificando-datos","text":"A continuaci\u00f3n, vamos a modificar los datos almacenados en la tabla de lesiones a partir de otro archivo injuries2.txt el cual, adem\u00e1s de los datos anteriores, nos indica el lugar de la lesi\u00f3n y un campo de comentarios: injuries2.txt person_id;injury_type;injury_side;injury_date;comment 153;elbow;left;2007-07-09;temporada 2018 198;shoulder;left;2007-07-15;jugando al futbol 2523;knee;;2007-07-31;ligamento cruzado anterior 9999;other-excused;;2021-01-06;jugando a la consola En aquellos casos que la lesi\u00f3n que leamos del fichero ya exista, queremos actualizar la informaci\u00f3n de la base de datos con los datos del fichero. Para ello, creamos una nueva transformaci\u00f3n y leemos el fichero mediante un CSV input file . A continuaci\u00f3n, a\u00f1adimos un paso Update de la categor\u00eda Output (en la imagen pod\u00e9is observar como hemos mapeado los campos de b\u00fasqueda y los de actualizaci\u00f3n) y finalizamos con un paso de Write to Log enlazando los posibles errores que podamos encontrar: Caso de Uso 6 - Modificando datos Una vez ejecutada, si comprob\u00e1is el log, podemos observar como el \u00faltimo registro no se ha procesado correctamente(En el mensaje Finished processing (I=4, O=0, R=4, W=3, U=0, E=1) el valor de E=1 indica la cantidad de errores). Para mejorar el mensaje de error, vamos a hacer bot\u00f3n derecho sobre el paso Actualiza lesiones y seleccionamos la opci\u00f3n de Error Handling.... y a\u00f1adimos los valores a los campos Error description fieldname y Error fields fieldname : Caso de Uso 6 - Configurando mensajes de error Lo que provocar\u00eda que al volver a ejectuar la transformaci\u00f3n, ya podamos obtener informaci\u00f3n del motivo del error: 2021/11/11 12:44:40 - Write to log.0 - 2021/11/11 12:44:40 - Write to log.0 - ------------> Linenr 1------------------------------ 2021/11/11 12:44:40 - Write to log.0 - person_id = 9999 2021/11/11 12:44:40 - Write to log.0 - injury_type = other-excused 2021/11/11 12:44:40 - Write to log.0 - injury_side = null 2021/11/11 12:44:40 - Write to log.0 - injury_date = 2021-01-06 2021/11/11 12:44:40 - Write to log.0 - comment = jugando a la consola 2021/11/11 12:44:40 - Write to log.0 - cantidad_errores = 1 2021/11/11 12:44:40 - Write to log.0 - msj_error = Entry to update with following key could not be found: [9999], [2021-01-06] 2021/11/11 12:44:40 - Write to log.0 - campos_error = person_id, injury_date 2021/11/11 12:44:40 - Write to log.0 - 2021/11/11 12:44:40 - Write to log.0 - ==================== 2021/11/11 12:44:40 - Write to log.0 - Finished processing (I=0, O=0, R=1, W=1, U=0, E=0) 2021/11/11 12:44:40 - Actualiza lesiones.0 - Finished processing (I=4, O=0, R=4, W=3, U=0, E=1) Upsert Si lo que queremos es que en vez de dar un mensaje de error al no encontrar el registro lo inserte, es necesario utilizar el paso Insert/Update .","title":"Modificando datos"},{"location":"apuntes/ingesta02pentaho.html#actividades","text":"Realiza los casos pr\u00e1cticos de uso del 1 al 5. En la entrega debes adjuntar tanto el archivo .ktr como capturas de pantallas de los flujos de datos. Antes de realizar cada captura, a\u00f1ade una nota donde aparezca vuestro nombre completo (bot\u00f3n derecho -> New Note ). (opcional) Realiza el caso de uso 6, utilizando Amazon RDS o una base de datos en local (puedes elegir PostgreSQL, MariaDB o MySQL). (opcional) Realiza el tutorial oficial sobre PDI, el cual genera una lista de mailing almacenada en una base de datos a partir de un CSV, sobre el cual realiza un proceso de limpieza, formateo, estandarizando y categorizaci\u00f3n sobre los datos.","title":"Actividades"},{"location":"apuntes/ingesta02pentaho.html#referencias","text":"Pentaho Data Integration Quick Start Guide de Mar\u00eda Carina Rold\u00e1n. Apuntes de Pentaho , dentro de la asignatura Sistemas Multidimensionales , impartida por Jos\u00e9 Samos Jim\u00e9nez en la Universidad de Granada. Taller sobre integraci\u00f3n de datos (abiertos) / Uso de Pentaho Data Integration , por Jose Norberto Maz\u00f3n (Universidad de Alicante) Tutorial oficial sobre PDI Curso de Spoon en youtube del usuario Learning-BI .","title":"Referencias"},{"location":"apuntes/ingesta03nifi1.html","text":"Nifi \u00b6 Logo de Apache Nifi Nifi es un proyecto de Apache (desarrollado en Java inicialmente por la NSA) que plantea un sistema distribuido dedicado a ingestar y transformar datos de nuestros sistemas mediante el paradigma streaming . Entre sus caracter\u00edsticas principales destacamos: proyecto Open Source flujos de datos escalables. m\u00e1s de 300 conectores. procesadores personalizados. ingesta de datos en streaming. usa el paradigma de programaci\u00f3n basado en flujos. define las aplicaciones como grafos de procesos dirigidos (DAG) a trav\u00e9s de conexiones que env\u00edan mensajes. entrega garantizada sin p\u00e9rdida de datos. Las ventajas de utilizar Nifi son: multiplataforma licencia Open Source facilidad de uso mediante un interfaz gr\u00e1fica y web escalabilidad horizontal mediante un cluster de m\u00e1quinas. evoluci\u00f3n y comunidad pol\u00edtica de usuarios (LDAP) validador de configuraciones linaje y procedencia del dato Sus casos de uso son: transferencias de datos entre sistemas, por ejemplo, de JSON a una base de datos, de un FTP a Hadoop , etc.. preparar y enriquecer los datos enrutar datos en funci\u00f3n de caracter\u00edsticas, con diferentes prioridades conversi\u00f3n de datos entre formatos En cambio, no es apropiado para: procesamiento y operaciones de c\u00f3mputo distribuido operaciones de streaming complejas con joins o agregaciones. procesamiento complejo de eventos. Puesta en marcha \u00b6 Para instalar Nifi, s\u00f3lo hemos de descargar la \u00faltima versi\u00f3n desde https://nifi.apache.org (en nuestro caso hemos descargado la version 1.15) y tras descomprimirla, debemos crear unas credenciales de acceso. Para ello, ejecutaremos el comando ./nifi.sh set-single-user-credentials <username> <password> indicando el usuario y contrase\u00f1a que queramos. Por ejemplo, nosotros hemos creado el usuario nifi / nifinifinifi : ./bin/nifi.sh set-single-user-credentials nifi nifinifinifi A continuaci\u00f3n, ya podemos arrancar Nifi ejecutando el comando ./nifi.sh start (se ejecutar\u00e1 en brackgound ): ./bin/nifi.sh start Si queremos detenerlo ejecutaremos ./bin/nifi.sh stop . Instalaci\u00f3n con AWS \u00b6 Si quieres trabajar con una m\u00e1quina en AWS has de seguir los siguientes pasos: Crear una instancia EC2 (se recomienda elegir el tipo t3.large para tener 8GB RAM), con un grupo de seguridad que permita tanto las conexiones SSH como el puerto 8443. Conectarnos via SSH a la ipPublicaAWS y descargar Nifi: wget https://downloads.apache.org/nifi/1.15.0/nifi-1.15.0-bin.zip unzip nifi-1.15.0-bin.zip Tras meternos dentro de la carpeta reci\u00e9n creada, configurar el archivo conf/nifi.properties para permitir el acceso remoto. Para ello, modificaremos las siguientes propiedades: nifi.web.https.host = 0.0.0.0 nifi.web.proxy.host = <ipPublicaAWS>:8443 Configurar el usuario con las credenciales de acceso ./bin/nifi.sh set-single-user-credentials nifi nifinifinifi Acceder desde un navegador a la direcci\u00f3n https://ipPublicaAWS:8443/nifi Instalaci\u00f3n con Docker \u00b6 Si no queremos instalarlo y hacer uso de un contenedor mediante Docker, ejecutaremos el siguiente comando: docker run --name nifi -p 8443 :8443 -d -e SINGLE_USER_CREDENTIALS_USERNAME = nifi -e SINGLE_USER_CREDENTIALS_PASSWORD = nifinifinifi -e NIFI_JVM_HEAP_MAX = 2g apache/nifi:latest Cluster de Nifi mediante Docker El siguiente art\u00edculo sobre ejecutar Nifi en un cluster utilizando Docker es muy interesante. Acceso \u00b6 Esperaremos un par de minutos tras arrancar Nifi para acceder al entorno de trabajo. Para ello, introduciremos en el navegador la URL https://localhost:8443/nifi y tras aceptar la alerta de seguridad respecto al certificado veremos un interfaz similar a la siguiente imagen: P\u00e1gina de inicio en Nifi Respecto al interfaz de usuario, cabe destacar cuatro zonas: Men\u00fa superior: con los iconos grandes (procesadores, puertos de entrada y salida, etc...) Barra de debajo con iconos: indican el estado de la ejecuci\u00f3n (hilos, procesadores en marcha, detenidos, etc..) Cuadro Navigate para hacer zoom Cuadro Operate con las opciones del flujo de trabajo o del recurso seleccionado Zona de trabajo drag&drop . Componentes \u00b6 Flowfile \u00b6 En castellano se conocen como ficheros de flujo o FF: es b\u00e1sicamente el dato, el cual se persiste en disco tras su creaci\u00f3n. Realmente es un puntero al dato en su almacenamiento local, de esta manera se acelera su rendimiento. El Flowfile a a su vez se compone de dos partes: contenido: el dato en s\u00ed atributos: metadatos en pares de clave/valor Procesador \u00b6 Encargado de ejecutar alguna transformaci\u00f3n o regla sobre los datos o el flujo para generar un nuevo Flowfile . La salida de un procesador es un Flowfile que ser\u00e1 la entrada de otro procesador. As\u00ed pues, para implementar un flujo de datos en NiFi, crearemos una secuencia de procesadores que reproduzcan las acciones y transformaciones que queremos realizar sobre sobre los datos. Todos los procesadores se ejecutan en paralelo (mediante diferentes hilos de ejecuci\u00f3n), abstrayendo la complejidad de la programaci\u00f3n concurrente y adem\u00e1s se pueden ejecutar en varios nodos de forma simult\u00e1nea o bien en el nodo primario de un cl\u00faster. Si bien es posible dise\u00f1ar a mano un procesador, por defecto, NiFi ofrece un amplio cat\u00e1logo con m\u00e1s de 300 procesadores, que cubre ampliamente las operaciones m\u00e1s frecuentes que se van a necesitar en un flujo de datos: a\u00f1adir o modificar los atributos del FF, capturar cambios en una base de datos, cambiar de formato el contenido del FF (JSON, CSV, Avro\u2026), extraer el contenido textual de un fichero, extraer el valor de un campo de un fichero JSON a partir de su ruta, extraer cabeceras de un email, consultar a ElasticSearch, geolocalizar una IP, obtener un fichero de una cola Kafka, escribir en un log, unir el contenido de varios FFs, realizar una petici\u00f3n HTTP, transformar un fichero XML, validar un fichero CSV, enviar mensajes a un web socket, etc. Tipos de procesadores Adem\u00e1s de en la documentaci\u00f3n oficial ( https://nifi.apache.org/docs.html ), puedes consultar informaci\u00f3n extra en https://www.nifi.rocks/apache-nifi-processors/ . A grosso modo , los m\u00e1s utilizados son: Tranformaci\u00f3n de datos: ReplaceText , JoltTransformJSON , CompressContent . Enrutado y mediaci\u00f3n: RouteOnAttribute , RouteOnContent Acceso a base de datos: ExecuteSQL , ConvertJSONToSQL , PutSQL Extracci\u00f3n de atributos: EvaluateJsonPath , ExtractText , UpdateAttribute Interacci\u00f3n con el sistema: ExecuteProcess Ingesti\u00f3n de datos: GetFile , GetFTP , GetHTTP , GetHDFS Env\u00edo de datos: PutEmail , PutFile , PutFTP , PutKafka , PutMongo Divisi\u00f3n y agregaci\u00f3n: SplitText , SplitJson , SplitXml , MergeContent HTTP: GetHTTP , ListenHTTP , PostHTTP AWS: FetchS3Object , PutS3Object , PutSNS , PutSQS Conector \u00b6 Es una cola dirigida (con un origen y un destino que determinan un sentido) que une diferentes procesadores y contiene los FF que todav\u00eda no se han ejecutado, pudiendo definir diferentes prioridades (por ejemplo, FIFO o LIFO seg\u00fan necesitemos). As\u00ed pues, los conectores van a unir la salida de un procesador con la entrada de otro (o un procesador consigo mismo, por ejemplo, para realizar reintentos sobre una operaci\u00f3n). Las conexiones se caracterizan y nombran por el tipo de puerto de salida del procesador del que nacen. En la mayor\u00eda de los casos nos enfrentaremos a conexiones de tipo success , que recogen el FF que devuelve un procesador cuando ha terminado satisfactoriamente su tarea, o de tipo failure , que conducen el FF en los casos en los que la tarea ha fallado. Tambi\u00e9n podemos crear nuestros propios conectores. Adem\u00e1s, existe la posibilidad de configurar algunos aspectos de la conexi\u00f3n, como el n\u00famero de FF que pueden viajar de forma simult\u00e1nea por ella, la prioridad de salida de los FF que hay en la conexi\u00f3n, o el tiempo que los FF deben permanecer a la espera para ser recogidos por el procesador de destino. Componentes de Nifi Caso 1 - Moviendo datos \u00b6 Vamos a hacer un peque\u00f1o ejercicio con Nifi para familiarizarnos con el entorno desarrollando un flujo de datos sencillo que mueva un fichero de un directorio a otro. A continuaci\u00f3n detallamos los pasos a realizar: Seleccionamos un procesador (primer icono grande) y lo arrastramos en nuestra \u00e1rea de trabajo. Nos aparece un dialogo con tres partes diferenciadas: Di\u00e1logo de elecci\u00f3n de procesador A la izquierda una nube de etiquetas para poder filtrar los procesador. Arriba a la derecha tenemos un buscador para buscar procesadores por su nombre La parte central con el listado de procesadores, desde donde lo podemos seleccionar. As\u00ed pues, buscamos el procesador GetFile y lo a\u00f1adimos al flujo. Damos doble click sobre el elemento gr\u00e1fico que representa nuestro procesador, y en la pesta\u00f1a properties indicamos el directorio de entrada de donde tendr\u00e1 que recoger el fichero mediante la propiedad Input Directory . En nuestro caso le pondremos el valor /home/iabd/Documentos/in : Propiedades de GetFile Ahora a\u00f1adimos un nuevo procesador de tipo PutFile , y en las propiedades indicamos el directorio de salida con la propiedad directory a /home/iabd/Documentos/out . Si visualizamos la pesta\u00f1a Settings , y nos centramos en el lado derecho, podemos configurar el comportamiento a seguir si el procesador se ejecuta correctamente ( success ) o falla ( failure ). Como vamos a hacer que este procesador sea el paso final, vamos a configurar que autoterminen marcando ambos casos: Finalizaci\u00f3n de PutFile Terminar las relaciones Si nos olvidamos de autoterminar las relaciones, o tenemos conexiones sin conectar, no podremos iniciar los procesadores implicados. Esto lo tenemos que realizar para todos los procesadores que tengamos en nuestro flujo de datos. Unimos ambos procesadores creando una conexi\u00f3n. Para ello, tras pulsar sobre el icono de la flecha que aparece al dejar el rat\u00f3n sobre el primer procesador y lo arrastramos hasta el segundo. Conexi\u00f3n mediante un conector entre procesadores Antes de arrancar el primer procesador, creamos un peque\u00f1o fichero en el directorio que hemos puesto como entrada: echo \"Hola Severo!\" > hola.txt Arrancamos el procesador mediante el bot\u00f3n derecho y la opci\u00f3n Start , y comprobamos que el fichero ya no est\u00e1 en la carpeta in , y que s\u00ed aparece en la cola ( Queued 1 ). Tambi\u00e9n podemos comprobar como tampoco est\u00e1 en la carpeta out . Finalmente, arrancamos el procesador de Poner Fichero , y veremos como la cola se vac\u00eda y el archivo aparece en la carpeta out . \u00a1Ya hemos creado nuestro primer flujo de datos! Gestionando los errores \u00b6 \u00bfQu\u00e9 sucede si leemos dos veces un archivo con el mismo nombre? Tal como lo hemos definido en nuestro flujo, s\u00f3lo se guardar\u00e1 la primera copia. Si vamos a la pesta\u00f1a Properties del procesador PonerFichero , podemos cambiar este comportamiento en la propiedad Conflict Resolution Strategy a replace , de esta manera, se guardar\u00e1 el \u00faltimo archivo. Propiedades de PutFile - gesti\u00f3n de conflictos Realmente, en vez de decidir si lo ignora o lo sobreescribe, lo ideal es definir un nuevo flujo que dependa del estado de finalizaci\u00f3n del procesador. De esta manera, podremos almacenar todos los archivos que han llegado con el mismo nombre para su posterior estudio. As\u00ed pues, vamos a quitar la autoterminaci\u00f3n que antes hab\u00edamos puesto al procesador de PonerFichero , para que cuando falle, redirija el flujo a un nuevo procesador PutFile que coloque el archivo en una nueva carpeta (en nuestro caso en /home/iabd/Documentos/conflictos ): Flujo failure para los ficheros repetidos Aunque ahora tenemos un mecanismo para almacenar los ficheros que coinciden en nombre, s\u00f3lo nos guardar\u00e1 uno (nos sucede lo mismo que antes, pero ahora s\u00f3lo con los repetidos). Asi pues, necesitamos renombrar los ficheros que vayamos a colocar en la carpeta conflictos para guardar el hist\u00f3rico. Para ello, necesitamos introducir un procesador previo que le cambie el nombre al archivo. Nifi a\u00f1ade la propiedad filename a todos los FF. Esta propiedad la podemos consultar mediante el Nifi Expression Language (Nifi EL) y haciendo uso del procesador UpdateAttribute modificar su valor. As\u00ed pues, vamos a colocar el procesador UpdateAttribute antes de colocar los archivos en la carpeta de conflictos : A\u00f1adimos el procesador UpdateAttribute Hemos decidido a\u00f1adir como prefijo al nombre del archivo la fecha del sistema en formato de milisegundos, de manera que obtendremos archivos similares a 1637151536113-fichero.txt . Para ello, a\u00f1adimos un nuevo atributo que llamaremos filename haciendo clic sobre el icono de + que aparece arriba a la derecha y en su valor utilizaremos la expresi\u00f3n ${now():toNumber()}-${filename} : A\u00f1adimos el procesador UpdateAttribute Caso 2 - Trabajando con atributos \u00b6 Cada vez que se generan FF (representa un registro de datos que viaja por el flujo) estos van a tener asignados ciertos atributos por defecto. Entre estos atributos est\u00e1n el UUID o identificador \u00fanico, su timestamp y el tama\u00f1o del fichero. Como ya hemos visto, mediante el uso de procesadores podremos modificar estos o a\u00f1adir nuevos atributos. Generando contenido \u00b6 Vamos a ver c\u00f3mo hacerlo realizando los siguientes pasos: Vamos a a\u00f1adir un procesador del tipo GenerateFlowFile (este procesador crea FF con datos aleatorios o contenidos personalizados, lo cual es muy \u00fatil para testear y depurar nuestros flujos de datos). En las opciones del procesador vamos a la pesta\u00f1a de propiedades y completamos los campos: Flow Size: 10 bytes Batch Size: 1 para que nos genere un FF por cada ejecuci\u00f3n Data Format: Text Unique Flowfiles: true e indicamos que los FF van a ser \u00fanicos. Configuraci\u00f3n del procesador GenerateFlowFile A continuaci\u00f3n, en la configuraci\u00f3n de planificaci\u00f3n ( Scheduling ) de este procesador vamos a indicar que se ejecute cada 3 segundos (en el campo Run Schedule le ponemos como valor 3s ). Una vez tenemos listo el generador, vamos a a\u00f1adir el procesador ReplaceText con el que cambiaremos el texto. Tras ello, conectamos ambos procesadores. Conexi\u00f3n con ReplaceText Si nos fijamos, a la izquierda del nombre del procesador, aparece un icono de aviso, el cual nos indica que necesitamos configurar el nuevo procesador, adem\u00e1s de indicarnos que ambas relaciones no est\u00e1n conectadas o que faltan por autocompletar. Avisos que aparecen Para ello, configuramos la estrategia de reemplazo para que lo haga siempre (en el campo Replacement Value seleccionamos Always Replace ), y al hacer esto el campo Search Value se invalida. Adem\u00e1s, en el Replacement Value vamos a indicar simplemente prueba . Finalmente, marcamos para que autotermine la conexi\u00f3n failure . A\u00f1adimos un procesador de tipo LogAttribute para mostrar en el log los atributos del FF, y conectamos el procesador anterior ( ReplaceText ) a \u00e9ste mediante la relaci\u00f3n success . Log con los atributos Arrancamos el primer procesador y visualizamos la cola para comprobar qu\u00e9 ha generado. Para ello, sobre la cola elegimos la opci\u00f3n list queue para ver su contenido, y tras elegir uno, sobre el icono del ojo, visualizamos su contenido y comprobado que ha generado datos aleatorios: Acceso y visualizaci\u00f3n de la cola Si ejecutamos el siguiente procesador, vemos que saca el FF de la cola anterior y aparecer\u00e1 en la siguiente. Si comprobamos su valor, veremos que ha cambiado el valor original por prueba . Resultado de visualizar la cola tras ReplaceText Si accedemos al log de la aplicaci\u00f3n (archivo nifi-app.log dentro de la carpeta logs ) veremos mensajes similares a: 2021-11-17 19:12:37,446 INFO [Timer-Driven Process Thread-2] o.a.n.processors.standard.LogAttribute LogAttribute [id=2f300ff5-017d-1000-9020-2744e67e8d04] logging for flow file StandardFlowFileRecord [uuid=f4181825-f996-40c0-9e3c-a78326837d60,claim=StandardContentClaim [resourceClaim=StandardResourceClaim [id=1637174667410-1, container=default, section=1], offset=7953, length=7], offset=0,name=f4181825-f996-40c0-9e3c-a78326837d60,size=7] -- ------------------------------------------------ Standard FlowFile Attributes Key: 'entryDate' Value: 'Wed Nov 17 19:12:37 UTC 2021' Key: 'lineageStartDate' Value: 'Wed Nov 17 19:12:37 UTC 2021' Key: 'fileSize' Value: '7' FlowFile Attribute Map Content Key: 'filename' Value: 'f4181825-f996-40c0-9e3c-a78326837d60' Key: 'path' Value: './' Key: 'uuid' Value: 'f4181825-f996-40c0-9e3c-a78326837d60' -------------------------------------------------- A\u00f1adiendo un atributo \u00b6 Ahora vamos a extraer el contenido del FF a un atributo mediante el procesador ExtractText . En las propiedades, creamos una nueva propiedad (bot\u00f3n + de la esquina superior derecha) que llamaremos contenido , y en cuyo valor vamos a poner la expresi\u00f3n .* que indica que queremos que coincida con todo. A\u00f1adimos la propiedad contenido a ExtractText Una vez creado, vamos a colocar este procesador entre los dos anteriores (para el segundo con el caso matched , que es cuando ha coincidido con la expresi\u00f3n regular). En la conexi\u00f3n unmatched la marcamos para que autotermine, y comprobamos que no tenemos ning\u00fan advertencia en ning\u00fan procesador. Flujo completo del caso 2 Finalmente, ejecutamos todos los procesadores y comprobamos como en el log aparece el nuevo atributo creado. Tambi\u00e9n podemos acceder a la cola, y en la parte izquierda de cada flujo, en el icono de la i , pulsar y comprobar la pesta\u00f1a Atributes . Comprobaci\u00f3n de los atributos de un FF Linaje de los datos \u00b6 Para comprobar el dato final, es muy \u00fatil utilizar la opci\u00f3n de Data provenance , la cual nos ofrece un linaje de los datos. Linaje de los datos El linaje de los datos describe el origen, movimientos, caracter\u00edsticas y calidad de los datos, aportando visibilidad de punto a punto para incrementar la calidad de los datos. Se puede considerar como el historial de los datos, facilitando la documentaci\u00f3n y gobernanza de los datos. Para ello, sobre el procesador final, con el bot\u00f3n derecho, elegimos la opci\u00f3n View data provenance . Si elegimos uno de los flujos, a la derecha de cada flujo, podemos pulsar sobre el primer icono podremos ver un grafo y un slider que modifica el grafo respecto al instante temporal (en cada uno de los pasos, podemos hacer doble clik y ver la informaci\u00f3n y el contenido del FF en dicho momento exacto): Linaje de los datos Caso 3 - Filtrado de datos \u00b6 En este caso, vamos a coger los datos de ventas que ya utilizamos en la sesi\u00f3n de Pentaho , el cual tiene la siguiente estructura: pdi_sales_small.csv ProductID;Date;Zip;Units;Revenue;Country 725;1/15/1999;41540 ;1;115.5;Germany 787;6/6/2002;41540 ;1;314.9;Germany 788;6/6/2002;41540 ;1;314.9;Germany Utilizando Nifi, vamos a crear un nuevo fichero CSV que contenga \u00fanicamente los datos de Francia que han realizado m\u00e1s de una venta. Para ello, tendremos que leer el fichero haciendo uso del procesador GetFile , separar cada fila en un FF mediante SplitRecord , filtrar los datos usando el procesador QueryRecord y finalmente los almacenaremos en disco gracias al procesador PutFile . Lectura y divisi\u00f3n \u00b6 As\u00ed pues, comenzamos leyendo el fichero con el procesador GetFile . En este caso vamos a dejar la opci\u00f3n keep source file a true para que no lo elimine. Mediante el procesador SplitRecord , vamos a separar cada fila del CSV a un FF. Para ello, primero hemos de crear un RecordReader y un RecordWriter para que sepa interactuar con el CSV ( Nifi ya tiene varios implementados que podemos utilizar). As\u00ed pues: En el Record Reader , seleccionamos Create new service , y elegimos CVSReader . A su vez, en el Record Writer elegimos CVSRecordSetWriter . Para configurar estos servicios, pulsaremos sobre la flecha, y veremos la pantalla de configuraci\u00f3n. Para cada uno de ellos, tendremos otros tres iconos: la rueda para configurar, el rayo para activar/desactivar el servicio y la papelera para eliminarlo. As\u00ed, pues, tras comprobar los valores de CVSReader y CSVSWriter (indicamos el ; como separador de campos tanto para la lectura como la escritura de CSV en el campo value separator y marcamos como true que el archivo contiene la primera fila con encabezados ( treat first line as header*)), pulsamos sobre el rayo para activar ambos servicios. Configuraci\u00f3n y activaci\u00f3n de Split Record Finalmente, en el campo Records per Split le indicamos 1 para que coloque cada fila en un FF. Filtrado de FF \u00b6 En este paso, mediante el procesador QueryRecord vamos a ejecutar una consulta SQL contra el FF. El resultado del nuevo FF ser\u00e1 el resultado de la consulta. En nuestro caso, como hemos comentado antes, vamos a quedarnos con las ventas de m\u00e1s de una unidad realizadas en Francia. Igual que antes, configuramos los mismos Record Reader y Record Writer . Adem\u00e1s, hemos de poner la propiedad Include Zero Record FlowFiles a false para que no vuelva a enrutar los FF que no cumplan la consulta. Finalmente, a\u00f1adimos una nueva propiedad para indicar la consulta. En nuestro caso la hemos denominado FranciaMayor1 y en el contenido ponemos la consulta: select * from Flowfile where Country = 'France' and Units > 1 campos Tambi\u00e9n podr\u00edamos haber filtrado los campos para recuperar menos contenido con una consulta similar a select ProductID, Date from FlowFile .... . Con este procesador podemos filtrar, hacer agrupaciones, c\u00e1lculos, del mismo modo que lo hacemos con SQL. Finalmente, igual que hicimos en el caso 1, vamos a cambiarle el nombre a cada FF para generar un archivo por cada resultado mediante UpdateAttribute y persistimos los datos con PutFile . El resultado del flujo de datos ser\u00e1 similar a: Flujo completo del caso 3 Caso 4 - Fusionar datos \u00b6 En esta caso vamos a realizar los siguientes pasos: Escuchar datos que recibimos de un servidor web. Reconocer si el mensaje contiene la cadena error . Fusionar los mensajes en varios ficheros, dependiendo de si contienen un error. Almacenar el fichero resultante en MongoDB . Recibiendo datos via HTTP \u00b6 Vamos a utilizar el procesador ListenHTTP para escuchar peticiones HTTP. Para ello, lo a\u00f1adimos a nuestro flujo de trabajo y configuramos: Listening port (puerto de escucha): 8081 Base Path ( endpoint de la petici\u00f3n): iabd A continuaci\u00f3n, para distinguir entre los diferentes datos de entrada, utilizaremos el procesador RouteOnContent , con el objetivo de separar en dos flujos de datos, los que contienen la cadena error y los que no. Para ello, tras a\u00f1adir el procesador, le conectamos al flujo success que viene de ListenHTTP , y configuramos: Cambiamos el Match Requirement (requisito de coincidencia) a: content must contain match (no tienen que coincidir exactamente, sino contener el valor). Y a\u00f1adimos la propiedad textoError con el valor ERROR . Propiedades de RouteOnContent Para poder probarlo, arrancamos el primer procesador, y desde un terminal, hacemos una petici\u00f3n a: curl --data \"texto de prueba\" http://localhost:8081/iabd Si comprobamos la cola, podremos ver como se ha creado un FF cuyo contenido es texto de prueba . Fusionando contenido \u00b6 Si nos fijamos en las propiedades del procesador RouteOnContent , tenemos dos flujos de salida textoError (para los mensajes que contienen el texto ERROR ) y unmatched (para los que no). Vamos a a\u00f1adir el procesador MergeContent , el cual recibe un conjunto de FF y los fusiona en uno a partir de la estrategia de fusi\u00f3n que defina el usuario. Las diferentes opciones incluye agrupando ciertos atributos de forma similar a como se realiza la fase reduce de un algoritmo MapReduce . As\u00ed pues, vamos a conectar las relaciones textoError y unmatched con MergeContent : Conexi\u00f3n con MergeContent Tras conectar los procesadores, vamos a configurar el procesador MergeContent : En la pesta\u00f1a de planificaci\u00f3n, vamos a poner que el procesador se ejecuta cada 30 segundos para poder agrupar varios FF. En las propiedades: Merge Strategy : Bin-Packing Algorithm (se fusionan en grupos). Correlation Attribute Name : RouteOnContent.Route (crea diferentes grupos para textoError y unmatched ) Merge Format : Binary Concatenation (concatena los contenidos de los FF en un \u00fanico FF, otra opci\u00f3n ser\u00eda comprimirlos en un zip) Maximum Number of Entries : 1000 (cantidad m\u00e1xima de elementos por grupo, por si tuvi\u00e9ramos muchas peticiones HTTP) Maximum Bin Age : 300s (fuerza que el fichero fusionado salga como muy tarde a los 300s) Delimiter Strategy : Text (para concatenar los fichero utilizando una nueva l\u00ednea como car\u00e1cter delimitador) y Demarcator : al abrir el campo, pulsar Shift/May\u00fas + Intro para poner el car\u00e1cter del salto de l\u00ednea. Para poder probar como se van creando y agrupando los mensajes, podemos ejecutar los siguientes comandos: curl --data \"texto de prueba\" http://localhost:8081/iabd curl --data \"este s\u00ed que tiene ERROR\" http://localhost:8081/iabd curl --data \"vaya ERROR m\u00e1s tonto\" http://localhost:8081/iabd curl --data \"nifi mola mucho\" http://localhost:8081/iabd Por ejemplo, si abrimos uno de los flujos podemos ver c\u00f3mo se han agrupado varias peticiones en un \u00fanico FF: Resultado de MergeContent Guardando el resultado a MongoDB \u00b6 Para almacenar el resultado de las fusiones anteriores, vamos a guardar los resultados en una colecci\u00f3n de MongoDB. Suponemos que ya tenemos instalado MongoDB en nuestro sistema. Si no, podemos lanzarlo mediante Docker : MongoDB + Nifi via Docker Si queremos utilizarlo mediante Docker, necesitamos que MongoDB y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml (si tuvieras alguna problema con la imagen de MongoDB y tu procesador, prueba a cambiar la l\u00ednea 15 mongo:latest por mongo:4.4 ): docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi NIFI_JVM_HEAP_MAX : 2g links : - mongodb mongodb : ports : - \"27017:27017\" image : mongo:latest Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifimongodb up -d Para poder meter los mensajes en MongoDB , vamos a preparar el contenido para que est\u00e9 en formato JSON. Adem\u00e1s del contenido, vamos a crear un atributo con el nombre del enrutador utilizado para posteriormente poder filtrar los mensajes de error. Para poder crear el formato JSON, utilizaremos el procesador AttributesToJSON . As\u00ed pues, previamente necesitamos pasar los mensajes desde el contenido de los FF a los atributos (para ello, igual que en el caso anterior, utilizaremos el procesador ExtracText ). A su vez, tambi\u00e9n crearemos un nuevo atributo con el nombre del enrutador mediante el procesador UpdateAttribute . El resultado final ser\u00e1 similar al siguiente flujo: Resultado completo del caso 4 Utilizamos el procesador ExtracText para pasar el contenido a los atributos. Dentro de las propiedades configuraremos: Enable DOTALL mode : true , para que incluya los saltos de l\u00ednea como contenido. A\u00f1adimos una nueva propiedad contenido , y como expresi\u00f3n regular introducimos (.*) . Una vez creado, conectamos MergeContent con ExtractText mediante la conexi\u00f3n merged , y el resto de conexiones las marcamos para que autoterminen. A\u00f1adimos el procesador UpdateAttribute , y dentro de las propiedades, a\u00f1adirmos una nueva propiedad que vamos a llamar estado cuyo valor ser\u00e1 ${RouteOnContent.Route} , es decir, le ponemos el mismo que contenga el atributo RouteOnContent.Route . Creando el atributo estado Una vez creado, conectamos ExtractText con UpdateAttribute mediante la conexi\u00f3n matched , y el resto de conexiones las marcamos para que autoterminen. A continuaci\u00f3n, vamos a utilizar el procesador AttributesToJSON para pasar los atributos contenido y estado como contenido de un FF en formato JSON. Para ello, configuramos las propiedades: Attribute List : contenido,estado Destination : flowfile-content Creando el atributo estado Una vez creado, conectamos UpdateAttribute con AttributesToJSON mediante la conexi\u00f3n success , y el resto de conexiones las marcamos para que autoterminen. Si ejecutamos los procesadores anteriores y comprobamos la salida, veremos como se est\u00e1n creando FF cuyo contenido es la petici\u00f3n introducida m\u00e1s el estado: Mensaje JSON creado Finalmente, a\u00f1adimos el procesador PutMongo para introducir el contenido JSON. Las propiedades que hay que configurar son: Mongo URI: mongodb://localhost Mongo Database Name: iabd Mongo Collection Name: caso4 En nuestro caso, hemos autoterminado la conexi\u00f3n success y reconectado con el mismo procesador la conexi\u00f3n failure . Si arrancamos el flujo de datos completo, y tras realizar las mismas peticiones de antes: curl --data \"texto de prueba\" http://localhost:8081/iabd curl --data \"este s\u00ed que tiene ERROR\" http://localhost:8081/iabd curl --data \"vaya ERROR m\u00e1s tonto\" http://localhost:8081/iabd curl --data \"nifi mola mucho\" http://localhost:8081/iabd S\u00f3lo nos queda entrar a MongoDB y comprobar que nos aparecen los datos: > use iabd swi t ched t o db iabd > db.caso 4. f i n d() { \"_id\" : Objec t Id( \"6197cca29c63ec4e825b8232\" ) , \"contenido\" : \"este s\u00ed que tiene ERROR\\nvaya ERROR m\u00e1s tonto\" , \"estado\" : \"textoError\" } { \"_id\" : Objec t Id( \"6197cca29c63ec4e825b8233\" ) , \"contenido\" : \"texto de prueba\\nnifi mola mucho\" , \"estado\" : \"unmatched\" } Actividades \u00b6 Realiza los casos de uso del 1 al 3. En la entrega debes adjuntar una captura de pantalla donde se vea el flujo de datos completo con una nota con tu nombre, y adjuntar la definici\u00f3n de cada flujo (sobre el \u00e1rea de trabajo, con el bot\u00f3n derecho, Download flow definition ). (opcional) Realiza el caso de uso 4. Referencias \u00b6 Getting started with Apache Nifi Apache Nifi User Guide Apache Nifi in Depth Apache Nifi en TutorialsPoint","title":"3.- Nifi I"},{"location":"apuntes/ingesta03nifi1.html#nifi","text":"Logo de Apache Nifi Nifi es un proyecto de Apache (desarrollado en Java inicialmente por la NSA) que plantea un sistema distribuido dedicado a ingestar y transformar datos de nuestros sistemas mediante el paradigma streaming . Entre sus caracter\u00edsticas principales destacamos: proyecto Open Source flujos de datos escalables. m\u00e1s de 300 conectores. procesadores personalizados. ingesta de datos en streaming. usa el paradigma de programaci\u00f3n basado en flujos. define las aplicaciones como grafos de procesos dirigidos (DAG) a trav\u00e9s de conexiones que env\u00edan mensajes. entrega garantizada sin p\u00e9rdida de datos. Las ventajas de utilizar Nifi son: multiplataforma licencia Open Source facilidad de uso mediante un interfaz gr\u00e1fica y web escalabilidad horizontal mediante un cluster de m\u00e1quinas. evoluci\u00f3n y comunidad pol\u00edtica de usuarios (LDAP) validador de configuraciones linaje y procedencia del dato Sus casos de uso son: transferencias de datos entre sistemas, por ejemplo, de JSON a una base de datos, de un FTP a Hadoop , etc.. preparar y enriquecer los datos enrutar datos en funci\u00f3n de caracter\u00edsticas, con diferentes prioridades conversi\u00f3n de datos entre formatos En cambio, no es apropiado para: procesamiento y operaciones de c\u00f3mputo distribuido operaciones de streaming complejas con joins o agregaciones. procesamiento complejo de eventos.","title":"Nifi"},{"location":"apuntes/ingesta03nifi1.html#puesta-en-marcha","text":"Para instalar Nifi, s\u00f3lo hemos de descargar la \u00faltima versi\u00f3n desde https://nifi.apache.org (en nuestro caso hemos descargado la version 1.15) y tras descomprimirla, debemos crear unas credenciales de acceso. Para ello, ejecutaremos el comando ./nifi.sh set-single-user-credentials <username> <password> indicando el usuario y contrase\u00f1a que queramos. Por ejemplo, nosotros hemos creado el usuario nifi / nifinifinifi : ./bin/nifi.sh set-single-user-credentials nifi nifinifinifi A continuaci\u00f3n, ya podemos arrancar Nifi ejecutando el comando ./nifi.sh start (se ejecutar\u00e1 en brackgound ): ./bin/nifi.sh start Si queremos detenerlo ejecutaremos ./bin/nifi.sh stop .","title":"Puesta en marcha"},{"location":"apuntes/ingesta03nifi1.html#instalacion-con-aws","text":"Si quieres trabajar con una m\u00e1quina en AWS has de seguir los siguientes pasos: Crear una instancia EC2 (se recomienda elegir el tipo t3.large para tener 8GB RAM), con un grupo de seguridad que permita tanto las conexiones SSH como el puerto 8443. Conectarnos via SSH a la ipPublicaAWS y descargar Nifi: wget https://downloads.apache.org/nifi/1.15.0/nifi-1.15.0-bin.zip unzip nifi-1.15.0-bin.zip Tras meternos dentro de la carpeta reci\u00e9n creada, configurar el archivo conf/nifi.properties para permitir el acceso remoto. Para ello, modificaremos las siguientes propiedades: nifi.web.https.host = 0.0.0.0 nifi.web.proxy.host = <ipPublicaAWS>:8443 Configurar el usuario con las credenciales de acceso ./bin/nifi.sh set-single-user-credentials nifi nifinifinifi Acceder desde un navegador a la direcci\u00f3n https://ipPublicaAWS:8443/nifi","title":"Instalaci\u00f3n con AWS"},{"location":"apuntes/ingesta03nifi1.html#instalacion-con-docker","text":"Si no queremos instalarlo y hacer uso de un contenedor mediante Docker, ejecutaremos el siguiente comando: docker run --name nifi -p 8443 :8443 -d -e SINGLE_USER_CREDENTIALS_USERNAME = nifi -e SINGLE_USER_CREDENTIALS_PASSWORD = nifinifinifi -e NIFI_JVM_HEAP_MAX = 2g apache/nifi:latest Cluster de Nifi mediante Docker El siguiente art\u00edculo sobre ejecutar Nifi en un cluster utilizando Docker es muy interesante.","title":"Instalaci\u00f3n con Docker"},{"location":"apuntes/ingesta03nifi1.html#acceso","text":"Esperaremos un par de minutos tras arrancar Nifi para acceder al entorno de trabajo. Para ello, introduciremos en el navegador la URL https://localhost:8443/nifi y tras aceptar la alerta de seguridad respecto al certificado veremos un interfaz similar a la siguiente imagen: P\u00e1gina de inicio en Nifi Respecto al interfaz de usuario, cabe destacar cuatro zonas: Men\u00fa superior: con los iconos grandes (procesadores, puertos de entrada y salida, etc...) Barra de debajo con iconos: indican el estado de la ejecuci\u00f3n (hilos, procesadores en marcha, detenidos, etc..) Cuadro Navigate para hacer zoom Cuadro Operate con las opciones del flujo de trabajo o del recurso seleccionado Zona de trabajo drag&drop .","title":"Acceso"},{"location":"apuntes/ingesta03nifi1.html#componentes","text":"","title":"Componentes"},{"location":"apuntes/ingesta03nifi1.html#flowfile","text":"En castellano se conocen como ficheros de flujo o FF: es b\u00e1sicamente el dato, el cual se persiste en disco tras su creaci\u00f3n. Realmente es un puntero al dato en su almacenamiento local, de esta manera se acelera su rendimiento. El Flowfile a a su vez se compone de dos partes: contenido: el dato en s\u00ed atributos: metadatos en pares de clave/valor","title":"Flowfile"},{"location":"apuntes/ingesta03nifi1.html#procesador","text":"Encargado de ejecutar alguna transformaci\u00f3n o regla sobre los datos o el flujo para generar un nuevo Flowfile . La salida de un procesador es un Flowfile que ser\u00e1 la entrada de otro procesador. As\u00ed pues, para implementar un flujo de datos en NiFi, crearemos una secuencia de procesadores que reproduzcan las acciones y transformaciones que queremos realizar sobre sobre los datos. Todos los procesadores se ejecutan en paralelo (mediante diferentes hilos de ejecuci\u00f3n), abstrayendo la complejidad de la programaci\u00f3n concurrente y adem\u00e1s se pueden ejecutar en varios nodos de forma simult\u00e1nea o bien en el nodo primario de un cl\u00faster. Si bien es posible dise\u00f1ar a mano un procesador, por defecto, NiFi ofrece un amplio cat\u00e1logo con m\u00e1s de 300 procesadores, que cubre ampliamente las operaciones m\u00e1s frecuentes que se van a necesitar en un flujo de datos: a\u00f1adir o modificar los atributos del FF, capturar cambios en una base de datos, cambiar de formato el contenido del FF (JSON, CSV, Avro\u2026), extraer el contenido textual de un fichero, extraer el valor de un campo de un fichero JSON a partir de su ruta, extraer cabeceras de un email, consultar a ElasticSearch, geolocalizar una IP, obtener un fichero de una cola Kafka, escribir en un log, unir el contenido de varios FFs, realizar una petici\u00f3n HTTP, transformar un fichero XML, validar un fichero CSV, enviar mensajes a un web socket, etc. Tipos de procesadores Adem\u00e1s de en la documentaci\u00f3n oficial ( https://nifi.apache.org/docs.html ), puedes consultar informaci\u00f3n extra en https://www.nifi.rocks/apache-nifi-processors/ . A grosso modo , los m\u00e1s utilizados son: Tranformaci\u00f3n de datos: ReplaceText , JoltTransformJSON , CompressContent . Enrutado y mediaci\u00f3n: RouteOnAttribute , RouteOnContent Acceso a base de datos: ExecuteSQL , ConvertJSONToSQL , PutSQL Extracci\u00f3n de atributos: EvaluateJsonPath , ExtractText , UpdateAttribute Interacci\u00f3n con el sistema: ExecuteProcess Ingesti\u00f3n de datos: GetFile , GetFTP , GetHTTP , GetHDFS Env\u00edo de datos: PutEmail , PutFile , PutFTP , PutKafka , PutMongo Divisi\u00f3n y agregaci\u00f3n: SplitText , SplitJson , SplitXml , MergeContent HTTP: GetHTTP , ListenHTTP , PostHTTP AWS: FetchS3Object , PutS3Object , PutSNS , PutSQS","title":"Procesador"},{"location":"apuntes/ingesta03nifi1.html#conector","text":"Es una cola dirigida (con un origen y un destino que determinan un sentido) que une diferentes procesadores y contiene los FF que todav\u00eda no se han ejecutado, pudiendo definir diferentes prioridades (por ejemplo, FIFO o LIFO seg\u00fan necesitemos). As\u00ed pues, los conectores van a unir la salida de un procesador con la entrada de otro (o un procesador consigo mismo, por ejemplo, para realizar reintentos sobre una operaci\u00f3n). Las conexiones se caracterizan y nombran por el tipo de puerto de salida del procesador del que nacen. En la mayor\u00eda de los casos nos enfrentaremos a conexiones de tipo success , que recogen el FF que devuelve un procesador cuando ha terminado satisfactoriamente su tarea, o de tipo failure , que conducen el FF en los casos en los que la tarea ha fallado. Tambi\u00e9n podemos crear nuestros propios conectores. Adem\u00e1s, existe la posibilidad de configurar algunos aspectos de la conexi\u00f3n, como el n\u00famero de FF que pueden viajar de forma simult\u00e1nea por ella, la prioridad de salida de los FF que hay en la conexi\u00f3n, o el tiempo que los FF deben permanecer a la espera para ser recogidos por el procesador de destino. Componentes de Nifi","title":"Conector"},{"location":"apuntes/ingesta03nifi1.html#caso-1-moviendo-datos","text":"Vamos a hacer un peque\u00f1o ejercicio con Nifi para familiarizarnos con el entorno desarrollando un flujo de datos sencillo que mueva un fichero de un directorio a otro. A continuaci\u00f3n detallamos los pasos a realizar: Seleccionamos un procesador (primer icono grande) y lo arrastramos en nuestra \u00e1rea de trabajo. Nos aparece un dialogo con tres partes diferenciadas: Di\u00e1logo de elecci\u00f3n de procesador A la izquierda una nube de etiquetas para poder filtrar los procesador. Arriba a la derecha tenemos un buscador para buscar procesadores por su nombre La parte central con el listado de procesadores, desde donde lo podemos seleccionar. As\u00ed pues, buscamos el procesador GetFile y lo a\u00f1adimos al flujo. Damos doble click sobre el elemento gr\u00e1fico que representa nuestro procesador, y en la pesta\u00f1a properties indicamos el directorio de entrada de donde tendr\u00e1 que recoger el fichero mediante la propiedad Input Directory . En nuestro caso le pondremos el valor /home/iabd/Documentos/in : Propiedades de GetFile Ahora a\u00f1adimos un nuevo procesador de tipo PutFile , y en las propiedades indicamos el directorio de salida con la propiedad directory a /home/iabd/Documentos/out . Si visualizamos la pesta\u00f1a Settings , y nos centramos en el lado derecho, podemos configurar el comportamiento a seguir si el procesador se ejecuta correctamente ( success ) o falla ( failure ). Como vamos a hacer que este procesador sea el paso final, vamos a configurar que autoterminen marcando ambos casos: Finalizaci\u00f3n de PutFile Terminar las relaciones Si nos olvidamos de autoterminar las relaciones, o tenemos conexiones sin conectar, no podremos iniciar los procesadores implicados. Esto lo tenemos que realizar para todos los procesadores que tengamos en nuestro flujo de datos. Unimos ambos procesadores creando una conexi\u00f3n. Para ello, tras pulsar sobre el icono de la flecha que aparece al dejar el rat\u00f3n sobre el primer procesador y lo arrastramos hasta el segundo. Conexi\u00f3n mediante un conector entre procesadores Antes de arrancar el primer procesador, creamos un peque\u00f1o fichero en el directorio que hemos puesto como entrada: echo \"Hola Severo!\" > hola.txt Arrancamos el procesador mediante el bot\u00f3n derecho y la opci\u00f3n Start , y comprobamos que el fichero ya no est\u00e1 en la carpeta in , y que s\u00ed aparece en la cola ( Queued 1 ). Tambi\u00e9n podemos comprobar como tampoco est\u00e1 en la carpeta out . Finalmente, arrancamos el procesador de Poner Fichero , y veremos como la cola se vac\u00eda y el archivo aparece en la carpeta out . \u00a1Ya hemos creado nuestro primer flujo de datos!","title":"Caso 1 - Moviendo datos"},{"location":"apuntes/ingesta03nifi1.html#gestionando-los-errores","text":"\u00bfQu\u00e9 sucede si leemos dos veces un archivo con el mismo nombre? Tal como lo hemos definido en nuestro flujo, s\u00f3lo se guardar\u00e1 la primera copia. Si vamos a la pesta\u00f1a Properties del procesador PonerFichero , podemos cambiar este comportamiento en la propiedad Conflict Resolution Strategy a replace , de esta manera, se guardar\u00e1 el \u00faltimo archivo. Propiedades de PutFile - gesti\u00f3n de conflictos Realmente, en vez de decidir si lo ignora o lo sobreescribe, lo ideal es definir un nuevo flujo que dependa del estado de finalizaci\u00f3n del procesador. De esta manera, podremos almacenar todos los archivos que han llegado con el mismo nombre para su posterior estudio. As\u00ed pues, vamos a quitar la autoterminaci\u00f3n que antes hab\u00edamos puesto al procesador de PonerFichero , para que cuando falle, redirija el flujo a un nuevo procesador PutFile que coloque el archivo en una nueva carpeta (en nuestro caso en /home/iabd/Documentos/conflictos ): Flujo failure para los ficheros repetidos Aunque ahora tenemos un mecanismo para almacenar los ficheros que coinciden en nombre, s\u00f3lo nos guardar\u00e1 uno (nos sucede lo mismo que antes, pero ahora s\u00f3lo con los repetidos). Asi pues, necesitamos renombrar los ficheros que vayamos a colocar en la carpeta conflictos para guardar el hist\u00f3rico. Para ello, necesitamos introducir un procesador previo que le cambie el nombre al archivo. Nifi a\u00f1ade la propiedad filename a todos los FF. Esta propiedad la podemos consultar mediante el Nifi Expression Language (Nifi EL) y haciendo uso del procesador UpdateAttribute modificar su valor. As\u00ed pues, vamos a colocar el procesador UpdateAttribute antes de colocar los archivos en la carpeta de conflictos : A\u00f1adimos el procesador UpdateAttribute Hemos decidido a\u00f1adir como prefijo al nombre del archivo la fecha del sistema en formato de milisegundos, de manera que obtendremos archivos similares a 1637151536113-fichero.txt . Para ello, a\u00f1adimos un nuevo atributo que llamaremos filename haciendo clic sobre el icono de + que aparece arriba a la derecha y en su valor utilizaremos la expresi\u00f3n ${now():toNumber()}-${filename} : A\u00f1adimos el procesador UpdateAttribute","title":"Gestionando los errores"},{"location":"apuntes/ingesta03nifi1.html#caso-2-trabajando-con-atributos","text":"Cada vez que se generan FF (representa un registro de datos que viaja por el flujo) estos van a tener asignados ciertos atributos por defecto. Entre estos atributos est\u00e1n el UUID o identificador \u00fanico, su timestamp y el tama\u00f1o del fichero. Como ya hemos visto, mediante el uso de procesadores podremos modificar estos o a\u00f1adir nuevos atributos.","title":"Caso 2 - Trabajando con atributos"},{"location":"apuntes/ingesta03nifi1.html#generando-contenido","text":"Vamos a ver c\u00f3mo hacerlo realizando los siguientes pasos: Vamos a a\u00f1adir un procesador del tipo GenerateFlowFile (este procesador crea FF con datos aleatorios o contenidos personalizados, lo cual es muy \u00fatil para testear y depurar nuestros flujos de datos). En las opciones del procesador vamos a la pesta\u00f1a de propiedades y completamos los campos: Flow Size: 10 bytes Batch Size: 1 para que nos genere un FF por cada ejecuci\u00f3n Data Format: Text Unique Flowfiles: true e indicamos que los FF van a ser \u00fanicos. Configuraci\u00f3n del procesador GenerateFlowFile A continuaci\u00f3n, en la configuraci\u00f3n de planificaci\u00f3n ( Scheduling ) de este procesador vamos a indicar que se ejecute cada 3 segundos (en el campo Run Schedule le ponemos como valor 3s ). Una vez tenemos listo el generador, vamos a a\u00f1adir el procesador ReplaceText con el que cambiaremos el texto. Tras ello, conectamos ambos procesadores. Conexi\u00f3n con ReplaceText Si nos fijamos, a la izquierda del nombre del procesador, aparece un icono de aviso, el cual nos indica que necesitamos configurar el nuevo procesador, adem\u00e1s de indicarnos que ambas relaciones no est\u00e1n conectadas o que faltan por autocompletar. Avisos que aparecen Para ello, configuramos la estrategia de reemplazo para que lo haga siempre (en el campo Replacement Value seleccionamos Always Replace ), y al hacer esto el campo Search Value se invalida. Adem\u00e1s, en el Replacement Value vamos a indicar simplemente prueba . Finalmente, marcamos para que autotermine la conexi\u00f3n failure . A\u00f1adimos un procesador de tipo LogAttribute para mostrar en el log los atributos del FF, y conectamos el procesador anterior ( ReplaceText ) a \u00e9ste mediante la relaci\u00f3n success . Log con los atributos Arrancamos el primer procesador y visualizamos la cola para comprobar qu\u00e9 ha generado. Para ello, sobre la cola elegimos la opci\u00f3n list queue para ver su contenido, y tras elegir uno, sobre el icono del ojo, visualizamos su contenido y comprobado que ha generado datos aleatorios: Acceso y visualizaci\u00f3n de la cola Si ejecutamos el siguiente procesador, vemos que saca el FF de la cola anterior y aparecer\u00e1 en la siguiente. Si comprobamos su valor, veremos que ha cambiado el valor original por prueba . Resultado de visualizar la cola tras ReplaceText Si accedemos al log de la aplicaci\u00f3n (archivo nifi-app.log dentro de la carpeta logs ) veremos mensajes similares a: 2021-11-17 19:12:37,446 INFO [Timer-Driven Process Thread-2] o.a.n.processors.standard.LogAttribute LogAttribute [id=2f300ff5-017d-1000-9020-2744e67e8d04] logging for flow file StandardFlowFileRecord [uuid=f4181825-f996-40c0-9e3c-a78326837d60,claim=StandardContentClaim [resourceClaim=StandardResourceClaim [id=1637174667410-1, container=default, section=1], offset=7953, length=7], offset=0,name=f4181825-f996-40c0-9e3c-a78326837d60,size=7] -- ------------------------------------------------ Standard FlowFile Attributes Key: 'entryDate' Value: 'Wed Nov 17 19:12:37 UTC 2021' Key: 'lineageStartDate' Value: 'Wed Nov 17 19:12:37 UTC 2021' Key: 'fileSize' Value: '7' FlowFile Attribute Map Content Key: 'filename' Value: 'f4181825-f996-40c0-9e3c-a78326837d60' Key: 'path' Value: './' Key: 'uuid' Value: 'f4181825-f996-40c0-9e3c-a78326837d60' --------------------------------------------------","title":"Generando contenido"},{"location":"apuntes/ingesta03nifi1.html#anadiendo-un-atributo","text":"Ahora vamos a extraer el contenido del FF a un atributo mediante el procesador ExtractText . En las propiedades, creamos una nueva propiedad (bot\u00f3n + de la esquina superior derecha) que llamaremos contenido , y en cuyo valor vamos a poner la expresi\u00f3n .* que indica que queremos que coincida con todo. A\u00f1adimos la propiedad contenido a ExtractText Una vez creado, vamos a colocar este procesador entre los dos anteriores (para el segundo con el caso matched , que es cuando ha coincidido con la expresi\u00f3n regular). En la conexi\u00f3n unmatched la marcamos para que autotermine, y comprobamos que no tenemos ning\u00fan advertencia en ning\u00fan procesador. Flujo completo del caso 2 Finalmente, ejecutamos todos los procesadores y comprobamos como en el log aparece el nuevo atributo creado. Tambi\u00e9n podemos acceder a la cola, y en la parte izquierda de cada flujo, en el icono de la i , pulsar y comprobar la pesta\u00f1a Atributes . Comprobaci\u00f3n de los atributos de un FF","title":"A\u00f1adiendo un atributo"},{"location":"apuntes/ingesta03nifi1.html#linaje-de-los-datos","text":"Para comprobar el dato final, es muy \u00fatil utilizar la opci\u00f3n de Data provenance , la cual nos ofrece un linaje de los datos. Linaje de los datos El linaje de los datos describe el origen, movimientos, caracter\u00edsticas y calidad de los datos, aportando visibilidad de punto a punto para incrementar la calidad de los datos. Se puede considerar como el historial de los datos, facilitando la documentaci\u00f3n y gobernanza de los datos. Para ello, sobre el procesador final, con el bot\u00f3n derecho, elegimos la opci\u00f3n View data provenance . Si elegimos uno de los flujos, a la derecha de cada flujo, podemos pulsar sobre el primer icono podremos ver un grafo y un slider que modifica el grafo respecto al instante temporal (en cada uno de los pasos, podemos hacer doble clik y ver la informaci\u00f3n y el contenido del FF en dicho momento exacto): Linaje de los datos","title":"Linaje de los datos"},{"location":"apuntes/ingesta03nifi1.html#caso-3-filtrado-de-datos","text":"En este caso, vamos a coger los datos de ventas que ya utilizamos en la sesi\u00f3n de Pentaho , el cual tiene la siguiente estructura: pdi_sales_small.csv ProductID;Date;Zip;Units;Revenue;Country 725;1/15/1999;41540 ;1;115.5;Germany 787;6/6/2002;41540 ;1;314.9;Germany 788;6/6/2002;41540 ;1;314.9;Germany Utilizando Nifi, vamos a crear un nuevo fichero CSV que contenga \u00fanicamente los datos de Francia que han realizado m\u00e1s de una venta. Para ello, tendremos que leer el fichero haciendo uso del procesador GetFile , separar cada fila en un FF mediante SplitRecord , filtrar los datos usando el procesador QueryRecord y finalmente los almacenaremos en disco gracias al procesador PutFile .","title":"Caso 3 - Filtrado de datos"},{"location":"apuntes/ingesta03nifi1.html#lectura-y-division","text":"As\u00ed pues, comenzamos leyendo el fichero con el procesador GetFile . En este caso vamos a dejar la opci\u00f3n keep source file a true para que no lo elimine. Mediante el procesador SplitRecord , vamos a separar cada fila del CSV a un FF. Para ello, primero hemos de crear un RecordReader y un RecordWriter para que sepa interactuar con el CSV ( Nifi ya tiene varios implementados que podemos utilizar). As\u00ed pues: En el Record Reader , seleccionamos Create new service , y elegimos CVSReader . A su vez, en el Record Writer elegimos CVSRecordSetWriter . Para configurar estos servicios, pulsaremos sobre la flecha, y veremos la pantalla de configuraci\u00f3n. Para cada uno de ellos, tendremos otros tres iconos: la rueda para configurar, el rayo para activar/desactivar el servicio y la papelera para eliminarlo. As\u00ed, pues, tras comprobar los valores de CVSReader y CSVSWriter (indicamos el ; como separador de campos tanto para la lectura como la escritura de CSV en el campo value separator y marcamos como true que el archivo contiene la primera fila con encabezados ( treat first line as header*)), pulsamos sobre el rayo para activar ambos servicios. Configuraci\u00f3n y activaci\u00f3n de Split Record Finalmente, en el campo Records per Split le indicamos 1 para que coloque cada fila en un FF.","title":"Lectura y divisi\u00f3n"},{"location":"apuntes/ingesta03nifi1.html#filtrado-de-ff","text":"En este paso, mediante el procesador QueryRecord vamos a ejecutar una consulta SQL contra el FF. El resultado del nuevo FF ser\u00e1 el resultado de la consulta. En nuestro caso, como hemos comentado antes, vamos a quedarnos con las ventas de m\u00e1s de una unidad realizadas en Francia. Igual que antes, configuramos los mismos Record Reader y Record Writer . Adem\u00e1s, hemos de poner la propiedad Include Zero Record FlowFiles a false para que no vuelva a enrutar los FF que no cumplan la consulta. Finalmente, a\u00f1adimos una nueva propiedad para indicar la consulta. En nuestro caso la hemos denominado FranciaMayor1 y en el contenido ponemos la consulta: select * from Flowfile where Country = 'France' and Units > 1 campos Tambi\u00e9n podr\u00edamos haber filtrado los campos para recuperar menos contenido con una consulta similar a select ProductID, Date from FlowFile .... . Con este procesador podemos filtrar, hacer agrupaciones, c\u00e1lculos, del mismo modo que lo hacemos con SQL. Finalmente, igual que hicimos en el caso 1, vamos a cambiarle el nombre a cada FF para generar un archivo por cada resultado mediante UpdateAttribute y persistimos los datos con PutFile . El resultado del flujo de datos ser\u00e1 similar a: Flujo completo del caso 3","title":"Filtrado de FF"},{"location":"apuntes/ingesta03nifi1.html#caso-4-fusionar-datos","text":"En esta caso vamos a realizar los siguientes pasos: Escuchar datos que recibimos de un servidor web. Reconocer si el mensaje contiene la cadena error . Fusionar los mensajes en varios ficheros, dependiendo de si contienen un error. Almacenar el fichero resultante en MongoDB .","title":"Caso 4 - Fusionar datos"},{"location":"apuntes/ingesta03nifi1.html#recibiendo-datos-via-http","text":"Vamos a utilizar el procesador ListenHTTP para escuchar peticiones HTTP. Para ello, lo a\u00f1adimos a nuestro flujo de trabajo y configuramos: Listening port (puerto de escucha): 8081 Base Path ( endpoint de la petici\u00f3n): iabd A continuaci\u00f3n, para distinguir entre los diferentes datos de entrada, utilizaremos el procesador RouteOnContent , con el objetivo de separar en dos flujos de datos, los que contienen la cadena error y los que no. Para ello, tras a\u00f1adir el procesador, le conectamos al flujo success que viene de ListenHTTP , y configuramos: Cambiamos el Match Requirement (requisito de coincidencia) a: content must contain match (no tienen que coincidir exactamente, sino contener el valor). Y a\u00f1adimos la propiedad textoError con el valor ERROR . Propiedades de RouteOnContent Para poder probarlo, arrancamos el primer procesador, y desde un terminal, hacemos una petici\u00f3n a: curl --data \"texto de prueba\" http://localhost:8081/iabd Si comprobamos la cola, podremos ver como se ha creado un FF cuyo contenido es texto de prueba .","title":"Recibiendo datos via HTTP"},{"location":"apuntes/ingesta03nifi1.html#fusionando-contenido","text":"Si nos fijamos en las propiedades del procesador RouteOnContent , tenemos dos flujos de salida textoError (para los mensajes que contienen el texto ERROR ) y unmatched (para los que no). Vamos a a\u00f1adir el procesador MergeContent , el cual recibe un conjunto de FF y los fusiona en uno a partir de la estrategia de fusi\u00f3n que defina el usuario. Las diferentes opciones incluye agrupando ciertos atributos de forma similar a como se realiza la fase reduce de un algoritmo MapReduce . As\u00ed pues, vamos a conectar las relaciones textoError y unmatched con MergeContent : Conexi\u00f3n con MergeContent Tras conectar los procesadores, vamos a configurar el procesador MergeContent : En la pesta\u00f1a de planificaci\u00f3n, vamos a poner que el procesador se ejecuta cada 30 segundos para poder agrupar varios FF. En las propiedades: Merge Strategy : Bin-Packing Algorithm (se fusionan en grupos). Correlation Attribute Name : RouteOnContent.Route (crea diferentes grupos para textoError y unmatched ) Merge Format : Binary Concatenation (concatena los contenidos de los FF en un \u00fanico FF, otra opci\u00f3n ser\u00eda comprimirlos en un zip) Maximum Number of Entries : 1000 (cantidad m\u00e1xima de elementos por grupo, por si tuvi\u00e9ramos muchas peticiones HTTP) Maximum Bin Age : 300s (fuerza que el fichero fusionado salga como muy tarde a los 300s) Delimiter Strategy : Text (para concatenar los fichero utilizando una nueva l\u00ednea como car\u00e1cter delimitador) y Demarcator : al abrir el campo, pulsar Shift/May\u00fas + Intro para poner el car\u00e1cter del salto de l\u00ednea. Para poder probar como se van creando y agrupando los mensajes, podemos ejecutar los siguientes comandos: curl --data \"texto de prueba\" http://localhost:8081/iabd curl --data \"este s\u00ed que tiene ERROR\" http://localhost:8081/iabd curl --data \"vaya ERROR m\u00e1s tonto\" http://localhost:8081/iabd curl --data \"nifi mola mucho\" http://localhost:8081/iabd Por ejemplo, si abrimos uno de los flujos podemos ver c\u00f3mo se han agrupado varias peticiones en un \u00fanico FF: Resultado de MergeContent","title":"Fusionando contenido"},{"location":"apuntes/ingesta03nifi1.html#guardando-el-resultado-a-mongodb","text":"Para almacenar el resultado de las fusiones anteriores, vamos a guardar los resultados en una colecci\u00f3n de MongoDB. Suponemos que ya tenemos instalado MongoDB en nuestro sistema. Si no, podemos lanzarlo mediante Docker : MongoDB + Nifi via Docker Si queremos utilizarlo mediante Docker, necesitamos que MongoDB y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml (si tuvieras alguna problema con la imagen de MongoDB y tu procesador, prueba a cambiar la l\u00ednea 15 mongo:latest por mongo:4.4 ): docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi NIFI_JVM_HEAP_MAX : 2g links : - mongodb mongodb : ports : - \"27017:27017\" image : mongo:latest Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifimongodb up -d Para poder meter los mensajes en MongoDB , vamos a preparar el contenido para que est\u00e9 en formato JSON. Adem\u00e1s del contenido, vamos a crear un atributo con el nombre del enrutador utilizado para posteriormente poder filtrar los mensajes de error. Para poder crear el formato JSON, utilizaremos el procesador AttributesToJSON . As\u00ed pues, previamente necesitamos pasar los mensajes desde el contenido de los FF a los atributos (para ello, igual que en el caso anterior, utilizaremos el procesador ExtracText ). A su vez, tambi\u00e9n crearemos un nuevo atributo con el nombre del enrutador mediante el procesador UpdateAttribute . El resultado final ser\u00e1 similar al siguiente flujo: Resultado completo del caso 4 Utilizamos el procesador ExtracText para pasar el contenido a los atributos. Dentro de las propiedades configuraremos: Enable DOTALL mode : true , para que incluya los saltos de l\u00ednea como contenido. A\u00f1adimos una nueva propiedad contenido , y como expresi\u00f3n regular introducimos (.*) . Una vez creado, conectamos MergeContent con ExtractText mediante la conexi\u00f3n merged , y el resto de conexiones las marcamos para que autoterminen. A\u00f1adimos el procesador UpdateAttribute , y dentro de las propiedades, a\u00f1adirmos una nueva propiedad que vamos a llamar estado cuyo valor ser\u00e1 ${RouteOnContent.Route} , es decir, le ponemos el mismo que contenga el atributo RouteOnContent.Route . Creando el atributo estado Una vez creado, conectamos ExtractText con UpdateAttribute mediante la conexi\u00f3n matched , y el resto de conexiones las marcamos para que autoterminen. A continuaci\u00f3n, vamos a utilizar el procesador AttributesToJSON para pasar los atributos contenido y estado como contenido de un FF en formato JSON. Para ello, configuramos las propiedades: Attribute List : contenido,estado Destination : flowfile-content Creando el atributo estado Una vez creado, conectamos UpdateAttribute con AttributesToJSON mediante la conexi\u00f3n success , y el resto de conexiones las marcamos para que autoterminen. Si ejecutamos los procesadores anteriores y comprobamos la salida, veremos como se est\u00e1n creando FF cuyo contenido es la petici\u00f3n introducida m\u00e1s el estado: Mensaje JSON creado Finalmente, a\u00f1adimos el procesador PutMongo para introducir el contenido JSON. Las propiedades que hay que configurar son: Mongo URI: mongodb://localhost Mongo Database Name: iabd Mongo Collection Name: caso4 En nuestro caso, hemos autoterminado la conexi\u00f3n success y reconectado con el mismo procesador la conexi\u00f3n failure . Si arrancamos el flujo de datos completo, y tras realizar las mismas peticiones de antes: curl --data \"texto de prueba\" http://localhost:8081/iabd curl --data \"este s\u00ed que tiene ERROR\" http://localhost:8081/iabd curl --data \"vaya ERROR m\u00e1s tonto\" http://localhost:8081/iabd curl --data \"nifi mola mucho\" http://localhost:8081/iabd S\u00f3lo nos queda entrar a MongoDB y comprobar que nos aparecen los datos: > use iabd swi t ched t o db iabd > db.caso 4. f i n d() { \"_id\" : Objec t Id( \"6197cca29c63ec4e825b8232\" ) , \"contenido\" : \"este s\u00ed que tiene ERROR\\nvaya ERROR m\u00e1s tonto\" , \"estado\" : \"textoError\" } { \"_id\" : Objec t Id( \"6197cca29c63ec4e825b8233\" ) , \"contenido\" : \"texto de prueba\\nnifi mola mucho\" , \"estado\" : \"unmatched\" }","title":"Guardando el resultado a MongoDB"},{"location":"apuntes/ingesta03nifi1.html#actividades","text":"Realiza los casos de uso del 1 al 3. En la entrega debes adjuntar una captura de pantalla donde se vea el flujo de datos completo con una nota con tu nombre, y adjuntar la definici\u00f3n de cada flujo (sobre el \u00e1rea de trabajo, con el bot\u00f3n derecho, Download flow definition ). (opcional) Realiza el caso de uso 4.","title":"Actividades"},{"location":"apuntes/ingesta03nifi1.html#referencias","text":"Getting started with Apache Nifi Apache Nifi User Guide Apache Nifi in Depth Apache Nifi en TutorialsPoint","title":"Referencias"},{"location":"apuntes/ingesta04nifi2.html","text":"Nifi Avanzado \u00b6 Grupos \u00b6 En Nifi s\u00f3lo hay un canvas de nivel superior, pero podemos construir tantos flujos l\u00f3gicos como deseemos. Normalmente, para organizar las flujos, se utilizan grupos de procesos , por lo que el canvas de nivel superior puede tener varios grupos de procesos, cada uno de los cuales representa un flujo l\u00f3gico, pero no necesariamente conectados entre s\u00ed. Trabajando con grupos Dentro de los grupos, para indicar como entran los datos se utiliza un Input port , el cual va a definir un puerto de entrada al grupo. Del mismo modo, para indicar c\u00f3mo salen los datos, se utiliza un Output port como puerto de salida para transferir la informaci\u00f3n fuera del grupo. As\u00ed pues, los grupos de procesos nos van a permitir refactorizar nuestros flujos para agruparlos y poder reutilizarlos en otros flujos, o bien mejorar la legibilidad de nuestro flujo de datos. En todo momento podemos ver el nivel en el que nos encontramos en la parte inferior izquierda, con una notaci\u00f3n similar a Nifi Flow >> Subnivel >> Otro Nivel . Creando un grupo \u00b6 Vamos a partir de un ejemplo sencillo de leer un fichero de texto, partirlo en fragmentos y nos saque por el log alguno de sus atributos. Para ello, vamos a conectar un procesador GetFile con un SplitText y finalmente con LogAttribute . El procesador SplitText nos permite dividir cualquier flujo de texto en fragmentos a partir del n\u00famero de l\u00edneas que queramos (pudiendo tener encabezados, ignorar las l\u00edneas en blanco, etc...) Dividimos un archivo en fragmentos Para probarlo, podemos copiar cualquier archivo (ya sea el README o el NOTICE ) en la carpeta que hayamos indicado de entrada y comprobar el log. Una vez lo tenemos funcionando, vamos a colocar el procesador SplitText dentro de un grupo. As\u00ed pues, un grupo encapsula la l\u00f3gica de un procesador haci\u00e9ndolo funcionar como una caja negra. Para ello, desde la barra superior arrastramos el icono de Process Group y lo nombramos como grupo . Creaci\u00f3n de un grupo Puertos \u00b6 Una vez creado, vamos a copiar nuestro procesador SplitText . Pulsamos doble click sobre el grupo y bajaremos un nivel en el canvas para meternos dentro de Grupo . Una vez dentro, vamos a pegar el procesador y a\u00f1adiremos tanto un Input port (al que nombramos como entrada ) como un Output port (al que llamamos salida ). Una vez creados, los conectamos con el procesador con las mismas conexiones que antes. Grupo con puertos de entrada y salida Ahora salimos del grupo, y conectamos los procesadores del nivel superior con el grupo creado y comprobamos como sigue funcionando. Sustituimos el procesador por el grupo creado Funnels \u00b6 Funnel Los funnels son un tipo de componente que permite trabajar en paralelo y despu\u00e9s unir los diferentes flujos en un \u00fanico flujo de ejecuci\u00f3n, adem\u00e1s de poder definir su propia prioridad de forma centralizada. Para ello vamos a poner varios GenerateFlowFile (4 en este caso) para mostrar sus datos mediante LogAttribute . Varios procesadores que apuntan a uno Si quisi\u00e9ramos cambiar el procesador de LogAttribute por otro tipo de procesador, deber\u00edamos borrar todas las conexiones y volver a conectarlo todo. Para evitar esto a\u00f1adimos un Funnel que va a centralizar todas las conexiones en un \u00fanico punto. El Funnel agrupa las conexiones Plantillas \u00b6 Nifi permite trabajar con plantillas para poder reutilizar flujos de datos, as\u00ed como importar y exportar nuestras plantillas. Creando plantillas \u00b6 Crear una plantilla es muy sencillo. Si partimos del ejemplo anterior, mediante shift y el rat\u00f3n, podemos seleccionar todos los elementos que queremos que formen parte de la plantilla. Una vez seleccionado, podemos utilizar el bot\u00f3n derecho o dentro del men\u00fa Operate , y elegir Create Template : Creaci\u00f3n de una plantilla Si queremos descargar una plantilla para usarla en otra instalaci\u00f3n, desde el men\u00fa de la esquina superior derecha, en la opci\u00f3n Templates , podemos consultar las plantillas que tenemos cargadas, y para cada una de ellas, tenemos la opci\u00f3n de descargarlas o eliminarlas. Descargando una plantilla Cargando plantillas \u00b6 En cambio, para cargar una plantilla, desde el propio men\u00fa de Operate , el icono con la plantilla y la flecha hacia arriba, nos permitir\u00e1 elegir un archivo .xml con el c\u00f3digo de la plantilla. Una vez cargada, usaremos el control del menu superior para arrastrarla al \u00e1rea de trabajo. Una de las mayores ventajas es el uso de plantillas ya existentes. Existe una colecci\u00f3n de plantillas mantenida por Cloudera en https://github.com/hortonworks-gallery/nifi-templates . Se recomienda darle un ojo a la hoja de c\u00e1lculo que contiene un resumen de las plantillas compartidas. Otros ejemplos a destacar se encuentran en https://github.com/xmlking/nifi-examples . En nuestro caso, vamos a utilizar la plantilla de CSV-to-JSON , la cual podemos descargar desde https://raw.githubusercontent.com/hortonworks-gallery/nifi-templates/master/templates/csv-to-json-flow.xml . Una vez descargado el archivo xml, lo subimos a Nifi. Tras ello, arrastramos el componente y vemos su resultado: Cargando una plantilla Caso 5: Trabajando con conjuntos de registros \u00b6 En los casos anteriores, ya hemos visto que haciendo uso de ExtractText y AttributesToJSON pod\u00edamos crear ficheros JSON a partir de CSV. Nifi ofrece una forma m\u00e1s c\u00f3moda de realizar esto. Haciendo uso de los FF como registros y los procesadores de tipo Record (ya utilizamos alguno en el caso 3, en el que filtr\u00e1bamos mediante una sentencia SQL), vamos a poder trabajar con los datos como un conjunto de registros en vez de hacerlo de forma individual. Estos procesadores hacen que los flujos de construcci\u00f3n para manejar datos sean m\u00e1s sencillos, ya que que podemos construir procesadores que acepten cualquier formato de datos sin tener que preocuparnos por el an\u00e1lisis y la l\u00f3gica de serializaci\u00f3n. Otra gran ventaja de este enfoque es que podemos mantener los FF m\u00e1s grandes, cada uno de los cuales consta de m\u00faltiples registros, lo que resulta en un mejor rendimiento. Tenemos tres componentes a destacar: De lectura: AvroReader , CsvReader , ParquetReader , JsonPathReader , JsonTreeReader , ScriptedReader , ... De escritura: AvroRecordSetWriter , CsvRecordSetWriter , JsonRecordSetWriter , FreeFormTextRecordSetWriter , ScriptedRecordSetWriter , ... Procesador de registros: ConvertRecord : convierte entre formatos y/o esquemas similares. Por ejemplo, la conversi\u00f3n de CSV a Avro se puede realizar configurando ConvertRecord con un CsvReader y un AvroRecordSetWriter . Adem\u00e1s, la conversi\u00f3n de esquemas entre esquemas similares se puede realizar cuando el esquema de escritura es un subconjunto de los campos del esquema de lectura o si el esquema de escritura tiene campos adicionales con valores propuestos. LookupRecord : extrae uno o m\u00e1s campos de un registro y busca un valor para esos campos en un LookupService (ya sea a un fichero CSV, XML, accediendo a una base de datos o un servicio REST, etc...). Estos servicios funcionan como un mapa, de manera que reciben la clave y el servicio devuelve el valor. Puedes consultar m\u00e1s informaci\u00f3n en la serie de art\u00edculos Data flow enrichment with NiFi part: LookupRecord processor y un ejemplo completo en Enriching Records with LookupRecord & REST APIs in NiFi . QueryRecord : ejecuta una declaraci\u00f3n SQL contra los registros y escribe los resultados en el contenido del archivo de flujo. Este es el procesador que usamos en el caso 3 de la sesi\u00f3n anterior . ConsumeKafkaRecord_N_M : utiliza el Reader de registros configurado para deserializar los datos sin procesar recuperados de Kafka, y luego utiliza el Writer de registros configurado para serializar los registros al contenido del archivo de flujo. PublicarKafkaRecord_N_M : utiliza el Reader de registros configurado para leer el archivo de flujo entrante como registros, y luego utiliza el Writer de registros configurado para serializar cada registro para publicarlo en Kafka. Convirtiendo formatos \u00b6 As\u00ed pues, para demostrar su uso vamos a convertir el archivo CSV del caso 3 de la sesi\u00f3n anterior que contiene informaci\u00f3n sobre ventas a formato JSON. Podr\u00edamos utilizar simplemente un GetFile conectado a un ConvertRecord y este a un PutFile . Para que el fichero generado contenga como extensi\u00f3n el formato al que convertimos, antes de serializar los datos, a\u00f1adimos un procesador UpdateAttribute para modificar el nombre del fichero. El flujo de datos resultante ser\u00e1 similar a: Conversi\u00f3n de formato mediante ConvertRecord En concreto, en el caso del ConvertRecord , hemos utilizado los siguientes elementos: Configuraci\u00f3n de ConvertRecord Para el CSVReader , hemos de configurar el separador de campos con el ; e indicar que la primera fila contiene un encabezado. Para el JSONRecordSetWriter no hemos configurado nada. Renombrando el destino \u00b6 Tal como hemos comentado, necesitamos renombrar el fichero de salida. Para ello, necesitamos hacer uso del procesador UpdateAttribute y utilizar el Nifi Expression Language para modificar la propiedad filename y recortar la extensi\u00f3n y concatenar con la nueva mediante la expresi\u00f3n ${filename:substringBefore('.csv')}.json : Modificando la extensi\u00f3n de filename Caso 6: Trabajando con Elasticsearch \u00b6 En bloques anteriores ya hemos trabajado con Elasticsearch . En nuestro caso, tenemos la versi\u00f3n 7.16 descargada en la carpeta /opt/elasticsearch-7.16.0 de nuestra m\u00e1quina virtual. Elasticsearch+Nifi via Docker Si queremos utilizarlo mediante Docker , necesitamos que ElasticSearch y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml : docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi links : - elasticsearch elasticsearch : ports : - \"9200:9200\" - \"9300:9300\" environment : discovery.type : single-node image : docker.elastic.co/elasticsearch/elasticsearch:7.15.2 Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifielasticsearch up -d Recordad que necesitamos arrancarla mediante el comando ./bin/elasticsearch . Para comprobar que ha ido todo bien, podemos ejecutar la siguiente petici\u00f3n: curl -X GET 'localhost:9200/_cat/health?v=true&pretty' El procesador con el que vamos a trabajar es del tipo PutElasticSearchHttp , en el cual vamos a configurar: Elasticsearch URL : http://localhost:9200 (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: http://elasticsearch:9200 ) Index : aqu\u00ed vamos a poner como valor la palabra peliculas . marcamos la opci\u00f3n de autoterminar para las conexiones retry y failure . Una vez creado el procesador, vamos a alimentarlo a partir de los datos de un fichero JSON, mediante el procesador que ya conocemos GetFile . Pod\u00e9is descargar el fichero de pruebas movies.json y colocarlo en la carpeta donde hayamos configurado. Lectura de JSON e inserci\u00f3n en Elasticsearch Una vez ejecutado, para comprobar que se han introducido los datos podemos ejecutar la siguiente petici\u00f3n: curl -X GET \"localhost:9200/peliculas/_search?pretty\" Y veremos c\u00f3mo se han introducido en Elasticsearch : { \"took\" : 46 , \"timed_out\" : false , \"_shards\" : { \"total\" : 1 , \"successful\" : 1 , \"skipped\" : 0 , \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1 , \"relation\" : \"eq\" }, \"max_score\" : 1 , \"hits\" : [ { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"FTUyU30BBEE3YF7Zlgn1\" , \"_score\" : 1 , \"_source\" : { \"movies\" : [ { \"title\" : \"The Shawshank Redemption\" , \"rank\" : \"1\" , \"id\" : \"tt0111161\" }, Si nos fijamos bien, realmente solo ha insertado un documento que contiene un array de pel\u00edculas, lo cual no est\u00e1 bien. Separando los datos \u00b6 As\u00ed pues, previamente debemos separar el array contenido dentro de movies.json en documentos individuales. Borrar el \u00edndice Recuerda que antes de meter nuevos datos, necesitamos eliminar el \u00edndice de ElasticSearch mediante curl -X DELETE \"localhost:9200/peliculas\" . Para dividir el documento de pel\u00edculas en documentos individuales, necesitamos utilizar el procesador SplitJSON . Lo conectamos entre los dos procesadores anteriores, y le indicamos en la propiedad JSonPath Expression la expresi\u00f3n donde se encuentran las pel\u00edculas: $.movies.* Separamos las pel\u00edculas Y ahora si volvemos a consultar el \u00edndice mediante curl -X GET \"localhost:9200/peliculas/_search?pretty\" , s\u00ed que veremos que ha insertado las cien pel\u00edculas por separado: { \"took\" : 5 , \"timed_out\" : false , \"_shards\" : { \"total\" : 1 , \"successful\" : 1 , \"skipped\" : 0 , \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 100 , \"relation\" : \"eq\" }, \"max_score\" : 1.0 , \"hits\" : [ { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"X-uaWH0BesFhA-H6MMxA\" , \"_score\" : 1.0 , \"_source\" : { \"title\" : \"The Shawshank Redemption\" , \"rank\" : \"1\" , \"id\" : \"tt0111161\" } }, { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"YOuaWH0BesFhA-H6MMxC\" , \"_score\" : 1.0 , \"_source\" : { \"title\" : \"The Godfather\" , \"rank\" : \"2\" , \"id\" : \"tt0068646\" } }, Caso 7: de Twitter a Elasticsearch/MongoDB \u00b6 Para este caso de uso, vamos a recoger datos de Twitter y los vamos a meter tanto en MongoDB como en ElasticSearch a la vez. El primer paso es obtener unas credenciales de desarrollador por parte de Twitter para poder acceder a su API. Para ello, en https://developer.twitter.com/ creamos una cuenta de desarrollador y creamos un proyecto. Claves necesarias para conectar con Twitter Nifi+Elasticsearch+MongDB via Docker Si queremos realizar este caso de uso mediante Docker, necesitamos que ElasticSearch, MongoDB y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml : docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi NIFI_JVM_HEAP_MAX : 2g links : - elasticsearch - mongodb elasticsearch : ports : - \"9200:9200\" - \"9300:9300\" environment : discovery.type : single-node image : docker.elastic.co/elasticsearch/elasticsearch:7.15.2 mongodb : ports : - \"27017:27017\" image : mongo:latest Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifielasticsearchmongodb up -d Leyendo tweets \u00b6 Vamos a recuperar cada 100 segundos los mensajes que contengan la palabra bigdata . Procesador GetTwitter - Twitter Elevated Para poder utilizar el procesador de Nifi GetTwitter es necesario acceder al Twitter API v1.1 , la cual solo est\u00e1 disponible para las cuentas con un nivel Elevated . Por ello, en vez de utilizar las credenciales habituales (API Key, API Key Secret, Access Token y Access Token Secret), vamos a realizar el acceso mediante una petici\u00f3n HTTP utilizando un token de validaci\u00f3n, conocido como Bearer Token (lo cual es una mejor pr\u00e1ctica de seguridad). Para ello, usamos un procesador InvokeHTTP y configuramos los siguientes par\u00e1metros: En la planificaci\u00f3n, vamos a configurar que se realice una petici\u00f3n cada 100 segundos, configurando Run Schedule a 100s . HTTP Method : GET Remote UR L: https://api.twitter.com/2/tweets/search/recent?query=bigdata&tweet.fields=created_at,lang,public_metrics Mediante el par\u00e1metro query indicamos el t\u00e9rmino a buscar. En nuestro caso, buscamos la palabra bigdata . Mediante el par\u00e1metro tweet.field indicamos los campos a recuperar (por defecto recupera el id y el texto de cada tweet ) Puedes comprobar todos los campos que podemos obtener de los mensajes en https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet A\u00f1adimos una propiedad (mediante el icono del signo + ) que nombramos como Authorization y le asignamos la palabra Bearer y el token que copiamos desde la administraci\u00f3n de Twitter: Bearer AAAAAAAAAAAAAAAAAAAAAIGNWAEAAAAA... Caso 7: Comprobando la lectura de mensajes de Twitter Tras realizar una petici\u00f3n, obtendremos un FF similar a la siguiente informaci\u00f3n (he dejado s\u00f3lo dos mensajes para facilitar la visualizaci\u00f3n): { \"data\" :[ { \"id\" : \"1468197767885561856\" , \"text\" : \"RT @CatherineAdenle: Machine Learning Algorithms in Python You Must Learn \\n#DataScience #MachineLearning\\n#BigData #Analytics #AI #Tech #Alg\u2026\" , \"created_at\" : \"2021-12-07T12:36:40.000Z\" , \"public_metrics\" :{ \"retweet_count\" : 22 , \"reply_count\" : 0 , \"like_count\" : 0 , \"quote_count\" : 0 }, \"lang\" : \"en\" }, ... { \"id\" : \"1468197570925277190\" , \"text\" : \"RT @gp_pulipaka: A Quick Intro to Deep Learning Course! #BigData #Analytics #DataScience #AI #MachineLearning #IoT #IIoT #Python #RStats #T\u2026\" , \"created_at\" : \"2021-12-07T12:35:53.000Z\" , \"public_metrics\" :{ \"retweet_count\" : 60 , \"reply_count\" : 0 , \"like_count\" : 0 , \"quote_count\" : 0 }, \"lang\" : \"en\" } ], \"meta\" :{ \"newest_id\" : \"1468197767885561856\" , \"oldest_id\" : \"1468197570925277190\" , \"result_count\" : 10 , \"next_token\" : \"b26v89c19zqg8o3fpdy8xh0ikrj9fhq1nywqt95oqkxh9\" } } A continuaci\u00f3n, vamos a separar los mensajes en diferentes FF del mismo modo que acabamos de hacer en el ejercicio anterior. As\u00ed pues, a\u00f1adimos el procesador SplitJson y configuramos la divisi\u00f3n de los mensajes mediante expresi\u00f3n con $.data.* . Evaluando el idioma \u00b6 Con esta informaci\u00f3n, hemos decidido enviar todos los mensajes que vengan en ingl\u00e9s ( \"lang\": \"en\" ) a ElasticSearch , y los que vengan en castellano a MongoDB. Mediante el procesador EvaluateJsonPath , vamos a colocar en atributos la siguiente informaci\u00f3n que queremos almacenar, definiendo la propiedad Destination como flow-attribute : Para ello creamos los siguientes atributos con la expresi\u00f3n para acceder al campo JSON asociado: twitter.id: $.id twitter.text: $.text twitter.lang: $.lang twitter.created_at: $.created_at twitter.rt: $.public_metrics.retweet_count twitter.likes: $.public_metrics.like_count Caso 7: Creamos atributos con informaci\u00f3n de los tweets A continuaci\u00f3n, conectamos con el procesador RouteOnAttribute . Caso 7: Conexiones para enrutar seg\u00fan el idioma Dentro del procesador RouteOnAttribute , creamos dos propiedas para enrutar los FF seg\u00fan el valor del atributo twitter.lang : lang-en : ${twitter.lang:equals(\"en\")} lang-es : ${twitter.lang:equals(\"es\")} Caso 7: Enrutamos seg\u00fan el atributo twitter.lang Guardando mensajes en ingl\u00e9s en ElasticSearch \u00b6 Tal como acabamos de hacer, s\u00f3lo tenemos que a\u00f1adir el procesador PutElasticsearchHttp y configurar las siguientes propiedades: Elasticsearch URL : http://localhost:9200 (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: http://elasticsearch:9200 ) Index : aqu\u00ed vamos a poner como valor la palabra tweets . marcamos la opci\u00f3n de autoterminar para las conexiones retry y failure . A continuaci\u00f3n, lo conectamos mediante la conexi\u00f3n lang-en que sale del procesador anterior. Para comprobar que los datos se est\u00e1n insertando correctamente, podemos hacer una petici\u00f3n a: curl -X GET \"localhost:9200/tweets/_search?pretty\" Guardando mensajes en castellano en MongoDB \u00b6 De forma similar, agregamos el procesador PutMongo y configuramos las siguientes propiedades: Mongo URI: mongodb://localhost (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: mongodb://mongodb ) Mongo Database Name: iabd Mongo Collection Name: tweets Tras ello, lo conectamos mediante la conexi\u00f3n lang-es (y si queremos, tambi\u00e9n podemos a\u00f1adir la conexi\u00f3n unmatched de manera que almacener\u00e1 los tambi\u00e9n los mensajes que no est\u00e9n ni en ingl\u00e9s ni en espa\u00f1ol. Cuando Twitter no reconoce el lenguaje del mensaje, le asigna como lenguaje undetermined mediante el c\u00f3digo und ). Para comprobar que los datos se est\u00e1n insertando correctamente, una vez conectados a mongo , podemos realizar la siguiente consulta: use iabd ; db . tweets . find (); As\u00ed pues, el flujo de datos queda tal que as\u00ed: Caso 7: Flujo de datos con ElasticSearch y MongoDB Renombrando id Si queremos que MongoDB utilice el id del tweet como la clave de los documentos en MongoDB y as\u00ed asegurar que no tenemos mensajes repetidos, debemos renombrar el campo a _id . Para ello, podemos utilizar el procesador UpdateAttribute para renombrar twitter.id a _id y luego el procesador AttritubesToJSON para generar la informaci\u00f3n a almacenar. REST API Nifi ofrece un API REST con el cual podemos interactuar de forma similar al interfaz gr\u00e1fico. Teniendo Nifi arrancado, prueba con las siguientes URL: https://localhost:8443/nifi-api/access y https://localhost:8443/nifi-api/flow/about . M\u00e1s informaci\u00f3n en https://nifi.apache.org/docs/nifi-docs/rest-api/index.html Actividades \u00b6 Realiza los casos de uso del 5 al 7. En la entrega debes adjuntar una captura de pantalla donde se vea el flujo de datos completo con una nota con tu nombre, y adjuntar la plantilla de cada flujo. (opcional) Modifica el caso 7 para que los tweets que no est\u00e1n ni en castellano ni en ingl\u00e9s se inserten en una base de datos MySQL / MariaDB . Para ello, debes recoger la informaci\u00f3n de los atributos que hemos separado y generar un nuevo JSON con los datos que quieres almacenar ( id , text , created_at , rt y likes ) mediante el procesador AttributestoJSON . Una vez tengas el JSON, utiliza los procesadores ConvertJSONToSQL y ExecuteSQL para insertar los datos. Antes deber\u00e1s crear la tabla en la base de datos. Tienes un ejemplo parecido en el art\u00edculo Using Apache Nifi to Load Tweets from Twitter API to MemSQL . Adjunta capturas de pantalla de la configuraci\u00f3n de los procesadores que has a\u00f1adido, as\u00ed como de una consulta sobre la base de datos donde aparezcan mensajes insertados y la plantilla del caso de uso completo. Referencias \u00b6 Apache Nifi User Guide Apache Nifi in Depth Apache Nifi en TutorialsPoint Libro Data Engineering with Python Art\u00edculo de Futurespace: Flujo de extracci\u00f3n, validaci\u00f3n, transformaci\u00f3n y carga de ficheros (Caso de uso real)","title":"4.- Nifi II"},{"location":"apuntes/ingesta04nifi2.html#nifi-avanzado","text":"","title":"Nifi Avanzado"},{"location":"apuntes/ingesta04nifi2.html#grupos","text":"En Nifi s\u00f3lo hay un canvas de nivel superior, pero podemos construir tantos flujos l\u00f3gicos como deseemos. Normalmente, para organizar las flujos, se utilizan grupos de procesos , por lo que el canvas de nivel superior puede tener varios grupos de procesos, cada uno de los cuales representa un flujo l\u00f3gico, pero no necesariamente conectados entre s\u00ed. Trabajando con grupos Dentro de los grupos, para indicar como entran los datos se utiliza un Input port , el cual va a definir un puerto de entrada al grupo. Del mismo modo, para indicar c\u00f3mo salen los datos, se utiliza un Output port como puerto de salida para transferir la informaci\u00f3n fuera del grupo. As\u00ed pues, los grupos de procesos nos van a permitir refactorizar nuestros flujos para agruparlos y poder reutilizarlos en otros flujos, o bien mejorar la legibilidad de nuestro flujo de datos. En todo momento podemos ver el nivel en el que nos encontramos en la parte inferior izquierda, con una notaci\u00f3n similar a Nifi Flow >> Subnivel >> Otro Nivel .","title":"Grupos"},{"location":"apuntes/ingesta04nifi2.html#creando-un-grupo","text":"Vamos a partir de un ejemplo sencillo de leer un fichero de texto, partirlo en fragmentos y nos saque por el log alguno de sus atributos. Para ello, vamos a conectar un procesador GetFile con un SplitText y finalmente con LogAttribute . El procesador SplitText nos permite dividir cualquier flujo de texto en fragmentos a partir del n\u00famero de l\u00edneas que queramos (pudiendo tener encabezados, ignorar las l\u00edneas en blanco, etc...) Dividimos un archivo en fragmentos Para probarlo, podemos copiar cualquier archivo (ya sea el README o el NOTICE ) en la carpeta que hayamos indicado de entrada y comprobar el log. Una vez lo tenemos funcionando, vamos a colocar el procesador SplitText dentro de un grupo. As\u00ed pues, un grupo encapsula la l\u00f3gica de un procesador haci\u00e9ndolo funcionar como una caja negra. Para ello, desde la barra superior arrastramos el icono de Process Group y lo nombramos como grupo . Creaci\u00f3n de un grupo","title":"Creando un grupo"},{"location":"apuntes/ingesta04nifi2.html#puertos","text":"Una vez creado, vamos a copiar nuestro procesador SplitText . Pulsamos doble click sobre el grupo y bajaremos un nivel en el canvas para meternos dentro de Grupo . Una vez dentro, vamos a pegar el procesador y a\u00f1adiremos tanto un Input port (al que nombramos como entrada ) como un Output port (al que llamamos salida ). Una vez creados, los conectamos con el procesador con las mismas conexiones que antes. Grupo con puertos de entrada y salida Ahora salimos del grupo, y conectamos los procesadores del nivel superior con el grupo creado y comprobamos como sigue funcionando. Sustituimos el procesador por el grupo creado","title":"Puertos"},{"location":"apuntes/ingesta04nifi2.html#funnels","text":"Funnel Los funnels son un tipo de componente que permite trabajar en paralelo y despu\u00e9s unir los diferentes flujos en un \u00fanico flujo de ejecuci\u00f3n, adem\u00e1s de poder definir su propia prioridad de forma centralizada. Para ello vamos a poner varios GenerateFlowFile (4 en este caso) para mostrar sus datos mediante LogAttribute . Varios procesadores que apuntan a uno Si quisi\u00e9ramos cambiar el procesador de LogAttribute por otro tipo de procesador, deber\u00edamos borrar todas las conexiones y volver a conectarlo todo. Para evitar esto a\u00f1adimos un Funnel que va a centralizar todas las conexiones en un \u00fanico punto. El Funnel agrupa las conexiones","title":"Funnels"},{"location":"apuntes/ingesta04nifi2.html#plantillas","text":"Nifi permite trabajar con plantillas para poder reutilizar flujos de datos, as\u00ed como importar y exportar nuestras plantillas.","title":"Plantillas"},{"location":"apuntes/ingesta04nifi2.html#creando-plantillas","text":"Crear una plantilla es muy sencillo. Si partimos del ejemplo anterior, mediante shift y el rat\u00f3n, podemos seleccionar todos los elementos que queremos que formen parte de la plantilla. Una vez seleccionado, podemos utilizar el bot\u00f3n derecho o dentro del men\u00fa Operate , y elegir Create Template : Creaci\u00f3n de una plantilla Si queremos descargar una plantilla para usarla en otra instalaci\u00f3n, desde el men\u00fa de la esquina superior derecha, en la opci\u00f3n Templates , podemos consultar las plantillas que tenemos cargadas, y para cada una de ellas, tenemos la opci\u00f3n de descargarlas o eliminarlas. Descargando una plantilla","title":"Creando plantillas"},{"location":"apuntes/ingesta04nifi2.html#cargando-plantillas","text":"En cambio, para cargar una plantilla, desde el propio men\u00fa de Operate , el icono con la plantilla y la flecha hacia arriba, nos permitir\u00e1 elegir un archivo .xml con el c\u00f3digo de la plantilla. Una vez cargada, usaremos el control del menu superior para arrastrarla al \u00e1rea de trabajo. Una de las mayores ventajas es el uso de plantillas ya existentes. Existe una colecci\u00f3n de plantillas mantenida por Cloudera en https://github.com/hortonworks-gallery/nifi-templates . Se recomienda darle un ojo a la hoja de c\u00e1lculo que contiene un resumen de las plantillas compartidas. Otros ejemplos a destacar se encuentran en https://github.com/xmlking/nifi-examples . En nuestro caso, vamos a utilizar la plantilla de CSV-to-JSON , la cual podemos descargar desde https://raw.githubusercontent.com/hortonworks-gallery/nifi-templates/master/templates/csv-to-json-flow.xml . Una vez descargado el archivo xml, lo subimos a Nifi. Tras ello, arrastramos el componente y vemos su resultado: Cargando una plantilla","title":"Cargando plantillas"},{"location":"apuntes/ingesta04nifi2.html#caso-5-trabajando-con-conjuntos-de-registros","text":"En los casos anteriores, ya hemos visto que haciendo uso de ExtractText y AttributesToJSON pod\u00edamos crear ficheros JSON a partir de CSV. Nifi ofrece una forma m\u00e1s c\u00f3moda de realizar esto. Haciendo uso de los FF como registros y los procesadores de tipo Record (ya utilizamos alguno en el caso 3, en el que filtr\u00e1bamos mediante una sentencia SQL), vamos a poder trabajar con los datos como un conjunto de registros en vez de hacerlo de forma individual. Estos procesadores hacen que los flujos de construcci\u00f3n para manejar datos sean m\u00e1s sencillos, ya que que podemos construir procesadores que acepten cualquier formato de datos sin tener que preocuparnos por el an\u00e1lisis y la l\u00f3gica de serializaci\u00f3n. Otra gran ventaja de este enfoque es que podemos mantener los FF m\u00e1s grandes, cada uno de los cuales consta de m\u00faltiples registros, lo que resulta en un mejor rendimiento. Tenemos tres componentes a destacar: De lectura: AvroReader , CsvReader , ParquetReader , JsonPathReader , JsonTreeReader , ScriptedReader , ... De escritura: AvroRecordSetWriter , CsvRecordSetWriter , JsonRecordSetWriter , FreeFormTextRecordSetWriter , ScriptedRecordSetWriter , ... Procesador de registros: ConvertRecord : convierte entre formatos y/o esquemas similares. Por ejemplo, la conversi\u00f3n de CSV a Avro se puede realizar configurando ConvertRecord con un CsvReader y un AvroRecordSetWriter . Adem\u00e1s, la conversi\u00f3n de esquemas entre esquemas similares se puede realizar cuando el esquema de escritura es un subconjunto de los campos del esquema de lectura o si el esquema de escritura tiene campos adicionales con valores propuestos. LookupRecord : extrae uno o m\u00e1s campos de un registro y busca un valor para esos campos en un LookupService (ya sea a un fichero CSV, XML, accediendo a una base de datos o un servicio REST, etc...). Estos servicios funcionan como un mapa, de manera que reciben la clave y el servicio devuelve el valor. Puedes consultar m\u00e1s informaci\u00f3n en la serie de art\u00edculos Data flow enrichment with NiFi part: LookupRecord processor y un ejemplo completo en Enriching Records with LookupRecord & REST APIs in NiFi . QueryRecord : ejecuta una declaraci\u00f3n SQL contra los registros y escribe los resultados en el contenido del archivo de flujo. Este es el procesador que usamos en el caso 3 de la sesi\u00f3n anterior . ConsumeKafkaRecord_N_M : utiliza el Reader de registros configurado para deserializar los datos sin procesar recuperados de Kafka, y luego utiliza el Writer de registros configurado para serializar los registros al contenido del archivo de flujo. PublicarKafkaRecord_N_M : utiliza el Reader de registros configurado para leer el archivo de flujo entrante como registros, y luego utiliza el Writer de registros configurado para serializar cada registro para publicarlo en Kafka.","title":"Caso 5: Trabajando con conjuntos de registros"},{"location":"apuntes/ingesta04nifi2.html#convirtiendo-formatos","text":"As\u00ed pues, para demostrar su uso vamos a convertir el archivo CSV del caso 3 de la sesi\u00f3n anterior que contiene informaci\u00f3n sobre ventas a formato JSON. Podr\u00edamos utilizar simplemente un GetFile conectado a un ConvertRecord y este a un PutFile . Para que el fichero generado contenga como extensi\u00f3n el formato al que convertimos, antes de serializar los datos, a\u00f1adimos un procesador UpdateAttribute para modificar el nombre del fichero. El flujo de datos resultante ser\u00e1 similar a: Conversi\u00f3n de formato mediante ConvertRecord En concreto, en el caso del ConvertRecord , hemos utilizado los siguientes elementos: Configuraci\u00f3n de ConvertRecord Para el CSVReader , hemos de configurar el separador de campos con el ; e indicar que la primera fila contiene un encabezado. Para el JSONRecordSetWriter no hemos configurado nada.","title":"Convirtiendo formatos"},{"location":"apuntes/ingesta04nifi2.html#renombrando-el-destino","text":"Tal como hemos comentado, necesitamos renombrar el fichero de salida. Para ello, necesitamos hacer uso del procesador UpdateAttribute y utilizar el Nifi Expression Language para modificar la propiedad filename y recortar la extensi\u00f3n y concatenar con la nueva mediante la expresi\u00f3n ${filename:substringBefore('.csv')}.json : Modificando la extensi\u00f3n de filename","title":"Renombrando el destino"},{"location":"apuntes/ingesta04nifi2.html#caso-6-trabajando-con-elasticsearch","text":"En bloques anteriores ya hemos trabajado con Elasticsearch . En nuestro caso, tenemos la versi\u00f3n 7.16 descargada en la carpeta /opt/elasticsearch-7.16.0 de nuestra m\u00e1quina virtual. Elasticsearch+Nifi via Docker Si queremos utilizarlo mediante Docker , necesitamos que ElasticSearch y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml : docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi links : - elasticsearch elasticsearch : ports : - \"9200:9200\" - \"9300:9300\" environment : discovery.type : single-node image : docker.elastic.co/elasticsearch/elasticsearch:7.15.2 Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifielasticsearch up -d Recordad que necesitamos arrancarla mediante el comando ./bin/elasticsearch . Para comprobar que ha ido todo bien, podemos ejecutar la siguiente petici\u00f3n: curl -X GET 'localhost:9200/_cat/health?v=true&pretty' El procesador con el que vamos a trabajar es del tipo PutElasticSearchHttp , en el cual vamos a configurar: Elasticsearch URL : http://localhost:9200 (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: http://elasticsearch:9200 ) Index : aqu\u00ed vamos a poner como valor la palabra peliculas . marcamos la opci\u00f3n de autoterminar para las conexiones retry y failure . Una vez creado el procesador, vamos a alimentarlo a partir de los datos de un fichero JSON, mediante el procesador que ya conocemos GetFile . Pod\u00e9is descargar el fichero de pruebas movies.json y colocarlo en la carpeta donde hayamos configurado. Lectura de JSON e inserci\u00f3n en Elasticsearch Una vez ejecutado, para comprobar que se han introducido los datos podemos ejecutar la siguiente petici\u00f3n: curl -X GET \"localhost:9200/peliculas/_search?pretty\" Y veremos c\u00f3mo se han introducido en Elasticsearch : { \"took\" : 46 , \"timed_out\" : false , \"_shards\" : { \"total\" : 1 , \"successful\" : 1 , \"skipped\" : 0 , \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1 , \"relation\" : \"eq\" }, \"max_score\" : 1 , \"hits\" : [ { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"FTUyU30BBEE3YF7Zlgn1\" , \"_score\" : 1 , \"_source\" : { \"movies\" : [ { \"title\" : \"The Shawshank Redemption\" , \"rank\" : \"1\" , \"id\" : \"tt0111161\" }, Si nos fijamos bien, realmente solo ha insertado un documento que contiene un array de pel\u00edculas, lo cual no est\u00e1 bien.","title":"Caso 6: Trabajando con Elasticsearch"},{"location":"apuntes/ingesta04nifi2.html#separando-los-datos","text":"As\u00ed pues, previamente debemos separar el array contenido dentro de movies.json en documentos individuales. Borrar el \u00edndice Recuerda que antes de meter nuevos datos, necesitamos eliminar el \u00edndice de ElasticSearch mediante curl -X DELETE \"localhost:9200/peliculas\" . Para dividir el documento de pel\u00edculas en documentos individuales, necesitamos utilizar el procesador SplitJSON . Lo conectamos entre los dos procesadores anteriores, y le indicamos en la propiedad JSonPath Expression la expresi\u00f3n donde se encuentran las pel\u00edculas: $.movies.* Separamos las pel\u00edculas Y ahora si volvemos a consultar el \u00edndice mediante curl -X GET \"localhost:9200/peliculas/_search?pretty\" , s\u00ed que veremos que ha insertado las cien pel\u00edculas por separado: { \"took\" : 5 , \"timed_out\" : false , \"_shards\" : { \"total\" : 1 , \"successful\" : 1 , \"skipped\" : 0 , \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 100 , \"relation\" : \"eq\" }, \"max_score\" : 1.0 , \"hits\" : [ { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"X-uaWH0BesFhA-H6MMxA\" , \"_score\" : 1.0 , \"_source\" : { \"title\" : \"The Shawshank Redemption\" , \"rank\" : \"1\" , \"id\" : \"tt0111161\" } }, { \"_index\" : \"peliculas\" , \"_type\" : \"_doc\" , \"_id\" : \"YOuaWH0BesFhA-H6MMxC\" , \"_score\" : 1.0 , \"_source\" : { \"title\" : \"The Godfather\" , \"rank\" : \"2\" , \"id\" : \"tt0068646\" } },","title":"Separando los datos"},{"location":"apuntes/ingesta04nifi2.html#caso-7-de-twitter-a-elasticsearchmongodb","text":"Para este caso de uso, vamos a recoger datos de Twitter y los vamos a meter tanto en MongoDB como en ElasticSearch a la vez. El primer paso es obtener unas credenciales de desarrollador por parte de Twitter para poder acceder a su API. Para ello, en https://developer.twitter.com/ creamos una cuenta de desarrollador y creamos un proyecto. Claves necesarias para conectar con Twitter Nifi+Elasticsearch+MongDB via Docker Si queremos realizar este caso de uso mediante Docker, necesitamos que ElasticSearch, MongoDB y Nifi est\u00e9n dentro del mismo contenedor. Para ello, podemos configurarlo mediante el siguiente archivo docker-compose.yml : docker-compose.yml services : nifi : ports : - \"8443:8443\" image : apache/nifi:latest environment : SINGLE_USER_CREDENTIALS_USERNAME : nifi SINGLE_USER_CREDENTIALS_PASSWORD : nifinifinifi NIFI_JVM_HEAP_MAX : 2g links : - elasticsearch - mongodb elasticsearch : ports : - \"9200:9200\" - \"9300:9300\" environment : discovery.type : single-node image : docker.elastic.co/elasticsearch/elasticsearch:7.15.2 mongodb : ports : - \"27017:27017\" image : mongo:latest Una vez creado el archivo, construimos el contenedor mediante: docker-compose -p nifielasticsearchmongodb up -d","title":"Caso 7: de Twitter a Elasticsearch/MongoDB"},{"location":"apuntes/ingesta04nifi2.html#leyendo-tweets","text":"Vamos a recuperar cada 100 segundos los mensajes que contengan la palabra bigdata . Procesador GetTwitter - Twitter Elevated Para poder utilizar el procesador de Nifi GetTwitter es necesario acceder al Twitter API v1.1 , la cual solo est\u00e1 disponible para las cuentas con un nivel Elevated . Por ello, en vez de utilizar las credenciales habituales (API Key, API Key Secret, Access Token y Access Token Secret), vamos a realizar el acceso mediante una petici\u00f3n HTTP utilizando un token de validaci\u00f3n, conocido como Bearer Token (lo cual es una mejor pr\u00e1ctica de seguridad). Para ello, usamos un procesador InvokeHTTP y configuramos los siguientes par\u00e1metros: En la planificaci\u00f3n, vamos a configurar que se realice una petici\u00f3n cada 100 segundos, configurando Run Schedule a 100s . HTTP Method : GET Remote UR L: https://api.twitter.com/2/tweets/search/recent?query=bigdata&tweet.fields=created_at,lang,public_metrics Mediante el par\u00e1metro query indicamos el t\u00e9rmino a buscar. En nuestro caso, buscamos la palabra bigdata . Mediante el par\u00e1metro tweet.field indicamos los campos a recuperar (por defecto recupera el id y el texto de cada tweet ) Puedes comprobar todos los campos que podemos obtener de los mensajes en https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet A\u00f1adimos una propiedad (mediante el icono del signo + ) que nombramos como Authorization y le asignamos la palabra Bearer y el token que copiamos desde la administraci\u00f3n de Twitter: Bearer AAAAAAAAAAAAAAAAAAAAAIGNWAEAAAAA... Caso 7: Comprobando la lectura de mensajes de Twitter Tras realizar una petici\u00f3n, obtendremos un FF similar a la siguiente informaci\u00f3n (he dejado s\u00f3lo dos mensajes para facilitar la visualizaci\u00f3n): { \"data\" :[ { \"id\" : \"1468197767885561856\" , \"text\" : \"RT @CatherineAdenle: Machine Learning Algorithms in Python You Must Learn \\n#DataScience #MachineLearning\\n#BigData #Analytics #AI #Tech #Alg\u2026\" , \"created_at\" : \"2021-12-07T12:36:40.000Z\" , \"public_metrics\" :{ \"retweet_count\" : 22 , \"reply_count\" : 0 , \"like_count\" : 0 , \"quote_count\" : 0 }, \"lang\" : \"en\" }, ... { \"id\" : \"1468197570925277190\" , \"text\" : \"RT @gp_pulipaka: A Quick Intro to Deep Learning Course! #BigData #Analytics #DataScience #AI #MachineLearning #IoT #IIoT #Python #RStats #T\u2026\" , \"created_at\" : \"2021-12-07T12:35:53.000Z\" , \"public_metrics\" :{ \"retweet_count\" : 60 , \"reply_count\" : 0 , \"like_count\" : 0 , \"quote_count\" : 0 }, \"lang\" : \"en\" } ], \"meta\" :{ \"newest_id\" : \"1468197767885561856\" , \"oldest_id\" : \"1468197570925277190\" , \"result_count\" : 10 , \"next_token\" : \"b26v89c19zqg8o3fpdy8xh0ikrj9fhq1nywqt95oqkxh9\" } } A continuaci\u00f3n, vamos a separar los mensajes en diferentes FF del mismo modo que acabamos de hacer en el ejercicio anterior. As\u00ed pues, a\u00f1adimos el procesador SplitJson y configuramos la divisi\u00f3n de los mensajes mediante expresi\u00f3n con $.data.* .","title":"Leyendo tweets"},{"location":"apuntes/ingesta04nifi2.html#evaluando-el-idioma","text":"Con esta informaci\u00f3n, hemos decidido enviar todos los mensajes que vengan en ingl\u00e9s ( \"lang\": \"en\" ) a ElasticSearch , y los que vengan en castellano a MongoDB. Mediante el procesador EvaluateJsonPath , vamos a colocar en atributos la siguiente informaci\u00f3n que queremos almacenar, definiendo la propiedad Destination como flow-attribute : Para ello creamos los siguientes atributos con la expresi\u00f3n para acceder al campo JSON asociado: twitter.id: $.id twitter.text: $.text twitter.lang: $.lang twitter.created_at: $.created_at twitter.rt: $.public_metrics.retweet_count twitter.likes: $.public_metrics.like_count Caso 7: Creamos atributos con informaci\u00f3n de los tweets A continuaci\u00f3n, conectamos con el procesador RouteOnAttribute . Caso 7: Conexiones para enrutar seg\u00fan el idioma Dentro del procesador RouteOnAttribute , creamos dos propiedas para enrutar los FF seg\u00fan el valor del atributo twitter.lang : lang-en : ${twitter.lang:equals(\"en\")} lang-es : ${twitter.lang:equals(\"es\")} Caso 7: Enrutamos seg\u00fan el atributo twitter.lang","title":"Evaluando el idioma"},{"location":"apuntes/ingesta04nifi2.html#guardando-mensajes-en-ingles-en-elasticsearch","text":"Tal como acabamos de hacer, s\u00f3lo tenemos que a\u00f1adir el procesador PutElasticsearchHttp y configurar las siguientes propiedades: Elasticsearch URL : http://localhost:9200 (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: http://elasticsearch:9200 ) Index : aqu\u00ed vamos a poner como valor la palabra tweets . marcamos la opci\u00f3n de autoterminar para las conexiones retry y failure . A continuaci\u00f3n, lo conectamos mediante la conexi\u00f3n lang-en que sale del procesador anterior. Para comprobar que los datos se est\u00e1n insertando correctamente, podemos hacer una petici\u00f3n a: curl -X GET \"localhost:9200/tweets/_search?pretty\"","title":"Guardando mensajes en ingl\u00e9s en ElasticSearch"},{"location":"apuntes/ingesta04nifi2.html#guardando-mensajes-en-castellano-en-mongodb","text":"De forma similar, agregamos el procesador PutMongo y configuramos las siguientes propiedades: Mongo URI: mongodb://localhost (en el caso de usar Docker , deber\u00e1s cambiar localhost por el nombre del servicio: mongodb://mongodb ) Mongo Database Name: iabd Mongo Collection Name: tweets Tras ello, lo conectamos mediante la conexi\u00f3n lang-es (y si queremos, tambi\u00e9n podemos a\u00f1adir la conexi\u00f3n unmatched de manera que almacener\u00e1 los tambi\u00e9n los mensajes que no est\u00e9n ni en ingl\u00e9s ni en espa\u00f1ol. Cuando Twitter no reconoce el lenguaje del mensaje, le asigna como lenguaje undetermined mediante el c\u00f3digo und ). Para comprobar que los datos se est\u00e1n insertando correctamente, una vez conectados a mongo , podemos realizar la siguiente consulta: use iabd ; db . tweets . find (); As\u00ed pues, el flujo de datos queda tal que as\u00ed: Caso 7: Flujo de datos con ElasticSearch y MongoDB Renombrando id Si queremos que MongoDB utilice el id del tweet como la clave de los documentos en MongoDB y as\u00ed asegurar que no tenemos mensajes repetidos, debemos renombrar el campo a _id . Para ello, podemos utilizar el procesador UpdateAttribute para renombrar twitter.id a _id y luego el procesador AttritubesToJSON para generar la informaci\u00f3n a almacenar. REST API Nifi ofrece un API REST con el cual podemos interactuar de forma similar al interfaz gr\u00e1fico. Teniendo Nifi arrancado, prueba con las siguientes URL: https://localhost:8443/nifi-api/access y https://localhost:8443/nifi-api/flow/about . M\u00e1s informaci\u00f3n en https://nifi.apache.org/docs/nifi-docs/rest-api/index.html","title":"Guardando mensajes en castellano en MongoDB"},{"location":"apuntes/ingesta04nifi2.html#actividades","text":"Realiza los casos de uso del 5 al 7. En la entrega debes adjuntar una captura de pantalla donde se vea el flujo de datos completo con una nota con tu nombre, y adjuntar la plantilla de cada flujo. (opcional) Modifica el caso 7 para que los tweets que no est\u00e1n ni en castellano ni en ingl\u00e9s se inserten en una base de datos MySQL / MariaDB . Para ello, debes recoger la informaci\u00f3n de los atributos que hemos separado y generar un nuevo JSON con los datos que quieres almacenar ( id , text , created_at , rt y likes ) mediante el procesador AttributestoJSON . Una vez tengas el JSON, utiliza los procesadores ConvertJSONToSQL y ExecuteSQL para insertar los datos. Antes deber\u00e1s crear la tabla en la base de datos. Tienes un ejemplo parecido en el art\u00edculo Using Apache Nifi to Load Tweets from Twitter API to MemSQL . Adjunta capturas de pantalla de la configuraci\u00f3n de los procesadores que has a\u00f1adido, as\u00ed como de una consulta sobre la base de datos donde aparezcan mensajes insertados y la plantilla del caso de uso completo.","title":"Actividades"},{"location":"apuntes/ingesta04nifi2.html#referencias","text":"Apache Nifi User Guide Apache Nifi in Depth Apache Nifi en TutorialsPoint Libro Data Engineering with Python Art\u00edculo de Futurespace: Flujo de extracci\u00f3n, validaci\u00f3n, transformaci\u00f3n y carga de ficheros (Caso de uso real)","title":"Referencias"},{"location":"apuntes/ingesta05python.html","text":"Python y AWS \u00b6 En esta sesi\u00f3n vamos a estudiar c\u00f3mo acceder a los servicios de AWS relacionados con el Big Data estudiados previamente, y mediante diversos casos de uso crear diferentes flujos de datos. Para los siguientes casos de uso, realizaremos los 5 primeros desde nuestro sistema local y el caso final haciendo uso AWS Lambda . Para autenticarnos en AWS desde nuestro sistema local, recuerda que necesitas copiar las credenciales de acceso en ~/.aws/credentials o mediante las variables de entorno . SDK Boto3 \u00b6 Para acceder a AWS desde Python , Amazon ofrece el SDK Boto3 . Para poder utilizarlo, la instalaremos mediante pip install boto3 Pod\u00e9is consultar toda la informaci\u00f3n relativa a Boto3 en su documentaci\u00f3n oficial en https://boto3.amazonaws.com/v1/documentation/api/latest/index.html Existen dos posibilidades para acceder a AWS mediante Boto3: Recursos : representan un interfaz orientado a objetos de AWS, de manera que cada recurso contendr\u00e1 un identificador, unos atributos y un conjunto de operaciones. Un ejemplo de recurso es el S3 . M\u00e1s informaci\u00f3n sobre recursos en la documentaci\u00f3n oficial . Clientes : ofrecen un interfaz de bajo nivel que se mapea 1:1 con el API de cada servicio. Los clientes se generan a partir de la definici\u00f3n JSON del servicio. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial . En resumen, los recursos son una abstracci\u00f3n a m\u00e1s alto nivel de los servicios AWS que los clientes. Se recomienda el uso de los recursos al no tener que preocuparse de c\u00f3mo se realiza por debajo la comunicaci\u00f3n e interacci\u00f3n con los servicios. Sin embargo, a d\u00eda de hoy no hay recursos para todos los servicios AWS, y por ello, en ocasiones no queda otra opci\u00f3n que utilizar los clientes. Para demostrar las diferencias, vamos a ver c\u00f3mo podemos realizar algunas operaciones haciendo uso del cliente o del recurso (en estos ejemplos nos vamos a centrar en el servicio S3 - https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html .): Buckets de un usuario Elementos de un bucket Creaci\u00f3n de un bucket Eliminaci\u00f3n de un recurso/bucket s3-buckets.py import boto3 # Opci\u00f3n 1 print ( 'Buckets mediante resource:' ) s3resource = boto3 . resource ( 's3' , region_name = 'us-east-1' ) buckets = s3resource . buckets . all () for bucket in buckets : print ( f ' \\t { bucket . name } ' ) # Opci\u00f3n 2 print ( 'Buckets mediante el cliente:' ) s3client = boto3 . client ( 's3' ) response = s3client . list_buckets () for bucket in response [ 'Buckets' ]: print ( f ' \\t { bucket [ \"Name\" ] } ' ) s3-bucket-objects.py import boto3 s3 = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3 . Bucket ( 's3severo2122python' ) for obj in bucket . objects . all (): print ( obj . key ) s3-create-bucket.py import boto3 # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2122python-r' ) bucket . create () # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) response = s3c . create_bucket ( Bucket = 's3severo2122python' ) s3-delete.py import boto3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2122python' ) # Elimina todos los objetos del bucket bucket . objects . delete () # Elimina el bucket bucket . delete () Caso de uso 1: Comunicaci\u00f3n con S3 \u00b6 Vamos a trabajar con el archivo datosPeliculas.json el cual contiene un listado de pel\u00edculas con las que trabajaremos en los siguientes casos de uso. Primero vamos a ver c\u00f3mo podemos subir el archivo a S3 mediante Python: upload-movies-s3.py import boto3 ficheroUpload = \"datosPeliculas.json\" nombreBucket = \"s3severo2122python\" # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) # 1.1 mediante upload_file bucket = s3r . Object ( nombreBucket , 'datosSubidosR1.txt' ) bucket . upload_file ( ficheroUpload ) # 1.2 mediante put object = s3r . Object ( nombreBucket , 'datosSubidosR2.txt' ) object . put ( Body = b 'Ejemplo de datos binarios' ) # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) # 2.1 mediante upload_file response = s3c . upload_file ( ficheroUpload , nombreBucket , \"datosSubidosC1.json\" ) # 2.2 mediante upload_fileobj with open ( ficheroUpload , \"rb\" ) as f : s3c . upload_fileobj ( f , nombreBucket , \"datosSubidosC2.json\" ) # Cliente: Ejemplo de como crear un objeto y a\u00f1adirle contenido desde Python s3c . put_object ( Body = b 'Ejemplo de datos binarios' , Bucket = nombreBucket , Key = \"datosSubidosC3\" ) Si lo que queremos es descargar un recurso de S3 para tenerlo en nuestro sistema local haremos: import boto3 # Opci\u00f3n 1 - descarga s3c = boto3 . client ( 's3' ) s3c . download_file ( 's3severo2122python' , 'datosPeliculas.json' , 'datosDescargados.json' ) # Opci\u00f3n 2 - Abrimos el fichero y metemos el contenido with open ( 'fichero.json' , 'wb' ) as f : s3c . download_fileobj ( 's3severo2122python' , 'datosPeliculas.json' , f ) Caso de uso 2: Cargar datos en DynamoDB \u00b6 Vamos a cargar un listado de pel\u00edculas en DynamoDB . El primer paso es elegir las claves de particionado y ordenaci\u00f3n. El archivo datosPeliculas.json contiene el siguiente contenido: [ { \"year\" : 2013 , \"title\" : \"Rush\" , \"info\" : { \"directors\" : [ \"Ron Howard\" ], \"release_date\" : \"2013-09-02T00:00:00Z\" , \"rating\" : 8.3 , \"genres\" : [ \"Action\" , \"Biography\" , \"Drama\" , \"Sport\" ], \"image_url\" : \"http://ia.media-imdb.com/images/M/MV5BMTQyMDE0MTY0OV5BMl5BanBnXkFtZTcwMjI2OTI0OQ@@._V1_SX400_.jpg\" , \"plot\" : \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\" , \"rank\" : 2 , \"running_time_secs\" : 7380 , \"actors\" : [ \"Daniel Bruhl\" , \"Chris Hemsworth\" , \"Olivia Wilde\" ] } }, ] Como los a\u00f1os de las pel\u00edculas permiten particionar de manera m\u00e1s o menos equilibrada los datos, en la mejor candidata para clave de particionado. Como s\u00ed que habr\u00e1 varias pel\u00edculas en el mismo a\u00f1o, elegimos el t\u00edtulo como clave de ordenaci\u00f3n, provocando que los documentos tengan una clave compuesta. As\u00ed pues, vamos a nombrar nuestra tabla como SeveroPeliculas y ponemos como clave de partici\u00f3n el atributo year de tipo num\u00e9rico, y como clave de ordenaci\u00f3n title de tipo cadena. Creaci\u00f3n de la tabla SeveroPeliculas Una vez creada la tabla, vamos a ver c\u00f3mo podemos cargar los datos. Haciendo uso de la librer\u00eda boto3 vamos a crear el archivo cargarDatosPeliculas.py : cargarDatosPeliculas.py import boto3 import json import decimal dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) # (1) tabla = dynamodb . Table ( 'SeveroPeliculas' ) with open ( \"datosPeliculas.json\" ) as ficheroJSON : peliculas = json . load ( ficheroJSON , parse_float = decimal . Decimal ) for pelicula in peliculas : year = int ( movie [ 'year' ]) title = movie [ 'title' ] info = movie [ 'info' ] print ( \"A\u00f1adida pel\u00edcula:\" , year , title ) tabla . put_item ( Item = { 'year' : year , 'title' : title , 'info' : info , } ) Nos conectamos a la regi\u00f3n e indicamos que vamos a utilizar el servicio de DynamoDB Si lo ejecutamos desde nuestro ordenador, nos aparecer\u00e1 por la consola cada una de las pel\u00edculas insertadas. Float y boto3 Mucho cuidado con boto3 y DynamoDB, ya que los tipos Float no est\u00e1n soportados, y en cambio, hemos de utilizar el tipo Decimal . Faker \u00b6 Si necesitamos escribir muchos datos, es muy \u00fatil emplear una librer\u00eda como Faker para generar los datos. Primero hemos de instalarla mediante pip: pip3 install faker Vamos a realizar un ejemplo para mostrar algunos datos aleatorios y comprobar su funcionamiento: C\u00f3digo Resultado holaFaker.py from faker import Faker fake = Faker () fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol print ( \"Nombre:\" , fake . name ()) print ( \"Direcci\u00f3n:\" , fake . address ()) print ( \"Nombre de hombre:\" , fake . first_name_male ()) print ( \"N\u00famero de tel\u00e9fono:\" , fake . phone_number ()) print ( \"Color:\" , fake . color_name ()) print ( \"Fecha:\" , fake . date ()) print ( \"Email:\" , fake . email ()) print ( \"Frase de 10 palabras\" , fake . sentence ( nb_words = 10 )) Nombre: Dani Pla Chico Direcci\u00f3n: Cuesta de Emiliano Milla 66 Albacete, 83227 Nombre de hombre: Mat\u00edas N\u00famero de tel\u00e9fono: +34 818 779 827 Color: Salm\u00f3n oscuro Fecha: 1984-09-29 Email: btome@example.net Frase de 10 palabras Perferendis saepe consequatur saepe sapiente est impedit eaque omnis temporibus excepturi repellat ducimus. Los diferentes grupos de datos que genera se agrupan en Providers : de direcci\u00f3n, fechas, relacionados con internet, bancarios, c\u00f3digos de barra, isbn, etc... Se recomienda consultar la documentaci\u00f3n en https://faker.readthedocs.io/en/master/providers.html . Locale ES Al trabajar con el idioma en espa\u00f1ol, puede que algunos m\u00e9todos no funcionen (m\u00e1s que no funcionar, posiblemente tengan otro nombre). Es recomendable comprobar las opciones disponibles en https://faker.readthedocs.io/en/master/locales/es_ES.html Generando CSV \u00b6 Vamos a generar un CSV con datos de 1000 personas. Primero creamos una lista con los encabezados y los escribimos en el fichero, para posteriormente, l\u00ednea a l\u00ednea, generar los datos de cada persona: C\u00f3digo Resultado generaCSV.py from faker import Faker import csv output = open ( 'datosFaker.csv' , 'w' ) fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol header = [ 'nombre' , 'edad' , 'calle' , 'ciudad' , 'provincia' , 'cp' , 'longitud' , 'latitud' ] mywriter = csv . writer ( output ) mywriter . writerow ( header ) for r in range ( 1000 ): mywriter . writerow ([ fake . name (), fake . random_int ( min = 18 , max = 80 , step = 1 ), fake . street_address (), fake . city (), fake . state (), fake . postcode (), fake . longitude (), fake . latitude ()]) output . close () datosFaker.csv nombre,edad,calle,ciudad,provincia,cp,longitud,latitud Jenaro Verd\u00fa Suarez,26,Urbanizaci\u00f3n Mohamed Vall\u00e9s 122,Sevilla,Guip\u00fazcoa,73198,2.657719,-69.679293 Eugenio Calzada Revilla,57,Camino Vanesa Amor 36 Piso 9 ,Huesca,\u00c1lava,75590,34.041399,-52.924628 Flavio del Lumbreras,76,Avenida de Beatriz Amaya 92,Ciudad,Murcia,86420,58.248903,-17.924926 Generando JSON \u00b6 Y a continuaci\u00f3n repetimos el mismo ejemplo, pero ahora generando un documento JSON. La principal diferencia es que primero vamos a rellenar un diccionario con toda la informaci\u00f3n, y luego persistimos el diccionario: C\u00f3digo Resultado generaJSON.py from faker import Faker import json fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol # Preparamos los datos datos = {} datos [ 'registros' ] = [] for x in range ( 1000 ): persona = { \"datos\" : fake . name (), \"edad\" : fake . random_int ( min = 18 , max = 80 , step = 1 ), \"calle\" : fake . street_address (), \"ciudad\" : fake . city (), \"provincia\" : fake . state (), \"cp\" : fake . postcode (), \"longitud\" : float ( fake . longitude ()), \"latitud\" : float ( fake . latitude ())} datos [ 'registros' ] . append ( persona ) # Los metemos en el fichero output = open ( 'datosFaker.json' , 'w' ) json . dump ( datos , output ) datosFaker.json { \"registros\" : [ { \"datos\" : \"Merche Moreno Roman\" , \"edad\" : 51 , \"calle\" : \"Paseo Amelia Folch 967\" , \"ciudad\" : \"Segovia\" , \"provincia\" : \"M\\u00e1laga\" , \"cp\" : \"71721\" , \"longitud\" : 84.603801 , \"latitud\" : 58.941349 }, { \"datos\" : \"Miguel Abascal Sanz\" , \"edad\" : 21 , Caso de uso 3 - Consultar datos en DynamoDB \u00b6 Una vez tenemos nuestra tabla de DynamoDB cargada con datos, llega el momento de recuperar los datos, ya sea un registro en concreto o la posibilidad de realizar una consulta, ya sea por su \u00edndice o su clave de ordenaci\u00f3n (o ambas). En la sesi\u00f3n que trabajamos con DynamoDB estudiamos que pod\u00edamos realizar consultas sobre el almac\u00e9n NoSQL haciendo uso de un subconjunto de SQL conocido como PartiQL . En los siguientes ejemplos vamos a mostrar c\u00f3mo realizar las operaciones v\u00eda el API de DynamoDb y mediante PartiQL. Si queremos recuperar la pel\u00edcula Interstellar de 2014 haremos: Mediante get_item Mediante get_item con excepciones Mediante PartiQL dynamodb_getitem.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) title = \"Interstellar\" year = 2014 response = tabla . get_item ( Key = { 'year' : year , 'title' : title }, ProjectionExpression = \"title, info.plot\" ) item = response [ 'Item' ] print ( item ) dynamodb_getitem_exc.py import boto3 from botocore.exceptions import ClientError dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) title = \"Interstellar\" year = 2014 # Recupera una pel\u00edcula print ( \"------ Datos de Interstellar\" ) try : response = tabla . get_item ( Key = { 'year' : year , 'title' : title }, ProjectionExpression = \"title, info.plot\" ) except ClientError as e : print ( e . response [ 'Error' ][ 'Message' ]) else : item = response [ 'Item' ] print ( item ) dynamodb_select.py import boto3 clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) # Recupera una pel\u00edcula con PartiQL print ( \"------ Datos de Interstellar mediante PartiQL\" ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, info.plot FROM SeveroPeliculas WHERE year = 2014 and title='Interstellar'\" ) item = resp [ 'Items' ][ 0 ] print ( item ) title = \"Interstellar\" year = 2014 # Recupera una pel\u00edcula con PartiQL con par\u00e1metros indicados mediante ? print ( \"------ Datos de Interstellar mediante PartiQL con par\u00e1metros\" ) resp = clientDDB . execute_statement ( Statement = 'SELECT * FROM SeveroPeliculas WHERE year = ? AND title = ?' , Parameters = [{ 'N' : str ( year )}, { 'S' : title }]) item = resp [ 'Items' ][ 0 ] print ( item ) En el caso de las consultas mediante PartiQL haciendo uso de execute_statement conviene destacar que: Las consultas son case sensitive . Los par\u00e1metros se indican mediante ? Los contenidos de los par\u00e1metros se indican mediante una lista con un diccionario por cada par\u00e1metro donde la clave es el tipo del par\u00e1metro, y el valor es el dato a pasar (el dato se pasa siempre como un string ) Las consultas siempre devuelven un diccionario con una propiedad Items que contiene los resultados devueltos. Destacar que es diferente la estructura del resultado de realizar una consulta mediante el API de DynamoDB (respeta la estructura definida en la base de datos) o mediante PartiQL (crea un atributo por columna recuperada cuyo valor contiene el tipo del dato): Resultado de get-item Resultado de PartiQL { 'i nf o' : { 'plo t ' : 'A group o f explorers make use o f a ne wly discovered wormhole t o surpass t he limi tat io ns o n huma n space tra vel a n d co n quer t he vas t dis tan ces i n volved i n a n i nterstellar voyage.' }, ' t i tle ' : 'I nterstellar ' } { ' t i tle ' : { 'S' : 'I nterstellar ' }, 'plo t ' : { 'S' : 'A group o f explorers make use o f a ne wly discovered wormhole t o surpass t he limi tat io ns o n huma n space tra vel a n d co n quer t he vas t dis tan ces i n volved i n a n i nterstellar voyage.' } } Tambi\u00e9n podemos realizar otro tipo de consultas: Pel\u00edculas de 2016 mediante query Pel\u00edculas cuyo t\u00edtulo est\u00e9 entre la A y la L import boto3 from boto3.dynamodb.conditions import Key # Mediante query dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de 2016\" ) resp = tabla . query ( KeyConditionExpression = Key ( 'year' ) . eq ( 2016 )) for i in resp [ 'Items' ]: print ( i [ 'year' ], \":\" , i [ 'title' ]) # Mediante PartiQL print ( \"-----Pel\u00edculas de 2016 con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, year FROM SeveroPeliculas WHERE year = 2016\" ) for i in resp [ 'Items' ]: print ( i [ 'year' ][ 'N' ], \":\" , i [ 'title' ][ 'S' ]) import boto3 import json import decimal from boto3.dynamodb.conditions import Key class DecimalEncoder ( json . JSONEncoder ): def default ( self , o ): if isinstance ( o , decimal . Decimal ): if o % 1 > 0 : return float ( o ) else : return int ( o ) return super ( DecimalEncoder , self ) . default ( o ) anyo = 2016 letraInicial = \"A\" letraFinal = \"F\" dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de 2016 cuyo t\u00edtulo empieza desde A hasta L\" ) resp = tabla . query ( ProjectionExpression = \"#yr, title, info.genres, info.actors[0]\" , # year es una palabra clave, por tanto necesitamos crear un alias ExpressionAttributeNames = { \"#yr\" : \"year\" }, KeyConditionExpression = Key ( 'year' ) . eq ( anyo ) & Key ( 'title' ) . between ( letraInicial , letraFinal ) ) for i in resp [ 'Items' ]: print ( i ) # {'info': {'actors': ['Zoe Saldana'], 'genres': ['Action', 'Adventure', 'Fantasy', 'Sci-Fi']}, 'year': Decimal('2016'), 'title': 'Avatar 2'} # Transforma los valores num\u00e9ricos de Decimal a Number print ( json . dumps ( i , cls = DecimalEncoder )) # {\"info\": {\"actors\": [\"Zoe Saldana\"], \"genres\": [\"Action\", \"Adventure\", \"Fantasy\", \"Sci-Fi\"]}, \"year\": 2016, \"title\": \"Avatar 2\"} for genero in i [ 'info' ][ 'genres' ]: print ( genero ) print ( \"-----Pel\u00edculas de 2016 cuyo t\u00edtulo empieza desde A hasta L con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) consulta = 'SELECT year, title, info.genres, info.actors[0] FROM SeveroPeliculas WHERE year = ? AND title between ? and ?' resp = clientDDB . execute_statement ( Statement = consulta , Parameters = [{ 'N' : str ( anyo )}, { 'S' : letraInicial }, { 'S' : letraFinal }]) for i in resp [ 'Items' ]: print ( i ) # [{'year': {'N': '2016'}, 'title': {'S': 'Avatar 2'}, 'actors[0]': {'S': 'Zoe Saldana'}, 'genres': {'L': [{'S': 'Action'}, {'S': 'Adventure'}, {'S': 'Fantasy'}, {'S': 'Sci-Fi'}]}}] for genero in i [ 'genres' ][ 'L' ]: print ( genero [ 'S' ]) La clase DecimalEncoder se utiliza para transformar los campos Decimal que utiliza DynamoDB para almacenar contenido num\u00e9rico a tipo entero o flotante seg\u00fan necesite. Full scan \u00b6 Cuando en PartiQL no le indicamos en la condici\u00f3n una expresi\u00f3n que busque por una de las claves, se realizar\u00e1 un full scan sobre toda la tabla, lo que puede implicar unos costes inesperados, tanto econ\u00f3micos como a nivel rendimiento provisionado. El m\u00e9todo scan lee cada elemento de la tabla y devuelve todos los datos de la tabla. Se le puede pasar una filter_expression opcional para que s\u00f3lo devuelva los elementos que cumplan el criterio. Sin embargo, el filtrado se aplica tras escanear toda la tabla. Ejemplo scan import boto3 import json import decimal from boto3.dynamodb.conditions import Key class DecimalEncoder ( json . JSONEncoder ): def default ( self , o ): if isinstance ( o , decimal . Decimal ): if o % 1 > 0 : return float ( o ) else : return int ( o ) return super ( DecimalEncoder , self ) . default ( o ) dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de sobresaliente mediante full scan\" ) # Escaneo y filtrado fe = Key ( 'info.rating' ) . gte ( 9 ) pe = \"#yr, title, info.rating\" ean = { \"#yr\" : \"year\" } resp = tabla . scan ( FilterExpression = fe , ProjectionExpression = pe , ExpressionAttributeNames = ean ) for i in resp [ 'Items' ]: print ( json . dumps ( i , cls = DecimalEncoder )) Full scan con PartiQL import boto3 from boto3.dynamodb.conditions import Key print ( \"-----Pel\u00edculas de 2016 con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, year, info.rating FROM SeveroPeliculas WHERE info.rating >= 9\" ) for i in resp [ 'Items' ]: print ( i [ 'year' ][ 'N' ], \":\" , i [ 'title' ][ 'S' ]) Caso de uso 4 - De S3 a DynamoDB \u00b6 En este caso, vamos a coger datos de pel\u00edculas de un dataset p\u00fablico disponible en https://www.kaggle.com/sankha1998/tmdb-top-10000-popular-movies-dataset El contenido del dataset es similar a: TMDb_updated.CSV ,title,overview,original_language,vote_count,vote_average 0,Ad Astra,\"The near future...\",en,2853,5.9 1,Bloodshot,\"After he ...\",en,1349,7.2 2,Bad Boys for Life,\"Marcus and Mike ...\",en,2530,7.1 Una vez descargado TMDb_updated.CSV , vamos a cargar la informaci\u00f3n en S3 dentro del bucket. Para este caso, en vez de cargar todos los datos desde el dataset en nuestra tabla NoSQL, vamos a meter en DynamoDB el t\u00edtulo, la nota media y la trama siempre y cuando hayan recibido al menos 10.000 votos. S3Select \u00b6 Para realizar esta consulta desde Python para poder automatizar el proceso ETL utilizaremos S3Select para recuperar el subconjunto de los datos. S3Select vs AWS Athena Este tipo de procesamiento es m\u00e1s c\u00f3modo realizarlo mediante AWS Athena, el cual s\u00ed que permite realizar join entre diferentes datasets. S3Select s\u00f3lo permite trabajar con una \u00fanica tabla. Para ello, mediante S3Select ejecutaremos la consulta SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000 y almacenaremos el resultado en un nuevo CSV dentro del mismo bucket: s3select.py import boto3 s3 = boto3 . client ( 's3' ) # 1.- Realizamos la consulta mediante S3Select resp = s3 . select_object_content ( Bucket = 's3severo2122python' , Key = 'TMDb_updated.CSV' , ExpressionType = 'SQL' , Expression = \"SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000\" , InputSerialization = { 'CSV' : { \"FileHeaderInfo\" : \"USE\" , 'AllowQuotedRecordDelimiter' : True }, 'CompressionType' : 'NONE' }, OutputSerialization = { 'CSV' : {}}, ) # 2.- Unimos los datos que vamos recibiendo en streaming registros = [ \"title,overview,vote_count,vote_average \\n \" ] for evento in resp [ 'Payload' ]: if 'Records' in evento : registros . append ( evento [ 'Records' ][ 'Payload' ] . decode ()) # 3.- Generamos el contenido en un String file_str = '' . join ( registros ) # 4.- Creamos un nuevo objeto en S3 s3 . put_object ( Body = file_str , Bucket = 's3severo2122python' , Key = \"TMDb_filtered.CSV\" ) De S3 a DynamoDB \u00b6 Una vez creado el fichero en S3, vamos cargar los datos en DynamoDB. Como el dataset no conten\u00eda la fecha de la pel\u00edcula, en nuestro caso le vamos a poner a todas las pel\u00edculas que son del 2022: import boto3 import pandas as pd from decimal import Decimal # 1.- Leemos el fichero desde S3 y lo metemos en un DataFrame s3c = boto3 . client ( 's3' ) bucketNombre = \"s3severo2122python\" ficheroNombre = \"TMDb_filtered.CSV\" response = s3c . get_object ( Bucket = bucketNombre , Key = ficheroNombre ) movies_df = pd . read_csv ( response [ 'Body' ], delimiter = ',' ) # 2.- Nos conectamos a DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) # 3.- Lo metemos en DynamoDB mediante un batch with tabla . batch_writer () as batch : for index , fila in movies_df . iterrows (): Item = { 'year' : 2022 , 'title' : str ( fila [ 'title' ]), 'info' : { 'plot' : fila [ 'overview' ], 'rating' : Decimal ( fila [ 'vote_average' ]) . quantize ( Decimal ( '1.00' )) } } batch . put_item ( Item = Item ) Caso de uso 5 - Desde RDS \u00b6 Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, bien sea mediante DBeaver o si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Vamos a utilizar la instancia de base de datos iabd que tenemos en RDS con la base de datos retail_db . MariaDB/MySQL y Python \u00b6 Para acceder a la base de datos desde Python necesitamos instalar la librer\u00eda correspondiente: pip3 install mariadb pip3 install mysql-connector-python Todo el c\u00f3digo a continuaci\u00f3n se basa en MariaDB como sistema gestor de base de datos. Si queremos conectarnos, debemos indicar los datos de conexion: import mariadb import sys try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaD: { e } \" ) sys . exit ( 1 ) # Obtenemos el cursor cur = conn . cursor () Una vez nos hemos conectado y tenemos abierto un cursor, ya podemos hacer consultas y recuperar datos. Por ejemplo, para recuperar toda la informaci\u00f3n de los clientes almacenada en la tabla customers : sql = \"select * from customers\" cur . execute ( sql ) resultado = cur . fetchAll () # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () # Mostramos el resultado print ( resultado ) De RDS a S3 \u00b6 Vamos a realizar otro ejemplo sencillo que recupere el nombre, apellido y email de los clientes mediante una consulta que reibe un par\u00e1metro: import mariadb import sys import json import boto3 try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaDB: { e } \" ) sys . exit ( 1 ) ciudad = \"Brownsville\" # Obtenemos el cursor cur = conn . cursor () sql = \"select customer_fname, customer_lname, customer_zipcode from customers where customer_city=?\" cur . execute ( sql , ( ciudad , )) # Generamos un JSON con los datos row_headers = [ x [ 0 ] for x in cur . description ] clientes = cur . fetchall () json_data = [] for cliente in clientes : json_data . append ( dict ( zip ( row_headers , cliente ))) # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () # Persistimos el JSON en S3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) object = s3r . Object ( 's3severo2122python' , 'clientesRDS.json' ) object . put ( Body = json . dumps ( json_data )) De RDS a DynamoDB \u00b6 Para este caso de uso, vamos a crear una nueva tabla en DynamoDB a la que llamaremos SeveroClientes y le pondremos como clave de particionado el campo Id de tipo num\u00e9rico y como clave de ordenamiento el Zip de tipo texto. Creaci\u00f3n de la tabla SeveroClientes Vamos a modificar el ejemplo anterior para que, una vez recuperado los datos de la base de datos, los almacene directamente en DynamoDB: import mariadb import sys import boto3 try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaDB: { e } \" ) sys . exit ( 1 ) ciudad = \"Brownsville\" # Obtenemos el cursor cur = conn . cursor () sql = \"select customer_id, customer_fname, customer_lname, customer_zipcode from customers where customer_city=?\" cur . execute ( sql , ( ciudad , )) # Recorremos el cursor e insertamos en DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroClientes' ) with tabla . batch_writer () as batch : for customer_id , customer_fname , customer_lname , customer_zipcode in cur : Item = { 'Id' : customer_id , 'Zip' : customer_zipcode , 'Nombre' : customer_fname , 'Apellidos' : customer_lname , } batch . put_item ( Item = Item ) # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () Bases de datos y Pandas Si est\u00e1s interesado en colocar dentro de Pandas los datos que recuperas desde una base de datos, es muy c\u00f3modo utilizar SQLAlchemy el cual ofrece una capa por encima de los drivers (adem\u00e1s de ofrecer un framework ORM). Un fragmento de c\u00f3digo que utiliza Pandas y SQLAlchmy ser\u00eda similar a: from sqlalchemy import create_engine import pymysql import pandas as pd sqlEngine = create_engine ( 'mysql+pymysql://iabd:@127.0.0.1' , pool_recycle = 3600 ) dbConnection = sqlEngine . connect () df = pd . read_sql ( \"select * from retail_db.customers\" , dbConnection ); Caso de uso 6 - AWS Lambda \u00b6 En este caso de uso, vamos a realizar los casos de uso 4 y 5 mediante AWS Lambda, de manera que accedamos a S3, RDS y DynamoDB mediante funciones serverless. Vamos a empezar con el caso 4.2 De S3 a DynamoDB . Para ello, nos vamos a crear una funci\u00f3n que se ejecutar\u00e1 autom\u00e1ticamente cada vez que se inserte en el bucket s3severo2122python el objeto TMDb_filtered.csv . Si repasamos el caso de uso, el objetivo es cargar los datos de dicho CSV a la tabla SeveroPeliculas de DynamoDB. As\u00ed pues, el primer paso, es acceder a AWS Lambda y crear una nueva funci\u00f3n desde cero con soporte para Python 3.9 y x86_64: import json import urllib.parse import boto3 import pandas as pd from decimal import Decimal print ( 'Loading function' ) s3 = boto3 . client ( 's3' ) bucketNombre = \"s3severo2122python\" ficheroNombre = \"TMDb_filtered.CSV\" def lambda_handler ( event , context ): print ( \"Received event: \" + json . dumps ( event , indent = 2 )) # Obtenemos el bucket y el objeto desde el evento bucket = event [ 'Records' ][ 0 ][ 's3' ][ 'bucket' ][ 'name' ] key = urllib . parse . unquote_plus ( event [ 'Records' ][ 0 ][ 's3' ][ 'object' ][ 'key' ], encoding = 'utf-8' ) try : if bucket == bucketNombre and key == ficheroNombre : # 1.- Leemos el fichero desde S3 y lo metemos en un DataFrame response = s3 . get_object ( Bucket = bucketNombre , Key = ficheroNombre ) movies_df = pd . read_csv ( response [ 'Body' ], delimiter = ',' ) # 2.- Nos conectamos a DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) # 3.- Lo metemos en DynamoDB mediante un batch with tabla . batch_writer () as batch : for index , fila in movies_df . iterrows (): Item = { 'year' : 2022 , 'title' : str ( fila [ 'title' ]), 'info' : { 'plot' : fila [ 'overview' ], 'rating' : Decimal ( fila [ 'vote_average' ]) . quantize ( Decimal ( '1.00' )) } } batch . put_item ( Item = Item ) return response [ 'ContentType' ] except Exception as e : print ( e ) print ( 'Error getting object {} from bucket {} . Make sure they exist and your bucket is in the same region as this function.' . format ( key , bucket )) raise e Para poder probar nuestra funci\u00f3n, vamos a crear un nuevo evento con el siguiente contenido (el cual hemos creado a partir de la plantilla de s3-put ): { \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"aws:s3\" , \"awsRegion\" : \"us-east-1\" , \"eventTime\" : \"1970-01-01T00:00:00.000Z\" , \"eventName\" : \"ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"EXAMPLE\" }, \"requestParameters\" : { \"sourceIPAddress\" : \"127.0.0.1\" }, \"responseElements\" : { \"x-amz-request-id\" : \"EXAMPLE123456789\" , \"x-amz-id-2\" : \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"testConfigRule\" , \"bucket\" : { \"name\" : \"s3severo2122python\" , \"ownerIdentity\" : { \"principalId\" : \"EXAMPLE\" }, \"arn\" : \"arn:aws:s3:::s3severo2122python\" }, \"object\" : { \"key\" : \"TMDb_filtered.CSV\" , \"size\" : 1024 , \"eTag\" : \"0123456789abcdef0123456789abcdef\" , \"sequencer\" : \"0A1B2C3D4E5F678901\" } } } ] } Si ahora desplegamos e intentamos probar la funci\u00f3n, nos dar\u00e1 error ya que no encuentra la librer\u00eda de Pandas . Para que funcione, necesitamos a\u00f1adirla como una capa de AWS Lambda. Para ello, en la parte inferior de la pantalla, tenemos la opci\u00f3n de a\u00f1adir una nueva capa. En nuestro caso, vamos a a\u00f1adirla a trav\u00e9s de un ARN: arn:aws:lambda:us-east-1:770693421928:layer:Klayers-p39-pandas:1 Capa con Pandas en AWS Lambda Ahora s\u00ed que podemos ejecutar nuestra funci\u00f3n y comprobar que todo funciona correctamente. Desencadenador \u00b6 El siguiente paso es conectar un desencadenador para que autom\u00e1ticamente se lance la funci\u00f3n. Para ello, desde el gr\u00e1fico con el esquema de la funci\u00f3n, sobre el bot\u00f3n de +Agregar desencadenador , configuramos la carga de objetos S3 en nuestro repositorio: Desencadenador en AWS Lambda Una vez que ya tenemos la funci\u00f3n desplegada y hemos comprobado que funciona correctamente, podemos probar primero a borrar y volver a crear la tabla de SeveroPeliculas y a continuaci\u00f3n, ejecutar el caso de uso 4.1: Borrando SeveroPeliculas Creando SeveroPeliculas Creando TMDB_filtered.CSV borrarTablaDDB.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) tabla . delete () print ( \"Estado:\" , tabla . table_status ) crearTablaDDB.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . create_table ( TableName = 'SeveroPeliculas' , KeySchema = [ { 'AttributeName' : 'year' , 'KeyType' : 'HASH' # clave principal }, { 'AttributeName' : 'title' , 'KeyType' : 'RANGE' # clave ordenacion } ], AttributeDefinitions = [ { 'AttributeName' : 'year' , 'AttributeType' : 'N' }, { 'AttributeName' : 'title' , 'AttributeType' : 'S' }, ], ProvisionedThroughput = { 'ReadCapacityUnits' : 10 , 'WriteCapacityUnits' : 10 } ) print ( \"Estado:\" , tabla . table_status ) crearTMDB_filtered.py import boto3 s3 = boto3 . client ( 's3' ) resp = s3 . select_object_content ( Bucket = 's3severo2122python' , Key = 'TMDb_updated.CSV' , ExpressionType = 'SQL' , Expression = \"SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000\" , InputSerialization = { 'CSV' : { \"FileHeaderInfo\" : \"USE\" , 'AllowQuotedRecordDelimiter' : True }, 'CompressionType' : 'NONE' }, OutputSerialization = { 'CSV' : {}}, ) registros = [ \"title,overview,vote_count,vote_average \\n \" ] for evento in resp [ 'Payload' ]: if 'Records' in evento : registros . append ( evento [ 'Records' ][ 'Payload' ] . decode ()) file_str = '' . join ( registros ) s3 . put_object ( Body = file_str , Bucket = 's3severo2122python' , Key = \"TMDb_filtered.CSV\" ) Si ejecutamos los tres programas uno detr\u00e1s de otro, al crearse el fichero TMDb_filtered.CSV , autom\u00e1ticamente se ejecutar\u00e1 la funci\u00f3n serverless y se rellenar\u00e1 la tabla SeveroPeliculas . Destino \u00b6 Finalmente, si queremos que quede registrada nuestra operaci\u00f3n, una posibilidad es crear un destino que se ejecute autom\u00e1ticamente. Para ello, vamos a hacer uso de una cola SQS en la cual se insertar\u00e1 un mensaje cada vez que se ejecute exitosamente nuestra funci\u00f3n. Entramos en el servicios SQS, y creamos una cola est\u00e1ndar con el nombre que queramos (en mi caso he elegido etl5sqs ). Tras ello, desde el gr\u00e1fico con el esquema de la funci\u00f3n, sobre el bot\u00f3n de +Agregar destino , configuramos la cola SQS que acabamos de crear: Destino en AWS Lambda Si hemos realizado todos los pasos, tendremos nuestra funci\u00f3n asociada a un desencadenador y a un destino: Diagrama completo en AWS Lambda Si volvemos a ejecutar el script crearTMDB_filtered.py para que forzar la creaci\u00f3n del fichero en S3 y que provoque que se lance la funci\u00f3n, se insertar\u00e1 autom\u00e1ticamente un mensaje en la cola. Para comprobar la cola, una vez dentro de SQS y habiendo seleccionado la cola creada, mediante el bot\u00f3n superior de Enviar y recibir mensajes , entramos en la propia cola y podemos sondear mensajes para ver el contenido de la cola e inspeccionar los mensajes. Sondeando mensajes en SQS Actividades \u00b6 Para las siguientes actividades, se pide entregar en Aules un zip con todo el c\u00f3digo fuente y con una captura de todos los recursos AWS empleados (buckets S3, tabla/s DynamoDB, instancias RDS, funci\u00f3n AWS Lambda, etc..). Realizar los casos de uso del 1 al 4. (opcional) Realiza los casos de uso 5 y 6. Referencias \u00b6 Python, Boto3, and AWS S3: Demystified: https://realpython.com/python-boto3-aws-s3/ DynamoDB mediante Python Ten Examples of Getting Data from DynamoDB with Python and Boto3 DynamoDB Insert: Performance Basics in Python/Boto3 S3Select mediante Python : Ditch the Database","title":"5.- Python y AWS"},{"location":"apuntes/ingesta05python.html#python-y-aws","text":"En esta sesi\u00f3n vamos a estudiar c\u00f3mo acceder a los servicios de AWS relacionados con el Big Data estudiados previamente, y mediante diversos casos de uso crear diferentes flujos de datos. Para los siguientes casos de uso, realizaremos los 5 primeros desde nuestro sistema local y el caso final haciendo uso AWS Lambda . Para autenticarnos en AWS desde nuestro sistema local, recuerda que necesitas copiar las credenciales de acceso en ~/.aws/credentials o mediante las variables de entorno .","title":"Python y AWS"},{"location":"apuntes/ingesta05python.html#sdk-boto3","text":"Para acceder a AWS desde Python , Amazon ofrece el SDK Boto3 . Para poder utilizarlo, la instalaremos mediante pip install boto3 Pod\u00e9is consultar toda la informaci\u00f3n relativa a Boto3 en su documentaci\u00f3n oficial en https://boto3.amazonaws.com/v1/documentation/api/latest/index.html Existen dos posibilidades para acceder a AWS mediante Boto3: Recursos : representan un interfaz orientado a objetos de AWS, de manera que cada recurso contendr\u00e1 un identificador, unos atributos y un conjunto de operaciones. Un ejemplo de recurso es el S3 . M\u00e1s informaci\u00f3n sobre recursos en la documentaci\u00f3n oficial . Clientes : ofrecen un interfaz de bajo nivel que se mapea 1:1 con el API de cada servicio. Los clientes se generan a partir de la definici\u00f3n JSON del servicio. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial . En resumen, los recursos son una abstracci\u00f3n a m\u00e1s alto nivel de los servicios AWS que los clientes. Se recomienda el uso de los recursos al no tener que preocuparse de c\u00f3mo se realiza por debajo la comunicaci\u00f3n e interacci\u00f3n con los servicios. Sin embargo, a d\u00eda de hoy no hay recursos para todos los servicios AWS, y por ello, en ocasiones no queda otra opci\u00f3n que utilizar los clientes. Para demostrar las diferencias, vamos a ver c\u00f3mo podemos realizar algunas operaciones haciendo uso del cliente o del recurso (en estos ejemplos nos vamos a centrar en el servicio S3 - https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html .): Buckets de un usuario Elementos de un bucket Creaci\u00f3n de un bucket Eliminaci\u00f3n de un recurso/bucket s3-buckets.py import boto3 # Opci\u00f3n 1 print ( 'Buckets mediante resource:' ) s3resource = boto3 . resource ( 's3' , region_name = 'us-east-1' ) buckets = s3resource . buckets . all () for bucket in buckets : print ( f ' \\t { bucket . name } ' ) # Opci\u00f3n 2 print ( 'Buckets mediante el cliente:' ) s3client = boto3 . client ( 's3' ) response = s3client . list_buckets () for bucket in response [ 'Buckets' ]: print ( f ' \\t { bucket [ \"Name\" ] } ' ) s3-bucket-objects.py import boto3 s3 = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3 . Bucket ( 's3severo2122python' ) for obj in bucket . objects . all (): print ( obj . key ) s3-create-bucket.py import boto3 # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2122python-r' ) bucket . create () # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) response = s3c . create_bucket ( Bucket = 's3severo2122python' ) s3-delete.py import boto3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) bucket = s3r . Bucket ( 's3severo2122python' ) # Elimina todos los objetos del bucket bucket . objects . delete () # Elimina el bucket bucket . delete ()","title":"SDK Boto3"},{"location":"apuntes/ingesta05python.html#caso-de-uso-1-comunicacion-con-s3","text":"Vamos a trabajar con el archivo datosPeliculas.json el cual contiene un listado de pel\u00edculas con las que trabajaremos en los siguientes casos de uso. Primero vamos a ver c\u00f3mo podemos subir el archivo a S3 mediante Python: upload-movies-s3.py import boto3 ficheroUpload = \"datosPeliculas.json\" nombreBucket = \"s3severo2122python\" # Opci\u00f3n 1 - resource s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) # 1.1 mediante upload_file bucket = s3r . Object ( nombreBucket , 'datosSubidosR1.txt' ) bucket . upload_file ( ficheroUpload ) # 1.2 mediante put object = s3r . Object ( nombreBucket , 'datosSubidosR2.txt' ) object . put ( Body = b 'Ejemplo de datos binarios' ) # Opci\u00f3n 2 - cliente s3c = boto3 . client ( 's3' ) # 2.1 mediante upload_file response = s3c . upload_file ( ficheroUpload , nombreBucket , \"datosSubidosC1.json\" ) # 2.2 mediante upload_fileobj with open ( ficheroUpload , \"rb\" ) as f : s3c . upload_fileobj ( f , nombreBucket , \"datosSubidosC2.json\" ) # Cliente: Ejemplo de como crear un objeto y a\u00f1adirle contenido desde Python s3c . put_object ( Body = b 'Ejemplo de datos binarios' , Bucket = nombreBucket , Key = \"datosSubidosC3\" ) Si lo que queremos es descargar un recurso de S3 para tenerlo en nuestro sistema local haremos: import boto3 # Opci\u00f3n 1 - descarga s3c = boto3 . client ( 's3' ) s3c . download_file ( 's3severo2122python' , 'datosPeliculas.json' , 'datosDescargados.json' ) # Opci\u00f3n 2 - Abrimos el fichero y metemos el contenido with open ( 'fichero.json' , 'wb' ) as f : s3c . download_fileobj ( 's3severo2122python' , 'datosPeliculas.json' , f )","title":"Caso de uso 1: Comunicaci\u00f3n con S3"},{"location":"apuntes/ingesta05python.html#caso-de-uso-2-cargar-datos-en-dynamodb","text":"Vamos a cargar un listado de pel\u00edculas en DynamoDB . El primer paso es elegir las claves de particionado y ordenaci\u00f3n. El archivo datosPeliculas.json contiene el siguiente contenido: [ { \"year\" : 2013 , \"title\" : \"Rush\" , \"info\" : { \"directors\" : [ \"Ron Howard\" ], \"release_date\" : \"2013-09-02T00:00:00Z\" , \"rating\" : 8.3 , \"genres\" : [ \"Action\" , \"Biography\" , \"Drama\" , \"Sport\" ], \"image_url\" : \"http://ia.media-imdb.com/images/M/MV5BMTQyMDE0MTY0OV5BMl5BanBnXkFtZTcwMjI2OTI0OQ@@._V1_SX400_.jpg\" , \"plot\" : \"A re-creation of the merciless 1970s rivalry between Formula One rivals James Hunt and Niki Lauda.\" , \"rank\" : 2 , \"running_time_secs\" : 7380 , \"actors\" : [ \"Daniel Bruhl\" , \"Chris Hemsworth\" , \"Olivia Wilde\" ] } }, ] Como los a\u00f1os de las pel\u00edculas permiten particionar de manera m\u00e1s o menos equilibrada los datos, en la mejor candidata para clave de particionado. Como s\u00ed que habr\u00e1 varias pel\u00edculas en el mismo a\u00f1o, elegimos el t\u00edtulo como clave de ordenaci\u00f3n, provocando que los documentos tengan una clave compuesta. As\u00ed pues, vamos a nombrar nuestra tabla como SeveroPeliculas y ponemos como clave de partici\u00f3n el atributo year de tipo num\u00e9rico, y como clave de ordenaci\u00f3n title de tipo cadena. Creaci\u00f3n de la tabla SeveroPeliculas Una vez creada la tabla, vamos a ver c\u00f3mo podemos cargar los datos. Haciendo uso de la librer\u00eda boto3 vamos a crear el archivo cargarDatosPeliculas.py : cargarDatosPeliculas.py import boto3 import json import decimal dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) # (1) tabla = dynamodb . Table ( 'SeveroPeliculas' ) with open ( \"datosPeliculas.json\" ) as ficheroJSON : peliculas = json . load ( ficheroJSON , parse_float = decimal . Decimal ) for pelicula in peliculas : year = int ( movie [ 'year' ]) title = movie [ 'title' ] info = movie [ 'info' ] print ( \"A\u00f1adida pel\u00edcula:\" , year , title ) tabla . put_item ( Item = { 'year' : year , 'title' : title , 'info' : info , } ) Nos conectamos a la regi\u00f3n e indicamos que vamos a utilizar el servicio de DynamoDB Si lo ejecutamos desde nuestro ordenador, nos aparecer\u00e1 por la consola cada una de las pel\u00edculas insertadas. Float y boto3 Mucho cuidado con boto3 y DynamoDB, ya que los tipos Float no est\u00e1n soportados, y en cambio, hemos de utilizar el tipo Decimal .","title":"Caso de uso 2: Cargar datos en DynamoDB"},{"location":"apuntes/ingesta05python.html#faker","text":"Si necesitamos escribir muchos datos, es muy \u00fatil emplear una librer\u00eda como Faker para generar los datos. Primero hemos de instalarla mediante pip: pip3 install faker Vamos a realizar un ejemplo para mostrar algunos datos aleatorios y comprobar su funcionamiento: C\u00f3digo Resultado holaFaker.py from faker import Faker fake = Faker () fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol print ( \"Nombre:\" , fake . name ()) print ( \"Direcci\u00f3n:\" , fake . address ()) print ( \"Nombre de hombre:\" , fake . first_name_male ()) print ( \"N\u00famero de tel\u00e9fono:\" , fake . phone_number ()) print ( \"Color:\" , fake . color_name ()) print ( \"Fecha:\" , fake . date ()) print ( \"Email:\" , fake . email ()) print ( \"Frase de 10 palabras\" , fake . sentence ( nb_words = 10 )) Nombre: Dani Pla Chico Direcci\u00f3n: Cuesta de Emiliano Milla 66 Albacete, 83227 Nombre de hombre: Mat\u00edas N\u00famero de tel\u00e9fono: +34 818 779 827 Color: Salm\u00f3n oscuro Fecha: 1984-09-29 Email: btome@example.net Frase de 10 palabras Perferendis saepe consequatur saepe sapiente est impedit eaque omnis temporibus excepturi repellat ducimus. Los diferentes grupos de datos que genera se agrupan en Providers : de direcci\u00f3n, fechas, relacionados con internet, bancarios, c\u00f3digos de barra, isbn, etc... Se recomienda consultar la documentaci\u00f3n en https://faker.readthedocs.io/en/master/providers.html . Locale ES Al trabajar con el idioma en espa\u00f1ol, puede que algunos m\u00e9todos no funcionen (m\u00e1s que no funcionar, posiblemente tengan otro nombre). Es recomendable comprobar las opciones disponibles en https://faker.readthedocs.io/en/master/locales/es_ES.html","title":"Faker"},{"location":"apuntes/ingesta05python.html#generando-csv","text":"Vamos a generar un CSV con datos de 1000 personas. Primero creamos una lista con los encabezados y los escribimos en el fichero, para posteriormente, l\u00ednea a l\u00ednea, generar los datos de cada persona: C\u00f3digo Resultado generaCSV.py from faker import Faker import csv output = open ( 'datosFaker.csv' , 'w' ) fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol header = [ 'nombre' , 'edad' , 'calle' , 'ciudad' , 'provincia' , 'cp' , 'longitud' , 'latitud' ] mywriter = csv . writer ( output ) mywriter . writerow ( header ) for r in range ( 1000 ): mywriter . writerow ([ fake . name (), fake . random_int ( min = 18 , max = 80 , step = 1 ), fake . street_address (), fake . city (), fake . state (), fake . postcode (), fake . longitude (), fake . latitude ()]) output . close () datosFaker.csv nombre,edad,calle,ciudad,provincia,cp,longitud,latitud Jenaro Verd\u00fa Suarez,26,Urbanizaci\u00f3n Mohamed Vall\u00e9s 122,Sevilla,Guip\u00fazcoa,73198,2.657719,-69.679293 Eugenio Calzada Revilla,57,Camino Vanesa Amor 36 Piso 9 ,Huesca,\u00c1lava,75590,34.041399,-52.924628 Flavio del Lumbreras,76,Avenida de Beatriz Amaya 92,Ciudad,Murcia,86420,58.248903,-17.924926","title":"Generando CSV"},{"location":"apuntes/ingesta05python.html#generando-json","text":"Y a continuaci\u00f3n repetimos el mismo ejemplo, pero ahora generando un documento JSON. La principal diferencia es que primero vamos a rellenar un diccionario con toda la informaci\u00f3n, y luego persistimos el diccionario: C\u00f3digo Resultado generaJSON.py from faker import Faker import json fake = Faker ( 'es_ES' ) # cambiamos el locale a espa\u00f1ol # Preparamos los datos datos = {} datos [ 'registros' ] = [] for x in range ( 1000 ): persona = { \"datos\" : fake . name (), \"edad\" : fake . random_int ( min = 18 , max = 80 , step = 1 ), \"calle\" : fake . street_address (), \"ciudad\" : fake . city (), \"provincia\" : fake . state (), \"cp\" : fake . postcode (), \"longitud\" : float ( fake . longitude ()), \"latitud\" : float ( fake . latitude ())} datos [ 'registros' ] . append ( persona ) # Los metemos en el fichero output = open ( 'datosFaker.json' , 'w' ) json . dump ( datos , output ) datosFaker.json { \"registros\" : [ { \"datos\" : \"Merche Moreno Roman\" , \"edad\" : 51 , \"calle\" : \"Paseo Amelia Folch 967\" , \"ciudad\" : \"Segovia\" , \"provincia\" : \"M\\u00e1laga\" , \"cp\" : \"71721\" , \"longitud\" : 84.603801 , \"latitud\" : 58.941349 }, { \"datos\" : \"Miguel Abascal Sanz\" , \"edad\" : 21 ,","title":"Generando JSON"},{"location":"apuntes/ingesta05python.html#caso-de-uso-3-consultar-datos-en-dynamodb","text":"Una vez tenemos nuestra tabla de DynamoDB cargada con datos, llega el momento de recuperar los datos, ya sea un registro en concreto o la posibilidad de realizar una consulta, ya sea por su \u00edndice o su clave de ordenaci\u00f3n (o ambas). En la sesi\u00f3n que trabajamos con DynamoDB estudiamos que pod\u00edamos realizar consultas sobre el almac\u00e9n NoSQL haciendo uso de un subconjunto de SQL conocido como PartiQL . En los siguientes ejemplos vamos a mostrar c\u00f3mo realizar las operaciones v\u00eda el API de DynamoDb y mediante PartiQL. Si queremos recuperar la pel\u00edcula Interstellar de 2014 haremos: Mediante get_item Mediante get_item con excepciones Mediante PartiQL dynamodb_getitem.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) title = \"Interstellar\" year = 2014 response = tabla . get_item ( Key = { 'year' : year , 'title' : title }, ProjectionExpression = \"title, info.plot\" ) item = response [ 'Item' ] print ( item ) dynamodb_getitem_exc.py import boto3 from botocore.exceptions import ClientError dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) title = \"Interstellar\" year = 2014 # Recupera una pel\u00edcula print ( \"------ Datos de Interstellar\" ) try : response = tabla . get_item ( Key = { 'year' : year , 'title' : title }, ProjectionExpression = \"title, info.plot\" ) except ClientError as e : print ( e . response [ 'Error' ][ 'Message' ]) else : item = response [ 'Item' ] print ( item ) dynamodb_select.py import boto3 clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) # Recupera una pel\u00edcula con PartiQL print ( \"------ Datos de Interstellar mediante PartiQL\" ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, info.plot FROM SeveroPeliculas WHERE year = 2014 and title='Interstellar'\" ) item = resp [ 'Items' ][ 0 ] print ( item ) title = \"Interstellar\" year = 2014 # Recupera una pel\u00edcula con PartiQL con par\u00e1metros indicados mediante ? print ( \"------ Datos de Interstellar mediante PartiQL con par\u00e1metros\" ) resp = clientDDB . execute_statement ( Statement = 'SELECT * FROM SeveroPeliculas WHERE year = ? AND title = ?' , Parameters = [{ 'N' : str ( year )}, { 'S' : title }]) item = resp [ 'Items' ][ 0 ] print ( item ) En el caso de las consultas mediante PartiQL haciendo uso de execute_statement conviene destacar que: Las consultas son case sensitive . Los par\u00e1metros se indican mediante ? Los contenidos de los par\u00e1metros se indican mediante una lista con un diccionario por cada par\u00e1metro donde la clave es el tipo del par\u00e1metro, y el valor es el dato a pasar (el dato se pasa siempre como un string ) Las consultas siempre devuelven un diccionario con una propiedad Items que contiene los resultados devueltos. Destacar que es diferente la estructura del resultado de realizar una consulta mediante el API de DynamoDB (respeta la estructura definida en la base de datos) o mediante PartiQL (crea un atributo por columna recuperada cuyo valor contiene el tipo del dato): Resultado de get-item Resultado de PartiQL { 'i nf o' : { 'plo t ' : 'A group o f explorers make use o f a ne wly discovered wormhole t o surpass t he limi tat io ns o n huma n space tra vel a n d co n quer t he vas t dis tan ces i n volved i n a n i nterstellar voyage.' }, ' t i tle ' : 'I nterstellar ' } { ' t i tle ' : { 'S' : 'I nterstellar ' }, 'plo t ' : { 'S' : 'A group o f explorers make use o f a ne wly discovered wormhole t o surpass t he limi tat io ns o n huma n space tra vel a n d co n quer t he vas t dis tan ces i n volved i n a n i nterstellar voyage.' } } Tambi\u00e9n podemos realizar otro tipo de consultas: Pel\u00edculas de 2016 mediante query Pel\u00edculas cuyo t\u00edtulo est\u00e9 entre la A y la L import boto3 from boto3.dynamodb.conditions import Key # Mediante query dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de 2016\" ) resp = tabla . query ( KeyConditionExpression = Key ( 'year' ) . eq ( 2016 )) for i in resp [ 'Items' ]: print ( i [ 'year' ], \":\" , i [ 'title' ]) # Mediante PartiQL print ( \"-----Pel\u00edculas de 2016 con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, year FROM SeveroPeliculas WHERE year = 2016\" ) for i in resp [ 'Items' ]: print ( i [ 'year' ][ 'N' ], \":\" , i [ 'title' ][ 'S' ]) import boto3 import json import decimal from boto3.dynamodb.conditions import Key class DecimalEncoder ( json . JSONEncoder ): def default ( self , o ): if isinstance ( o , decimal . Decimal ): if o % 1 > 0 : return float ( o ) else : return int ( o ) return super ( DecimalEncoder , self ) . default ( o ) anyo = 2016 letraInicial = \"A\" letraFinal = \"F\" dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de 2016 cuyo t\u00edtulo empieza desde A hasta L\" ) resp = tabla . query ( ProjectionExpression = \"#yr, title, info.genres, info.actors[0]\" , # year es una palabra clave, por tanto necesitamos crear un alias ExpressionAttributeNames = { \"#yr\" : \"year\" }, KeyConditionExpression = Key ( 'year' ) . eq ( anyo ) & Key ( 'title' ) . between ( letraInicial , letraFinal ) ) for i in resp [ 'Items' ]: print ( i ) # {'info': {'actors': ['Zoe Saldana'], 'genres': ['Action', 'Adventure', 'Fantasy', 'Sci-Fi']}, 'year': Decimal('2016'), 'title': 'Avatar 2'} # Transforma los valores num\u00e9ricos de Decimal a Number print ( json . dumps ( i , cls = DecimalEncoder )) # {\"info\": {\"actors\": [\"Zoe Saldana\"], \"genres\": [\"Action\", \"Adventure\", \"Fantasy\", \"Sci-Fi\"]}, \"year\": 2016, \"title\": \"Avatar 2\"} for genero in i [ 'info' ][ 'genres' ]: print ( genero ) print ( \"-----Pel\u00edculas de 2016 cuyo t\u00edtulo empieza desde A hasta L con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) consulta = 'SELECT year, title, info.genres, info.actors[0] FROM SeveroPeliculas WHERE year = ? AND title between ? and ?' resp = clientDDB . execute_statement ( Statement = consulta , Parameters = [{ 'N' : str ( anyo )}, { 'S' : letraInicial }, { 'S' : letraFinal }]) for i in resp [ 'Items' ]: print ( i ) # [{'year': {'N': '2016'}, 'title': {'S': 'Avatar 2'}, 'actors[0]': {'S': 'Zoe Saldana'}, 'genres': {'L': [{'S': 'Action'}, {'S': 'Adventure'}, {'S': 'Fantasy'}, {'S': 'Sci-Fi'}]}}] for genero in i [ 'genres' ][ 'L' ]: print ( genero [ 'S' ]) La clase DecimalEncoder se utiliza para transformar los campos Decimal que utiliza DynamoDB para almacenar contenido num\u00e9rico a tipo entero o flotante seg\u00fan necesite.","title":"Caso de uso 3 - Consultar datos en DynamoDB"},{"location":"apuntes/ingesta05python.html#full-scan","text":"Cuando en PartiQL no le indicamos en la condici\u00f3n una expresi\u00f3n que busque por una de las claves, se realizar\u00e1 un full scan sobre toda la tabla, lo que puede implicar unos costes inesperados, tanto econ\u00f3micos como a nivel rendimiento provisionado. El m\u00e9todo scan lee cada elemento de la tabla y devuelve todos los datos de la tabla. Se le puede pasar una filter_expression opcional para que s\u00f3lo devuelva los elementos que cumplan el criterio. Sin embargo, el filtrado se aplica tras escanear toda la tabla. Ejemplo scan import boto3 import json import decimal from boto3.dynamodb.conditions import Key class DecimalEncoder ( json . JSONEncoder ): def default ( self , o ): if isinstance ( o , decimal . Decimal ): if o % 1 > 0 : return float ( o ) else : return int ( o ) return super ( DecimalEncoder , self ) . default ( o ) dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) print ( \"-----Pel\u00edculas de sobresaliente mediante full scan\" ) # Escaneo y filtrado fe = Key ( 'info.rating' ) . gte ( 9 ) pe = \"#yr, title, info.rating\" ean = { \"#yr\" : \"year\" } resp = tabla . scan ( FilterExpression = fe , ProjectionExpression = pe , ExpressionAttributeNames = ean ) for i in resp [ 'Items' ]: print ( json . dumps ( i , cls = DecimalEncoder )) Full scan con PartiQL import boto3 from boto3.dynamodb.conditions import Key print ( \"-----Pel\u00edculas de 2016 con PartiQL\" ) clientDDB = boto3 . client ( 'dynamodb' , region_name = 'us-east-1' ) resp = clientDDB . execute_statement ( Statement = \"SELECT title, year, info.rating FROM SeveroPeliculas WHERE info.rating >= 9\" ) for i in resp [ 'Items' ]: print ( i [ 'year' ][ 'N' ], \":\" , i [ 'title' ][ 'S' ])","title":"Full scan"},{"location":"apuntes/ingesta05python.html#caso-de-uso-4-de-s3-a-dynamodb","text":"En este caso, vamos a coger datos de pel\u00edculas de un dataset p\u00fablico disponible en https://www.kaggle.com/sankha1998/tmdb-top-10000-popular-movies-dataset El contenido del dataset es similar a: TMDb_updated.CSV ,title,overview,original_language,vote_count,vote_average 0,Ad Astra,\"The near future...\",en,2853,5.9 1,Bloodshot,\"After he ...\",en,1349,7.2 2,Bad Boys for Life,\"Marcus and Mike ...\",en,2530,7.1 Una vez descargado TMDb_updated.CSV , vamos a cargar la informaci\u00f3n en S3 dentro del bucket. Para este caso, en vez de cargar todos los datos desde el dataset en nuestra tabla NoSQL, vamos a meter en DynamoDB el t\u00edtulo, la nota media y la trama siempre y cuando hayan recibido al menos 10.000 votos.","title":"Caso de uso 4 - De S3 a DynamoDB"},{"location":"apuntes/ingesta05python.html#s3select","text":"Para realizar esta consulta desde Python para poder automatizar el proceso ETL utilizaremos S3Select para recuperar el subconjunto de los datos. S3Select vs AWS Athena Este tipo de procesamiento es m\u00e1s c\u00f3modo realizarlo mediante AWS Athena, el cual s\u00ed que permite realizar join entre diferentes datasets. S3Select s\u00f3lo permite trabajar con una \u00fanica tabla. Para ello, mediante S3Select ejecutaremos la consulta SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000 y almacenaremos el resultado en un nuevo CSV dentro del mismo bucket: s3select.py import boto3 s3 = boto3 . client ( 's3' ) # 1.- Realizamos la consulta mediante S3Select resp = s3 . select_object_content ( Bucket = 's3severo2122python' , Key = 'TMDb_updated.CSV' , ExpressionType = 'SQL' , Expression = \"SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000\" , InputSerialization = { 'CSV' : { \"FileHeaderInfo\" : \"USE\" , 'AllowQuotedRecordDelimiter' : True }, 'CompressionType' : 'NONE' }, OutputSerialization = { 'CSV' : {}}, ) # 2.- Unimos los datos que vamos recibiendo en streaming registros = [ \"title,overview,vote_count,vote_average \\n \" ] for evento in resp [ 'Payload' ]: if 'Records' in evento : registros . append ( evento [ 'Records' ][ 'Payload' ] . decode ()) # 3.- Generamos el contenido en un String file_str = '' . join ( registros ) # 4.- Creamos un nuevo objeto en S3 s3 . put_object ( Body = file_str , Bucket = 's3severo2122python' , Key = \"TMDb_filtered.CSV\" )","title":"S3Select"},{"location":"apuntes/ingesta05python.html#de-s3-a-dynamodb","text":"Una vez creado el fichero en S3, vamos cargar los datos en DynamoDB. Como el dataset no conten\u00eda la fecha de la pel\u00edcula, en nuestro caso le vamos a poner a todas las pel\u00edculas que son del 2022: import boto3 import pandas as pd from decimal import Decimal # 1.- Leemos el fichero desde S3 y lo metemos en un DataFrame s3c = boto3 . client ( 's3' ) bucketNombre = \"s3severo2122python\" ficheroNombre = \"TMDb_filtered.CSV\" response = s3c . get_object ( Bucket = bucketNombre , Key = ficheroNombre ) movies_df = pd . read_csv ( response [ 'Body' ], delimiter = ',' ) # 2.- Nos conectamos a DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) # 3.- Lo metemos en DynamoDB mediante un batch with tabla . batch_writer () as batch : for index , fila in movies_df . iterrows (): Item = { 'year' : 2022 , 'title' : str ( fila [ 'title' ]), 'info' : { 'plot' : fila [ 'overview' ], 'rating' : Decimal ( fila [ 'vote_average' ]) . quantize ( Decimal ( '1.00' )) } } batch . put_item ( Item = Item )","title":"De S3 a DynamoDB"},{"location":"apuntes/ingesta05python.html#caso-de-uso-5-desde-rds","text":"Preparaci\u00f3n MariaBD Para estos actividades y futuras sesiones, vamos a utilizar una base de datos ( retail_db ) que contiene informaci\u00f3n sobre un comercio (clientes, productos, pedidos, etc...). Para ello, descargaremos el archivo create_db.sql con las sentencias para crear la base de datos y los datos como instrucciones SQL. Tras ello, bien sea mediante DBeaver o si nos conectamos a MariaDB ( mariadb -u iabd -p ) desde la misma carpeta que hemos descargado el archivo, ejecutaremos los siguientes comando: create database retail_db ; use retail_db ; source create_db . sql ; show tables ; Vamos a utilizar la instancia de base de datos iabd que tenemos en RDS con la base de datos retail_db .","title":"Caso de uso 5 - Desde RDS"},{"location":"apuntes/ingesta05python.html#mariadbmysql-y-python","text":"Para acceder a la base de datos desde Python necesitamos instalar la librer\u00eda correspondiente: pip3 install mariadb pip3 install mysql-connector-python Todo el c\u00f3digo a continuaci\u00f3n se basa en MariaDB como sistema gestor de base de datos. Si queremos conectarnos, debemos indicar los datos de conexion: import mariadb import sys try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaD: { e } \" ) sys . exit ( 1 ) # Obtenemos el cursor cur = conn . cursor () Una vez nos hemos conectado y tenemos abierto un cursor, ya podemos hacer consultas y recuperar datos. Por ejemplo, para recuperar toda la informaci\u00f3n de los clientes almacenada en la tabla customers : sql = \"select * from customers\" cur . execute ( sql ) resultado = cur . fetchAll () # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () # Mostramos el resultado print ( resultado )","title":"MariaDB/MySQL y Python"},{"location":"apuntes/ingesta05python.html#de-rds-a-s3","text":"Vamos a realizar otro ejemplo sencillo que recupere el nombre, apellido y email de los clientes mediante una consulta que reibe un par\u00e1metro: import mariadb import sys import json import boto3 try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaDB: { e } \" ) sys . exit ( 1 ) ciudad = \"Brownsville\" # Obtenemos el cursor cur = conn . cursor () sql = \"select customer_fname, customer_lname, customer_zipcode from customers where customer_city=?\" cur . execute ( sql , ( ciudad , )) # Generamos un JSON con los datos row_headers = [ x [ 0 ] for x in cur . description ] clientes = cur . fetchall () json_data = [] for cliente in clientes : json_data . append ( dict ( zip ( row_headers , cliente ))) # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () # Persistimos el JSON en S3 s3r = boto3 . resource ( 's3' , region_name = 'us-east-1' ) object = s3r . Object ( 's3severo2122python' , 'clientesRDS.json' ) object . put ( Body = json . dumps ( json_data ))","title":"De RDS a S3"},{"location":"apuntes/ingesta05python.html#de-rds-a-dynamodb","text":"Para este caso de uso, vamos a crear una nueva tabla en DynamoDB a la que llamaremos SeveroClientes y le pondremos como clave de particionado el campo Id de tipo num\u00e9rico y como clave de ordenamiento el Zip de tipo texto. Creaci\u00f3n de la tabla SeveroClientes Vamos a modificar el ejemplo anterior para que, una vez recuperado los datos de la base de datos, los almacene directamente en DynamoDB: import mariadb import sys import boto3 try : conn = mariadb . connect ( user = \"admin\" , password = \"adminadmin\" , host = \"iabd.cllw9xnmy9av.us-east-1.rds.amazonaws.com\" , port = 3306 , database = \"retail_db\" ) except mariadb . Error as e : print ( f \"Error conectando a MariaDB: { e } \" ) sys . exit ( 1 ) ciudad = \"Brownsville\" # Obtenemos el cursor cur = conn . cursor () sql = \"select customer_id, customer_fname, customer_lname, customer_zipcode from customers where customer_city=?\" cur . execute ( sql , ( ciudad , )) # Recorremos el cursor e insertamos en DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroClientes' ) with tabla . batch_writer () as batch : for customer_id , customer_fname , customer_lname , customer_zipcode in cur : Item = { 'Id' : customer_id , 'Zip' : customer_zipcode , 'Nombre' : customer_fname , 'Apellidos' : customer_lname , } batch . put_item ( Item = Item ) # Cerramos el cursor y la conexi\u00f3n cur . close () conn . close () Bases de datos y Pandas Si est\u00e1s interesado en colocar dentro de Pandas los datos que recuperas desde una base de datos, es muy c\u00f3modo utilizar SQLAlchemy el cual ofrece una capa por encima de los drivers (adem\u00e1s de ofrecer un framework ORM). Un fragmento de c\u00f3digo que utiliza Pandas y SQLAlchmy ser\u00eda similar a: from sqlalchemy import create_engine import pymysql import pandas as pd sqlEngine = create_engine ( 'mysql+pymysql://iabd:@127.0.0.1' , pool_recycle = 3600 ) dbConnection = sqlEngine . connect () df = pd . read_sql ( \"select * from retail_db.customers\" , dbConnection );","title":"De RDS a DynamoDB"},{"location":"apuntes/ingesta05python.html#caso-de-uso-6-aws-lambda","text":"En este caso de uso, vamos a realizar los casos de uso 4 y 5 mediante AWS Lambda, de manera que accedamos a S3, RDS y DynamoDB mediante funciones serverless. Vamos a empezar con el caso 4.2 De S3 a DynamoDB . Para ello, nos vamos a crear una funci\u00f3n que se ejecutar\u00e1 autom\u00e1ticamente cada vez que se inserte en el bucket s3severo2122python el objeto TMDb_filtered.csv . Si repasamos el caso de uso, el objetivo es cargar los datos de dicho CSV a la tabla SeveroPeliculas de DynamoDB. As\u00ed pues, el primer paso, es acceder a AWS Lambda y crear una nueva funci\u00f3n desde cero con soporte para Python 3.9 y x86_64: import json import urllib.parse import boto3 import pandas as pd from decimal import Decimal print ( 'Loading function' ) s3 = boto3 . client ( 's3' ) bucketNombre = \"s3severo2122python\" ficheroNombre = \"TMDb_filtered.CSV\" def lambda_handler ( event , context ): print ( \"Received event: \" + json . dumps ( event , indent = 2 )) # Obtenemos el bucket y el objeto desde el evento bucket = event [ 'Records' ][ 0 ][ 's3' ][ 'bucket' ][ 'name' ] key = urllib . parse . unquote_plus ( event [ 'Records' ][ 0 ][ 's3' ][ 'object' ][ 'key' ], encoding = 'utf-8' ) try : if bucket == bucketNombre and key == ficheroNombre : # 1.- Leemos el fichero desde S3 y lo metemos en un DataFrame response = s3 . get_object ( Bucket = bucketNombre , Key = ficheroNombre ) movies_df = pd . read_csv ( response [ 'Body' ], delimiter = ',' ) # 2.- Nos conectamos a DynamoDB dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) # 3.- Lo metemos en DynamoDB mediante un batch with tabla . batch_writer () as batch : for index , fila in movies_df . iterrows (): Item = { 'year' : 2022 , 'title' : str ( fila [ 'title' ]), 'info' : { 'plot' : fila [ 'overview' ], 'rating' : Decimal ( fila [ 'vote_average' ]) . quantize ( Decimal ( '1.00' )) } } batch . put_item ( Item = Item ) return response [ 'ContentType' ] except Exception as e : print ( e ) print ( 'Error getting object {} from bucket {} . Make sure they exist and your bucket is in the same region as this function.' . format ( key , bucket )) raise e Para poder probar nuestra funci\u00f3n, vamos a crear un nuevo evento con el siguiente contenido (el cual hemos creado a partir de la plantilla de s3-put ): { \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"aws:s3\" , \"awsRegion\" : \"us-east-1\" , \"eventTime\" : \"1970-01-01T00:00:00.000Z\" , \"eventName\" : \"ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"EXAMPLE\" }, \"requestParameters\" : { \"sourceIPAddress\" : \"127.0.0.1\" }, \"responseElements\" : { \"x-amz-request-id\" : \"EXAMPLE123456789\" , \"x-amz-id-2\" : \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"testConfigRule\" , \"bucket\" : { \"name\" : \"s3severo2122python\" , \"ownerIdentity\" : { \"principalId\" : \"EXAMPLE\" }, \"arn\" : \"arn:aws:s3:::s3severo2122python\" }, \"object\" : { \"key\" : \"TMDb_filtered.CSV\" , \"size\" : 1024 , \"eTag\" : \"0123456789abcdef0123456789abcdef\" , \"sequencer\" : \"0A1B2C3D4E5F678901\" } } } ] } Si ahora desplegamos e intentamos probar la funci\u00f3n, nos dar\u00e1 error ya que no encuentra la librer\u00eda de Pandas . Para que funcione, necesitamos a\u00f1adirla como una capa de AWS Lambda. Para ello, en la parte inferior de la pantalla, tenemos la opci\u00f3n de a\u00f1adir una nueva capa. En nuestro caso, vamos a a\u00f1adirla a trav\u00e9s de un ARN: arn:aws:lambda:us-east-1:770693421928:layer:Klayers-p39-pandas:1 Capa con Pandas en AWS Lambda Ahora s\u00ed que podemos ejecutar nuestra funci\u00f3n y comprobar que todo funciona correctamente.","title":"Caso de uso 6 - AWS Lambda"},{"location":"apuntes/ingesta05python.html#desencadenador","text":"El siguiente paso es conectar un desencadenador para que autom\u00e1ticamente se lance la funci\u00f3n. Para ello, desde el gr\u00e1fico con el esquema de la funci\u00f3n, sobre el bot\u00f3n de +Agregar desencadenador , configuramos la carga de objetos S3 en nuestro repositorio: Desencadenador en AWS Lambda Una vez que ya tenemos la funci\u00f3n desplegada y hemos comprobado que funciona correctamente, podemos probar primero a borrar y volver a crear la tabla de SeveroPeliculas y a continuaci\u00f3n, ejecutar el caso de uso 4.1: Borrando SeveroPeliculas Creando SeveroPeliculas Creando TMDB_filtered.CSV borrarTablaDDB.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . Table ( 'SeveroPeliculas' ) tabla . delete () print ( \"Estado:\" , tabla . table_status ) crearTablaDDB.py import boto3 dynamodb = boto3 . resource ( 'dynamodb' , region_name = 'us-east-1' ) tabla = dynamodb . create_table ( TableName = 'SeveroPeliculas' , KeySchema = [ { 'AttributeName' : 'year' , 'KeyType' : 'HASH' # clave principal }, { 'AttributeName' : 'title' , 'KeyType' : 'RANGE' # clave ordenacion } ], AttributeDefinitions = [ { 'AttributeName' : 'year' , 'AttributeType' : 'N' }, { 'AttributeName' : 'title' , 'AttributeType' : 'S' }, ], ProvisionedThroughput = { 'ReadCapacityUnits' : 10 , 'WriteCapacityUnits' : 10 } ) print ( \"Estado:\" , tabla . table_status ) crearTMDB_filtered.py import boto3 s3 = boto3 . client ( 's3' ) resp = s3 . select_object_content ( Bucket = 's3severo2122python' , Key = 'TMDb_updated.CSV' , ExpressionType = 'SQL' , Expression = \"SELECT s.title, s.overview, s.vote_count, s.vote_average FROM s3object s WHERE cast(s.vote_count as int)> 10000\" , InputSerialization = { 'CSV' : { \"FileHeaderInfo\" : \"USE\" , 'AllowQuotedRecordDelimiter' : True }, 'CompressionType' : 'NONE' }, OutputSerialization = { 'CSV' : {}}, ) registros = [ \"title,overview,vote_count,vote_average \\n \" ] for evento in resp [ 'Payload' ]: if 'Records' in evento : registros . append ( evento [ 'Records' ][ 'Payload' ] . decode ()) file_str = '' . join ( registros ) s3 . put_object ( Body = file_str , Bucket = 's3severo2122python' , Key = \"TMDb_filtered.CSV\" ) Si ejecutamos los tres programas uno detr\u00e1s de otro, al crearse el fichero TMDb_filtered.CSV , autom\u00e1ticamente se ejecutar\u00e1 la funci\u00f3n serverless y se rellenar\u00e1 la tabla SeveroPeliculas .","title":"Desencadenador"},{"location":"apuntes/ingesta05python.html#destino","text":"Finalmente, si queremos que quede registrada nuestra operaci\u00f3n, una posibilidad es crear un destino que se ejecute autom\u00e1ticamente. Para ello, vamos a hacer uso de una cola SQS en la cual se insertar\u00e1 un mensaje cada vez que se ejecute exitosamente nuestra funci\u00f3n. Entramos en el servicios SQS, y creamos una cola est\u00e1ndar con el nombre que queramos (en mi caso he elegido etl5sqs ). Tras ello, desde el gr\u00e1fico con el esquema de la funci\u00f3n, sobre el bot\u00f3n de +Agregar destino , configuramos la cola SQS que acabamos de crear: Destino en AWS Lambda Si hemos realizado todos los pasos, tendremos nuestra funci\u00f3n asociada a un desencadenador y a un destino: Diagrama completo en AWS Lambda Si volvemos a ejecutar el script crearTMDB_filtered.py para que forzar la creaci\u00f3n del fichero en S3 y que provoque que se lance la funci\u00f3n, se insertar\u00e1 autom\u00e1ticamente un mensaje en la cola. Para comprobar la cola, una vez dentro de SQS y habiendo seleccionado la cola creada, mediante el bot\u00f3n superior de Enviar y recibir mensajes , entramos en la propia cola y podemos sondear mensajes para ver el contenido de la cola e inspeccionar los mensajes. Sondeando mensajes en SQS","title":"Destino"},{"location":"apuntes/ingesta05python.html#actividades","text":"Para las siguientes actividades, se pide entregar en Aules un zip con todo el c\u00f3digo fuente y con una captura de todos los recursos AWS empleados (buckets S3, tabla/s DynamoDB, instancias RDS, funci\u00f3n AWS Lambda, etc..). Realizar los casos de uso del 1 al 4. (opcional) Realiza los casos de uso 5 y 6.","title":"Actividades"},{"location":"apuntes/ingesta05python.html#referencias","text":"Python, Boto3, and AWS S3: Demystified: https://realpython.com/python-boto3-aws-s3/ DynamoDB mediante Python Ten Examples of Getting Data from DynamoDB with Python and Boto3 DynamoDB Insert: Performance Basics in Python/Boto3 S3Select mediante Python : Ditch the Database","title":"Referencias"},{"location":"apuntes/nube01.html","text":"Cloud Computing \u00b6 La Nube \u00b6 Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Ventajas \u00b6 As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx. CapEx vs OpEx \u00b6 Hay dos tipos diferentes de gastos que se deben tener en cuenta: CapEx vs OpEx La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Esta elasticidad facilita que la capacidad de c\u00f3mputo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes. Consumo respecto a Capacidad / Tiempo Coste total de propiedad \u00b6 El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micos. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala, el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo. Inconvenientes \u00b6 Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...). Servicios en la nube \u00b6 Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. IaaS \u00b6 La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas. PaaS \u00b6 La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos. SaaS \u00b6 Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. Este concepto se conoce como principio de responsabilidad compartida y lo estudiaremos en detalle en la pr\u00f3xima sesi\u00f3n . \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa. Tipos de arquitectura seg\u00fan la infraestructura \u00b6 Arquitecturas on premise \u00b6 Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware. Arquitecturas cloud \u00b6 Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel (serverless, base de datos administradas...). Arquitecturas h\u00edbridas \u00b6 Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados. Plataformas Cloud \u00b6 En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente... Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: Infraestructura cloud \u00b6 Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas. Regiones y Zonas de disponibilidad \u00b6 A lo largo de todo el globo terr\u00e1queo, se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante enlaces de baja latencia. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestro centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cluster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centro de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que si cae un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n. Ubicaciones de borde \u00b6 Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generar\u00e1n vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Por ejemplo, Amazon despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional) a trav\u00e9s de 90 ciudades en 47 paises. El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , que redirige el tr\u00e1fico a los nodos Cloudfront ( https://aws.amazon.com/es/cloudfront/ ). Despliegue \u00b6 Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Regiones AWS Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos. Actividades \u00b6 A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . Referencias \u00b6 Curso Academy Cloud Foundation de Amazon Web Services . Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure AWS Certified Cloud Practitioner Training 2020 - Full Course","title":"1.- Cloud Computing"},{"location":"apuntes/nube01.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"apuntes/nube01.html#la-nube","text":"Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet de manera que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software.","title":"La Nube"},{"location":"apuntes/nube01.html#ventajas","text":"As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Adem\u00e1s, permite escalar la aplicaci\u00f3n a nivel mundial, desplegando las aplicaciones en diferente regiones de todo el mundo con s\u00f3lo unos clicks. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto, de manera casi instant\u00e1nea. No hay que esperar a adquirir y montar los recursos (en vez de tardar del orden de semanas pasamos a minutos). Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx.","title":"Ventajas"},{"location":"apuntes/nube01.html#capex-vs-opex","text":"Hay dos tipos diferentes de gastos que se deben tener en cuenta: CapEx vs OpEx La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan pago previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costes asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las virtudes, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo, con lo cual el riesgo se reduce al m\u00ednimo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Esta elasticidad facilita que la capacidad de c\u00f3mputo se ajuste a la demanda real, en contraposici\u00f3n por un planteamiento de infraestructura in-house/on-premise donde tenemos que estimar cual va a ser la necesidad de la empresa y adquirir la infraestructura por adelantado teniendo en cuenta que: hay que aprovisionar por encima de la demanda, lo que es un desperdicio econ\u00f3mico. si la demanda crece por encima de la estimaci\u00f3n, tendr\u00e9 un impacto negativo en la demanda con la consiguiente p\u00e9rdida de clientes. Consumo respecto a Capacidad / Tiempo","title":"CapEx vs OpEx"},{"location":"apuntes/nube01.html#coste-total-de-propiedad","text":"El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memoria): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Se dice que una soluci\u00f3n cloud supone una mejora de un orden de magnitud, es decir, 10 veces m\u00e1s econ\u00f3micos. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para construir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Un concepto que conviene conocer es el de econom\u00eda de escala, el cual plantea que al disponer de miles de clientes, la plataforma cloud adquiere los productos a un precio inferior al de mercado y que luego repercute en los clientes, que acaban pagando un precio por uso m\u00e1s bajo.","title":"Coste total de propiedad"},{"location":"apuntes/nube01.html#inconvenientes","text":"Ya hemos comentado las virtudes de utilizar una soluci\u00f3n cloud, pero tambi\u00e9n cabe destacar sus desventajas: Necesita una conexi\u00f3n a internet continua y r\u00e1pida. En las arquitecturas h\u00edbridas, puede haber bastante latencia. Hay funcionalidades que todav\u00eda no est\u00e1n implementadas, aunque su avance es continuo y salen soluciones nuevas cada mes. Puede haber una falta de confianza: Los datos guardados pueden ser accedidos por otros Nuestros datos ya no est\u00e1n en la empresa Problemas legales (datos protegidos por leyes europeas que se encuentran en servidor americanos, ...) Dependencia tecnol\u00f3gica con compa\u00f1\u00edas ajenas (Amazon, Microsoft, ...).","title":"Inconvenientes"},{"location":"apuntes/nube01.html#servicios-en-la-nube","text":"Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo.","title":"Servicios en la nube"},{"location":"apuntes/nube01.html#iaas","text":"La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas.","title":"IaaS"},{"location":"apuntes/nube01.html#paas","text":"La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos.","title":"PaaS"},{"location":"apuntes/nube01.html#saas","text":"Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. Este concepto se conoce como principio de responsabilidad compartida y lo estudiaremos en detalle en la pr\u00f3xima sesi\u00f3n . \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa.","title":"SaaS"},{"location":"apuntes/nube01.html#tipos-de-arquitectura-segun-la-infraestructura","text":"","title":"Tipos de arquitectura seg\u00fan la infraestructura"},{"location":"apuntes/nube01.html#arquitecturas-on-premise","text":"Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware.","title":"Arquitecturas on premise"},{"location":"apuntes/nube01.html#arquitecturas-cloud","text":"Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. El planteamiento de todo en la nube suele utilizarse en proyectos nuevos o en la actualizaci\u00f3n de cero de los proyectos existentes. Abarca implementaciones que s\u00f3lo utilizan recursos de bajo nivel (redes, servidores, etc) o bien servicios de alto nivel (serverless, base de datos administradas...).","title":"Arquitecturas cloud"},{"location":"apuntes/nube01.html#arquitecturas-hibridas","text":"Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. El planteamiento h\u00edbrido es el m\u00e1s habitual (respecto a un cloud puro), donde los servicios se van migrando poco a poco (buscando primero ampliar o resolver carencias) coexistiendo con la infraestructura actual que est\u00e1 en la organizaci\u00f3n, normalmente conectada mediante VPN y enlaces dedicados.","title":"Arquitecturas h\u00edbridas"},{"location":"apuntes/nube01.html#plataformas-cloud","text":"En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub, Dropbox, Spotify Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente... Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure:","title":"Plataformas Cloud"},{"location":"apuntes/nube01.html#infraestructura-cloud","text":"Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas.","title":"Infraestructura cloud"},{"location":"apuntes/nube01.html#regiones-y-zonas-de-disponibilidad","text":"A lo largo de todo el globo terr\u00e1queo, se han construido enormes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ) situadas en ubicaciones aisladas. Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Cada zona de disponibilidad est\u00e1 aislada, pero las zonas de disponibilidad de una regi\u00f3n est\u00e1n conectadas mediante enlaces de baja latencia. Una zona de disponibilidad se representa mediante un c\u00f3digo de regi\u00f3n seguido de un identificador de letra, por ejemplo, us-east-1a . Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita, y adem\u00e1s, es la m\u00e1s econ\u00f3mica). Por ejemplo, en AWS, dentro de la regi\u00f3n us-east-1 del Norte de Virginia, se encuentran 6 zonas de disponibilidad: us-east-1a , us-east-1b , us-east-1c , us-east-1d , us-east-1e , us-east-1f . En cambio, en us-east-2 s\u00f3lo tiene tres AZ: us-east-2a , us-east-2b y us-east-2c . Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. En otras ocasiones elegiremos la regi\u00f3n que asegura las leyes y regulaciones que nuestras aplicaciones deben cumplir. Finalmente, en el caso de una nube h\u00edbrida, elegiremos la regi\u00f3n m\u00e1s cercana a nuestro centro de datos corporativo. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente, mediante el dise\u00f1o de una arquitectura con un cluster que reparta las peticiones a partir de un balanceador de carga entre, al menos, dos AZ distintas. As\u00ed, si cae una AZ, la otra dar\u00e1 respuesta a todas las peticiones. Un fallo en una AZ (normalmente en uno de los centro de datos que contiene) no afectar\u00e1 los servicios que est\u00e1n dise\u00f1ados para trabajar fuera de las AZ, como las diferentes opciones de almacenamiento ni de los servicios globales como DNS o CDN. Cabe destacar que si cae un centro de datos de una AZ no implica que caigan el resto de centros de datos de la misma AZ donde nuestras aplicaciones pueden estar replicadas. Adem\u00e1s, cada AZ est\u00e1 aislada del resto de AZ dentro de la misma regi\u00f3n.","title":"Regiones y Zonas de disponibilidad"},{"location":"apuntes/nube01.html#ubicaciones-de-borde","text":"Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. A menudo, las ubicaciones de borde est\u00e1n cerca de las zonas de gran poblaci\u00f3n que generar\u00e1n vol\u00famenes de tr\u00e1fico elevados. As\u00ed pues, se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Por ejemplo, Amazon despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional) a trav\u00e9s de 90 ciudades en 47 paises. El acceso a estos CDN se realiza gracias al DNS interno que utiliza cada proveedor. En el caso de AWS se conoce como Amazon Route 53 , que redirige el tr\u00e1fico a los nodos Cloudfront ( https://aws.amazon.com/es/cloudfront/ ).","title":"Ubicaciones de borde"},{"location":"apuntes/nube01.html#despliegue","text":"Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Regiones AWS Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions La localizaci\u00f3n exacta de cada una de estas regiones y zonas de disponibilidad es difusa a prop\u00f3sito. A los proveedores, por temas de seguridad, no les interesa que se sepa donde se localizan los recursos.","title":"Despliegue"},{"location":"apuntes/nube01.html#actividades","text":"A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realiza el m\u00f3dulo 1 (Informaci\u00f3n general sobre los conceptos de la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube01.html#referencias","text":"Curso Academy Cloud Foundation de Amazon Web Services . Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure AWS Certified Cloud Practitioner Training 2020 - Full Course","title":"Referencias"},{"location":"apuntes/nube02aws.html","text":"Amazon Web Services \u00b6 Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones. Servicios \u00b6 Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados: Almacenamiento \u00b6 Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo coste para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para utilizarlo tanto con los servicios cloud de AWS como con los recursos en on-premise. Dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Almacenamiento en AWS . Inform\u00e1tica / Computaci\u00f3n \u00b6 Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 3.- Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores Docker en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS. Bases de Datos \u00b6 Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Datos en AWS . Redes \u00b6 Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS. Seguridad en AWS \u00b6 Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS. Servicios de administraci\u00f3n de costes \u00b6 Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo. Administraci\u00f3n y gobernanza de datos \u00b6 La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorizar recursos y aplicaciones. Si CloudTrail monitoriza personas, CloudWatch monitoriza servicios. CloudWatch es perfecto para asegurar de que los servicios de la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se pueden pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla: Redes en AWS \u00b6 Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad. Amazon VPC \u00b6 AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00edamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalle, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b VPC Wizard \u00b6 Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad. IP El\u00e1stica \u00b6 Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html Seguridad en la Nube \u00b6 La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero. Modelo de responsabilidad compartida de AWS \u00b6 La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general. Responsabilidad de AWS \u00b6 AWS es responsable de proteger la infraestructura en la que se ejecutan todos los servicios ofrecidos por la nube de AWS (en algunas preguntas de la certificaci\u00f3n se refieren a ellos por servicios de la nube): Seguridad f\u00edsica de los centros de datos con acceso controlado basado en las necesidades en instalaciones sin identificaci\u00f3n, con guardias de seguridad permanentes, autenticaci\u00f3n de dos factores, revisi\u00f3n y registro de accesos, videovigilancia, y destrucci\u00f3n y desmagnetizaci\u00f3n de discos. Infraestructura de hardware , como servidores, dispositivos de almacenamiento y otros dispositivos de los que dependen los servicios de AWS. Infraestructura de software , que aloja sistemas operativos, aplicaciones de servicios y software de virtualizaci\u00f3n. Infraestructura de red , como routers, conmutadores, balanceadores de carga, firewalls y cableado. AWS tambi\u00e9n monitoriza la red en l\u00edmites externos, protege los puntos de acceso y proporciona infraestructura redundante con detecci\u00f3n de intrusiones de forma constante Responsabilidad del cliente \u00b6 El cliente es responsable del cifrado de los datos en reposo y los datos en tr\u00e1nsito, de todo lo que se pone en la nube. Los pasos de seguridad que debe tomar depender\u00e1n de los servicios que utilice y de la complejidad del sistema. Si entramos en m\u00e1s detalle, es responsable de: El sistema operativo de la instancia de Amazon EC2 : incluidas las actualizaciones, los parches de seguridad y su mantenimiento. La protecci\u00f3n de las aplicaciones que se lanzan en los recursos AWS: contrase\u00f1as, acceso basado en roles, etc. Configuraci\u00f3n del grupo de seguridad . SO o firewalls basados en host: incluidos los sistemas de detecci\u00f3n o prevenci\u00f3n de intrusiones. Configuraciones de red . Administraci\u00f3n de cuentas : Configuraci\u00f3n de inicio de sesi\u00f3n y credenciales para cada usuario. Respecto al contenido cr\u00edtico, el cliente es responsable de administrar: El contenido que elige almacenar en AWS. Los servicios de AWS que se utilizan con el contenido. En qu\u00e9 pa\u00eds se almacena ese contenido. El formato y la estructura de ese contenido y si est\u00e1 enmascarado, cifrado o es an\u00f3nimo. Qui\u00e9n tiene acceso a ese contenido y c\u00f3mo se conceden, administran y revocan esos derechos de acceso. AWS IAM \u00b6 AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM. IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz. Autenticaci\u00f3n \u00b6 Cuando se define un usuario de IAM se indica qu\u00e9 tipo de acceso puede utilizar el usuario para obtener acceso a los recursos de AWS: acceso mediante programaci\u00f3n: mediante email y clave de acceso secreta cuando realice una llamada a la API de AWS mediante la CLI de AWS, el SDK de AWS o cualquier otra herramienta de desarrollo. acceso a la consola de administraci\u00f3n de AWS: mediante usuario / contrase\u00f1a m\u00e1s el ID/alias de cuenta. Es recomendable activar MFA ( Multi-Factor Authentication ) para a\u00f1adir una capa m\u00e1s de seguridad. acceso mediante ambos tipos Autorizaci\u00f3n \u00b6 Una vez que el usuario se ha autenticado, se ha de determinar qu\u00e9 permisos debe concederse a un usuario, servicio o aplicaci\u00f3n. De forma predeterminada, los usuarios de IAM no tienen permiso para obtener acceso a los recursos o los datos en una cuenta de AWS. En su lugar, debe conceder permisos de forma expl\u00edcita a un usuario, grupo o rol mediante la creaci\u00f3n de una pol\u00edtica de IAM, ya que por defecto, se denegar\u00e1n todas las acciones que no se hayan permitido expl\u00edcitamente. Consejo Seguir el principio de m\u00ednimo privilegio: conceder \u00fanicamente los privilegios de usuario m\u00ednimos que necesita el usuario. El alcance de las configuraciones del servicio de IAM es global, se aplican en todas las regiones de AWS. Pol\u00edticas IAM \u00b6 Una pol\u00edtica de IAM es una instrucci\u00f3n formal mediante un documento JSON con los permisos que se conceder\u00e1 a una entidad. Las entidad es incluyen usuarios, grupos, roles o recursos. Las pol\u00edticas especifican cu\u00e1les son las acciones permitidas, cu\u00e1les son los recursos a los que estas tienen permiso y cu\u00e1l ser\u00e1 el efecto cuando el usuario solicite acceso a los recursos. Info Una sola pol\u00edtica se puede asociar a varias entidades. Una sola entidad puede tener varias pol\u00edticas asociadas a ella. Hay dos tipos de pol\u00edticas de IAM: pol\u00edticas basadas en identidad : controlan qu\u00e9 acciones puede realizar dicha identidad, en qu\u00e9 recursos y en qu\u00e9 condiciones. A su vez se dividen en administradas (asociada a varios usuarios/grupos/roles) o insertadas (un \u00fanico usuario/grupo/rol). pol\u00edticas basadas en recursos : son documentos de pol\u00edtica JSON que se asocian a un recurso (por ejemplo, un bucket de S3). Estas pol\u00edticas controlan qu\u00e9 acciones puede realizar una entidad principal especificada en dicho recurso y en qu\u00e9 condiciones. Destacar que no todos los servicios de AWS soportan este tipo de pol\u00edticas. Pol\u00edticas y permisos \u00b6 El usuario solo podr\u00e1 realizar la acci\u00f3n si la acci\u00f3n solicitada no est\u00e1 denegada de forma expl\u00edcita y adem\u00e1s est\u00e1 permitida de forma expl\u00edcita. Cuando IAM determina si se concede un permiso, primero comprueba la existencia de cualquier pol\u00edtica de denegaci\u00f3n expl\u00edcita aplicable. Si no existe ninguna denegaci\u00f3n expl\u00edcita, comprueba si existe alguna pol\u00edtica de permisos expl\u00edcitos aplicable. Si no existe una pol\u00edtica de denegaci\u00f3n expl\u00edcita ni de permiso expl\u00edcito, IAM vuelve a la forma predeterminada, que consiste en denegar el acceso. Este proceso se denomina denegaci\u00f3n impl\u00edcita . Permisos IAM Otros servicios relacionados con la seguridad AWS Organizations : Permite configurar los permisos de una organizaci\u00f3n que contiene varias cuentas de usuario en unidades organizativas (UO), y unificar tanto la seguridad como la facturaci\u00f3n AWS Key Management Service (AWS KMS): servicio que permite crear y administrar claves de cifrado Amazon Cognito : permite controlar el acceso a recursos de AWS desde aplicaciones con una credencial \u00fanica mediante SAML. AWS Shield : servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones ejecutadas en AWS. Pr\u00e1cticas recomendadas \u00b6 Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS. AWS CLI \u00b6 AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema operativo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola del curso Leaner Labs , tras arrancar el laboratorio, pulsaremos (1) en la opci\u00f3n AWS Details , y posteriormente veremos los datos de acceso temporales al pulsar (2) en Show de la opci\u00f3n AWS CLI : Credenciales de AWS Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> aws configure Otra forma de configurar estos valores es mediante el comando aws configure (), el cual nos preguntar\u00e1 los siguientes datos: AWS Access Key ID [****************6YUJ]: AWS Secret Access Key [****************4TEz]: Default region name [us-east-1]: Default output format [None]: El problema es que no nos solicita el token de sesi\u00f3n, por lo cual s\u00f3lo lo podemos utilizar si tenemos una cuenta real de AWS. Orden Al ejecutar comandos AWS CLI, AWS CLI buscar\u00e1 las credenciales primero en las variables de entorno y, a continuaci\u00f3n, en el archivo de configuraci\u00f3n. Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias. AWS Cloudshell \u00b6 Es un shell integrado en la consola web que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operacionales ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Interfaz de Cloudshell Actividades \u00b6 Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity . (opcional) Realiza el m\u00f3dulo 4 (Seguridad en la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS . Referencias \u00b6 Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS","title":"2.- AWS"},{"location":"apuntes/nube02aws.html#amazon-web-services","text":"Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones.","title":"Amazon Web Services"},{"location":"apuntes/nube02aws.html#servicios","text":"Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados:","title":"Servicios"},{"location":"apuntes/nube02aws.html#almacenamiento","text":"Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo coste para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de ficheros NFS el\u00e1stico, escalable y completamente administrado para utilizarlo tanto con los servicios cloud de AWS como con los recursos en on-premise. Dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento. Servicios de almacenamiento de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Almacenamiento en AWS .","title":"Almacenamiento"},{"location":"apuntes/nube02aws.html#informatica-computacion","text":"Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores ( serverless ). S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada). Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 3.- Computaci\u00f3n en AWS . Los servicios que est\u00e1n relacionado con el uso de contenedores son: Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento (permite lanzar miles de contenedores Docker en segundos), compatible con los contenedores Docker . Mantiene y escala la flota de nodos que ejecutan los contenedores eliminando la complejidad de poner en marcha la infraestructura. Amazon Fargate : motor para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de almacenamiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS.","title":"Inform\u00e1tica / Computaci\u00f3n"},{"location":"apuntes/nube02aws.html#bases-de-datos","text":"Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Servicios de datos de AWS Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Datos en AWS .","title":"Bases de Datos"},{"location":"apuntes/nube02aws.html#redes","text":"Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones y APIs de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( on-premise ) a un \u00fanico gateway . Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS.","title":"Redes"},{"location":"apuntes/nube02aws.html#seguridad-en-aws","text":"Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones autorizadas en sus cuentas. Amazon Cognito facilita incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves de acceso. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS.","title":"Seguridad en AWS"},{"location":"apuntes/nube02aws.html#servicios-de-administracion-de-costes","text":"Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y gasto de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS facilita la definici\u00f3n de presupuestos personalizados que generar\u00e1n una alerta cuando los costes o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costes y el uso de AWS a lo largo del tiempo.","title":"Servicios de administraci\u00f3n de costes"},{"location":"apuntes/nube02aws.html#administracion-y-gobernanza-de-datos","text":"La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que facilita realizar un seguimiento del inventario de recursos y sus cambios. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Esto significa que cada vez que alguien carga datos, ejecuta c\u00f3digo, crea una instancia EC2, cambia un tipo de unidad S3 o cualquier otra acci\u00f3n que se pueda realizar en AWS, CloudTrail lo registrar\u00e1. Esto resulta muy \u00fatil por razones de seguridad para que los administradores puedan saber qui\u00e9n est\u00e1 utilizando su cuenta y qu\u00e9 est\u00e1n haciendo. Si algo sale mal o si surge un problema de seguridad, CloudTrail ser\u00e1 la mejor prueba para averiguar lo ocurrido. Amazon CloudWatch : permite monitorizar recursos y aplicaciones. Si CloudTrail monitoriza personas, CloudWatch monitoriza servicios. CloudWatch es perfecto para asegurar de que los servicios de la nube funcionan sin problemas y no utilizan m\u00e1s o menos recursos de los esperados, lo que es importante para el seguimiento del presupuesto. CloudWatch es excelente para asegurarse de que todos los recursos est\u00e1n funcionando, lo que puede resultar complicado si una gran empresa utiliza cientos de m\u00e1quinas y unidades diferentes. Para ello, se pueden pueden configurar alertas para que se lancen cuando una m\u00e9trica alcanza un l\u00edmite espec\u00edfico. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : proporciona consejos para optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar las cargas de trabajo. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla:","title":"Administraci\u00f3n y gobernanza de datos"},{"location":"apuntes/nube02aws.html#redes-en-aws","text":"Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Cabe destacar que AWS reserva las primeras cuatro direcciones IP y la \u00faltima direcci\u00f3n IP de cada subred para fines de redes internas. Por ejemplo, una subred / 28 tendr\u00eda 16 direcciones IP disponibles. De ah\u00ed hay que restar las 5 IP reservadas por AWS para obtener 11 direcciones IP para nuestro uso dentro de la subred. Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , con la ventaja que en la nube nos ahorraremos gran parte de la complejidad.","title":"Redes en AWS"},{"location":"apuntes/nube02aws.html#amazon-vpc","text":"AWS utiliza las VPC ( Amazon Virtual Private Cloud ) como redes privadas virtuales donde est\u00e1n conectados todos los recursos con los que trabajamos, de manera que el acceso queda aislado de otros usuarios. Dicho de otro modo, Amazon VPC permite lanzar recursos de AWS en la red virtual que definamos. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00edamos en nuestro propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS, pudiendo crear una VPC que abarque varias AZ. Al definir la red virtual podemos seleccionar nuestro propio intervalo de direcciones IP, crear subredes y configurar las tablas de enrutamiento y gateways de red. Tambi\u00e9n podemos colocar el backend (servidores de aplicaciones o de bases de datos) en una subred privada sin acceso a Internet p\u00fablico. Finalmente, podemos a\u00f1adir varias capas de seguridad, como grupos de seguridad y listas de control de acceso a la red (ACL de red), para ayudar a controlar el acceso a las instancias de EC2 en cada subred. Sin entrar en mayor detalle, ya que se sale del \u00e1mbito del curso, vamos a repasar algunos de los componentes m\u00e1s importantes: Un gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. Un caso espec\u00edfico es un Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Despu\u00e9s de crear una VPC, podemos agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. Para que las subredes privadas puedan conectarse a Internet dirigiendo el tr\u00e1fico al gateway NAT hemos de configurar las tablas enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se colocan de m\u00e1s a menos restrictivas. Tienen una ruta local integrada, la cual no se puede eliminar.\u200b Las rutas adicionales se agregan a la tabla.\u200b Aunque lo veremos en el siguiente apartado, las VPC utilizan un grupo de seguridad , que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Los grupos de seguridad tienen reglas que controlan el tr\u00e1fico de entrada y de salida de las instancias, las cuales podemos modificar. \u200bLos grupos de seguridad predeterminados deniegan todo el tr\u00e1fico de entrada y permiten todo el tr\u00e1fico de salida.\u200b","title":"Amazon VPC"},{"location":"apuntes/nube02aws.html#vpc-wizard","text":"Cada vez que vayamos a crear un recurso en AWS nos va a preguntar en qu\u00e9 VPC queremos desplegar la soluci\u00f3n. Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard , la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. VPC Wizard: Paso 1 Como podemos ver en el imagen, el asistente nos ofrece 4 modelos de redes: VPC con un \u00fanica subred p\u00fablica VPC con subredes p\u00fablicas y privadas VPC con subredes p\u00fablicas y privadas y acceso VPN a hardware on-premise VPC con un \u00fanica subred privada solo accesible via VPN con hardware on-premise. Si elegimos el primero, podemos ver que la informaci\u00f3n a completar se reduce al bloque de direcciones (se suele dejar el bloque por defecto) y un nombre para la VPC. VPC Wizard: Paso 2 Una vez creada ya podemos modificar la configuraci\u00f3n DHCP, la tabla de enrutamiento o los permisos via ACL, crear subredes sobre la propia VPC, etc... Redes y subredes Mientras que las VPC pertenecen a una \u00fanica regi\u00f3n de AWS y pueden abarcar varias zonas de disponibilidad, las subredes pertenecen a una \u00fanica zona de disponibilidad.","title":"VPC Wizard"},{"location":"apuntes/nube02aws.html#ip-elastica","text":"Una IP el\u00e1stica es una direcci\u00f3n IP p\u00fablica que AWS reserva para que la podamos asignar a una instancia para poder acceder a ella a trav\u00e9s de internet de forma fija. Normalmente salvo que decidamos hacer una estructura de red m\u00e1s compleja, mediante un VPC personalizado, en realidad AWS da una IP al azar a nuestras instancias al arrancarlas. La diferencia es que si le asignamos una IP el\u00e1stica ya quedar\u00e1 fija entre reinicios, especialmente \u00fatil si nuestra m\u00e1quina aloja un dominio. Tambi\u00e9n es muy \u00fatil para poder reasignar instancias y otros recursos en caso de fallo, de manera que podamos desconectar la ip el\u00e1stica de la instancia y asociarla a otra para redirigir el tr\u00e1fico de red\u200b. IP El\u00e1sticas Para evitar el acaparamiento de direcciones IP, AWS cobra 0,005\u20ac por cada hora y direcci\u00f3n IP el\u00e1stica que tengamos reservada sin asignar a ninguna instancia. Sin embargo, su uso es gratuito si la tenemos asignadas a una instancia o recurso en ejecuci\u00f3n. De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco IP el\u00e1sticas por regi\u00f3n, aunque se puede solicitar un aumento del l\u00edmite. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html","title":"IP El\u00e1stica"},{"location":"apuntes/nube02aws.html#seguridad-en-la-nube","text":"La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero.","title":"Seguridad en la Nube"},{"location":"apuntes/nube02aws.html#modelo-de-responsabilidad-compartida-de-aws","text":"La seguridad es una caracter\u00edstica que tiene una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general.","title":"Modelo de responsabilidad compartida de AWS"},{"location":"apuntes/nube02aws.html#aws-iam","text":"AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). Una sola cuenta de AWS puede tener servicios administrados por decenas de personas diferentes que pueden estar en distintos departamentos u oficinas, tener diferentes responsabilidades o niveles de antig\u00fcedad, e incluso estar en distintos pa\u00edses. Para mantener un entorno seguro en la nube con todas estas variables en cuesti\u00f3n, es esencial seguir las pr\u00e1cticas recomendadas de IAM. IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz.","title":"AWS IAM"},{"location":"apuntes/nube02aws.html#practicas-recomendadas","text":"Proteger las claves de acceso de usuario ra\u00edz de la cuenta de AWS. Crear usuarios individuales de IAM. Utilizar grupos de usuarios para asignar permisos a los usuarios de IAM. Conceder menos privilegios. Comenzar a utilizar los permisos con las pol\u00edticas administradas de AWS. Validar las pol\u00edticas que hayamos creado. Utilizar pol\u00edticas administradas (se pueden asignar a varias identidades) por el cliente en lugar de pol\u00edticas en integradas (s\u00f3lo existen en una identidad de IAM). Utilizar los niveles de acceso para revisar los permisos de IAM. Configurar una pol\u00edtica de contrase\u00f1as seguras para los usuarios. Habilitar la autenticaci\u00f3n multifactor (MFA). Utilizar roles para aplicaciones que se ejecutan en instancias de Amazon EC2. Utilizar roles para delegar permisos. No compartir claves de acceso. Cambiar las credenciales regularmente. Eliminar credenciales innecesarias. Utilizar las condiciones de la pol\u00edtica para obtener mayor seguridad. Supervisar la actividad de nuestra cuenta de AWS.","title":"Pr\u00e1cticas recomendadas"},{"location":"apuntes/nube02aws.html#aws-cli","text":"AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema operativo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola del curso Leaner Labs , tras arrancar el laboratorio, pulsaremos (1) en la opci\u00f3n AWS Details , y posteriormente veremos los datos de acceso temporales al pulsar (2) en Show de la opci\u00f3n AWS CLI : Credenciales de AWS Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno (es importante poner el nombre de las claves en may\u00fasculas): export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> aws configure Otra forma de configurar estos valores es mediante el comando aws configure (), el cual nos preguntar\u00e1 los siguientes datos: AWS Access Key ID [****************6YUJ]: AWS Secret Access Key [****************4TEz]: Default region name [us-east-1]: Default output format [None]: El problema es que no nos solicita el token de sesi\u00f3n, por lo cual s\u00f3lo lo podemos utilizar si tenemos una cuenta real de AWS. Orden Al ejecutar comandos AWS CLI, AWS CLI buscar\u00e1 las credenciales primero en las variables de entorno y, a continuaci\u00f3n, en el archivo de configuraci\u00f3n. Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias.","title":"AWS CLI"},{"location":"apuntes/nube02aws.html#aws-cloudshell","text":"Es un shell integrado en la consola web que facilita la gesti\u00f3n, exploraci\u00f3n e interacci\u00f3n con los recursos AWS. Al acceder ya estaremos pre-autenticados con las credencias de la consola, y la mayor\u00eda de herramientas operacionales ya est\u00e1n pre-instaladas, con lo que es entrar y ponerse a trabajar. De esta manera podemos trabajar con AWS CLI con solo entrar a nuestro usuario de AWS. Interfaz de Cloudshell","title":"AWS Cloudshell"},{"location":"apuntes/nube02aws.html#actividades","text":"Realiza el m\u00f3dulo 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) del curso ACF de AWS . Instala en tu ordenador AWS CLI y con\u00e9ctate a AWS desde el terminal. Realiza una captura donde se vea los datos de ejecutar aws sts get-caller-identity . (opcional) Realiza el m\u00f3dulo 4 (Seguridad en la nube) del curso ACF de AWS . (opcional) Realiza el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube02aws.html#referencias","text":"Overview of Amazon Web Services Redes y entrega de contenido en AWS Seguridad en la nube con AWS","title":"Referencias"},{"location":"apuntes/nube03computacion.html","text":"Computaci\u00f3n en la nube \u00b6 Introducci\u00f3n \u00b6 Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio. Amazon EC2 \u00b6 Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual. Paso 1: AMI \u00b6 Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde. Tipos de AMI \u00b6 Puede elegir entre los siguientes tipos de AMI: Quick Start : AWS ofrece una serie de AMI predise\u00f1adas, tanto Linux como Windows, para lanzar las instancias. Mis AMI : estas son las AMI que hemos creado nosotros, ya sea a partir de m\u00e1quinas locales que hayamos creado en VmWare, VirtualBox, o una previa que hemos creado en una instancia EC2, configurado y luego exportado. AWS Marketplace : cat\u00e1logo que incluye miles de soluciones de software creadas por empresas terceras (las cuales pueden cobrar por su uso). Estas AMI pueden ofrecer casos de uso espec\u00edficos para que pueda ponerse en marcha r\u00e1pidamente. AMI de la comunidad : estas son AMI creadas por personas de todo el mundo.AWS no controla estas AMI, as\u00ed que deben utilizarse bajo la propia responsabilidad, evitando su uso en entornos corporativos o de producci\u00f3n. Las AMI dependen de la regi\u00f3n Las AMI que creamos se hacen en la regi\u00f3n en la que estamos conectados. Si la necesitamos en otra regi\u00f3n, debemos realizar un proceso de copia. Paso 2: Tipo de instancias \u00b6 El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino. Categor\u00edas \u00b6 Las categor\u00edas de tipos de instancia incluyen instancias de uso general, optimizadas para inform\u00e1tica, optimizadas para memoria, optimizadas para almacenamiento y de inform\u00e1tica acelerada. Categor\u00eda Tipo de instancia Caso de uso Uso general a1, m4, m5, t2, t3 Amplio Computaci\u00f3n c4, c5 Alto rendimiento Memoria r4, r5 , x1, z1 Big Data Inform\u00e1tica acelerada f1, g3, g4, p2, p3 Machine Learning Almacenamiento d2, h1, i3 Sistemas de archivos distribuidos Tipos de instancias \u00b6 Los tipos de instancias ( https://aws.amazon.com/es/ec2/instance-types/ ) ofrecen familias, generaciones y tama\u00f1os . As\u00ed pues, el tipo de instancia t3.large referencia a la familia T , de la tercera generaci\u00f3n y con un tama\u00f1o large . En general, los tipos de instancia que son de una generaci\u00f3n superior son m\u00e1s potentes y ofrecen una mejor relaci\u00f3n calidad/precio. Comparando tipos de instancias Cuando se comparan los tama\u00f1os hay que examinar la parte del coeficiente en la categor\u00eda de tama\u00f1o. Por ejemplo, una instancia t3.2xlarge tiene el doble de CPU virtual y memoria que una t3.xlarge . A su vez, la instancia t3.xlarge tiene el doble de CPU virtual y memoria que una t3.large . Tambi\u00e9n se debe tener en cuenta que el ancho de banda de red tambi\u00e9n est\u00e1 vinculado al tama\u00f1o de la instancia de Amazon EC2. Si ejecutar\u00e1 trabajos que requieren un uso muy intensivo de la red, es posible que deba aumentar las especificaciones de la instancia para que satisfaga sus necesidades. A la hora de elegir un tipo de instancia, nos centraremos en la cantidad de n\u00facleos, el tama\u00f1o de la memoria, el rendimiento de la red y las tecnolog\u00edas de la propia CPU (si tiene habilitada GPU y FPGA) Paso 3: Configuraci\u00f3n de la instancia / red \u00b6 El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita. Asociar un rol de IAM \u00b6 Si necesitamos que nuestras instancias EC2 ejecuten una aplicaci\u00f3n que debe realizar llamadas seguras de la API a otros servicios de AWS, en vez de dejar anotadas las credenciales en el c\u00f3digo de la aplicaci\u00f3n (esto es una muy mala pr\u00e1ctica que puede acarrear problemas de seguridad), debemos asociar un rol de IAM a una instancia EC2. El rol de IAM asociado a una instancia EC2 se almacena en un perfil de instancia . Si creamos el rol desde esta misma pantalla, AWS crear\u00e1 un perfil de instancia autom\u00e1ticamente y le otorgar\u00e1 el mismo nombre que al rol. En el desplegable la lista que se muestra es, en realidad, una lista de nombres de perfiles de instancia. Cuando definimos un rol que una instancia EC2 puede utilizar, estamos configurando qu\u00e9 cuentas o servicios de AWS pueden asumir dicho rol, as\u00ed como qu\u00e9 acciones y recursos de la API puede utilizar la aplicaci\u00f3n despu\u00e9s de asumir el rol. Si cambia un rol, el cambio se extiende a todas las instancias que tengan el rol asociado. La asociaci\u00f3n del rol no est\u00e1 limitada al momento del lanzamiento de la instancia, tambi\u00e9n se puede asociar un rol a una instancia que ya exista. Script de datos de usuario \u00b6 Al momento de crear las instancias EC2, de forma opcional, podemos especificar un script de datos de usuario durante el lanzamiento de la instancia. Los datos de usuario pueden automatizar la finalizaci\u00f3n de las instalaciones y las configuraciones durante el lanzamiento de la instancia. Por ejemplo, un script de datos de usuario podr\u00eda colocar parches en el sistema operativo de la instancia y actualizarlo, recuperar e instalar claves de licencia de software, o instalar sistemas de software adicionales. Por ejemplo, si queremos instalar un servidor de Apache, de manera que arranque autom\u00e1ticamente y que muestre un Hola Mundo podr\u00edamos poner #!/bin/bash yum update -y yum -y install httpd systemctl enable httpd systemctl start httpd echo '<html><h1>Hola Mundo desde el Severo!</h1></html>' > /var/www/html/index.html Script en Windows Si nuestra instancia es de Windows, el script de datos de usuario debe escribirse en un formato que sea compatible con una ventana del s\u00edmbolo del sistema (comandos por lotes) o con Windows PowerShell. De forma predeterminada, los datos de usuario s\u00f3lo se ejecutan la primera vez que se inicia la instancia. Paso 4: Almacenamiento \u00b6 Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias). Paso 5: Etiquetas \u00b6 Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso. Paso 6: Grupo de seguridad \u00b6 Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. Servidor Web Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad. Paso 7: An\u00e1lisis e identificaci\u00f3n \u00b6 El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia Por ejemplo, si utilizamos la Amazon Linux AMI y descargamos las claves de AWS Academy (suponiendo que la ip p\u00fablica de la m\u00e1quina que hemos creado es 3.83.80.52 ) nos conectar\u00edamos mediante: ssh -i labsuser.pem ec2-user@3.83.80.52 M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Claves en AWS Academy Nuestro usuario tiene creado por defecto un par de claves que se conocen como vockey . Esta claves se pueden descargar desde la opci\u00f3n AWS Details del laboratorio de Learner Lab . M\u00e1s adelante, en esta misma sesi\u00f3n, veremos c\u00f3mo utilizarlas. Uso de la consola \u00b6 En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a comentar un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-04ad2567c9e3d7893 --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia t2.micro key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Comandos AWS CLI Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s \u00fatiles a la hora de trabajar con AWS CLI. Caso de uso mediante AWS CLI \u00b6 A continuaci\u00f3n vamos a crear un grupo de seguridad que permita el acceso via HTTP al puerto 80 y HTTPS al puerto 443 y conexi\u00f3n mediante SSH al puerto 22. Para ello, primero creamos el grupo de seguridad utilizando el comando create-security-group : aws ec2 create-security-group --group-name iabd-front \\ --description \"Grupo de seguridad para frontend\" A continuaci\u00f3n a\u00f1adimos el acceso a ssh utilizando el comando authorize-security-group-ingress : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 22 --cidr 0 .0.0.0/0 A continuaci\u00f3n habilitamos el acceso http : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 80 --cidr 0 .0.0.0/0 A continuaci\u00f3n habilitamos el acceso https : aws ec2 authorize-security-group-ingress --group-name iabd-front \\ --protocol tcp --port 443 --cidr 0 .0.0.0/0 Si queremos consultar el grupo de seguridad: aws ec2 describe-security-groups --group-name iabd-front Una vez creado y configurado el grupo de seguridad, vamos a crear una instancia utilizando el comando run-instances a partir de la AMI ami-03ae0589c3c7b8599 (es una imagen Ubuntu 20.04), y crearemos un instancia de tipo t3.large (su coste aproximado es de menos de 10 c\u00e9ntimos por hora) con el grupo de seguridad que acabamos de crear y 30GB de almacenamiento EBS: aws ec2 run-instances --image-id ami-03ae0589c3c7b8599 \\ --count 1 --instance-type t3.large \\ --key-name vockey --security-groups iabd-front \\ --tag-specifications \"ResourceType=instance,Tags=[{Key=Name,Value=iabd}]\" \\ --ebs-optimized \\ --block-device-mapping \"[ { \\\"DeviceName\\\": \\\"/dev/sda1\\\", \\\"Ebs\\\": { \\\"VolumeSize\\\": 30 } } ]\" El comando describe-instances tambi\u00e9n nos permite obtener informaci\u00f3n de nuestras instancias utilizando filtros. Asi pues, por ejemplo, si queremos obtener la ip p\u00fablica de la instancia que acabamos de crear podemos hacer: aws ec2 describe-instances \\ --filters \"Name=tag:Name,Values=iabd\" \\ --query \"Reservations[*].Instances[*].PublicIpAddress\" \\ --output text El siguiente paso es conectarnos a nuestra instancia. Como hemos creado la instancia utilizando las credenciales vockey , vamos a descargar las claves desde la consola de Learner Labs donde antes hab\u00edamos consultado nuestras credenciales. Claves SSH en AWS Academy Una vez descargada la clave, ya sea mediante Download PEM o Download PPK , nos conectaremos utilizando el usuario ubuntu mediante: SSH mediante Linux / Mac Putty mediante Windows Si nos situamos sobre la carpeta que contiene en el archivo descargado chmod 400 labsuser.pem Una vez que ya tenemos permisos de lectura sobre el archivo, nos conectamos mediante el comando ssh : ssh -i labsuser.pem ubuntu@<ip-publica> Descargamos el archivo labuser.ppk , y una vez configurada la ip p\u00fablica, en Connection -> SSH -> Auth , en la parte inferior donde podemos cargarlo mediante el bot\u00f3n Browse : Configuraci\u00f3n de Putty Ciclo de vida de las instancias \u00b6 Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de terminaci\u00f3n / apag\u00e1ndose) Terminated (terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping (deteni\u00e9ndose) : las instancias que cuentan con vol\u00famenes EBS se pueden detener. Stopped (detenida) : no generar\u00e1 los mismos costes que una instancia en el estado running . S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que utilizan como almacenamiento EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asoci\u00e1ndola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se pagan por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n. Monitorizaci\u00f3n \u00b6 Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias. Costes de las instancias \u00b6 Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS Academy es en esa modalidad), pero conviene conocer el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y es apto para la capa gratuita de AWS. bajo coste y flexibilidad. Cargas de trabajo de corto plazo, con picos o impredecibles. Tambi\u00e9n para desarrollo o prueba de aplicaciones. spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot. AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos. Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda. Carga de trabajo din\u00e1mica y a gran escala. Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional. instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve. Descuento en el cargo por hora por el uso de la instancia (hasta 72%). Plazo de 1 o 3 a\u00f1os. Asegura capacidad de c\u00f3mputo disponible cuando se la necesita. Cargas de trabajo de uso predecible o estado estable. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n. host reservado / dedicado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso. Ahorro de dinero en costes de licencia. Asistencia para cumplir los requisitos normativos y de conformidad. Licencia Bring your own (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo. PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cuotas mensuales. NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual. Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/ Optimizaci\u00f3n de costes \u00b6 Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las instancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las instancias EBS que ya no se necesitan o las copias de seguridad ya pasadas. AWS Lambda \u00b6 La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions . Creando una funci\u00f3n \u00b6 Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n (en el caso de AWS Academy, elegimos el rol LabRole ), mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda Cargando c\u00f3digo Adem\u00e1s de poder utilizar el IDE que ofrece AWS, podemos subir nuestras propias funciones en formato zip o desde S3. El fichero que contiene las funciones por defecto se llamar\u00e1 lambda_function y el manejador def_handler . Si queremos cambiar alguno de esos nombres, hay que editar el controlador en la configuraci\u00f3n en tiempo de ejecuci\u00f3n de la funci\u00f3n. Restricciones \u00b6 Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos. AWS Elastic Beanstalk \u00b6 AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Beanstalk autom\u00e1ticamente administra la implementaci\u00f3n, desde el aprovisionamiento de capacidad, el balanceo de carga y el escalado autom\u00e1tico hasta la monitorizaci\u00f3n del estado de las aplicaciones. Al mismo tiempo, si queremos, podemos mantener el control total de los recursos de AWS que alimentan la aplicaci\u00f3n y acceder a los recursos subyacentes en cualquier momento. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker, y se desplegan en servidores como Apache, Nginx o IIS. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga por los recursos que AWS utilice (instancia, base de datos, almacenamiento S3, etc...) Actividades \u00b6 Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Crea una instancia ec2 mediante AWS CLI , siguiendo todos los pasos del apartado Uso de la consola . Adjunta una captura con todos los comandos empleados y el resultado que aparece en la consola. Adem\u00e1s, con\u00e9ctate mediante SSH a la m\u00e1quina creada, y realiza una nueva captura. (opcional) Mediante AWS Lambda, realiza una funci\u00f3n que reciba del evento dos n\u00fameros (por ejemplo, a y b ) y devuelva un objeto JSON con el total de la suma. Adjunta captura del c\u00f3digo fuente, del evento de prueba y de las m\u00e9tricas capturadas tras probar la funci\u00f3n 10 veces. Referencias \u00b6 Amazon EC2 Funciones Lambda en AWS","title":"3.- Computaci\u00f3n"},{"location":"apuntes/nube03computacion.html#computacion-en-la-nube","text":"","title":"Computaci\u00f3n en la nube"},{"location":"apuntes/nube03computacion.html#introduccion","text":"Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio.","title":"Introducci\u00f3n"},{"location":"apuntes/nube03computacion.html#amazon-ec2","text":"Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual.","title":"Amazon EC2"},{"location":"apuntes/nube03computacion.html#paso-1-ami","text":"Una imagen de Amazon Machine ( AMI ) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde.","title":"Paso 1: AMI"},{"location":"apuntes/nube03computacion.html#paso-2-tipo-de-instancias","text":"El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino.","title":"Paso 2: Tipo de instancias"},{"location":"apuntes/nube03computacion.html#paso-3-configuracion-de-la-instancia-red","text":"El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita.","title":"Paso 3: Configuraci\u00f3n de la instancia / red"},{"location":"apuntes/nube03computacion.html#paso-4-almacenamiento","text":"Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias).","title":"Paso 4: Almacenamiento"},{"location":"apuntes/nube03computacion.html#paso-5-etiquetas","text":"Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera podemos clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso.","title":"Paso 5: Etiquetas"},{"location":"apuntes/nube03computacion.html#paso-6-grupo-de-seguridad","text":"Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia, formando parte de la VPC. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. Servidor Web Si hemos seguido el ejemplo anterior y hemos a\u00f1adido en los datos de usuario el script para instalar Apache, debemos habilitar las peticiones entrantes en el puerto 80. Para ello crearemos una regla que permita el tr\u00e1fico HTTP. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad.","title":"Paso 6: Grupo de seguridad"},{"location":"apuntes/nube03computacion.html#paso-7-analisis-e-identificacion","text":"El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente (formato X.509), continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que la clave privada la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia Por ejemplo, si utilizamos la Amazon Linux AMI y descargamos las claves de AWS Academy (suponiendo que la ip p\u00fablica de la m\u00e1quina que hemos creado es 3.83.80.52 ) nos conectar\u00edamos mediante: ssh -i labsuser.pem ec2-user@3.83.80.52 M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... IAM Recuerda que en el caso de otros recursos cloud, como el almacenamiento masivo, bases de datos, serverless, etc, lo normal ser\u00e1 controlar el acceso mediante la estructura de permisos IAM, que permite establecer pol\u00edticas definidas y el uso de roles. En resumen, las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de nuestra cuenta. Podemos elegir entre muchos tipos de instancias, con diferentes combinaciones de CPU, RAM, almacenamiento y redes. Adem\u00e1s, podemos configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos). Al crear una instancia, mediante los datos de usuario, podemos especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Claves en AWS Academy Nuestro usuario tiene creado por defecto un par de claves que se conocen como vockey . Esta claves se pueden descargar desde la opci\u00f3n AWS Details del laboratorio de Learner Lab . M\u00e1s adelante, en esta misma sesi\u00f3n, veremos c\u00f3mo utilizarlas.","title":"Paso 7: An\u00e1lisis e identificaci\u00f3n"},{"location":"apuntes/nube03computacion.html#uso-de-la-consola","text":"En la sesi\u00f3n anterior ya utilizamos AWS CLI para conectarnos a AWS. En el caso concreto de EC2, es muy \u00fatil para crear, arrancar y detener instancias. Todos los comandos comenzar\u00e1n por aws ec2 , seguida de la opci\u00f3n deseada. Si usamos el comando aws ec2 help obtendremos un listado enorme con todas las posibilidades. Vamos a comentar un par de casos de uso. Por ejemplo, para ejecutar una instancia utilizaremos el comando: aws ec2 run-instances --image-id ami-04ad2567c9e3d7893 --count 1 --instance-type c3.large --key-name MiParejaDeClaves --security-groups MiGrupoSeguridad --region us-east-1 Los par\u00e1metros que permiten configurar la instancia son: image-id : este par\u00e1metro va seguido de un ID de AMI. Recordad que todas las AMI tienen un ID de \u00fanico. count : puede especificar m\u00e1s de una instancia. instance-type : tipo de instancia que se crear\u00e1, como una instancia t2.micro key-name : supongamos que MiParejaDeClaves ya existe. security-groups : supongamos que MiGrupoSeguridad ya existe. region : las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. Para que cree la instancia EC2, se debe cumplir que el comando tiene el formato correcto, y que todos los recursos y permisos existen, as\u00ed como saldo suficiente. Si queremos ver las instancias que tenemos creadas ejecutaremos el comando: aws ec2 describe-instances Comandos AWS CLI Es muy \u00fatil utilizar alguna de las cheatsheet disponibles en la red con los comandos m\u00e1s \u00fatiles a la hora de trabajar con AWS CLI.","title":"Uso de la consola"},{"location":"apuntes/nube03computacion.html#ciclo-de-vida-de-las-instancias","text":"Las instancias en todo momento tienen un estado que se puede consultar: Pending (pendiente) : nada m\u00e1s lanzarse o al arrancar una instancia detenida. Running (en ejecuci\u00f3n) : cuando arranc\u00f3 la instancia por completo y est\u00e1 lista para su uso. En este momento se empieza a facturar. Rebooting (reiniciada) : AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de utilizar el reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo DNS p\u00fablico y la misma IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shutting down (en proceso de terminaci\u00f3n / apag\u00e1ndose) Terminated (terminada) : las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se destruya la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping (deteni\u00e9ndose) : las instancias que cuentan con vol\u00famenes EBS se pueden detener. Stopped (detenida) : no generar\u00e1 los mismos costes que una instancia en el estado running . S\u00f3lo se paga por el almacenamiento de datos. Solo se pueden detener las instancias que utilizan como almacenamiento EBS. Ciclo de vida de una instancia IPs est\u00e1ticas A cada instancia que recibe una IP p\u00fablica se le asigna tambi\u00e9n un DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25 , el nombre de host DNS externo podr\u00eda ser ec2-203-0-113-25.compute-1.amazonaws.com . AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia. Si necesitamos una IP p\u00fablica fija, se recomienda utilizar una IP el\u00e1stica, asoci\u00e1ndola primero a la regi\u00f3n donde vaya a residir la instancia EC2. Recuerda que las IP el\u00e1sticas se pagan por cada hora que las tenemos reservadas y se deja de pagar por ellas si est\u00e1n asociadas a una instancia en ejecuci\u00f3n.","title":"Ciclo de vida de las instancias"},{"location":"apuntes/nube03computacion.html#monitorizacion","text":"Aunque ya lo veremos en una sesi\u00f3n m\u00e1s adelante, podemos monitorizar las instancias EC2 mediante la herramienta Amazon CloudWatch con los datos que recopila y procesa, los cuales convierte en m\u00e9tricas legibles en intervalos por defecto de 5 minutos (aunque se puede habilitar el monitoreo detallado y monitorizar cada minuto) Estas estad\u00edsticas se registran durante un periodo de 15 meses, lo que nos permite obtener informaci\u00f3n hist\u00f3rica y sobre el rendimiento de nuestras instancias.","title":"Monitorizaci\u00f3n"},{"location":"apuntes/nube03computacion.html#costes-de-las-instancias","text":"Normalmente cuando iniciemos una instancia usaremos instancias bajo demanda (el cr\u00e9dito concedido por AWS Academy es en esa modalidad), pero conviene conocer el resto de formas que ofrecen diferentes facturaciones. AWS ofrece diferentes tipos pago de instancia: Tipo Descripci\u00f3n Beneficios Uso bajo demanda se paga por hora, no tiene compromisos a largo plazo, y es apto para la capa gratuita de AWS. bajo coste y flexibilidad. Cargas de trabajo de corto plazo, con picos o impredecibles. Tambi\u00e9n para desarrollo o prueba de aplicaciones. spot Se puja por ellas. Se ejecutan siempre que est\u00e9n disponibles y que su oferta est\u00e9 por encima del precio de la instancia de spot. AWS puede interrumpirlas con una notificaci\u00f3n de 2 minutos. Los precios pueden ser considerablemente m\u00e1s econ\u00f3micos en comparaci\u00f3n con las instancias bajo demanda. Carga de trabajo din\u00e1mica y a gran escala. Aplicaciones con horarios flexibles de inicio y finalizaci\u00f3n. Aplicaciones que solo son viables con precios de computaci\u00f3n muy bajos. Usuarios con necesidades de computaci\u00f3n urgentes de grandes cantidades de capacidad adicional. instancia reservada Pago inicial completo, parcial o nulo para las instancias que reserve. Descuento en el cargo por hora por el uso de la instancia (hasta 72%). Plazo de 1 o 3 a\u00f1os. Asegura capacidad de c\u00f3mputo disponible cuando se la necesita. Cargas de trabajo de uso predecible o estado estable. Aplicaciones que requieren capacidad reservada, incluida la recuperaci\u00f3n de desastres. Usuarios capaces de afrontar pagos iniciales para reducir a\u00fan m\u00e1s los costes de computaci\u00f3n. host reservado / dedicado Servidor f\u00edsico con capacidad de instancias EC2 totalmente dedicado a su uso. Ahorro de dinero en costes de licencia. Asistencia para cumplir los requisitos normativos y de conformidad. Licencia Bring your own (BYOL). Conformidad y restricciones normativas. Seguimiento del uso y las licencias. Control de la ubicaci\u00f3n de instancias. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias reservadas supondr\u00e1n un ahorro econ\u00f3mico importante, si hay posibilidades econ\u00f3micas y previsi\u00f3n (de 12 a 36 meses), hasta de un 75% seg\u00fan las diferentes opciones: AURI - All up-front reserved instance : se realiza un pago inicial completo. PURI - Partial up-front reserved instance : se realiza una pago inicial parcial y cuotas mensuales. NURI - No up-front reserved instance : sin pago inicial, se realiza un pago mensual. Modelos de pago de las instancias reservadas El planteamiento ideal es utilizar instancias reservadas para la carga m\u00ednima de base de nuestro sistema, bajo demanda para autoescalar seg\u00fan necesidades y quiz\u00e1 las instancias spot para cargas opcionales que se contemplar\u00e1n s\u00f3lo si el coste es bajo. Puedes consultar el coste de las diferentes instancias en https://aws.amazon.com/es/ec2/pricing/reserved_instances , y consultar precios en https://aws.amazon.com/es/ec2/pricing/reserved-instances/pricing/","title":"Costes de las instancias"},{"location":"apuntes/nube03computacion.html#optimizacion-de-costes","text":"Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o : consiste en conseguir el equilibrio adecuado de los tipos de instancias. Los servidores pueden desactivarse o reducirse y seguir cumpliendo con sus requisitos de rendimiento. Si seguimos las m\u00e9tricas de Amazon Cloudwatch podremos ver el porcentaje de actividades de las instancias o los rangos horarios donde est\u00e1n inactivas. Se recomienda primero adaptar el tama\u00f1o, y una vez que ya es estable la configuraci\u00f3n, utilizar instancias reservadas. Aumento de la elasticidad : mediante soluciones el\u00e1sticas podemos reducir la capacidad del servidor (por ejemplo, deteniendo o hibernando las instancias que utilizan Amazon EBS que no est\u00e1n activas, como puedan ser entornos de prueba o durante las noches) o utilizar el escalado autom\u00e1tico para administrar picos de cargas. Modelo de precios \u00f3ptimo : hay que conocer las opciones de precios disponibles, analizando los patrones de uso para combinar los tipos de compra. Por ejemplo, utilizar instancias bajo demanda e instancias de spot para las cargas de trabajo variables, incluso el uso de funciones serverless . Optimizaci\u00f3n de las opciones de almacenamiento : hay que reducir la sobrecarga de almacenamiento sin utilizar siempre que sea posible (reduciendo el tama\u00f1o de los vol\u00famenes) y elegir las opciones de almacenamiento m\u00e1s econ\u00f3micas si cumplen los requisitos de rendimiento de almacenamiento. Otro caso puede ser el eliminar las instancias EBS que ya no se necesitan o las copias de seguridad ya pasadas.","title":"Optimizaci\u00f3n de costes"},{"location":"apuntes/nube03computacion.html#aws-lambda","text":"La inform\u00e1tica serverless permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona tolerancia a errores y escalado autom\u00e1tico, y que se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , Elastic Load Balancing ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Podemos encadenar funciones Lambda para flujos de trabajo mediante AWS Step Functions .","title":"AWS Lambda"},{"location":"apuntes/nube03computacion.html#creando-una-funcion","text":"Al crear una funci\u00f3n Lambda, primero le asignaremos un nombre a la funci\u00f3n. Tras elegir el entorno de ejecuci\u00f3n (versi\u00f3n de Python, Node.js, etc...), hemos de elegir el rol de ejecuci\u00f3n (en el caso de AWS Academy, elegimos el rol LabRole ), mediante un permiso de IAM, dependiendo de los servicios con los que tenga que interactuar...(al menos el rol AWSLambdaBasicExecutionRole y AWSLambdaVPCAccessExecutionRole ). Respecto a la configuraci\u00f3n de la funci\u00f3n, deberemos: Agregar un desencadenador / origen de evento. Agregar el c\u00f3digo de la funci\u00f3n. Especificar la cantidad de memoria en MB que se asignar\u00e1 a la funci\u00f3n (de 128MB a 3008MB) Si queremos, podemos configurar las variables del entorno, la descripci\u00f3n, el tiempo de espera, la VPC espec\u00edfica en la que se debe ejecutar la funci\u00f3n, las etiquetas que desea utilizar y otros ajustes. Ejemplo de funci\u00f3n Lambda Cargando c\u00f3digo Adem\u00e1s de poder utilizar el IDE que ofrece AWS, podemos subir nuestras propias funciones en formato zip o desde S3. El fichero que contiene las funciones por defecto se llamar\u00e1 lambda_function y el manejador def_handler . Si queremos cambiar alguno de esos nombres, hay que editar el controlador en la configuraci\u00f3n en tiempo de ejecuci\u00f3n de la funci\u00f3n.","title":"Creando una funci\u00f3n"},{"location":"apuntes/nube03computacion.html#restricciones","text":"Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos.","title":"Restricciones"},{"location":"apuntes/nube03computacion.html#aws-elastic-beanstalk","text":"AWS ElasticBeanstalk es un servicio PaaS que facilita la implementaci\u00f3n, el escalado y la administraci\u00f3n de aplicaciones y servicios web con rapidez. Nosotros, como desarrolladores, s\u00f3lo deberemos cargar el c\u00f3digo, elegir el tipo de instancia y de base de datos, configurar y ajustar el escalador autom\u00e1tico. Beanstalk autom\u00e1ticamente administra la implementaci\u00f3n, desde el aprovisionamiento de capacidad, el balanceo de carga y el escalado autom\u00e1tico hasta la monitorizaci\u00f3n del estado de las aplicaciones. Al mismo tiempo, si queremos, podemos mantener el control total de los recursos de AWS que alimentan la aplicaci\u00f3n y acceder a los recursos subyacentes en cualquier momento. Ejemplo de despliegue con Beanstalk Es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker, y se desplegan en servidores como Apache, Nginx o IIS. No se aplican cargos por utilizar ElasticBeanstalk , solo se paga por los recursos que AWS utilice (instancia, base de datos, almacenamiento S3, etc...)","title":"AWS Elastic Beanstalk"},{"location":"apuntes/nube03computacion.html#actividades","text":"Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . (opcional) Crea una instancia ec2 mediante AWS CLI , siguiendo todos los pasos del apartado Uso de la consola . Adjunta una captura con todos los comandos empleados y el resultado que aparece en la consola. Adem\u00e1s, con\u00e9ctate mediante SSH a la m\u00e1quina creada, y realiza una nueva captura. (opcional) Mediante AWS Lambda, realiza una funci\u00f3n que reciba del evento dos n\u00fameros (por ejemplo, a y b ) y devuelva un objeto JSON con el total de la suma. Adjunta captura del c\u00f3digo fuente, del evento de prueba y de las m\u00e9tricas capturadas tras probar la funci\u00f3n 10 veces.","title":"Actividades"},{"location":"apuntes/nube03computacion.html#referencias","text":"Amazon EC2 Funciones Lambda en AWS","title":"Referencias"},{"location":"apuntes/nube04almacenamiento.html","text":"Almacenamiento en la nube \u00b6 El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar. Amazon EBS \u00b6 Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad. Vol\u00famenes \u00b6 IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS. Instant\u00e1neas \u00b6 Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS. Amazon S3 \u00b6 S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . Es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket (los objetos pueden ser de hasta 5TB), permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivo se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Tambi\u00e9n tenemos la posibilidad de activar el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Clases de almacenamiento \u00b6 S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/S3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas. Buckets \u00b6 Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Podemos controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n podemos obtener registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, v\u00eddeos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar los datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre severo2122 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato). Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://severo2122.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos. Casos de uso \u00b6 Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada. Costes \u00b6 Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ . Sitio web est\u00e1tico \u00b6 Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo2122web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo2122web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablico M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html S3 Select \u00b6 Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejectuar consulta SQL . Si nos fijamos en la imagen, se crea una tabla fictia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST. Acceso \u00b6 Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo2122cli --region us-east-1 { \"Location\" : \"/severo2122cli\" } Otra forma que veremos m\u00e1s adelante es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremos S3 mediante Python directamente y utilizando AWS Lambda. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Mediante la configuraci\u00f3n de IAM, podemos obtener un control detallado sobre qui\u00e9n puede acceder a los datos. Tambi\u00e9n podemos utilizar las pol\u00edticas de bucket de S3 e, incluso, las listas de control de acceso por objeto (ACL). Seguridad Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Para ello, se recomienda hacer uso de IAM, creando un grupo de usuarios donde definamos los permisos mediante pol\u00edticas. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html . Amazon EFS \u00b6 Amazon Elastic File System (Amazon EFS - https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS. Actividades \u00b6 Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web. (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado. Referencias \u00b6 Amazon EBS Amazon S3 Amazon EFS","title":"4.- Almacenamiento"},{"location":"apuntes/nube04almacenamiento.html#almacenamiento-en-la-nube","text":"El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar.","title":"Almacenamiento en la nube"},{"location":"apuntes/nube04almacenamiento.html#amazon-ebs","text":"Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de EC2 solamente dentro de la misma zona de disponibilidad.","title":"Amazon EBS"},{"location":"apuntes/nube04almacenamiento.html#volumenes","text":"IOPS El t\u00e9rmino IOPS, operaciones de entrada y salida por segundo , representa una medida de rendimiento frecuente que se utiliza para comparar dispositivos de almacenamiento. Un art\u00edculo muy interesante es What you need to know about IOPS . Los vol\u00famenes de EBS proporcionan almacenamiento externo a EC2 que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones: Unidades de estado s\u00f3lido (SSD) : optimizadas para cargas de trabajo de transacciones que implican operaciones de lectura/escritura frecuentes de peque\u00f1o tama\u00f1o de E/S. Proporciona un equilibrio entre precio y rendimiento, y es el tipo recomendado para la mayor\u00eda de las cargas de trabajo. Los tipos existentes son gp3 (1.000 MiB/s) y gp2 (128-250 MiB/s) ambas con un m\u00e1ximo de 16.000 IOPS. SSD de IOPS provisionadas : proporciona un rendimiento elevado con cargas de trabajo cr\u00edticas, baja latencia o alto rendimiento. Los tipos existentes con io2 Block Express (4.000 MiB/s con un m\u00e1ximo 246.000 IOPS) e io2 (1.000 MiB/s con 64.000 IOPS) Unidades de disco duro (HDD) : optimizadas para grandes cargas de trabajo de streaming. Los tipos existentes con st1 (500 MiB/s con 500 IOPS) y sc1 (250 MiB/s con 250 IOPS). M\u00e1s informaci\u00f3n sobre los diferentes vol\u00famenes: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/ebs-volume-types.html Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de Elastic Block Store y el submen\u00fa Vol\u00famenes : Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Al tener un alto nivel de disponibilidad y de confianza, pueden aprovecharse como particiones de arranque de instancias de EC2 o asociarse a una instancia de EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen mayor durabilidad que los almacenes de instancias de EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: ofrecen al menos el mismo o m\u00e1s rendimiento que las unidades de Amazon EC2 locales. Nivel de fiabilidad alto: tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Un volumen en una instancia S\u00f3lo una instancia de Amazon EC2 a la vez puede montarse en un volumen de Amazon EBS.","title":"Vol\u00famenes"},{"location":"apuntes/nube04almacenamiento.html#instantaneas","text":"Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon S3 y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS (clonando o restaurando copias de seguridad) y permiten proteger la durabilidad de los datos a largo plazo. Como todo recurso S3, tambi\u00e9n se pueden compartir f\u00e1cilmente con compa\u00f1eros del equipo de desarrollo y otros desarrolladores de AWS.","title":"Instant\u00e1neas"},{"location":"apuntes/nube04almacenamiento.html#amazon-s3","text":"S3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que adem\u00e1s de que los datos contengan metadatos que ayudan a catalogar el objeto, si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Esto puede tener implicaciones de rendimiento y consistencia, que conviene tener en cuenta. Los datos se almacenan como objetos dentro de recursos conocidos como buckets . Es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket (los objetos pueden ser de hasta 5TB), permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Replicaci\u00f3n en S3 Los datos que almacenamos en S3 no est\u00e1n asociados a ning\u00fan servidor en particular (aunque los buckets se asocien a regiones, los archivo se dice que est\u00e1n almacenados de forma global), con lo que no necesitamos administrar ning\u00fan tipo de servidor. Replicaci\u00f3n en S3 Tambi\u00e9n tenemos la posibilidad de activar el versionado de los archivos , de manera que cuando actualicemos un objeto, en vez de sustituirlo, se crea una nuevo versi\u00f3n manteniendo un hist\u00f3rico. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/versioning-workflows.html Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor.","title":"Amazon S3"},{"location":"apuntes/nube04almacenamiento.html#clases-de-almacenamiento","text":"S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/S3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: S3 Est\u00e1ndar : dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, es una opci\u00f3n adecuada para aplicaciones en la nube, sitios web din\u00e1micos, distribuci\u00f3n de contenido, aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de big data . S3 Est\u00e1ndar - Acceso poco frecuente : se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. Es una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. S3 \u00danica zona \u2013 Acceso poco frecuente : dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario, pero sin tener replicas (la clase S3 est\u00e1ndar replica los datos en un m\u00ednimo de tres AZ). Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. S3 Intelligent-Tiering : dise\u00f1ada para optimizar los costes mediante la migraci\u00f3n autom\u00e1tica de los datos entre capas, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Se encarga de monitorizar los patrones de acceso de los objetos y traslada aquellos a los que no se ha accedido durante 30 d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. S3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): es una clase de almacenamiento seguro, duradero y de bajo coste para archivar datos a largo plazo. Para que los costes se mantengan bajos, S3 Glacier proporciona tres opciones de recuperaci\u00f3n (recuperaci\u00f3n acelerada, est\u00e1ndar y masiva), que van desde unos pocos minutos a unas horas. Podemos cargar objetos directamente en S3 Glacier o utilizar pol\u00edticas de ciclo de vida para transferir datos entre cualquiera de las clases de almacenamiento de S3 para datos activos y S3 Glacier . Pol\u00edtica de ciclo de vida Una pol\u00edtica de ciclo de vida define qu\u00e9 va a pasar con los datos partiendo de su almacenamiento masivo en S3 est\u00e1ndar, pasando a uso poco frecuente y seguidamente a Glacier y finalmente para su eliminaci\u00f3n, en base a plazos o m\u00e9tricas y reduciendo costes de forma autom\u00e1tica. Pol\u00edtica de ciclo de vida Para ello, se puede monitorizar un bucket completo, un prefijo o una etiqueta de objeto, de manera que podamos evaluar los patrones de acceso y ajustar la pol\u00edtica de ciclo de vida. S3 Glacier Deep Archive : es la clase de almacenamiento de menor coste en S3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Dise\u00f1ado inicialmente los a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de 7 a 10 a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. Tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Todos los objetos almacenados en S3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12 horas.","title":"Clases de almacenamiento"},{"location":"apuntes/nube04almacenamiento.html#buckets","text":"Amazon S3 almacena los datos en buckets, los cuales son los bloques b\u00e1sicos donde se estructura la informaci\u00f3n, actuando como contenedores l\u00f3gicos de objetos. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo Amazon S3 a nivel mundial. Podemos controlar el acceso a cada bucket mediante mecanismos de control de acceso (ACL) que pueden aplicarse tanto a objetos individuales como a los buckets, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n podemos obtener registros de acceso al bucket y a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde Amazon S3 almacenar\u00e1 el bucket y su contenido. Para cargar los datos (como fotos, v\u00eddeos o documentos), primero hemos de crear un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargar casi cualquier cantidad de objetos en el bucket (los objetos pueden ocupar hasta 5TB). Cuando creamos un bucket en S3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacenamos datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. S3 est\u00e1 dise\u00f1ado para almacenar los datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. Creamos el bucket Por ejemplo, vamos a crear un bucket dentro de la regi\u00f3n us-east-1 con el nombre severo2122 (recuerda que el nombre debe ser \u00fanico y en min\u00fasculas, as\u00ed como evitar las tildes, \u00f1, etc...). Para almacenar un objeto en S3 , debemos cargarlo en un bucket. Cargando el bucket Para cargar un archivo, una vez elegido el bucket sobre el que queremos cargar, simplemente arrastrando el fichero , \u00e9ste se subir\u00e1 a S3 (tambi\u00e9n podemos establecer permisos sobre los datos y cualquier metadato). Ya hemos comentado que un objeto est\u00e1 compuesto por los datos y cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. En nuestro caso su URL ser\u00eda https://severo2122.s3.amazonaws.com/labS3.csv S3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de cada bucket a medida que aumenta la cantidad de datos. S3 tambi\u00e9n es escalable, lo que permite gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se facturar\u00e1 por lo que utilicemos.","title":"Buckets"},{"location":"apuntes/nube04almacenamiento.html#casos-de-uso","text":"Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a S3 en un servicio adecuado para distintos casos: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, ya sea nuestra propia aplicaci\u00f3n hospedada on-premise , como las aplicaciones de EC2 o mediante servidores en otros hostings . Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, podemos delegar la entrega de contenido de nuestra aplicaci\u00f3n y permitir que los clientes la consigan ellos mismos. Para el alojamiento web est\u00e1tico. S3 puede entregar el contenido est\u00e1tico de un sitio web, que incluye HTML, CSS, JavaScript y otros archivos. Para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, S3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucket de S3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de S3 . Diferencias entre EBS y S3 EBS solo se puede utilizar cuando se conecta a una instancia EC2 y se puede acceder a Amazon S3 por s\u00ed solo. EBS no puede contener tantos datos como S3 . EBS solo se puede adjuntar a una instancia EC2 , mientras que varias instancias EC2 pueden acceder a los datos de un bucket de S3 . S3 experimenta m\u00e1s retrasos que Amazon EBS al escribir datos. As\u00ed pues, es el usuario o el dise\u00f1ador de la aplicaci\u00f3n quien debe decidir si el almacenamiento de Amazon S3 o de Amazon EBS es el m\u00e1s apropiado para una aplicaci\u00f3n determinada.","title":"Casos de uso"},{"location":"apuntes/nube04almacenamiento.html#costes","text":"Con S3 , los costes espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizan. Solo se paga por lo que se utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo se paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantes a S3 ni por las transferencias salientes desde S3 a las ubicaciones de borde de Amazon CloudFront dentro de esa misma regi\u00f3n. Para calcular los costes de S3 hay que tener en cuenta: Clase de almacenamiento y cantidad almacenada: El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999.999.999% (11 nueves) de durabilidad y 99,99% (4 nueves) de disponibilidad. Por ejemplo, los primeros 50 TB/mes cuestan 0,023$ por GB. El almacenamiento Est\u00e1ndar - Acceso poco frecuente ofrece la misma durabilidad de 99,999.999.999% (11 nueves) de S3 , pero con 99,9% (3 nueves) de disponibilidad en un a\u00f1o concreto. Su precio parte desde los 0,0125$ por GB. Y si elegimos el almacenamiento poco frecuente pero en una \u00fanica zona, el precio pasa a ser de 0,01$ por GB. Si fuese a la capa Glacier, con una opci\u00f3n de recuperaci\u00f3n de 1 minutos a 12 horas el precio baja a 0,004$ por GB. Finalmente, con Glacier Deep Archive (archivos que se recuperan 1 o 2 veces al a\u00f1o con plazos de recuperaci\u00f3n de 12 horas) baja hasta 0,000.99$ por GB Solicitudes: se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos (0,000.4$ por cada 1.000 solicitudes) a tasas diferentes de las de otras solicitudes, como PUT y COPY (0,005$ cada 1.000 solicitudes). Transferencia de datos: se considera la cantidad de datos transferidos fuera de la regi\u00f3n de S3 , los datos salientes, siendo el primer GB gratuito y luego comienza a facturar a 0,09$ por GB. La transferencia entrante de datos es gratuita. La informaci\u00f3n actualizada y detallada se encuentra disponible en https://aws.amazon.com/es/s3/pricing/ .","title":"Costes"},{"location":"apuntes/nube04almacenamiento.html#sitio-web-estatico","text":"Vamos a hacer un caso pr\u00e1ctico de uso de S3. AWS permite que un bucket funcione como un sitio web est\u00e1tico. Para ello, una vez creado el bucket , sobre sus propiedades, al final de la p\u00e1gina, podemos habilitar el alojamiento de web est\u00e1ticas. Para este ejemplo, primero creamos un bucket llamado severo2122web . A continuaci\u00f3n subiremos nuestro archivo siteEstatico.zip descomprimido al bucket. Para que la web sea visible, tenemos que modificar los permisos para que no bloquee el acceso p\u00fablico. As\u00ed pues, en la pesta\u00f1a de permisos del bucket deshabilitamos todas las opciones. Haciendo el bucket p\u00fablico Una vez que tenemos el bucket visible, tenemos que a\u00f1adir una pol\u00edtica para acceder a los recursos del mismo (la pol\u00edtica tambi\u00e9n la podemos crear desde el generador de pol\u00edticas que tenemos disponible en la misma p\u00e1gina de edici\u00f3n): { \"Id\" : \"Policy1633602259164\" , \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , \"Effect\" : \"Allow\" , \"Principal\" : \"*\" , \"Action\" : \"s3:GetObject\" , \"Resource\" : \"arn:aws:s3:::severo2122web/*\" } ] } Tras ello, ahora tenemos que configurar el bucket como un sitio web. Para ello, en las propiedades, en la parte final de la p\u00e1gina, tenemos la opci\u00f3n de Alojamiento de sitios web est\u00e1ticos , la cual debemos habilitar y posteriormente nos mostrar\u00e1 la URL de acceso a nuestro sitio web. Sitio Web p\u00fablico M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html","title":"Sitio web est\u00e1tico"},{"location":"apuntes/nube04almacenamiento.html#s3-select","text":"Amazon S3 Select permite utilizar instrucciones SQL sencillas para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesitemos. Si utilizamos S3 Select para filtrar los datos, podemos reducir la cantidad de datos que Amazon transfiere, lo que reduce tambi\u00e9n los costes y la latencia para recuperarlos. Admite los formatos CSV , JSON o Apache Parquet , ya sea en crudo o comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON ), as\u00ed como objetos cifrados del lado del servidor. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles es recomendable consultar la referencia SQL de S3 . Cargando el bucket Por ejemplo, si trabajamos sobre el bucket que hab\u00edamos creado, tras seleccionarlo, en las Acciones de objeto , elegiremos la opci\u00f3n de Consultar con S3 Select , y si no queremos configurar nada, podemos ejecutar una consulta de tipo select desde la propia ventana mediante el bot\u00f3n Ejectuar consulta SQL . Si nos fijamos en la imagen, se crea una tabla fictia denominada s3object que referencia al documento cargado. Si queremos hacer referencia a columna, podemos hacerlo por su posici\u00f3n (por ejemplo s._1 referencia a la primera columna) o por el nombre de la columna (en nuestro caso, s.VendorID ). Es importante marcar la casilla Excluir la primera l\u00ednea de CSV datos si la primera fila de nuestro CSV contiene etiquetas a modo de encabezado. Si pulsamos sobre el bot\u00f3n de Agregar SQL desde plantillas , podremos seleccionar entre algunas consultas predefinidas (contar, elegir columnas, filtrar los datos, etc...). Autoevaluaci\u00f3n Los datos que hemos cargado en el ejemplo est\u00e1n relacionados con trayectos de taxis. 1. El campo VendorID tiene dos posibles valores: 1 y 2: \u00bf Cuantos viajes han hecho los vendor de tipo 1? 2. Cuando el campo payment_type tiene el valor 1, est\u00e1 indicando que el pago se ha realizado mediante tarjeta de cr\u00e9dito. A su vez, el campo total_amount almacena el coste total de cada viaje \u00bfCuantos viajes se han realizado y cuanto han recaudado los trayectos que se han pagado mediante tarjeta de cr\u00e9dito? Para transformar el tipo de un campo, se emplea la funci\u00f3n cast .Por ejemplo si queremos que interprete el campo total como de tipo float har\u00edamos cast(s.total as float) o si fuera entero como cast(s.total as int) . Puedes probar tambi\u00e9n con los datos almacenados en un fichero comprimido . La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, deberemos utilizar la AWS CLI o la API REST.","title":"S3 Select"},{"location":"apuntes/nube04almacenamiento.html#acceso","text":"Podemos obtener acceso a S3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n se puede acceder a S3 de forma privada a trav\u00e9s de una VPC. Por ejemplo, como ya conoces la AWS CLI, podr\u00edamos utilizarla para crear un bucket : Creando un bucket Resultado aws s3api create-bucket --bucket severo2122cli --region us-east-1 { \"Location\" : \"/severo2122cli\" } Otra forma que veremos m\u00e1s adelante es el acceso a los datos de los bucket directamente a trav\u00e9s de servicios REST, mediante puntos de enlace que admiten el acceso HTTP o HTTPS. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremos S3 mediante Python directamente y utilizando AWS Lambda. Para facilitar la integraci\u00f3n de S3 con otros servicios, S3 ofrece notificaciones de eventos que permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucket espec\u00edfico. Estas notificaciones se pueden enviar o utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Mediante la configuraci\u00f3n de IAM, podemos obtener un control detallado sobre qui\u00e9n puede acceder a los datos. Tambi\u00e9n podemos utilizar las pol\u00edticas de bucket de S3 e, incluso, las listas de control de acceso por objeto (ACL). Seguridad Recuerda que hay que controlar el acceso a los recursos, y en especial a S3. Si lo dejamos abierto, cualquier podr\u00e1 introducir datos con el consiguiente incremento en el coste. Para ello, se recomienda hacer uso de IAM, creando un grupo de usuarios donde definamos los permisos mediante pol\u00edticas. Tambi\u00e9n podemos cifrar los datos en tr\u00e1nsito y habilitar el cifrado del lado del servidor en nuestros objetos. M\u00e1s informaci\u00f3n en https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/selecting-content-from-objects.html .","title":"Acceso"},{"location":"apuntes/nube04almacenamiento.html#amazon-efs","text":"Amazon Elastic File System (Amazon EFS - https://aws.amazon.com/es/efs/ ) ofrece almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea , de manera similar a un NAS ( Network Area Storage ). Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS), al que acceden varios miles de instancia EC2 as\u00ed como servidores on-premise a traves de una VPN o conexiones directas ( AWS Direct Connect ). Se trata de un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Mediante una interfaz sencilla permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. EFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos, no necesitando asignar espacio inicial. Respecto al rendimiento, su IOPS escala de forma autom\u00e1tica conforme crece el tama\u00f1o del sistema de archivos, ofreciendo dos modos, el de uso general (ofrece alrededor de 7000 operaciones por segundo y fichero) y el max I/O (para miles de instancias que acceden al mismo archivo de forma simultanea), pudiendo admitir un rendimiento superior a 10 GB/seg y hasta 500.000 IOPS. Las instancias se conectan a EFS desde cualquier AZ de la regi\u00f3n. Todas las lecturas y escrituras son consistentes en todas las AZ. Por ejemplo, una lectura en una AZ garantiza que tendr\u00e1 la misma informaci\u00f3n, aunque los datos se hayan escrito en otra AZ. EFS compartido entre instancias Respecto al coste ( https://aws.amazon.com/es/efs/pricing/ ), dependiendo del tipo de acceso y la administraci\u00f3n del ciclo de vida, el acceso est\u00e1ndard se factura desde 0,30$ Gb/mes, mientras que si el acceso es poco frecuente, baja a 0,013$ Gb/mes m\u00e1s 0,01$ por transferencia y Gb/mes. Su casos de uso m\u00e1s comunes son para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. Respecto a su acceso, de manera similar al resto de servicios de almacenamiento, es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS.","title":"Amazon EFS"},{"location":"apuntes/nube04almacenamiento.html#actividades","text":"Realizar el m\u00f3dulo 7 (Almacenamiento) del curso ACF de AWS . (opcional) Sigue el ejemplo de la web est\u00e1tica para crear un bucket que muestre el contenido como un sitio web. Adjunta una captura de pantalla del navegador una vez puedas acceder a la web. (opcional) A partir del ejemplo de S3 Select, realiza las consultas propuestas en la autoevaluaci\u00f3n (utilizando el archivo comprimido) y realiza capturas donde se vea tanto la consulta como su resultado.","title":"Actividades"},{"location":"apuntes/nube04almacenamiento.html#referencias","text":"Amazon EBS Amazon S3 Amazon EFS","title":"Referencias"},{"location":"apuntes/nube05datos.html","text":"Datos en la nube \u00b6 Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches. Datos relacionales - Amazon RDS \u00b6 AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones. Instancias de bases de datos \u00b6 Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB Alta disponibilidad \u00b6 Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal. R\u00e9plica de lectura \u00b6 RDS tambi\u00e9n admite la creaci\u00f3n de r\u00e9plicas de lectura para MySQL, MariaDB, PostgreSQLy Amazon Aurora. R\u00e9plica de lectura Las actualizaciones que se realizan en la instancia principal se copian de manera as\u00edncrona en la instancia de r\u00e9plica de lectura, de manera que direccionando las consultas a esta nueva r\u00e9plica reduciremos la carga de la instancia principal. Las r\u00e9plicas de lectura tambi\u00e9n pueden convertirse en la instancia de base de datos principal, pero, debido a la replicaci\u00f3n as\u00edncrona, este proceso debe hacerse de forma manual. Las r\u00e9plicas de lectura pueden crearse en una regi\u00f3n diferente a la utilizada por la base de datos principal, lo que puede mejorar la recuperaci\u00f3n de desastres y/o disminuir la latencia al dirigir las lecturas a una r\u00e9plica de lectura lo m\u00e1s cercana al usuario. Casos de uso \u00b6 AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2). Costes \u00b6 El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducir\u00edan en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os). Ejemplo RDS \u00b6 A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndar con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Instancias permitidas en AWS Academy Si quer\u00e9is crear bases de datos con m\u00e1quinas m\u00e1s potentes, pod\u00e9is utilizar instancias hasta nivel medium , y a ser posible a r\u00e1fagas (instancias t ). Dentro de la Configuraci\u00f3n adicional , es importante deshabilitar la monitorizaci\u00f3n mejorada (no tenemos permiso para su uso en AWS Academy ). Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL. Amazon Aurora \u00b6 Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Cabe destacar que si creamos una base de datos serverless, Amazon no permite hacerla p\u00fablica, de manera que \u00fanicamente se puede acceder desde otro servicio de AWS. Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales. Datos NoSQL - DynamoDB \u00b6 DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una. Componentes y particiones \u00b6 Los componentes principales son: las tablas : son conjuntos de datos, formada por los elementos. los elementos : grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos : elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, ya que est\u00e1 formada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. Adem\u00e1s, DynamoDB permite crear \u00edndices para optimizar las consultas que realicemos sobre atributos que no forman parte de la clave de partici\u00f3n u ordenamiento. Infraestructura \u00b6 Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente, distribuyendo los datos entre diferentes dispositivos de almacenamiento. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Otras caracter\u00edsticas clave son las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elijamos, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Costes \u00b6 Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes. Ejemplo DynamoDB \u00b6 A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Tambi\u00e9n pod\u00edamos haber creado la tabla mediante el comando create-table de AWS CLI: aws dynamodb create-table \\ --table-name ProductCatalog \\ --attribute-definitions AttributeName = Id,AttributeType = N \\ --key-schema AttributeName = Id,KeyType = HASH \\ --billing-mode PAY_PER_REQUEST Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos consultar informaci\u00f3n de la tabla mediante el comando describe-table de AWS CLi, ejecutaremos: aws dynamodb describe-table --table-name ProductCatalog Si queremos hacer la consulta de la tabla para ver los datos que contiene desde el comando scan de AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL . As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL . Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Consultas PartiQL mediante Python M\u00e1s adelante mediante Python , accederemos a DynamoDB y realizaremos consultas con PartiQL , adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos. Actividades \u00b6 Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instant\u00e1nea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.t4g.medium , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV. Referencias \u00b6 Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"5.- Datos"},{"location":"apuntes/nube05datos.html#datos-en-la-nube","text":"Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia plataforma cloud se encarga de gestionar el escalado, las copias de seguridad autom\u00e1ticas, la tolerancia a errores y la alta disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostgreSQL , ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches.","title":"Datos en la nube"},{"location":"apuntes/nube05datos.html#datos-relacionales-amazon-rds","text":"AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones.","title":"Datos relacionales - Amazon RDS"},{"location":"apuntes/nube05datos.html#instancias-de-bases-de-datos","text":"Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Cuando vamos a crear una instancia de base de datos, primero hemos de indicar qu\u00e9 motor de base de datos ejecutar. Actualmente, RDS admite seis motores de bases de datos: MySQL , compatible con las versiones 5.6, 5.7 y 8.0. Amazon Aurora Microsoft SQL Server , que permite implementar varias versiones de SQL Server (2012, 2014, 2016, 2017 y 2019), incluidas las Express, Web, Standard y Enterprise. PostgreSQL , compatible con las versiones 9.6, 10, 11 y 12. MariaDB , compatible con las versiones 10.2, 10.3, 10.4 y 10.5 y Oracle , compatible con Oracle 12 y Oracle 19, con dos modelos de licencia diferentes: Licencia incluida y Bring-Your-Own-License (BYOL) . Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que permite adaptar el coste y el rendimiento a las necesidades de nuestra base de datos. Instancia de RDS Por ejemplo, si seleccionamos el motor de MariaDB , podemos observar como mediante la creaci\u00f3n sencilla nos ofrece tres propuestas de tama\u00f1o, dependiendo de si es para el entorno de producci\u00f3n, desarrollo y pruebas o el de la capa gratuita. Configuraci\u00f3n de tama\u00f1o de la instancia con MariaDB","title":"Instancias de bases de datos"},{"location":"apuntes/nube05datos.html#alta-disponibilidad","text":"Una de las caracter\u00edsticas m\u00e1s importantes de RDS es la capacidad de configurar la instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ . Al hacerlo, se genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma s\u00edncrona a la copia en espera. Alta disponibilidad en Multi-AZ Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, RDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal.","title":"Alta disponibilidad"},{"location":"apuntes/nube05datos.html#casos-de-uso","text":"AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Se recomienda RDS cuando nuestra aplicaci\u00f3n necesite: Transacciones o consultas complejas Tasa de consulta o escritura media a alta: hasta 30.000 IOPS (15.000 lecturas + 15.000 escrituras) No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo Alta durabilidad En cambio, no se recomienda cuando: Tasas de lectura o escritura muy grandes (por ejemplo, 150.000 escrituras por segundo) Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento Solicitudes y consultas GET o PUT simples que una base de datos NoSQL puede manejar Personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (en este caso, es mejor instalar por nuestra cuenta el SGBD que necesitemos en una instancia EC2).","title":"Casos de uso"},{"location":"apuntes/nube05datos.html#costes","text":"El coste se calcula en base al tiempo de ejecuci\u00f3n (calculado en horas) as\u00ed como las caracter\u00edsticas de la base de datos. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tipo de instancia y su cantidad, as\u00ed como la clase de memoria de la base de datos. Otros gastos asociados son: almacenamiento aprovisionado: el almacenamiento para copias de seguridad de hasta el 100% del almacenamiento de nuestra base de datos activa es gratuito. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. cantidad de solicitudes de entrada y de salida. Aunque se recomienda utilizar la calculadora de costes para afinar en el presupuesto, por ejemplo, una base de datos con MariaDB con una instancia db.m4.large con 2 procesadores y 8GB de RAM, en una \u00fanica AZ, con un porcentaje de utilizaci\u00f3n del 100% y 30GB para almacenar los datos, cuesta alrededor de 131$ mensuales. En cambio si la cambiamos por dos instancias m\u00e1s potentes, como puede ser la db.m4.4xlarge , con 16 procesadores y 64 GB de RAM, en multi-AZ ya sube a unos 4.100$ al mes. Es importante recordar que si reservamos las instancias estos costes se reducir\u00edan en proporci\u00f3n a 2350$ (reserva de un a\u00f1o) o 1526$ (reserva de tres a\u00f1os).","title":"Costes"},{"location":"apuntes/nube05datos.html#ejemplo-rds","text":"A continuaci\u00f3n vamos a hacer un ejemplo sencillo donde vamos a crear una base de datos con la informaci\u00f3n que vimos en el bloque de SQL. Para ello, crearemos una instancia de MariaDB y nos conectaremos desde HeidiSQL . Creaci\u00f3n de la BD en RDS As\u00ed pues, desde la consola de AWS, crearemos nuestra base de datos a la que llamaremos instituto . En nuestro caso hemos seguido la creaci\u00f3n est\u00e1ndar con una plantilla de la capa gratuita (utiliza una instancia db.t2.micro ). Una vez configurado el usuario admin y la contrase\u00f1a adminadmin (al menos debe tener ocho caracteres), debemos configurar la conectividad. Instancias permitidas en AWS Academy Si quer\u00e9is crear bases de datos con m\u00e1quinas m\u00e1s potentes, pod\u00e9is utilizar instancias hasta nivel medium , y a ser posible a r\u00e1fagas (instancias t ). Dentro de la Configuraci\u00f3n adicional , es importante deshabilitar la monitorizaci\u00f3n mejorada (no tenemos permiso para su uso en AWS Academy ). Como vamos a querer acceder a nuestro servidor de MariaDB desde fuera de una VPC de EC2, necesitamos configurar el acceso p\u00fablico. Al hacerlo, no quiere decir que ya sea accesible desde fuera de internet, ya que necesitamos configurar su grupo de seguridad (recordad que funciona a modo de firewall ). As\u00ed pues, es recomendable crear un nuevo grupo de seguridad para que permitamos las conexiones del puerto 3306 a nuestra IP. Configuraci\u00f3n de la conectividad en RDS As\u00ed pues, una vez creada (lo cual tarda unos minutos), podremos seleccionar la instancia creada y ver su panel de informaci\u00f3n: Resumen de instancia en RDS As\u00ed pues, si copiamos la informaci\u00f3n del punto de enlace y creamos una conexi\u00f3n en HeidiSQL , veremos que nos conectamos correctamente (si no hemos creado un nuevo grupo de seguridad, deberemos editar el grupo de seguridad por defecto, y a\u00f1adir una regla de entrada para el protocolo TCP para el puerto 3306, y por ejemplo para todo internet - 0.0.0.0/0 ). Configuraci\u00f3n en HeidiSQL Una vez conectado, ya procedemos de la misma manera que hemos trabajado en el m\u00f3dulo de repaso de SQL.","title":"Ejemplo RDS"},{"location":"apuntes/nube05datos.html#amazon-aurora","text":"Amazon Aurora es una base de datos relacional compatible con MySQL y PostgreSQL optimizada para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. Ofrece dos modelos, el cl\u00e1sico basado en instancias y un modelo serverless en el cual se contratan unidades de computaci\u00f3n (ACU). Cabe destacar que si creamos una base de datos serverless, Amazon no permite hacerla p\u00fablica, de manera que \u00fanicamente se puede acceder desde otro servicio de AWS. Al estar desarrollado de forma nativa por Amazon se adapta mejor a su infraestructura en coste, rendimiento y alta disponibilidad. Est\u00e1 pensado como un subsistema de almacenamiento distribuido de alto rendimiento, ofreciendo automatizaci\u00f3n de las tareas que requieren mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, las copias \u200bde seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y su reparaci\u00f3n. Alta disponibles con Aurora Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en S3 . Respecto a la seguridad, hay varios niveles disponibles, incluidos el aislamiento de la red con VPC , el cifrado en reposo por medio de claves creadas y controladas con AWS KMS y el cifrado de los datos en tr\u00e1nsito mediante SSL. Respecto al coste, si cogemos el mismo ejemplo anterior de una instancia de Aurora compatible con MySQL con dos procesadores y 8GB de RAM, en este caso, la db.t4g.large , el precio se queda en 106$ mensuales.","title":"Amazon Aurora"},{"location":"apuntes/nube05datos.html#datos-nosql-dynamodb","text":"DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio administrado de base de datos NoSQL clave-valor y documental, r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de un solo d\u00edgito de milisegundos a cualquier escala y una capacidad de almacenamiento pr\u00e1cticamente ilimitado. As\u00ed pues, es un almac\u00e9n de claves/valor (similar a Redis y MongoDB a la vez), flexible y sin estructura fija (los elementos pueden tener atributos diferentes), dise\u00f1ado para garantizar un determinado rendimiento as\u00ed como una determinada disponibilidad para cada tabla (en NoSQL suele haber pocas tablas), es decir, se definen elementos por tabla y se paga seg\u00fan lo exigido en cada una.","title":"Datos NoSQL - DynamoDB"},{"location":"apuntes/nube05datos.html#componentes-y-particiones","text":"Los componentes principales son: las tablas : son conjuntos de datos, formada por los elementos. los elementos : grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos los atributos : elemento de datos fundamental que no es preciso seguir dividiendo. DynamoDB soporta dos tipos de claves principales: La clave de partici\u00f3n es una clave principal simple. La clave de partici\u00f3n y de ordenamiento , tambi\u00e9n conocidas como clave principal compuesta, ya que est\u00e1 formada por dos atributos. Claves A medida que aumenta el volumen de datos, la clave principal particiona e indexa los datos de la tabla. Podemos recuperar los datos de una tabla de DynamoDB de dos formas distintas, bien por la clave y hacer una consulta directa, o utilizar un escaneo de todos los elementos en busca de aquello que coincida con el par\u00e1metro de b\u00fasqueda. Consultas por clave o escaneo Para aprovechar al m\u00e1ximo las operaciones de consulta, es importante que la clave utilizada identifique de forma un\u00edvoca los elementos de la tabla de DynamoDB. Podemos configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme. De forma alternativa, podemos especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. Adem\u00e1s, DynamoDB permite crear \u00edndices para optimizar las consultas que realicemos sobre atributos que no forman parte de la clave de partici\u00f3n u ordenamiento.","title":"Componentes y particiones"},{"location":"apuntes/nube05datos.html#infraestructura","text":"Amazon administra toda la infraestructura subyacente de datos y los almacena de manera redundante en varias instalaciones dentro de una regi\u00f3n, como parte de la arquitectura tolerante a errores. El sistema particiona los datos autom\u00e1ticamente, distribuyendo los datos entre diferentes dispositivos de almacenamiento. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Todos los datos de DynamoDB se almacenan en unidades SSD, y su lenguaje de consulta simple ( PartiQL ) permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDB permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para cada tabla. Tambi\u00e9n permite habilitar el escalado autom\u00e1tico, monitorizando la carga de la tabla e incrementando o disminuyendo el rendimiento aprovisionado de manera autom\u00e1tica. Otras caracter\u00edsticas clave son las tablas globales que permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elijamos, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos.","title":"Infraestructura"},{"location":"apuntes/nube05datos.html#costes_1","text":"Con DynamoDB se cobran las operaciones de lectura, escritura y almacenamiento de datos en sus tablas, junto con las caracter\u00edsticas opcionales que decidamos habilitar. Ofrece dos modos de capacidad con opciones de facturaci\u00f3n: Bajo demanda : se cobran las operaciones de lectura y escritura de datos realizada en las tablas. No necesitamos especificar el rendimiento de lectura y escritura que espera de nuestras aplicaciones. Apropiado cuando: Creamos nuevas tablas con cargas de trabajo desconocidas. El tr\u00e1fico de la aplicaci\u00f3n es impredecible. Aprovisionada : se configura el n\u00famero de operaciones de lectura y escritura por segundo que consideramos que necesitar\u00e1 nuestra aplicaci\u00f3n. Permite usar el escalado autom\u00e1tico para ajustar autom\u00e1ticamente la capacidad de la tabla en funci\u00f3n de la tasa de uso especificada. Apropiado cuando: El tr\u00e1fico de la aplicaci\u00f3n es predecible. Las aplicaciones tienen un tr\u00e1fico uniforme o aumenta gradualmente. Los requisitos de capacidad se pueden predecir para controlar los costos Por ejemplo, una tabla donde especificamos un rendimiento garantizado de 1000 millones lecturas y 1 mill\u00f3n de escrituras al mes, con una coherencia eventual (es decir, que permite desorden de peticiones ) nos costar\u00e1 $67,17 al mes.","title":"Costes"},{"location":"apuntes/nube05datos.html#ejemplo-dynamodb","text":"A continuaci\u00f3n vamos a crear un ejemplo donde tras crear una tabla, la cargaremos con datos para posteriormente realizar alguna consulta. Supongamos que tenemos datos relativos a un cat\u00e1logo de productos, almacenados en el archivo ProductCatalog.json , el cual queremos poder consultar. Si visualizamos el primer registro podemos observar su estructura. Esta estructura es espec\u00edfica de DynamoDB , ya que indica en el primer elemento el nombre de la tabla (en nuestro caso ProductCatalog ), y a continuaci\u00f3n el tipo de operaci\u00f3n ( PutRequest ): { \"ProductCatalog\" : [ { \"PutRequest\" : { \"Item\" : { \"Id\" : { \"N\" : \"101\" }, \"Title\" : { \"S\" : \"Book 101 Title\" }, \"ISBN\" : { \"S\" : \"111-1111111111\" }, \"Authors\" : { \"L\" : [ { \"S\" : \"Author1\" } ] }, \"Price\" : { \"N\" : \"2\" }, \"Dimensions\" : { \"S\" : \"8.5 x 11.0 x 0.5\" }, \"PageCount\" : { \"N\" : \"500\" }, \"InPublication\" : { \"BOOL\" : true }, \"ProductCategory\" : { \"S\" : \"Book\" } } } }, Para ello, primero vamos a crear la tabla desde el interfaz web de AWS. Tras seleccionar Amazon DynamoDB , creamos una tabla que llamamos ProductCatalog , cuyo identificador ser\u00e1 Id de tipo n\u00famero . El resto de campos se crear\u00e1n autom\u00e1ticamente al importar los datos. Creando la tabla Tambi\u00e9n pod\u00edamos haber creado la tabla mediante el comando create-table de AWS CLI: aws dynamodb create-table \\ --table-name ProductCatalog \\ --attribute-definitions AttributeName = Id,AttributeType = N \\ --key-schema AttributeName = Id,KeyType = HASH \\ --billing-mode PAY_PER_REQUEST Para introducir los datos, podemos hacerlo de varias maneras. Si pulsamos sobra la tabla y luego en elementos podemos rellenar un formulario indicando el tipo de los elementos y su valor. Otra manera m\u00e1s \u00e1gil es mediante AWS CLI (recordad antes configurar las variables de entorno con la informaci\u00f3n de la conexi\u00f3n): El comando batch-write-item permite importar los datos desde un archivo JSON siempre y cuando cumpla con el formato comentado anteriormente. As\u00ed pues, el comando ser\u00eda: aws dynamodb batch-write-item --request-items file://ProductCatalog.json Una vez ejecutado tendremos un mensaje de UnprocessedItems: {} . Si volvemos a la consola web, tras entrar en la tabla y pulsar en Ver elementos veremos los datos ya introducidos. Ver elementos Si queremos consultar informaci\u00f3n de la tabla mediante el comando describe-table de AWS CLi, ejecutaremos: aws dynamodb describe-table --table-name ProductCatalog Si queremos hacer la consulta de la tabla para ver los datos que contiene desde el comando scan de AWS CLI, ejecutaremos: aws dynamodb scan --table-name ProductCatalog Y veremos algo similar a: { \"Items\" : [ { \"Title\" : { \"S\" : \"18-Bike-204\" }, \"Price\" : { \"N\" : \"500\" }, \"Brand\" : { \"S\" : \"Brand-Company C\" }, \"Description\" : { \"S\" : \"205 Description\" }, \"Color\" : { \"L\" : [ { \"S\" : \"Red\" }, { \"S\" : \"Black\" } ] }, \"ProductCategory\" : { \"S\" : \"Bicycle\" }, \"Id\" : { \"N\" : \"205\" }, \"BicycleType\" : { \"S\" : \"Hybrid\" Como se puede observar, los datos salen desordenados. Vamos a realizar consultas sobre estos datos haciendo uso de PartiQL . As\u00ed pues, en el men\u00fa de la izquierda, seleccionamos el editor PartiQL . Consultas con PartiQL En el panel de la derecha podremos realizar consultas del tipo: select * from ProductCatalog where Id = 101 select Title from ProductCatalog where ProductCategory = 'Book' select * from ProductCatalog where Price >= 300 Consultas PartiQL mediante Python M\u00e1s adelante mediante Python , accederemos a DynamoDB y realizaremos consultas con PartiQL , adem\u00e1s de operaciones de inserci\u00f3n, modificaci\u00f3n y borrado de datos.","title":"Ejemplo DynamoDB"},{"location":"apuntes/nube05datos.html#actividades","text":"Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Siguiendo el ejemplo de RDS, crea una instancia ( instituto ) de una base de datos de tipo MariaDB y c\u00e1rgala con todos los datos de las sesiones de repaso de SQL (las tablas iniciales y las de inserci\u00f3n). (opcional) A partir de la instancia del ejercicio anterior, crea una instant\u00e1nea de forma manual. A continuaci\u00f3n, restaura esta instant\u00e1nea en una nueva instancia (por ejemplo, instituto2 ) de tipo db.t4g.medium , y tras conectarte mediante HeidiSQL , comprueba que tiene los datos ya cargados. Adjunta una captura de pantalla donde se vean las caracter\u00edsticas de las dos instancias. Siguiendo el ejemplo de DynamoDB , crea la tabla ( ProductCatalog ), c\u00e1rgala con los datos del ejemplo y realiza un consulta para obtener bicicletas h\u00edbridas. Exporta el resultado a CSV.","title":"Actividades"},{"location":"apuntes/nube05datos.html#referencias","text":"Gu\u00eda de usuario de Amazon RDS Gu\u00eda de referencias de Amazon DynamoDB Laboratorios con ejemplos y modelado con Amazon DynamoDB","title":"Referencias"},{"location":"apuntes/spark01rdd.html","text":"Spark \u00b6 La anal\u00edtica de datos es el proceso de inspeccionar, limpiar, transformar y modelar los datos con el objetivo de descubrir informaci\u00f3n \u00fatil, obtener conclusiones sobre los datos y ayudar en la toma de decisiones. Para ello, el uso de Spark de la mano de Python , NumPy y Pandas como interfaz de la anal\u00edtica es clave en el d\u00eda a d\u00eda de un cient\u00edfico/ingeniero de datos. La version 3.0 de Apache Spark se lanz\u00f3 en 2020, diez a\u00f1os despu\u00e9s de su nacimiento. Esta versi\u00f3n incluye mejoras de rendimiento (el doble en consultas adaptativas), facilidad en el uso del API de Pandas, un nuevo interfaz gr\u00e1fico para el streaming que facilita el seguimiento y depuraci\u00f3n de las consultas y ajustes de rendimiento. Introducci\u00f3n \u00b6 Logo de Apache Spark Spark es un framework de computaci\u00f3n distribuido en paralelo similar a Hadoop-MapReduce (as\u00ed pues, Spark no es un lenguaje de programaci\u00f3n), pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. El hecho de almacenar en memoria los c\u00e1lculos intermedios implica que sea mucho m\u00e1s eficiente que Hadoop MapReduce . En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. El dise\u00f1o de Spark se basa principalmente en cuatro caracter\u00edsticas: Velocidad : enfocado al uso en un cl\u00faster de commodity hardware con una gesti\u00f3n eficiente de multihilo y procesamiento paralelo. Spark construye sus consultas de computaci\u00f3n mediante un grafo dirigido ac\u00edclico (DAG) y utiliza un planificador para descomponer el grafo en tareas que se ejecutan en paralelo mediante los nodos de los cl\u00fasters. Finalmente, utiliza un motor de ejecuci\u00f3n ( Tungsten ) que genera c\u00f3digo compacto para optimizar la ejecuci\u00f3n. Todo ello teniendo en cuenta que los resultados intermedios se almacenan en memoria. Facilidad de uso : Spark ofrece varias capas de abstracci\u00f3n sobre los datos, como son los RDD , DataFrames y Dataset . Al ofrecer un conjunto de transformaciones y acciones como operaciones de su API, Spark facilita el desarrollo de aplicaciones Big data. Modularidad : soporte para todo tipo de cargas mediante cualquiera de los lenguajes de programaci\u00f3n soportados: Scala , Java , Python , SQL y R , as\u00ed como los m\u00f3dulos de Spark SQL para consultas interactivas, Spark Structured Streaming para procesamiento de datos en streaming , Spark MLlib para machine learning y GraphX. De esta manera, mediante una \u00fanica aplicaci\u00f3n Spark se puede hacer todo sin necesidad de utilizar APIs separadas. Extensibilidad : Al centrarse unicamente en el procesamiento, la gesti\u00f3n de los datos se puede realizar a partir de Hadoop , Cassandra , HBase , MongoDB , Hive o cualquier SGBD relacional, haciendo todo en memoria. Adem\u00e1s, se puede extender el API para utilizar otras fuentes de datos, como Apache Kafka , Amazon S3 o Azure Storage . En t\u00e9rminos de flexibilidad, Spark ofrece un stack unificado que permite resolver m\u00faltiples tipos de procesamiento de datos, tanto aplicaciones batch como consultas interactivas, algoritmos de machine learning que quieren muchas iteraciones, aplicaciones de ingesta en streaming con rendimiento cercado al tiempo real, etc... Antes de Spark , para cada uno de estos tipos de procesamiento necesit\u00e1bamos una herramienta diferente, ahora con Spark tenemos una bala de plata que reduce los costes y recursos necesarios. Spark vs Hadoop \u00b6 La principal diferencia es que la computaci\u00f3n se realiza en memoria, lo que puede implicar un mejora de hasta 100 veces mejor rendimiento. Para ello, se realiza una evaluaci\u00f3n perezosa de las operaciones, de manera, que hasta que no se realiza una operaci\u00f3n, los datos realmente no se cargan. Para solucionar los problemas asociados a MapReduce , Spark crea un espacio de memoria RAM compartida entre los ordenadores del cl\u00faster. Este permite que los NodeManager/WorkerNode compartan variables (y su estado), eliminando la necesidad de escribir los resultados intermedios en disco. Esta zona de memoria compartida se traduce en el uso de RDD, DataFrames y DataSets , permitiendo realizar procesamiento en memoria a lo largo de un cl\u00faster con tolerancia a fallos. Stack unificado \u00b6 El elemento principal es Spark Core aporta toda la funcionalidad necesaria para preparar y ejecutar las aplicaciones distribuidas, gestionando la planificaci\u00f3n y tolerancia a fallos de las diferentes tareas. Para ello, el n\u00facleo ofrece un entorno NoSQL id\u00f3neo para el an\u00e1lisis exploratorio e interactivo de los datos. Spark se puede ejecutar en batch o en modo interactivo y tiene soporte para Python. Independientemente del lenguaje utilizado (ya sea Python, Java, Scala, R o SQL) el c\u00f3digo se despliega entre todos los nodos a lo largo del cl\u00faster. Adem\u00e1s, contiene otros 4 grandes componentes construidos sobre el core : Componentes de Spark Spark Streaming es una herramienta para la creaci\u00f3n de aplicaciones que procesamiento en streaming que ofrece un gran rendimiento con soporte para la tolerancia a fallos. Los datos pueden venir desde fuentes de datos tan diversas como Kafka , Flume , Twitter y tratarse en tiempo real. Spark SQL ofrece un interfaz SQL para trabajar con Spark , permitiendo la lectura de datos tanto de una tabla de cualquier base de datos relacional como de ficheros con formatos estructurados ( CSV , texto, JSON , Avro , ORC , Parquet , etc...) y construir tablas permanentes o temporales en Spark . Tras la lectura, permite combinar sentencias SQL para trabajar con los datos y cargar los resultados en un DataFrame de Spark . Por ejemplo, con este fragmento leemos un fichero JSON desde S3, creamos una tabla temporal y mediante una consulta SQL cargamos los datos en un DataFrame de Spark: // In Scala // Read data off Amazon S3 bucket into a Spark DataFrame spark . read . json ( \"s3://apache_spark/data/committers.json\" ) . createOrReplaceTempView ( \"committers\" ) // Issue a SQL query and return the result as a Spark DataFrame val results = spark . sql ( \"\"\"SELECT name, org, module, release, num_commits FROM committers WHERE module = 'mllib' AND num_commits > 10 ORDER BY num_commits DESC\"\"\" ) Spark MLlib es un m\u00f3dulo de machine learning que ofrece la gran mayor\u00eda de algoritmos de ML y permite construir pipelines para el entrenamiento y evaluaci\u00f3n de los modelos IA. GraphX permite procesar estructuras de datos en grafo, siendo muy \u00fatiles para recorrer las relaciones es una red social u ofrecer recomendaciones sobre gustos/afinidades. En este curso no vamos a entrar en detalle en este m\u00f3dulo. Adem\u00e1s, la comunidad de Spark dispone de un gran n\u00famero de conectores para diferentes fuentes de datos, herramientas de monitorizaci\u00f3n, etc... que conforman su propio ecosistema: Ecosistema de Spark Puesta en Marcha \u00b6 En nuestra m\u00e1quina virtual, \u00fanicamente necesitamos ejecutar el comando pyspark el cual arrancar\u00e1 directamente un cuaderno Jupyter : iabd@iabd-virtualbox:~/Spark$ pyspark [ I 16 :50:57.168 NotebookApp ] Serving notebooks from local directory: /home/iabd/Spark [ I 16 :50:57.168 NotebookApp ] The Jupyter Notebook is running at: [ I 16 :50:57.168 NotebookApp ] http://localhost:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ I 16 :50:57.168 NotebookApp ] or http://127.0.0.1:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ I 16 :50:57.168 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 16 :50:57.968 NotebookApp ] To access the notebook, open this file in a browser: file:///home/iabd/.local/share/jupyter/runtime/nbserver-9654-open.html Or copy and paste one of these URLs: http://localhost:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 or http://127.0.0.1:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ W 16 :51:02.666 NotebookApp ] 404 GET /api/kernels/a8119b9f-91ce-4eee-b32b-9be48a0d281e/channels?session_id = 5860cf5e65fa481d9110c9ff9904d3f7 ( 127 .0.0.1 ) : Kernel does not exist: a8119b9f-91ce-4eee-b32b-9be48a0d281e [ W 16 :51:02.676 NotebookApp ] 404 GET /api/kernels/a8119b9f-91ce-4eee-b32b-9be48a0d281e/channels?session_id = 5860cf5e65fa481d9110c9ff9904d3f7 ( 127 .0.0.1 ) 12 .30ms referer = None Jupyter Notebook Si instalamos PySpark seg\u00fan las instrucciones de la propia web , al ejecutar pyspark , se lanzara un shell. Para que se abra autom\u00e1ticamente Jupyter Lab , necesitamos exportar las siguientes variables de entorno: ~/.bashrc export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook' M\u00e1s informaci\u00f3n en https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes As\u00ed pues, autom\u00e1ticamente se abrir\u00e1 una ventana en el navegador web donde crear/trabajar con los cuadernos Jupyter: Cuadernos Jupyter con PySpark Otra posibilidad es utilizar alguna de las im\u00e1genes Docker disponibles que facilitan su uso. En nuestro caso, recomendamos las im\u00e1genes disponibles en https://github.com/jupyter/docker-stacks . Para lanzar la imagen de PySpark con cuadernos Jupyter utilizaremos: docker run -d -p 8888 :8888 -p 4040 :4040 -p 4041 :4041 jupyter/pyspark-notebook O si queremos crear un volumen con la carpeta actual: docker run -d -v ${ PWD } :/home/jovyan/work -p 8888 :8888 -p 4040 :4040 -p 4041 :4041 --name pyspark jupyter/pyspark-notebook Cl\u00faster de Spark \u00b6 Si queremos montar nosotros mismo un cl\u00faster de Spark, una vez tenemos todas las m\u00e1quinas instaladas con Java , Python y Spark , debemos distinguir entre: Nodo maestro/ driver - el cual deberemos arrancar con: $SPARK_HOME /sbin/start-master.sh -h 0 .0.0.0 Workers (esclavos) - los cuales arrancaremos con: $SPARK_HOME /sbin/start-worker.sh spark://<ip-servidor-driver>:7077 Sobre los workers, le podemos indicar la cantidad de cpus mediante la opci\u00f3n -c y la cantidad de RAM con -m . Por ejemplo, si quisi\u00e9ramos lanzar un worker con 8 n\u00facleos y 16GB de RAM har\u00edamos: $SPARK_HOME /sbin/start-slave.sh spark://<ip-servidor-driver>:7077 -c 8 -m 16G Una vez arrancado, si accedemos a http://ip-servidor-driver:8080 veremos el IU de Spark con los workers arrancados. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial . Uso en la nube \u00b6 Para trabajar con Spark desde la nube disponemos de varias alternativas, ya sean herramientas que permiten trabajar con cuadernos Jupyter como pueden ser Google Colab o Databricks , o montar un cl\u00faster mediante AWS EMR ( Elastic MapReduce ) o Azure HDInsight . Google Colab \u00b6 Primero nos vamos a centrar en Google Colab . A lo largo del curso, ya hemos empleado esta herramienta tanto en sistemas de aprendizaje como en el an\u00e1lisis exploratorio de los datos. Para que funcione Spark dentro de Google Colab , \u00fanicamente hemos de instalar las librer\u00edas. Se adjunta un cuaderno con ejemplo de c\u00f3digo : # 1. Instalar las dependencias !apt-get install openjdk-8-jdk-headless -qq > /dev/null !wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz !tar xvf spark-3.2.1-bin-hadoop3.2.tgz !pip install -q pyspark # 2. Configurar el entorno import os os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os.environ[\"SPARK_HOME\"] = f\"/content/spark-3.2.1-bin-hadoop3.2\" # 3. Cargar Pyspark from pyspark.sql import SparkSession spark = SparkSession.builder.appName(\"s8a\").master(\"local[*]\").getOrCreate() spark El cual podemos observar c\u00f3mo se ejecuta en Colab: Ejemplo de cuaderno en Google Colab Databricks \u00b6 Databricks es una plataforma anal\u00edtica de datos basada en Apache Spark desarrollada por la compa\u00f1\u00eda con el mismo nombre. La empresa, creada en eln 2013 por los desarrolladores principales de Spark , permite realizar anal\u00edtica Big Data e Inteligencia Artificial con Spark de una forma sencilla y colaborativa. Databricks se integra de forma transparente con AWS , Azure y Google Cloud . En una entrada del blog de la empresa de noviembre de 2021 anuncian un nuevo record de procesamiento que implica que su rendimiento es 3 veces superior a la competencia y con un coste menor. Para poder trabajar con Databricks de forma gratuita, podemos hacer uso de Databricks Community Edition , donde podemos crear nuestros propios cuadernos Jupyter y trabajar con Spark sin necesidad de instalar nada. El \u00fanico paso inicial tras registrarnos, es crear un cl\u00faster b\u00e1sico (con 16GB de memoria y dos n\u00facleos) desde la opci\u00f3n Create del men\u00fa de la izquierda: Creaci\u00f3n de un cl\u00faster en Databricks Tras un par de minutos se habr\u00e1 creado y lanzado el cl\u00faster, ya estaremos listos para crear un nuevo notebook y tener acceso a Spark directamente desde el objeto spark : Ejemplo de cuaderno en Databricks Si queremos, podemos hacer p\u00fablico el cuaderno y compartirlo con la comunidad. SparkContext vs SparkSession \u00b6 SparkContext es el punto de entrada a Spark desde las versiones 1.x y se utiliza para crear de forma programativa RDD, acumuladores y variables broadcast en el cl\u00faster. Desde Spark 2.0, la mayor\u00eda de funcionalidades (m\u00e9todos) disponibles en SparkContext tambi\u00e9n los est\u00e1n en SparkSession . Su objeto sc est\u00e1 disponible en el spark-shell y se puede crear de forma programativa mediante la clase SparkContext . from pyspark import SparkContext sc = SparkContext . getOrCreate () SparkSession se introdujo en la versi\u00f3n 2.0 y es el punto de entrada para crear RDD , DataFrames y DataSets . El objeto spark se encuentra disponible por defecto en el spark-shell y se puede crear de forma programativa mediante el patr\u00f3n builder de SparkSession . from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () Adem\u00e1s, desde una sesi\u00f3n de Spark podemos obtener un contexto a trav\u00e9s de la propiedad context : from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext Hola Spark \u00b6 Lo primero que debemos hacer siempre es conectarnos al contexto de Spark , el cual le indica a Spark como acceder al cl\u00faster. Si utilizamos la imagen de Docker , debemos obtener siempre el contexto a partir de la clase SparkSession : ejemploDockerSpark.py from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () # SparkSession de forma programativa sc = spark . sparkContext # SparkContext a partir de la sesi\u00f3n # Suma de los 100 primeros n\u00fameros rdd = sc . parallelize ( range ( 100 + 1 )) rdd . sum () En cambio, si utilizamos la instalaci\u00f3n de PySpark que tenemos en la m\u00e1quina virtual, directamente podemos acceder a la instancia de SparkSession a trav\u00e9s del objeto global spark : ejemploPySpark.py sc = spark . sparkContext # spark es una instancia de la clase SparkSession rdd = sc . parallelize ( range ( 100 + 1 )) rdd . sum () En ambos casos, si mostramos el contenido del contexto obtendremos algo similar a: Version v3.2.0 Master local[*] AppName PySparkShell A continuaci\u00f3n podemos ver el resultado completo en su ejecuci\u00f3n dentro de un cuaderno Jupyter: Hola Spark Nombre de la aplicaci\u00f3n Si queremos darle nombre a la aplicaci\u00f3n Spark, lo podemos hacer al obtener la SparkSession: spark = SparkSession . builder . appName ( \"spark-s8a\" ) . getOrCreate () Spark Submit \u00b6 De la misma manera que mediante Hadoop pod\u00edamos lanzar un proceso al cl\u00faster para su ejecuci\u00f3n, Spark ofrece el comando spark-submit para enviar un script al driver para su ejecuci\u00f3n de forma distribuida. As\u00ed pues, si colocamos nuestro c\u00f3digo en un archivo de Python: holaSpark.py from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext # Suma de los 100 primeros n\u00fameros rdd = sc . parallelize ( range ( 100 + 1 )) suma = rdd . sum () print ( \"--------------\" ) print ( suma ) print ( \"--------------\" ) Lo podemos ejecutar mediante (en nuestra m\u00e1quina virtual antes debemos resetear una variable de entorno para que no ejecute autom\u00e1ticamente el cuaderno jupyter: unset PYSPARK_DRIVER_PYTHON ): spark-submit holaMundo.py Si nuestro servidor estuviera en otra direcci\u00f3n IP, deber\u00edamos indicarle donde encontrar el master : spark-submit --master spark://<ip-servidor-driver>:7077 holaMundo.py M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial Arquitectura \u00b6 Ya hemos comentado que Spark es un sistema distribuido dise\u00f1ado para procesar grandes vol\u00famenes de datos de forma r\u00e1pida y eficiente. Este sistema normalmente se despliega en un conjunto de m\u00e1quinas que se conocen como un cl\u00faster Spark , pudiendo estar compuesta de unas pocas m\u00e1quinas o miles de ellas. Seg\u00fan el FAQ de Spark , el cl\u00faster m\u00e1s grande de Spark est\u00e1 compuesto por m\u00e1s de 8000 nodos. Normalmente se utiliza un sistema de gesti\u00f3n de recursos como YARN para gestionar de forma inteligente y eficiente el cl\u00faster. Los dos componentes principales del cl\u00faster zon: el gestor de cluster: nodo maestro que sabe donde se localizan los esclavos, cuanta memoria disponen y el n\u00famero de cores CPU de cada nodo. Su mayor responsabilidad es orquestar el trabajo asign\u00e1ndolo a los diferentes nodos. los nodos trabajadores ( workers ): cada nodo ofrece recursos (memoria, CPU, etc...) al gestor del cl\u00faster y realiza las tareas que se le asignen. Aplicaciones Spark \u00b6 Una aplicaci\u00f3n Spark se compone de dos partes: La l\u00f3gica de procesamiento de los datos, la cual realizamos mediante alguna de las API que ofrece Spark (Java, Scala, Python, etc...), desde algo sencillo que realice una ETL sobre los datos a problemas m\u00e1s complejos que requieran m\u00faltiples iteraciones y tarden varias horas como entrenar un modelo de machine learning . Driver: coordinador central encargado de interactuar con el cl\u00faster Spark y averiguar qu\u00e9 m\u00e1quinas deben ejecutar la l\u00f3gica de procesamiento. Para cada una de esas m\u00e1quinas, el driver realiza una petici\u00f3n al cl\u00faster para lanchar un proceso conocido como un ejecutor ( executor ). Adem\u00e1s, el driver Spark es responsable de gestionar y distribuir las tareas a cada ejecutor, y si es necesario, recoger y fusionar los datos resultantes para presentarlos al usuario. Estas tareas se realizan a trav\u00e9s de la SparkSession . Cada ejecutor es un proceso JVM ( Java Virtual Machine ) dedicado para una aplicaci\u00f3n Spark espec\u00edfica. Un ejecutor vivir\u00e1 tanto como dure la aplicaci\u00f3n Spark, lo cual puede ser minutos o d\u00edas, dependiendo de la complejidad de la aplicaci\u00f3n. Conviene destacar que los ejecutor son elementos aislados que no se comparten entre aplicaciones Spark, por lo que la \u00fanica manera de compartir informaci\u00f3n entre diferente ejecutores es mediante un sistema de almacenamiento externo como HDFS. Arquitectura entre una aplicaci\u00f3n Spark y el gestor del cl\u00faster As\u00ed pues, Spark utiliza una arquitectura maestro/esclavo, donde el driver es el maestro, y los ejecutores los esclavos. Cada uno de estos componentes se ejecutan como un proceso independiente en el cl\u00faster Spark. Por lo tanto, una aplicaci\u00f3n Spark se compone de un driver y m\u00faltiples ejecutores. Cada ejecutor realiza lo que se le pide en forma de tareas,ejecutando cada una de ellas en un n\u00facleo CPU separado. As\u00ed es como el procesamiento paralelo acelera el tratamiento de los datos. Adem\u00e1s, cada ejecutor, bajo petici\u00f3n de la l\u00f3gica de la aplicaci\u00f3n, se responsabiliza de cachear un fragmento de los datos en memoria y/o disco. Al lanzar una aplicaci\u00f3n Spark, podemos indicar el n\u00famero de ejecutores que necesita la aplicaci\u00f3n, as\u00ed com la cantidad de memoria y n\u00famero de n\u00facleos que cada ejecutor deber\u00eda tener. Cl\u00faster compuesto por un driver y tres ejecutores Job, Stage y Task \u00b6 Cuando creamos una aplicaci\u00f3n Spark, por debajo, se distinguen los siguientes elementos: Job (trabajo): computaci\u00f3n paralela compuesta de m\u00faltiples tareas que se crean tras una acci\u00f3n de Spark ( save , collect , etc...). Al codificar nuestro c\u00f3digo mediante PySpark , el driver convierte la aplicaci\u00f3n Spark en uno o m\u00e1s jobs , y a continuaci\u00f3n, estos jobs los transforma en un DAG (grafo). Este grafo, en esencia, es el plan de ejecuci\u00f3n, donde cada elemento dentro del DAG puede implicar una o varias stages (escenas). Stage (escena): cada job se divide en peque\u00f1os conjuntos de tareas que forman un escenario. Como parte del grafo, las stages se crean a partir de si las operaciones se pueden realizar de forma paralela o de forma secuencial. Como no todas las operaciones pueden realizarse en una \u00fanica stage , en ocasiones de dividen en varias, normalmente debido a los l\u00edmites computacionales de los diferentes ejecutores. Task (tarea): unida de trabajo m\u00e1s peque\u00f1a que se envia a los ejecutores Spark . Cada escenarios se compone de varias tareas. Cada una de las tareas se asigna a un \u00fanico n\u00facleo y trabaja con una \u00fanica partici\u00f3n de los datos. Por ello, un ejecutor con 16 n\u00facleos puede tener 16 o m\u00e1s tareas trabajando en 16 o m\u00e1s particiones en paralelo. Driver -> Job -> Stage -> Task DataFrame \u00b6 La principal abstracci\u00f3n de los datos en Spark es el Dataset . Se pueden crear desde las fuentes de entrada de Hadoop (como ficheros HDFS) o mediante transformaciones de otros Datasets . Dado el cariz de Python , no necesitamos que los Dataset est\u00e9n fuertemente tipados, por eso, todos los Dataset que usemos ser\u00e1n Dataset[Row] (si trabaj\u00e1semos mediante Java o Scala s\u00ed deber\u00edamos indicar el tipo de sus datos), y por consistencia con el concepto de Pandas y R, los llamaremos DataFrame . Por ejemplo, veamos c\u00f3mo podemos crear un DataFrame a partir de un fichero de texto: from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () quijoteTxt = spark . read . text ( \"el_quijote.txt\" ) quijoteTxt . count () # n\u00famero de filas del DataFrame - 2186 quijoteTxt . first () # primera fila - Row(value='DON QUIJOTE DE LA MANCHA') # Transformamos un DataFrame en otro nuevo lineasConQuijote = quijoteTxt . filter ( quijoteTxt . value . contains ( \"Quijote\" )) # DataFrame con las l\u00edneas que contiene la palabra Quijote lineasConQuijote . count () # cantidad de l\u00edneas con la palabra Quijote - 584 # Las transformaciones se pueden encadenar quijoteTxt . filter ( quijoteTxt . value . contains ( \"Quijote\" )) . count () # idem - 584 Estudiaremos los DataFrame en profundidad en la siguiente sesi\u00f3n . RDD \u00b6 Un RDD ( Resilient Distributed Datasets ) es una estructura de datos que abstrae los datos para su procesamiento en paralelo. Antes de Spark 2.0, los RDD eran el interfaz principal para interactuar con los datos. Se trata de una colecci\u00f3n de elementos tolerantes a fallos que son immutables (una vez creados, no se pueden modificar) y dise\u00f1ados para su procesamiento distribuido. Cada conjunto de datos en los RDD se divide en particiones l\u00f3gicas, que se pueden calcular en diferentes nodos del cl\u00faster. Hay dos formas de crear un RDD: Paralelizando una colecci\u00f3n ya existente en nuestra aplicaci\u00f3n Spark . Referenciando un dataset de un sistema externo como HDFS , HBase , etc... Sobre los RDD se pueden realizar dos tipos de operaciones: Acci\u00f3n: devuelven un valor tras ejecutar una computaci\u00f3n sobre el conjunto de datos. Transformaci\u00f3n: es una operaci\u00f3n perezosa que crea un nuevo conjunto de datos a partir de otro RDD/Dataset, tras realizar un filtrado, join , etc... \u00bfRDD obsoleto? Antes de la versi\u00f3n 2.0, el principal interfaz para programar en Spark eran los RDD. Tras la versi\u00f3n 2.0, fueron sustituidos por los Dataset , que son RDD fuertemente tipados que adem\u00e1s est\u00e1n optimizados a bajo nivel. El interfaz RDD todav\u00eda tiene soporte, sin embargo, se recomienda el uso de los Dataset por su mejor rendimiento. A lo largo de estas sesiones iremos combinando ambos interfaces para conocer las similitudes y diferencias. Acciones \u00b6 A continuaci\u00f3n vamos a revisar las acciones m\u00e1s comunes. Puedes consultar todas las acciones disponibles en la documentaci\u00f3n oficial : Parallelize \u00b6 Podemos crear RDD directamente desde cero sin necesidad de leer los datos desde un fichero. Para ello, podemos utilizar parallelize . Esta acci\u00f3n divide una colecci\u00f3n de elementos entre los nodos de nuestro cl\u00fasters. Por ejemplo: miRDD = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) lista = [ 'Hola' , 'Adi\u00f3s' , 'Hasta luego' ] listaRDD = sc . parallelize ( lista ) # Creamos un RDD a partir de una lista listaRDD4 = sc . parallelize ( lista , 4 ) # Creamos un RDD con 4 particiones Take y Sample \u00b6 Cuando queremos recuperar un n\u00famero determinado de resultado, de forma similar a limit en SQL, tenemos la acci\u00f3n take : miRDD . take ( 3 ) # [1, 2, 3] listaRDD . take ( 2 ) # ['Hola', 'Adi\u00f3s'] Otra opci\u00f3n es utilizar sample para obtener una muestra de los datos, aunque en este caso no es una acci\u00f3n sino una transformaci\u00f3n: miRDDmuestra = miRDD . sample ( False , 0.5 ) miRDDmuestra . collect () # [2, 4, 6, 7, 8, 9] / [1, 2, 3, 4, 6] / [5, 8, 9] Esta transformaci\u00f3n recibe varios par\u00e1metros: withReplacement : booleano para indicar si queremos elementos repetidos fraction : valor entre 0 y 1 que expresa la probabilidad de elegir cada elemento opcionalmente se le puede pasar un tercer valor con la semilla As\u00ed pues, en el ejemplo anterior, cada llamada a sample ha generado un RDD diferente, sin valores repetidos, pero con un tama\u00f1o de RDD variable. Para obtener una muestra mediante una acci\u00f3n, tenemos la opci\u00f3n takeSample que funciona de forma similar pero sin hacer shuffle y devuelve una lista: miRDDmuestraT = miRDD . takeSample ( False , 5 ) print ( miRDDmuestraT ) # [1, 8, 9, 7, 2] El primer par\u00e1metro vuelve a indicar si hay repetidos, pero el segundo fija la cantidad de elementos a devolver. Por \u00faltimo, mediante top obtenemos los primeros elementos una vez ordenado el RDD: miRDD . top ( 3 ) # [9, 8, 7] De forma similar, tenemos takeOrdered que recupera la cantidad de registros necesarios pero ordenados ascendentemente (al contrario que top ), con la opci\u00f3n de ordenarlos descendentemente (similar a top ): miRDD . takeOrdered ( 3 ) # [1, 2, 3] miRDD . takeOrdered ( 3 , lambda x : - x ) # [9, 8, 7] Hay que tener cuidado si el conjunto de datos es muy grande, porque tanto take como takeSample , takeOrdered y top llevar\u00e1n todos los datos a memoria. Collect \u00b6 Un fallo muy posible a la hora de mostrar los datos de un RDD es utilizar rdd.foreach(print) o rdd.map(print) . En una \u00fanica m\u00e1quina, esta operaci\u00f3n generar\u00eda la salida esperada mostrando todos los elementos del RDD. Sin embargo, al trabajar en un cl\u00faster, la salida a stdout la realizar\u00edan los diferentes nodos y no el nodo principal. As\u00ed pues, para mostrar todos los elementos de un RDD / DataFrame / Dataset hemos de emplear el m\u00e9todo collect , el cual primero mostrar\u00e1 los RDD del nodo principal ( driver node ), y luego para cada nodo del cluster mostrar\u00e1 sus datos. rdd . collect () Out of memory Hay que tener mucho cuidado, ya que nos podemos quedar f\u00e1cilmente sin memoria, ya que collect se trae los datos de todos los ejecutores a un \u00fanico nodo, el que \u00e9sta ejecutando el c\u00f3digo ( driver ). Si s\u00f3lo necesitamos mostrar unos pocos elementos, un enfoque m\u00e1s seguro es utilizar take : rdd . take ( 100 ) . foreach ( print ) Transformaciones \u00b6 En Spark , las estructuras de datos son inmutables, de manera que una vez creadas no se pueden modificar. Para poder modificar un RDD/DataFrame , hace falta realizar una transformaci\u00f3n , siendo el modo de expresar la l\u00f3gica de negocio mediante Spark . Todas las transformaciones en Spark se eval\u00faan de manera perezosa ( lazy evaluation ), de manera que los resultados no se computan inmediatamente, sino que se retrasa el c\u00e1lculo hasta que el valor sea necesario. Para ello, se van almacenando los pasos necesarios y se ejecutan \u00fanicamente cuando una acci\u00f3n requiere devolver un resultado al driver . Este dise\u00f1o facilita un mejor rendimiento de Spark (por ejemplo, imagina que tras una operaci\u00f3n map se realiza un reduce y en vez de devolver todo el conjunto de datos tras el map , s\u00f3lo le enviamos al driver el resultado de la reducci\u00f3n). As\u00ed pues, las acciones provocan la evaluaci\u00f3n de todas las transformaciones previas que se hab\u00edan evaluado de forma perezosa y estaban a la espera. Por defecto, cada transformaci\u00f3n RDD/DataSet se puede recalcular cada vez que se ejecute una acci\u00f3n. Sin embargo, podemos persistir un RDD en memoria mediante los m\u00e9todos persist (o cache ), de manera que Spark mantendr\u00e1 los datos para un posterior acceso m\u00e1s eficiente. Tambi\u00e9n podemos persistir RDD en disco o replicarlo en m\u00faltiples nodos. Tipos de transformaciones \u00b6 Existen dos tipos de transformaciones, dependiendo de las dependencias entre las particiones de datos: Transformaciones Narrow : consisten en dependencias estrechas en las que cada partici\u00f3n de entrada contribuye a una \u00fanica partici\u00f3n de salida. Transformaciones Wide : consisten en dependencias anchas de manera que varias particiones de entrada contribuyen a muchas otras particiones de salida, es decir, cada partici\u00f3n de salida depende de diferentes particiones de entrada. Este proceso tambi\u00e9n se conoce como shuffle , ya que Spark baraja los datos entre las particiones del cl\u00faster. Transformaciones Narrow vs Wide Con las transformaciones narrow , Spark realiza un pipeline de las dependencias, de manera que si especificamos m\u00faltiples filtros sobre DataFrames/RDD, se realizar\u00e1n todos en memoria. Esto no sucede con las transformaciones wide , ya que al realizar un shuffle los resultados se persisten en disco. Cuidado con shuffle Las operaciones shuffle son computacionalmente caras, ya que implican E/S en disco, serializaci\u00f3n de datos y E/S en red. Para organizar los datos previos al shuffle , Spark genera un conjunto de tareas (tareas map para organizar los datos, y reduce para agregar los resultados). Internamente, el resultado de las tareas map se mantienen en memoria hasta que no caben. Entonces, se ordenan en la partici\u00f3n destino y se persisten en un \u00fanico archivo. En la fase de reducci\u00f3n, las tareas leen los bloques ordenados que son relevantes. Las operaciones reduceByKey y aggregateByKey son de las que m\u00e1s memoria consumen, al tener que crear las estructuras de datos para organizar los registros en las tareas de map , y luego generar los resultados agregados en la de reduce . Si los datos no caben en memoria, Spark los lleva a disco, incurriendo en operaciones adiciones de E/S en disco y del recolector de basura. A continuaci\u00f3n veremos las diferentes transformaciones que podemos realizar con Spark. Transformaciones Narrow \u00b6 Para los siguientes ejemplo, utilizaremos el siguiente fichero de empleados.txt que ya utilizamos en la sesi\u00f3n de Hive : empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead Map \u00b6 La transformaci\u00f3n map aplica la funci\u00f3n recibida a cada elemento del RDD, de manera que vamos a poder a\u00f1adir una nueva columna, modificar una existente, etc... Por ejemplo, si la entrada es un RDD que contiene [1, 2, 3, 4, 5] , al hacer rdd.map(x=>x*2) obtendr\u00edamos un nuevo RDD con [2, 4, 6, 8, 10] : rdd = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . map ( lambda x : x * 2 ) resultRDD . collect () # [2, 4, 6, 8, 10] Mediante la funci\u00f3n textFile podemos cargar un archivo. Supongamos que tenemos cargado en Hadoop el archivo empleados.txt : rddLocal = sc . textFile ( \"empleados.txt\" ) rdd = sc . textFile ( \"hdfs://iabd-virtualbox:9000/user/iabd/datos/empleados.txt\" ) rdd . count () # 4 - cantidad de l\u00edneas resultRDD = rdd . map ( len ) # obtenemos la cantidad de caracteres cada l\u00ednea resultRDD . collect () # [61, 52, 60, 50] Si quisi\u00e9ramos mostrar los datos de los empleados, podr\u00edamos recoger los datos del RDD y recorrerlo: empleados = rdd . collect () for empleado in empleados : print ( empleado ) FlatMap \u00b6 La transformaci\u00f3n flatMap es muy similar a la anterior, pero en vez de devolver un elemento por cada entrada, devuelve una lista por cada entrada, deshaciendo las colecciones en elementos individuales: rdd = sc . textFile ( \"empleados.txt\" ) resultFM = rdd . flatMap ( lambda x : x . split ( \"|\" )) resultFM . collect () Obtendr\u00edamos cada atributo separado y todos dentro de la misma lista: [ 'Michael' , 'Mo ntreal , Toro nt o' , 'Male , 30 ' , 'DB : 80 ' , 'Produc t : Developer\\x 04 Lead' , 'Will' , 'Mo ntreal ' , 'Male , 35 ' , 'Perl : 85 ' , 'Produc t : Lead , Tes t : Lead' , 'Shelley' , 'New York' , 'Female , 27 ' , 'Py t ho n : 80 ' , 'Tes t : Lead , COE : Archi te c t ' , 'Lucy' , 'Va n couver' , 'Female , 57 ' , 'Sales : 89 , HR : 94 ' , 'Sales : Lead' ] Filter \u00b6 Permite filtrar los elementos que cumplen una condici\u00f3n mediante filter : rdd = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . filter ( lambda x : x % 2 == 0 ) resultRDD . collect () # [2, 4] Por ejemplo, si queremos filtrar los empleados que son hombres, primero separamos por las | y nos quedamos con el tercer elemento que contiene el sexo y la edad. A continuaci\u00f3n, separamos por la coma para quedarnos en el sexo en la posici\u00f3n 0 y la edad en el 1, y comparamos con el valor deseado: rdd = sc . textFile ( \"empleados.txt\" ) hombres = rdd . filter ( lambda x : x . split ( \"|\" )[ 2 ] . split ( \",\" )[ 0 ] == \"Male\" ) resultFM . collect () Obteniendo: [ 'Michael|Mo ntreal , Toro nt o|Male , 30 |DB : 80 |Produc t : Developer\\x 04 Lead' , 'Will|Mo ntreal |Male , 35 |Perl : 85 |Produc t : Lead , Tes t : Lead' ] Tambi\u00e9n podemos anidar diferentes transformaciones. Para este ejemplo, vamos a crear tuplas formadas por un n\u00famero y su cuadrado, luego quitar los que no coincide el n\u00famero su potencia (el 0 y el 1), y luego aplanarlo en una lista: rdd10 = sc . parallelize ( range ( 10 + 1 )) rddPares = rdd10 . map ( lambda x : ( x , x ** 2 )) . filter ( lambda x : ( x [ 0 ] != x [ 1 ])) . flatMap ( lambda x : x ) rddPares . collect () # [2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81, 10, 100] Union \u00b6 Mediante union unimos dos RDD en uno: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 5 , 6 , 7 , 8 ]) resultRDD = rdd1 . union ( rdd2 ) resultRDD . collect () # [1, 2, 3, 4, 5, 6, 7, 8] Intersection \u00b6 Mediante intersection , obtendremos los elementos que tengan en com\u00fan: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 3 , 4 , 5 , 6 ]) resultRDD = rdd1 . intersection ( rdd2 ) resultRDD . collect () # [1, 2, 3, 4, 5, 6, 7, 8] Subtract \u00b6 Mediante subtract , obtendremos los elementos propios que no est\u00e9n en el RDD recibido: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 3 , 4 , 5 , 6 ]) resultRDD = rdd1 . subtract ( rdd2 ) resultRDD . collect () # [1, 2] Autoevaluaci\u00f3n Si tenemos dos RDD (A y B): rddA = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rddB = sc . parallelize ([ 3 , 4 , 5 , 6 ]) Enunciado Soluci\u00f3n \u00bfC\u00f3mo conseguimos los elementos que est\u00e1n en A y no B y los de B que no est\u00e1n en A? (es decir [1, 2, 5, 6] )): resultRDD = rddA . subtract ( rddB ) . union ( rddB . subtract ( rddA )) resultRDD . collect () # [1, 2, 5, 6] Distinct \u00b6 Si utilizamos distinct eliminaremos los elementos repetidos: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . distinct () resultRDD . collect () # [4, 1, 5, 2, 3] RDD de Pares \u00b6 Una t\u00e9cnica muy com\u00fan a la hora de trabajar con RDD es hacerlo con elementos que tienen el formato (clave,valor), las claves y los valores pueden ser de cualquier tipo. listaTuplas = [( 1 , 'a' ), ( 2 , 'b' ), ( 3 , 'c' ), ( 4 , 'd' )] rddTuplas = sc . parallelize ( listaTuplas ) Sobre estos RDD podemos realizar algoritmos MapReduce para muchas funciones de procesamiento de datos, como es la agrupaci\u00f3n, ordenaci\u00f3n, join , count , etc... Para generar un RDD de pares, adem\u00e1s de generarlo nosotros a partir de una lista, podemos emplear las siguientes operaciones: zip : uniendo dos listas del mismo tama\u00f1o: lista1 = [ 'a' , 'b' , 'c' , 'e' , 'f' , 'g' , 'h' ] lista2 = [ 4 , 5 , 6 , 7 , 8 , 9 , 10 ] rddZip = sc . parallelize ( lista1 ) . zip ( sc . parallelize ( lista2 )) . collect () # [('a', 4), ('b', 5), ('c', 6), ('e', 7), ('f', 8), ('g', 9), ('h', 10)] rddZipSecuencia = sc . parallelize ( zip ( lista1 , range ( len ( lista1 )))) # usando el tama\u00f1o de la lista # [('a', 0), ('b', 1), ('c', 2), ('e', 3), ('f', 4), ('g', 5), ('h', 6)] Otros m\u00e9todos relacionados son zipWithIndex , zipWithUniqueId o unir dos RDD mediante zip . map : asignando a cada elemento un valor o c\u00e1lculo sobre \u00e9l mismo: lista = [ 'Hola' , 'Adi\u00f3s' , 'Hasta luego' ] rddMap = sc . parallelize ( lista ) . map ( lambda x : ( x , len ( c )) # [('Hola', 4), ('Adi\u00f3s', 5), ('Hasta luego', 11)] keyBy : permite crear las claves a partir de cada elemento: rddKeyBy = sc . parallelize ( lista ) . keyBy ( lambda x : x [ 0 ]) # creamos una clave con la primera letra # [('H', 'Hola'), ('A', 'Adi\u00f3s'), ('H', 'Hasta luego')] Autoevaluaci\u00f3n A partir de la lista \"Perro Gato Loro Pez Le\u00f3n Tortuga Gallina\" Enunciado Soluci\u00f3n Crea un RDD a partir de esta lista Convierte el RDD normal en un RDD de pares cuya clave sea la primera letra del animal Crea otro RDD de pares pero poniendo como clave un n\u00famero incremental \u00bfY si queremos que el \u00edndice incremental empiece en 100? animales = \"Perro Gato Loro Pez Le\u00f3n Tortuga Gallina\" animalesLista = animales . split ( \" \" ) rdd1 = sc . parallelize ( animalesLista ) rdd2 = rdd1 . keyBy ( lambda x : x [ 0 ]) rdd3 = sc . parallelize ( zip ( range ( len ( animalesLista )), animalesLista )) rdd4 = sc . parallelize ( zip ( range ( 100 , 100 + len ( animalesLista )), animalesLista )) Sobre los RDD de pares, podemos realizar las siguientes transformaciones: keys : devuelve las claves values : devuelve los valores mapValues : Aplica la funci\u00f3n sobre los valores flatMapValues Aplica la funci\u00f3n sobre los valores y los aplana. A continuaci\u00f3n se muestra un fragmento de c\u00f3digo para poner en pr\u00e1ctica las transformaciones comentadas: listaTuplas = [( 'a' , 1 ),( 'z' , 3 ),( 'b' , 4 ),( 'c' , 3 ),( 'a' , 4 )] rddTuplas = sc . parallelize ( listaTuplas ) claves = rddTuplas . keys () # ['a', 'z', 'b', 'c', 'a'] valores = rddTuplas . values () # [1, 3, 4, 3, 4] rddMapValues = rddTuplas . mapValues ( lambda x : ( x , x * 2 )) # [('a', (1, 2)), ('z', (3, 6)), ('b', (4, 8)), ('c', (3, 6)), ('a', (4, 8))] rddFMV = rddTuplas . flatMapValues ( lambda x : ( x , x * 2 )) # [('a', 1), # ('a', 2), # ('z', 3), # ('z', 6), # ('b', 4), # ... Transformaciones Wide \u00b6 Las siguientes transformaciones, adem\u00e1s de trabajar con RDD de pares, mezclan los datos de las particiones mediante el shuffle de los elementos. ReduceByKey \u00b6 Mediante la transformaci\u00f3n reducedByKey los datos se calculan utilizando una funci\u00f3n de reducci\u00f3n a partir de la clave combinando en la misma m\u00e1quina las parejas con la misma clave antes de que los datos se barajen. Por ejemplo, vamos a calcular el total de unidades vendidas por pa\u00eds: rdd = sc . textFile ( \"pdi_sales.csv\" ) # Recogemos el pa\u00eds y las unidades de las ventas paisesUnidades = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" )[ 3 ])) # Le quitamos el encabezado header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) # Reducimos por el pais y sumamos las unidades paisesTotalUnidades = paisesUnidadesSinHeader . reduceByKey ( lambda a , b : int ( a ) + int ( b )) paisesTotalUnidades . collect () Funciones lambda en reduce Al aplicar una transformaci\u00f3n de tipo reduce , la funci\u00f3n lambda recibir\u00e1 dos par\u00e1metros, siendo el primero el valor acumulado y el segundo el valor del elemento a operar. Y el resultado: [ ('Ca na da ' , 77609 ) , ('Fra n ce ' , 327730 ) , ('Germa n y' , 244265 ) , ('Mexico ' , 223463 ) ] Autoevaluaci\u00f3n Dada la siguiente lista de compra: lista = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] Enunciado Soluci\u00f3n Calcula: El total que se ha gastado por cada producto Cu\u00e1nto es lo m\u00e1ximo que se ha pagado por cada producto lista = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] rdd = sc . parallelize ( lista ) rdd1 = rdd . reduceByKey ( lambda x , y : x + y ) rdd2 = rdd . reduceByKey ( lambda x , y : max ( x , y )) GroupByKey \u00b6 Permite agrupar los datos a partir de una clave, repartiendo los resultados ( shuffle ) entre todos los nodos: rdd = sc . textFile ( \"pdi_sales.csv\" ) # Creamos un RDD de pares con el nombre del pa\u00eds como clave, y una lista con los valores ventas = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" ))) # Quitamos el primer elemento que es el encabezado del CSV header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) # Agrupamos las ventas por nombre del pa\u00eds paisesAgrupados = ventas . groupByKey () paisesAgrupados . collect () Obtendremos para cada pa\u00eds, un iterable con todos sus datos: [ ('Ca na da ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 4 b 2e0 >) , ('Fra n ce ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 816 c 3 a 9700 >) , ('Germa n y' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 96e b 0 >) , ('Mexico ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 965e0 >) ] Podemos transformar los iterables a una lista: paisesAgrupadosLista = paisesAgrupados . map ( lambda x : ( x [ 0 ], list ( x [ 1 ]))) paisesAgrupadosLista . collect () Obteniendo: [ ('Ca na da ' , [[ ' 725 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 1 ' , ' 115.4 ' , 'Ca na da ' ], [ ' 2235 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 2 ' , ' 131.1 ' , 'Ca na da ' ], [ ' 713 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 1 ' , ' 160.1 ' , 'Ca na da ' ], ... Autoevaluaci\u00f3n Ahora tenemos las cuentas de las compras de 3 d\u00edas: d\u00eda 1: pan 3\u20ac, agua 2\u20ac, az\u00facar 1\u20ac, leche 2\u20ac, pan 4\u20ac d\u00eda 2: pan 1\u20ac, cereales 3\u20ac, agua 0.5\u20ac, leche 2\u20ac, filetes 5\u20ac d\u00eda 3: filetes 2\u20ac, cereales 1\u20ac Dada la siguiente lista de compra: dia1 = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 4 )] dia2 = [( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] dia3 = [( 'filetes' , 2 ), ( 'cereales' , 1 )] Enunciado Soluci\u00f3n \u00bfC\u00f3mo obtenemos lo que hemos gastado en cada producto? \u00bfY el gasto medio que hemos realizado en cada uno de ellos? dia1 = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 4 )] dia2 = [( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] dia3 = [( 'filetes' , 2 ), ( 'cereales' , 1 )] rdd = sc . parallelize ( dia1 ) . union ( sc . parallelize ( dia2 )) . union ( sc . parallelize ( dia3 )) rdd1 = rdd . groupByKey () # [('leche', <pyspark.resultiterable.ResultIterable at 0x7f3494aea130>), # ('agua', <pyspark.resultiterable.ResultIterable at 0x7f3494acd4c0>), # ('pan', <pyspark.resultiterable.ResultIterable at 0x7f3494acd2b0>), # ('cereales', <pyspark.resultiterable.ResultIterable at 0x7f3494acd850>), # ('filetes', <pyspark.resultiterable.ResultIterable at 0x7f34944e3be0>), # ('az\u00facar', <pyspark.resultiterable.ResultIterable at 0x7f34944e3fa0>)] rdd1a = [( x , list ( y )) for x , y in rdd1 . collect ()] # rdd1a ya no es un RDD, es una lista que se ha cargado en el driver rdd1b = rdd1 . map ( lambda x : ( x [ 0 ], list ( x [ 1 ]))) #rdd1b sigue siendo un RDD # [('leche', [2, 2]), # ('agua', [2, 0.5]), # ('pan', [3, 4, 1]), # ('cereales', [3, 1]), # ('filetes', [5, 2]), # ('az\u00facar', [1])] rdd2 = rdd1 . map ( lambda x : ( x [ 0 ], sum ( x [ 1 ]) / len ( x [ 1 ]))) # [('leche', 2.0), # ('agua', 1.25), # ('pan', 2.6666666666666665), # ('cereales', 2.0), # ('filetes', 3.5), # ('az\u00facar', 1.0)] Mejor reduceByKey que groupByKey Si el tipo de operaci\u00f3n a realizar es posible mediante una operaci\u00f3n de reduce , su rendimiento ser\u00e1 una soluci\u00f3n m\u00e1s eficiente. M\u00e1s informaci\u00f3n en el art\u00edculo Avoid Group By SortByKey \u00b6 sortByKey permite ordenar los datos a partir de una clave. Los pares de la misma m\u00e1quina se ordenan primero por la misma clave, y luego los datos de las diferentes particiones se barajan. Para ello crearemos una tupla, siendo el primer elemento un valor num\u00e9rico por el cual ordenar, y el segundo el dato asociado. Vamos a partir del ejemplo anterior para ordenar los paises por la cantidad de ventas: # Ejemplo anterior rdd = sc . textFile ( \"pdi_sales.csv\" ) paisesUnidades = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" )[ 3 ])) header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) paisesTotalUnidades = paisesUnidadesSinHeader . reduceByKey ( lambda a , b : int ( a ) + int ( b )) # Le damos la vuelta a la lista unidadesPaises = paisesTotalUnidades . map ( lambda x : ( x [ 1 ], x [ 0 ])) unidadesPaises . collect () Ahora tendr\u00edamos: [ ( 77609 , 'Ca na da ') , ( 327730 , 'Fra n ce ') , ( 244265 , 'Germa n y') , ( 223463 , 'Mexico ') ] Y a continuaci\u00f3n los ordenamos: unidadesPaisesOrdenadas = unidadesPaises . sortByKey () unidadesPaisesOrdenadas . collect () Y comprobamos el resultado: [ ( 77609 , 'Ca na da ') , ( 223463 , 'Mexico ') , ( 244265 , 'Germa n y') , ( 327730 , 'Fra n ce ') ] Si quisi\u00e9ramos obtener los datos en orden descendente, le pasamos False a la transformaci\u00f3n: unidadesPaisesOrdenadasDesc = unidadesPaises . sortByKey ( False ) SortBy \u00b6 Mediante sortBy podemos ordenar los datos indicando nosotros la funci\u00f3n de ordenaci\u00f3n: paisesTotalUnidades . sortBy ( lambda x : x [ 1 ]) . collect () Obteniendo: [ ('Ca na da ' , 77609 ) , ('Mexico ' , 223463 ) , ('Germa n y' , 244265 ) , ('Fra n ce ' , 327730 ) ] Si queremos ordenar descendentemente, le pasamos un segundo par\u00e1metro a con valor False (indica si la ordenaci\u00f3n es ascendente): paisesTotalUnidades . sortBy ( lambda x : x [ 1 ], False ) . collect () Join Aunque los RDD permitan realizar operaciones join , realmente este tipo de operaciones se realizan mediante DataSet, por lo que omitimos su explicaci\u00f3n en esta sesi\u00f3n. Particiones \u00b6 Spark organiza los datos en particiones, consider\u00e1ndolas divisiones l\u00f3gicas de los datos entre los nodos del cl\u00faster. Por ejemplo, si el almacenamiento es HDFS cada partici\u00f3n se almacena en un bloque. Cada una de las particiones va a llevar asociada una tarea de ejecuci\u00f3n, de manera que a m\u00e1s particiones, mayor paralelizaci\u00f3n del proceso. Veamos con c\u00f3digo como podemos trabajar con las particiones: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ]) rdd . getNumPartitions () # 4 rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 2 ) rdd . getNumPartitions () # 2 rddE = sc . textFile ( \"empleados.txt\" ) rddE . getNumPartitions () # 2 rddE = sc . textFile ( \"empleados.txt\" , 3 ) rddE . getNumPartitions () # 3 La mayor\u00eda de operaciones / transformaciones / acciones que trabajan con los datos admiten un par\u00e1metro extra indicando la cantidad de particiones con las que queremos trabajar. +FIXME: revisar https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/ y https://www.projectpro.io/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297 MapPartitions \u00b6 A diferencia de la transformaci\u00f3n map que se invoca por cada elemento del RDD/DataSet, mapPartitions se llama por cada partici\u00f3n. La funci\u00f3n que recibe como par\u00e1metro recoger\u00e1 como entrada un iterador con los elementos de cada partici\u00f3n: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 2 ) def f ( iterator ): yield sum ( iterator ) resultadoRdd = rdd . mapPartitions ( f ) resultadoRdd . collect () # [6, 15] resultadoRdd2 = rdd . mapPartitions ( lambda iterator : [ list ( iterator )]) resultadoRdd2 . collect () # [[1, 1, 2, 2], [3, 3, 4, 5]] En el ejemplo, ha dividido los datos en dos particiones, la primera con [1, 1, 2, 2] y la otra con [3, 3, 4, 5] , y de ah\u00ed el resultado de sumar sus elementos es [6, 15] . FIXME: revisar https://www.mytechmint.com/pyspark-mappartitions/ y https://sparkbyexamples.com/pyspark/pyspark-mappartitions/ mapPartitionsWithIndex \u00b6 De forma similar al caso anterior, pero ahora mapPartitionsWithIndex recibe una funci\u00f3n cuyos par\u00e1metros son el \u00edndice de la partici\u00f3n y el iterador con los datos de la misma: def mpwi ( indice , iterador ): return [( indice , list ( iterador ))] resultadoRdd = rdd . mapPartitionsWithIndex ( mpwi ) resultadoRdd . collect () # [(0, [1, 1, 2, 2]), (1, [3, 3, 4, 5])] Modificando las particiones \u00b6 Podemos modificar la cantidad de particiones mediante dos transformaciones wide : coalesce y repartition . Mediante coalesce podemos obtener un nuevo RDD con la cantidad de particiones reducidas: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 3 ) rdd . getNumPartitions () # 3 rdd1p = rdd . coalesce ( 1 ) rdd1p . getNumPartitions () # 2 En cambio, mediante repartition podemos obtener un nuevo RDD con la cantidad exacta de particiones deseadas (al reducir las particiones, repartition realiza un shuffle para redistribuir los datos, por lo tanto, si queremos reducir la cantidad de particiones, es m\u00e1s eficiente utilizar coalesce ): rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 3 ) rdd . getNumPartitions () # 3 rdd2p = rdd . repartition ( 2 ) rdd2p . getNumPartitions () # 2 Acumuladores y Broadcast \u00b6 import sys from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"app1\" ) . getOrCreate () # Broadcast broadcastVar = spark . sparkContext . broadcast ([ 1 , 2 , 3 ]) print ( \"\" ) print ( broadcastVar . value ) print ( \"\" ) # Accumulador accum = spark . sparkContext . accumulator ( 0 ) sumatorioError = 0 def myFunc ( x ): global sumatorioError accum . add ( x ) sumatorioError += x rdd = spark . sparkContext . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) rdd . foreach ( myFunc ) print ( \"\" ) print ( accum ) print ( sumatorioError ) print ( \"\" ) Spark UI \u00b6 Si accedemos a la direcci\u00f3n http://127.0.0.1:4040/ , veremos un interfaz gr\u00e1fico donde podemos monitorizar y analizar el c\u00f3digo Spark ejecutado. La barra superior muestra un men\u00fa con las opciones para visualizar las jobs , stages , almacenamiento, el entorno y sus variables de configuraci\u00f3n, y finalmente los ejecutores: Spark Shell UI Por ejemplo, si ejecutamos el ejemplo de groupByKey , obtenemos el siguiente DAG: Ejemplo de DAG Si pulsamos por ejemplo sobre la fase de groupBy obtendremos sus estad\u00edsticas de ejecuci\u00f3n: Estad\u00edsticas de una fase Actividades \u00b6 En las siguientes actividades vamos a familiarizarnos con el uso de Spark con RDD y las diferentes acciones y transformaciones para trabajar con los datos. A partir de la lista siguiente ['Alicante','Elche','Valencia','Madrid','Barcelona','Bilbao','Sevilla'] : Almacena s\u00f3lo las ciudades que tengan la letra e en su nombre y mu\u00e9stralas. Muestra las ciudades que tienen la letra e y el n\u00famero de veces que aparece en cada nombre. Por ejemplo ('Elche', 2) . Averigua las ciudades que solo tengan una \u00fanica e . Nos han enviado una nueva lista pero no han separado bien las ciudades. Reorganiza la lista y colocalas correctamente, y cuenta las apariciones de la letra e de cada ciudad. ciudades_mal = [['Alicante.Elche','Valencia','Madrid.Barcelona','Bilbao.Sevilla'],['Murcia','San Sebasti\u00e1n','Melilla.Aspe']] (opcional) A partir del fichero de El Quijote : Crear un RDD a partir del fichero y crea una lista con todas las palabras del documento. \u00bfCuantas veces aparece la palabra Dulcinea (independientemente de si est\u00e1 en may\u00fasculas o min\u00fasculas)? \u00bfY Rocinante ? (86 y 120 ocurrencias respectivamente) Devuelve una lista ordenada seg\u00fan el n\u00famero de veces que sale cada palabra de m\u00e1s a menos (las primeras ocurrencias deben ser [('que', 10731), ('de', 9035), ('y', 8668), ('la', 5014), ... ). Almacena el resultado en HDFS en /user/iabd/spark/wcQuijote . Dada una cadena que contiene una lista de nombres Juan, Jimena, Luis, Cristian, Laura, Lorena, Cristina, Jacobo, Jorge , una vez transformada la cadena en una lista y luego en un RDD: Agr\u00fapalos seg\u00fan su inicial, de manera que tengamos tuplas formadas por la letra inicial y todos los nombres que comienzan por dicha letra: [( 'J' , [ 'Juan' , 'Jimena' , 'Jacobo' , 'Jorge' ]), ( 'L' , [ 'Luis' , 'Laura' , 'Lorena' ]), ( 'C' , [ 'Cristian' , 'Cristina' ])] De la lista original, obt\u00e9n una muestra de 5 elementos sin repetir valores. Devuelve una muestra de datos de aproximadamente la mitad de registros que la lista original con datos que pudieran llegar a repetirse. A partir de las siguientes listas: Ingl\u00e9s: hello, table, angel, cat, dog, animal, chocolate, dark, doctor, hospital, computer Espa\u00f1ol: hola, mesa, angel, gato, perro, animal, chocolate, oscuro, doctor, hospital, ordenador Una vez creado un RDD con tuplas de palabras y su traducci\u00f3n (puedes usar zip para unir dos listas): [( 'hello' , 'hola' ), ( 'table' , 'mesa' ), ( 'angel' , 'angel' ), ( 'cat' , 'gato' ) ... Averigua: Palabras que se escriben igual en ingl\u00e9s y en espa\u00f1ol Palabras que en espa\u00f1ol son distintas que en ingl\u00e9s Obt\u00e9n una \u00fanica lista con las palabras en ambos idiomas que son distintas entre ellas ( ['hello', 'hola', 'table', ... ) Haz dos grupos con todas las palabras, uno con las que empiezan por vocal y otro con las que empiecen por consonante. (opcional) Dada una lista de elementos desordenados y algunos repetidos, devolver una muestra de 5 elementos, que est\u00e9n en la lista, sin repetir y ordenados descendentemente. lista = 4 , 6 , 34 , 7 , 9 , 2 , 3 , 4 , 4 , 21 , 4 , 6 , 8 , 9 , 7 , 8 , 5 , 4 , 3 , 22 , 34 , 56 , 98 Selecciona el elemento mayor de la lista resultante. Muestra los dos elementos menores. En una red social sobre cine, tenemos un fichero ratings.txt compuesta por el c\u00f3digo de la pel\u00edcula, el c\u00f3digo del usuario, la calificaci\u00f3n asignada y el timestamp de la votaci\u00f3n con el siguiente formato: 1::1193::5::978300760 1::661::3::978302109 1::914::3::978301968 Se pide crear dos script en Python , as\u00ed como los comandos necesarios para ejecutarlos (mediante spark-submit ) para: Obtener para cada pel\u00edcula, la nota media de todas sus votaciones. Pel\u00edculas cuya nota media sea superior a 3. Proyectos \u00b6 A continuaci\u00f3n planteamos dos proyectos para realizar en clase: Tenemos las calificaciones de las asignaturas de matem\u00e1ticas, ingl\u00e9s y f\u00edsica de los alumnos del instituto en 3 documentos de texto. A partir de estos ficheros: Crea 3 RDD de pares, uno para cada asignatura, con los alumnos y sus notas Crea un solo RDD con todas las notas \u00bfCu\u00e1l es la nota m\u00e1s baja que ha tenido cada alumno? \u00bfCu\u00e1l es la nota media de cada alumno? \u00bfCuantos estudiantes suspende cada asignatura? [('Mates', 7), ('F\u00edsica', 8), ('Ingl\u00e9s', 7)] \u00bfEn qu\u00e9 asignatura suspende m\u00e1s gente? Total de notables o sobresalientes por alumno, es decir, cantidad de notas superiores o igual a 7. \u00bfQu\u00e9 alumno no se ha presentado a ingl\u00e9s? \u00bfA cu\u00e1ntas asignaturas se ha presentado cada alumno? Obten un RDD con cada alumno con sus notas Dada la carpeta weblog , con un conjunto de ficheros de log de una web con el el formato Common Log Format : <ipUsuario> - <idUsuario> - [<fecha>] \"<peticionHTTP>\" <codigoHTTP> <bytesTransmitidos> <dominio> <browser> Un fragmento de un fichero ser\u00eda similar a: 116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] \"GET /hive-00031.html HTTP/1.0\" 200 1388 \"http://www.cursoDeFormacionBigData.com\" \"Opera 3.4\" 116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] \"GET /theme.css HTTP/1.0\" 200 5531 \"http://www.cursoDeFormacionBigData.com\" \"Opera 3.4\" Carga los logs de la carpeta weblogs Crea un RDD de pares a partir de estos ficheros con el usuario como clave y un valor 1 [('128', 1), ('128', 1), ('94', 1),... ) Crea un nuevo RDD con la suma de los valores para cada identificador de usuario [('54126', 32), ('54', 498), ('21443', 35), ... ) Muestra los 10 usuarios que m\u00e1s peticiones han realizado. [('193', 667), ('13', 636), ('24', 620),... Crea un nuevo RDD de pares donde la clave sea el c\u00f3digo del usuario y el valor una lista de IP desde donde se ha conectado el usuario. En cvs de cuentas, tenemos informaci\u00f3n de los clientes, podr\u00edas a\u00f1adir el nombre y apellido del usuario junto a las apariciones de ese usuario? por ejemplo: Usuario1,(Nombre apellido,30) Dados los ficheros de log de la carpeta weblogs, queremos contar cuantas veces se han enviado datos en formato png,jpg,html,css y txt. Usa acumuladores para recorrer los ficheros y calcular cuantos hay de cada formato en total entre todos los logs Referencias \u00b6 Documentaci\u00f3n oficial de Apache Spark RDD Programming Guide Learning Apache Spark, 2nd Edition https://github.com/vivek-bombatkar/MyLearningNotes/tree/master/spark","title":"Spark"},{"location":"apuntes/spark01rdd.html#spark","text":"La anal\u00edtica de datos es el proceso de inspeccionar, limpiar, transformar y modelar los datos con el objetivo de descubrir informaci\u00f3n \u00fatil, obtener conclusiones sobre los datos y ayudar en la toma de decisiones. Para ello, el uso de Spark de la mano de Python , NumPy y Pandas como interfaz de la anal\u00edtica es clave en el d\u00eda a d\u00eda de un cient\u00edfico/ingeniero de datos. La version 3.0 de Apache Spark se lanz\u00f3 en 2020, diez a\u00f1os despu\u00e9s de su nacimiento. Esta versi\u00f3n incluye mejoras de rendimiento (el doble en consultas adaptativas), facilidad en el uso del API de Pandas, un nuevo interfaz gr\u00e1fico para el streaming que facilita el seguimiento y depuraci\u00f3n de las consultas y ajustes de rendimiento.","title":"Spark"},{"location":"apuntes/spark01rdd.html#introduccion","text":"Logo de Apache Spark Spark es un framework de computaci\u00f3n distribuido en paralelo similar a Hadoop-MapReduce (as\u00ed pues, Spark no es un lenguaje de programaci\u00f3n), pero que en vez de almacenar los datos en un sistema de ficheros distribuidos o utilizar un sistema de gesti\u00f3n de recursos, lo hace en memoria. El hecho de almacenar en memoria los c\u00e1lculos intermedios implica que sea mucho m\u00e1s eficiente que Hadoop MapReduce . En el caso de tener la necesidad de almacenar los datos o gestionar los recursos, se apoya en sistemas ya existentes como HDFS , YARN o Apache Mesos . Por lo tanto, Hadoop y Spark son sistemas complementarios. El dise\u00f1o de Spark se basa principalmente en cuatro caracter\u00edsticas: Velocidad : enfocado al uso en un cl\u00faster de commodity hardware con una gesti\u00f3n eficiente de multihilo y procesamiento paralelo. Spark construye sus consultas de computaci\u00f3n mediante un grafo dirigido ac\u00edclico (DAG) y utiliza un planificador para descomponer el grafo en tareas que se ejecutan en paralelo mediante los nodos de los cl\u00fasters. Finalmente, utiliza un motor de ejecuci\u00f3n ( Tungsten ) que genera c\u00f3digo compacto para optimizar la ejecuci\u00f3n. Todo ello teniendo en cuenta que los resultados intermedios se almacenan en memoria. Facilidad de uso : Spark ofrece varias capas de abstracci\u00f3n sobre los datos, como son los RDD , DataFrames y Dataset . Al ofrecer un conjunto de transformaciones y acciones como operaciones de su API, Spark facilita el desarrollo de aplicaciones Big data. Modularidad : soporte para todo tipo de cargas mediante cualquiera de los lenguajes de programaci\u00f3n soportados: Scala , Java , Python , SQL y R , as\u00ed como los m\u00f3dulos de Spark SQL para consultas interactivas, Spark Structured Streaming para procesamiento de datos en streaming , Spark MLlib para machine learning y GraphX. De esta manera, mediante una \u00fanica aplicaci\u00f3n Spark se puede hacer todo sin necesidad de utilizar APIs separadas. Extensibilidad : Al centrarse unicamente en el procesamiento, la gesti\u00f3n de los datos se puede realizar a partir de Hadoop , Cassandra , HBase , MongoDB , Hive o cualquier SGBD relacional, haciendo todo en memoria. Adem\u00e1s, se puede extender el API para utilizar otras fuentes de datos, como Apache Kafka , Amazon S3 o Azure Storage . En t\u00e9rminos de flexibilidad, Spark ofrece un stack unificado que permite resolver m\u00faltiples tipos de procesamiento de datos, tanto aplicaciones batch como consultas interactivas, algoritmos de machine learning que quieren muchas iteraciones, aplicaciones de ingesta en streaming con rendimiento cercado al tiempo real, etc... Antes de Spark , para cada uno de estos tipos de procesamiento necesit\u00e1bamos una herramienta diferente, ahora con Spark tenemos una bala de plata que reduce los costes y recursos necesarios.","title":"Introducci\u00f3n"},{"location":"apuntes/spark01rdd.html#spark-vs-hadoop","text":"La principal diferencia es que la computaci\u00f3n se realiza en memoria, lo que puede implicar un mejora de hasta 100 veces mejor rendimiento. Para ello, se realiza una evaluaci\u00f3n perezosa de las operaciones, de manera, que hasta que no se realiza una operaci\u00f3n, los datos realmente no se cargan. Para solucionar los problemas asociados a MapReduce , Spark crea un espacio de memoria RAM compartida entre los ordenadores del cl\u00faster. Este permite que los NodeManager/WorkerNode compartan variables (y su estado), eliminando la necesidad de escribir los resultados intermedios en disco. Esta zona de memoria compartida se traduce en el uso de RDD, DataFrames y DataSets , permitiendo realizar procesamiento en memoria a lo largo de un cl\u00faster con tolerancia a fallos.","title":"Spark vs Hadoop"},{"location":"apuntes/spark01rdd.html#stack-unificado","text":"El elemento principal es Spark Core aporta toda la funcionalidad necesaria para preparar y ejecutar las aplicaciones distribuidas, gestionando la planificaci\u00f3n y tolerancia a fallos de las diferentes tareas. Para ello, el n\u00facleo ofrece un entorno NoSQL id\u00f3neo para el an\u00e1lisis exploratorio e interactivo de los datos. Spark se puede ejecutar en batch o en modo interactivo y tiene soporte para Python. Independientemente del lenguaje utilizado (ya sea Python, Java, Scala, R o SQL) el c\u00f3digo se despliega entre todos los nodos a lo largo del cl\u00faster. Adem\u00e1s, contiene otros 4 grandes componentes construidos sobre el core : Componentes de Spark Spark Streaming es una herramienta para la creaci\u00f3n de aplicaciones que procesamiento en streaming que ofrece un gran rendimiento con soporte para la tolerancia a fallos. Los datos pueden venir desde fuentes de datos tan diversas como Kafka , Flume , Twitter y tratarse en tiempo real. Spark SQL ofrece un interfaz SQL para trabajar con Spark , permitiendo la lectura de datos tanto de una tabla de cualquier base de datos relacional como de ficheros con formatos estructurados ( CSV , texto, JSON , Avro , ORC , Parquet , etc...) y construir tablas permanentes o temporales en Spark . Tras la lectura, permite combinar sentencias SQL para trabajar con los datos y cargar los resultados en un DataFrame de Spark . Por ejemplo, con este fragmento leemos un fichero JSON desde S3, creamos una tabla temporal y mediante una consulta SQL cargamos los datos en un DataFrame de Spark: // In Scala // Read data off Amazon S3 bucket into a Spark DataFrame spark . read . json ( \"s3://apache_spark/data/committers.json\" ) . createOrReplaceTempView ( \"committers\" ) // Issue a SQL query and return the result as a Spark DataFrame val results = spark . sql ( \"\"\"SELECT name, org, module, release, num_commits FROM committers WHERE module = 'mllib' AND num_commits > 10 ORDER BY num_commits DESC\"\"\" ) Spark MLlib es un m\u00f3dulo de machine learning que ofrece la gran mayor\u00eda de algoritmos de ML y permite construir pipelines para el entrenamiento y evaluaci\u00f3n de los modelos IA. GraphX permite procesar estructuras de datos en grafo, siendo muy \u00fatiles para recorrer las relaciones es una red social u ofrecer recomendaciones sobre gustos/afinidades. En este curso no vamos a entrar en detalle en este m\u00f3dulo. Adem\u00e1s, la comunidad de Spark dispone de un gran n\u00famero de conectores para diferentes fuentes de datos, herramientas de monitorizaci\u00f3n, etc... que conforman su propio ecosistema: Ecosistema de Spark","title":"Stack unificado"},{"location":"apuntes/spark01rdd.html#puesta-en-marcha","text":"En nuestra m\u00e1quina virtual, \u00fanicamente necesitamos ejecutar el comando pyspark el cual arrancar\u00e1 directamente un cuaderno Jupyter : iabd@iabd-virtualbox:~/Spark$ pyspark [ I 16 :50:57.168 NotebookApp ] Serving notebooks from local directory: /home/iabd/Spark [ I 16 :50:57.168 NotebookApp ] The Jupyter Notebook is running at: [ I 16 :50:57.168 NotebookApp ] http://localhost:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ I 16 :50:57.168 NotebookApp ] or http://127.0.0.1:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ I 16 :50:57.168 NotebookApp ] Use Control-C to stop this server and shut down all kernels ( twice to skip confirmation ) . [ C 16 :50:57.968 NotebookApp ] To access the notebook, open this file in a browser: file:///home/iabd/.local/share/jupyter/runtime/nbserver-9654-open.html Or copy and paste one of these URLs: http://localhost:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 or http://127.0.0.1:8888/?token = b7b4c7232e5d9d3f7c7fdd51d75e5fe314c3f2c637e90652 [ W 16 :51:02.666 NotebookApp ] 404 GET /api/kernels/a8119b9f-91ce-4eee-b32b-9be48a0d281e/channels?session_id = 5860cf5e65fa481d9110c9ff9904d3f7 ( 127 .0.0.1 ) : Kernel does not exist: a8119b9f-91ce-4eee-b32b-9be48a0d281e [ W 16 :51:02.676 NotebookApp ] 404 GET /api/kernels/a8119b9f-91ce-4eee-b32b-9be48a0d281e/channels?session_id = 5860cf5e65fa481d9110c9ff9904d3f7 ( 127 .0.0.1 ) 12 .30ms referer = None Jupyter Notebook Si instalamos PySpark seg\u00fan las instrucciones de la propia web , al ejecutar pyspark , se lanzara un shell. Para que se abra autom\u00e1ticamente Jupyter Lab , necesitamos exportar las siguientes variables de entorno: ~/.bashrc export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS='notebook' M\u00e1s informaci\u00f3n en https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes As\u00ed pues, autom\u00e1ticamente se abrir\u00e1 una ventana en el navegador web donde crear/trabajar con los cuadernos Jupyter: Cuadernos Jupyter con PySpark Otra posibilidad es utilizar alguna de las im\u00e1genes Docker disponibles que facilitan su uso. En nuestro caso, recomendamos las im\u00e1genes disponibles en https://github.com/jupyter/docker-stacks . Para lanzar la imagen de PySpark con cuadernos Jupyter utilizaremos: docker run -d -p 8888 :8888 -p 4040 :4040 -p 4041 :4041 jupyter/pyspark-notebook O si queremos crear un volumen con la carpeta actual: docker run -d -v ${ PWD } :/home/jovyan/work -p 8888 :8888 -p 4040 :4040 -p 4041 :4041 --name pyspark jupyter/pyspark-notebook","title":"Puesta en Marcha"},{"location":"apuntes/spark01rdd.html#cluster-de-spark","text":"Si queremos montar nosotros mismo un cl\u00faster de Spark, una vez tenemos todas las m\u00e1quinas instaladas con Java , Python y Spark , debemos distinguir entre: Nodo maestro/ driver - el cual deberemos arrancar con: $SPARK_HOME /sbin/start-master.sh -h 0 .0.0.0 Workers (esclavos) - los cuales arrancaremos con: $SPARK_HOME /sbin/start-worker.sh spark://<ip-servidor-driver>:7077 Sobre los workers, le podemos indicar la cantidad de cpus mediante la opci\u00f3n -c y la cantidad de RAM con -m . Por ejemplo, si quisi\u00e9ramos lanzar un worker con 8 n\u00facleos y 16GB de RAM har\u00edamos: $SPARK_HOME /sbin/start-slave.sh spark://<ip-servidor-driver>:7077 -c 8 -m 16G Una vez arrancado, si accedemos a http://ip-servidor-driver:8080 veremos el IU de Spark con los workers arrancados. M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial .","title":"Cl\u00faster de Spark"},{"location":"apuntes/spark01rdd.html#uso-en-la-nube","text":"Para trabajar con Spark desde la nube disponemos de varias alternativas, ya sean herramientas que permiten trabajar con cuadernos Jupyter como pueden ser Google Colab o Databricks , o montar un cl\u00faster mediante AWS EMR ( Elastic MapReduce ) o Azure HDInsight .","title":"Uso en la nube"},{"location":"apuntes/spark01rdd.html#sparkcontext-vs-sparksession","text":"SparkContext es el punto de entrada a Spark desde las versiones 1.x y se utiliza para crear de forma programativa RDD, acumuladores y variables broadcast en el cl\u00faster. Desde Spark 2.0, la mayor\u00eda de funcionalidades (m\u00e9todos) disponibles en SparkContext tambi\u00e9n los est\u00e1n en SparkSession . Su objeto sc est\u00e1 disponible en el spark-shell y se puede crear de forma programativa mediante la clase SparkContext . from pyspark import SparkContext sc = SparkContext . getOrCreate () SparkSession se introdujo en la versi\u00f3n 2.0 y es el punto de entrada para crear RDD , DataFrames y DataSets . El objeto spark se encuentra disponible por defecto en el spark-shell y se puede crear de forma programativa mediante el patr\u00f3n builder de SparkSession . from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () Adem\u00e1s, desde una sesi\u00f3n de Spark podemos obtener un contexto a trav\u00e9s de la propiedad context : from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext","title":"SparkContext vs SparkSession"},{"location":"apuntes/spark01rdd.html#hola-spark","text":"Lo primero que debemos hacer siempre es conectarnos al contexto de Spark , el cual le indica a Spark como acceder al cl\u00faster. Si utilizamos la imagen de Docker , debemos obtener siempre el contexto a partir de la clase SparkSession : ejemploDockerSpark.py from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () # SparkSession de forma programativa sc = spark . sparkContext # SparkContext a partir de la sesi\u00f3n # Suma de los 100 primeros n\u00fameros rdd = sc . parallelize ( range ( 100 + 1 )) rdd . sum () En cambio, si utilizamos la instalaci\u00f3n de PySpark que tenemos en la m\u00e1quina virtual, directamente podemos acceder a la instancia de SparkSession a trav\u00e9s del objeto global spark : ejemploPySpark.py sc = spark . sparkContext # spark es una instancia de la clase SparkSession rdd = sc . parallelize ( range ( 100 + 1 )) rdd . sum () En ambos casos, si mostramos el contenido del contexto obtendremos algo similar a: Version v3.2.0 Master local[*] AppName PySparkShell A continuaci\u00f3n podemos ver el resultado completo en su ejecuci\u00f3n dentro de un cuaderno Jupyter: Hola Spark Nombre de la aplicaci\u00f3n Si queremos darle nombre a la aplicaci\u00f3n Spark, lo podemos hacer al obtener la SparkSession: spark = SparkSession . builder . appName ( \"spark-s8a\" ) . getOrCreate ()","title":"Hola Spark"},{"location":"apuntes/spark01rdd.html#spark-submit","text":"De la misma manera que mediante Hadoop pod\u00edamos lanzar un proceso al cl\u00faster para su ejecuci\u00f3n, Spark ofrece el comando spark-submit para enviar un script al driver para su ejecuci\u00f3n de forma distribuida. As\u00ed pues, si colocamos nuestro c\u00f3digo en un archivo de Python: holaSpark.py from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () sc = spark . sparkContext # Suma de los 100 primeros n\u00fameros rdd = sc . parallelize ( range ( 100 + 1 )) suma = rdd . sum () print ( \"--------------\" ) print ( suma ) print ( \"--------------\" ) Lo podemos ejecutar mediante (en nuestra m\u00e1quina virtual antes debemos resetear una variable de entorno para que no ejecute autom\u00e1ticamente el cuaderno jupyter: unset PYSPARK_DRIVER_PYTHON ): spark-submit holaMundo.py Si nuestro servidor estuviera en otra direcci\u00f3n IP, deber\u00edamos indicarle donde encontrar el master : spark-submit --master spark://<ip-servidor-driver>:7077 holaMundo.py M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial","title":"Spark Submit"},{"location":"apuntes/spark01rdd.html#arquitectura","text":"Ya hemos comentado que Spark es un sistema distribuido dise\u00f1ado para procesar grandes vol\u00famenes de datos de forma r\u00e1pida y eficiente. Este sistema normalmente se despliega en un conjunto de m\u00e1quinas que se conocen como un cl\u00faster Spark , pudiendo estar compuesta de unas pocas m\u00e1quinas o miles de ellas. Seg\u00fan el FAQ de Spark , el cl\u00faster m\u00e1s grande de Spark est\u00e1 compuesto por m\u00e1s de 8000 nodos. Normalmente se utiliza un sistema de gesti\u00f3n de recursos como YARN para gestionar de forma inteligente y eficiente el cl\u00faster. Los dos componentes principales del cl\u00faster zon: el gestor de cluster: nodo maestro que sabe donde se localizan los esclavos, cuanta memoria disponen y el n\u00famero de cores CPU de cada nodo. Su mayor responsabilidad es orquestar el trabajo asign\u00e1ndolo a los diferentes nodos. los nodos trabajadores ( workers ): cada nodo ofrece recursos (memoria, CPU, etc...) al gestor del cl\u00faster y realiza las tareas que se le asignen.","title":"Arquitectura"},{"location":"apuntes/spark01rdd.html#aplicaciones-spark","text":"Una aplicaci\u00f3n Spark se compone de dos partes: La l\u00f3gica de procesamiento de los datos, la cual realizamos mediante alguna de las API que ofrece Spark (Java, Scala, Python, etc...), desde algo sencillo que realice una ETL sobre los datos a problemas m\u00e1s complejos que requieran m\u00faltiples iteraciones y tarden varias horas como entrenar un modelo de machine learning . Driver: coordinador central encargado de interactuar con el cl\u00faster Spark y averiguar qu\u00e9 m\u00e1quinas deben ejecutar la l\u00f3gica de procesamiento. Para cada una de esas m\u00e1quinas, el driver realiza una petici\u00f3n al cl\u00faster para lanchar un proceso conocido como un ejecutor ( executor ). Adem\u00e1s, el driver Spark es responsable de gestionar y distribuir las tareas a cada ejecutor, y si es necesario, recoger y fusionar los datos resultantes para presentarlos al usuario. Estas tareas se realizan a trav\u00e9s de la SparkSession . Cada ejecutor es un proceso JVM ( Java Virtual Machine ) dedicado para una aplicaci\u00f3n Spark espec\u00edfica. Un ejecutor vivir\u00e1 tanto como dure la aplicaci\u00f3n Spark, lo cual puede ser minutos o d\u00edas, dependiendo de la complejidad de la aplicaci\u00f3n. Conviene destacar que los ejecutor son elementos aislados que no se comparten entre aplicaciones Spark, por lo que la \u00fanica manera de compartir informaci\u00f3n entre diferente ejecutores es mediante un sistema de almacenamiento externo como HDFS. Arquitectura entre una aplicaci\u00f3n Spark y el gestor del cl\u00faster As\u00ed pues, Spark utiliza una arquitectura maestro/esclavo, donde el driver es el maestro, y los ejecutores los esclavos. Cada uno de estos componentes se ejecutan como un proceso independiente en el cl\u00faster Spark. Por lo tanto, una aplicaci\u00f3n Spark se compone de un driver y m\u00faltiples ejecutores. Cada ejecutor realiza lo que se le pide en forma de tareas,ejecutando cada una de ellas en un n\u00facleo CPU separado. As\u00ed es como el procesamiento paralelo acelera el tratamiento de los datos. Adem\u00e1s, cada ejecutor, bajo petici\u00f3n de la l\u00f3gica de la aplicaci\u00f3n, se responsabiliza de cachear un fragmento de los datos en memoria y/o disco. Al lanzar una aplicaci\u00f3n Spark, podemos indicar el n\u00famero de ejecutores que necesita la aplicaci\u00f3n, as\u00ed com la cantidad de memoria y n\u00famero de n\u00facleos que cada ejecutor deber\u00eda tener. Cl\u00faster compuesto por un driver y tres ejecutores","title":"Aplicaciones Spark"},{"location":"apuntes/spark01rdd.html#job-stage-y-task","text":"Cuando creamos una aplicaci\u00f3n Spark, por debajo, se distinguen los siguientes elementos: Job (trabajo): computaci\u00f3n paralela compuesta de m\u00faltiples tareas que se crean tras una acci\u00f3n de Spark ( save , collect , etc...). Al codificar nuestro c\u00f3digo mediante PySpark , el driver convierte la aplicaci\u00f3n Spark en uno o m\u00e1s jobs , y a continuaci\u00f3n, estos jobs los transforma en un DAG (grafo). Este grafo, en esencia, es el plan de ejecuci\u00f3n, donde cada elemento dentro del DAG puede implicar una o varias stages (escenas). Stage (escena): cada job se divide en peque\u00f1os conjuntos de tareas que forman un escenario. Como parte del grafo, las stages se crean a partir de si las operaciones se pueden realizar de forma paralela o de forma secuencial. Como no todas las operaciones pueden realizarse en una \u00fanica stage , en ocasiones de dividen en varias, normalmente debido a los l\u00edmites computacionales de los diferentes ejecutores. Task (tarea): unida de trabajo m\u00e1s peque\u00f1a que se envia a los ejecutores Spark . Cada escenarios se compone de varias tareas. Cada una de las tareas se asigna a un \u00fanico n\u00facleo y trabaja con una \u00fanica partici\u00f3n de los datos. Por ello, un ejecutor con 16 n\u00facleos puede tener 16 o m\u00e1s tareas trabajando en 16 o m\u00e1s particiones en paralelo. Driver -> Job -> Stage -> Task","title":"Job, Stage y Task"},{"location":"apuntes/spark01rdd.html#dataframe","text":"La principal abstracci\u00f3n de los datos en Spark es el Dataset . Se pueden crear desde las fuentes de entrada de Hadoop (como ficheros HDFS) o mediante transformaciones de otros Datasets . Dado el cariz de Python , no necesitamos que los Dataset est\u00e9n fuertemente tipados, por eso, todos los Dataset que usemos ser\u00e1n Dataset[Row] (si trabaj\u00e1semos mediante Java o Scala s\u00ed deber\u00edamos indicar el tipo de sus datos), y por consistencia con el concepto de Pandas y R, los llamaremos DataFrame . Por ejemplo, veamos c\u00f3mo podemos crear un DataFrame a partir de un fichero de texto: from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () quijoteTxt = spark . read . text ( \"el_quijote.txt\" ) quijoteTxt . count () # n\u00famero de filas del DataFrame - 2186 quijoteTxt . first () # primera fila - Row(value='DON QUIJOTE DE LA MANCHA') # Transformamos un DataFrame en otro nuevo lineasConQuijote = quijoteTxt . filter ( quijoteTxt . value . contains ( \"Quijote\" )) # DataFrame con las l\u00edneas que contiene la palabra Quijote lineasConQuijote . count () # cantidad de l\u00edneas con la palabra Quijote - 584 # Las transformaciones se pueden encadenar quijoteTxt . filter ( quijoteTxt . value . contains ( \"Quijote\" )) . count () # idem - 584 Estudiaremos los DataFrame en profundidad en la siguiente sesi\u00f3n .","title":"DataFrame"},{"location":"apuntes/spark01rdd.html#rdd","text":"Un RDD ( Resilient Distributed Datasets ) es una estructura de datos que abstrae los datos para su procesamiento en paralelo. Antes de Spark 2.0, los RDD eran el interfaz principal para interactuar con los datos. Se trata de una colecci\u00f3n de elementos tolerantes a fallos que son immutables (una vez creados, no se pueden modificar) y dise\u00f1ados para su procesamiento distribuido. Cada conjunto de datos en los RDD se divide en particiones l\u00f3gicas, que se pueden calcular en diferentes nodos del cl\u00faster. Hay dos formas de crear un RDD: Paralelizando una colecci\u00f3n ya existente en nuestra aplicaci\u00f3n Spark . Referenciando un dataset de un sistema externo como HDFS , HBase , etc... Sobre los RDD se pueden realizar dos tipos de operaciones: Acci\u00f3n: devuelven un valor tras ejecutar una computaci\u00f3n sobre el conjunto de datos. Transformaci\u00f3n: es una operaci\u00f3n perezosa que crea un nuevo conjunto de datos a partir de otro RDD/Dataset, tras realizar un filtrado, join , etc... \u00bfRDD obsoleto? Antes de la versi\u00f3n 2.0, el principal interfaz para programar en Spark eran los RDD. Tras la versi\u00f3n 2.0, fueron sustituidos por los Dataset , que son RDD fuertemente tipados que adem\u00e1s est\u00e1n optimizados a bajo nivel. El interfaz RDD todav\u00eda tiene soporte, sin embargo, se recomienda el uso de los Dataset por su mejor rendimiento. A lo largo de estas sesiones iremos combinando ambos interfaces para conocer las similitudes y diferencias.","title":"RDD"},{"location":"apuntes/spark01rdd.html#acciones","text":"A continuaci\u00f3n vamos a revisar las acciones m\u00e1s comunes. Puedes consultar todas las acciones disponibles en la documentaci\u00f3n oficial :","title":"Acciones"},{"location":"apuntes/spark01rdd.html#parallelize","text":"Podemos crear RDD directamente desde cero sin necesidad de leer los datos desde un fichero. Para ello, podemos utilizar parallelize . Esta acci\u00f3n divide una colecci\u00f3n de elementos entre los nodos de nuestro cl\u00fasters. Por ejemplo: miRDD = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ]) lista = [ 'Hola' , 'Adi\u00f3s' , 'Hasta luego' ] listaRDD = sc . parallelize ( lista ) # Creamos un RDD a partir de una lista listaRDD4 = sc . parallelize ( lista , 4 ) # Creamos un RDD con 4 particiones","title":"Parallelize"},{"location":"apuntes/spark01rdd.html#take-y-sample","text":"Cuando queremos recuperar un n\u00famero determinado de resultado, de forma similar a limit en SQL, tenemos la acci\u00f3n take : miRDD . take ( 3 ) # [1, 2, 3] listaRDD . take ( 2 ) # ['Hola', 'Adi\u00f3s'] Otra opci\u00f3n es utilizar sample para obtener una muestra de los datos, aunque en este caso no es una acci\u00f3n sino una transformaci\u00f3n: miRDDmuestra = miRDD . sample ( False , 0.5 ) miRDDmuestra . collect () # [2, 4, 6, 7, 8, 9] / [1, 2, 3, 4, 6] / [5, 8, 9] Esta transformaci\u00f3n recibe varios par\u00e1metros: withReplacement : booleano para indicar si queremos elementos repetidos fraction : valor entre 0 y 1 que expresa la probabilidad de elegir cada elemento opcionalmente se le puede pasar un tercer valor con la semilla As\u00ed pues, en el ejemplo anterior, cada llamada a sample ha generado un RDD diferente, sin valores repetidos, pero con un tama\u00f1o de RDD variable. Para obtener una muestra mediante una acci\u00f3n, tenemos la opci\u00f3n takeSample que funciona de forma similar pero sin hacer shuffle y devuelve una lista: miRDDmuestraT = miRDD . takeSample ( False , 5 ) print ( miRDDmuestraT ) # [1, 8, 9, 7, 2] El primer par\u00e1metro vuelve a indicar si hay repetidos, pero el segundo fija la cantidad de elementos a devolver. Por \u00faltimo, mediante top obtenemos los primeros elementos una vez ordenado el RDD: miRDD . top ( 3 ) # [9, 8, 7] De forma similar, tenemos takeOrdered que recupera la cantidad de registros necesarios pero ordenados ascendentemente (al contrario que top ), con la opci\u00f3n de ordenarlos descendentemente (similar a top ): miRDD . takeOrdered ( 3 ) # [1, 2, 3] miRDD . takeOrdered ( 3 , lambda x : - x ) # [9, 8, 7] Hay que tener cuidado si el conjunto de datos es muy grande, porque tanto take como takeSample , takeOrdered y top llevar\u00e1n todos los datos a memoria.","title":"Take y Sample"},{"location":"apuntes/spark01rdd.html#collect","text":"Un fallo muy posible a la hora de mostrar los datos de un RDD es utilizar rdd.foreach(print) o rdd.map(print) . En una \u00fanica m\u00e1quina, esta operaci\u00f3n generar\u00eda la salida esperada mostrando todos los elementos del RDD. Sin embargo, al trabajar en un cl\u00faster, la salida a stdout la realizar\u00edan los diferentes nodos y no el nodo principal. As\u00ed pues, para mostrar todos los elementos de un RDD / DataFrame / Dataset hemos de emplear el m\u00e9todo collect , el cual primero mostrar\u00e1 los RDD del nodo principal ( driver node ), y luego para cada nodo del cluster mostrar\u00e1 sus datos. rdd . collect () Out of memory Hay que tener mucho cuidado, ya que nos podemos quedar f\u00e1cilmente sin memoria, ya que collect se trae los datos de todos los ejecutores a un \u00fanico nodo, el que \u00e9sta ejecutando el c\u00f3digo ( driver ). Si s\u00f3lo necesitamos mostrar unos pocos elementos, un enfoque m\u00e1s seguro es utilizar take : rdd . take ( 100 ) . foreach ( print )","title":"Collect"},{"location":"apuntes/spark01rdd.html#transformaciones","text":"En Spark , las estructuras de datos son inmutables, de manera que una vez creadas no se pueden modificar. Para poder modificar un RDD/DataFrame , hace falta realizar una transformaci\u00f3n , siendo el modo de expresar la l\u00f3gica de negocio mediante Spark . Todas las transformaciones en Spark se eval\u00faan de manera perezosa ( lazy evaluation ), de manera que los resultados no se computan inmediatamente, sino que se retrasa el c\u00e1lculo hasta que el valor sea necesario. Para ello, se van almacenando los pasos necesarios y se ejecutan \u00fanicamente cuando una acci\u00f3n requiere devolver un resultado al driver . Este dise\u00f1o facilita un mejor rendimiento de Spark (por ejemplo, imagina que tras una operaci\u00f3n map se realiza un reduce y en vez de devolver todo el conjunto de datos tras el map , s\u00f3lo le enviamos al driver el resultado de la reducci\u00f3n). As\u00ed pues, las acciones provocan la evaluaci\u00f3n de todas las transformaciones previas que se hab\u00edan evaluado de forma perezosa y estaban a la espera. Por defecto, cada transformaci\u00f3n RDD/DataSet se puede recalcular cada vez que se ejecute una acci\u00f3n. Sin embargo, podemos persistir un RDD en memoria mediante los m\u00e9todos persist (o cache ), de manera que Spark mantendr\u00e1 los datos para un posterior acceso m\u00e1s eficiente. Tambi\u00e9n podemos persistir RDD en disco o replicarlo en m\u00faltiples nodos.","title":"Transformaciones"},{"location":"apuntes/spark01rdd.html#tipos-de-transformaciones","text":"Existen dos tipos de transformaciones, dependiendo de las dependencias entre las particiones de datos: Transformaciones Narrow : consisten en dependencias estrechas en las que cada partici\u00f3n de entrada contribuye a una \u00fanica partici\u00f3n de salida. Transformaciones Wide : consisten en dependencias anchas de manera que varias particiones de entrada contribuyen a muchas otras particiones de salida, es decir, cada partici\u00f3n de salida depende de diferentes particiones de entrada. Este proceso tambi\u00e9n se conoce como shuffle , ya que Spark baraja los datos entre las particiones del cl\u00faster. Transformaciones Narrow vs Wide Con las transformaciones narrow , Spark realiza un pipeline de las dependencias, de manera que si especificamos m\u00faltiples filtros sobre DataFrames/RDD, se realizar\u00e1n todos en memoria. Esto no sucede con las transformaciones wide , ya que al realizar un shuffle los resultados se persisten en disco. Cuidado con shuffle Las operaciones shuffle son computacionalmente caras, ya que implican E/S en disco, serializaci\u00f3n de datos y E/S en red. Para organizar los datos previos al shuffle , Spark genera un conjunto de tareas (tareas map para organizar los datos, y reduce para agregar los resultados). Internamente, el resultado de las tareas map se mantienen en memoria hasta que no caben. Entonces, se ordenan en la partici\u00f3n destino y se persisten en un \u00fanico archivo. En la fase de reducci\u00f3n, las tareas leen los bloques ordenados que son relevantes. Las operaciones reduceByKey y aggregateByKey son de las que m\u00e1s memoria consumen, al tener que crear las estructuras de datos para organizar los registros en las tareas de map , y luego generar los resultados agregados en la de reduce . Si los datos no caben en memoria, Spark los lleva a disco, incurriendo en operaciones adiciones de E/S en disco y del recolector de basura. A continuaci\u00f3n veremos las diferentes transformaciones que podemos realizar con Spark.","title":"Tipos de transformaciones"},{"location":"apuntes/spark01rdd.html#transformaciones-narrow","text":"Para los siguientes ejemplo, utilizaremos el siguiente fichero de empleados.txt que ya utilizamos en la sesi\u00f3n de Hive : empleados.txt Michael|Montreal,Toronto|Male,30|DB:80|Product:Developer\u0004Lead Will|Montreal|Male,35|Perl:85|Product:Lead,Test:Lead Shelley|New York|Female,27|Python:80|Test:Lead,COE:Architect Lucy|Vancouver|Female,57|Sales:89,HR:94|Sales:Lead","title":"Transformaciones Narrow"},{"location":"apuntes/spark01rdd.html#map","text":"La transformaci\u00f3n map aplica la funci\u00f3n recibida a cada elemento del RDD, de manera que vamos a poder a\u00f1adir una nueva columna, modificar una existente, etc... Por ejemplo, si la entrada es un RDD que contiene [1, 2, 3, 4, 5] , al hacer rdd.map(x=>x*2) obtendr\u00edamos un nuevo RDD con [2, 4, 6, 8, 10] : rdd = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . map ( lambda x : x * 2 ) resultRDD . collect () # [2, 4, 6, 8, 10] Mediante la funci\u00f3n textFile podemos cargar un archivo. Supongamos que tenemos cargado en Hadoop el archivo empleados.txt : rddLocal = sc . textFile ( \"empleados.txt\" ) rdd = sc . textFile ( \"hdfs://iabd-virtualbox:9000/user/iabd/datos/empleados.txt\" ) rdd . count () # 4 - cantidad de l\u00edneas resultRDD = rdd . map ( len ) # obtenemos la cantidad de caracteres cada l\u00ednea resultRDD . collect () # [61, 52, 60, 50] Si quisi\u00e9ramos mostrar los datos de los empleados, podr\u00edamos recoger los datos del RDD y recorrerlo: empleados = rdd . collect () for empleado in empleados : print ( empleado )","title":"Map"},{"location":"apuntes/spark01rdd.html#flatmap","text":"La transformaci\u00f3n flatMap es muy similar a la anterior, pero en vez de devolver un elemento por cada entrada, devuelve una lista por cada entrada, deshaciendo las colecciones en elementos individuales: rdd = sc . textFile ( \"empleados.txt\" ) resultFM = rdd . flatMap ( lambda x : x . split ( \"|\" )) resultFM . collect () Obtendr\u00edamos cada atributo separado y todos dentro de la misma lista: [ 'Michael' , 'Mo ntreal , Toro nt o' , 'Male , 30 ' , 'DB : 80 ' , 'Produc t : Developer\\x 04 Lead' , 'Will' , 'Mo ntreal ' , 'Male , 35 ' , 'Perl : 85 ' , 'Produc t : Lead , Tes t : Lead' , 'Shelley' , 'New York' , 'Female , 27 ' , 'Py t ho n : 80 ' , 'Tes t : Lead , COE : Archi te c t ' , 'Lucy' , 'Va n couver' , 'Female , 57 ' , 'Sales : 89 , HR : 94 ' , 'Sales : Lead' ]","title":"FlatMap"},{"location":"apuntes/spark01rdd.html#filter","text":"Permite filtrar los elementos que cumplen una condici\u00f3n mediante filter : rdd = sc . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . filter ( lambda x : x % 2 == 0 ) resultRDD . collect () # [2, 4] Por ejemplo, si queremos filtrar los empleados que son hombres, primero separamos por las | y nos quedamos con el tercer elemento que contiene el sexo y la edad. A continuaci\u00f3n, separamos por la coma para quedarnos en el sexo en la posici\u00f3n 0 y la edad en el 1, y comparamos con el valor deseado: rdd = sc . textFile ( \"empleados.txt\" ) hombres = rdd . filter ( lambda x : x . split ( \"|\" )[ 2 ] . split ( \",\" )[ 0 ] == \"Male\" ) resultFM . collect () Obteniendo: [ 'Michael|Mo ntreal , Toro nt o|Male , 30 |DB : 80 |Produc t : Developer\\x 04 Lead' , 'Will|Mo ntreal |Male , 35 |Perl : 85 |Produc t : Lead , Tes t : Lead' ] Tambi\u00e9n podemos anidar diferentes transformaciones. Para este ejemplo, vamos a crear tuplas formadas por un n\u00famero y su cuadrado, luego quitar los que no coincide el n\u00famero su potencia (el 0 y el 1), y luego aplanarlo en una lista: rdd10 = sc . parallelize ( range ( 10 + 1 )) rddPares = rdd10 . map ( lambda x : ( x , x ** 2 )) . filter ( lambda x : ( x [ 0 ] != x [ 1 ])) . flatMap ( lambda x : x ) rddPares . collect () # [2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81, 10, 100]","title":"Filter"},{"location":"apuntes/spark01rdd.html#union","text":"Mediante union unimos dos RDD en uno: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 5 , 6 , 7 , 8 ]) resultRDD = rdd1 . union ( rdd2 ) resultRDD . collect () # [1, 2, 3, 4, 5, 6, 7, 8]","title":"Union"},{"location":"apuntes/spark01rdd.html#intersection","text":"Mediante intersection , obtendremos los elementos que tengan en com\u00fan: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 3 , 4 , 5 , 6 ]) resultRDD = rdd1 . intersection ( rdd2 ) resultRDD . collect () # [1, 2, 3, 4, 5, 6, 7, 8]","title":"Intersection"},{"location":"apuntes/spark01rdd.html#subtract","text":"Mediante subtract , obtendremos los elementos propios que no est\u00e9n en el RDD recibido: rdd1 = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rdd2 = sc . parallelize ([ 3 , 4 , 5 , 6 ]) resultRDD = rdd1 . subtract ( rdd2 ) resultRDD . collect () # [1, 2] Autoevaluaci\u00f3n Si tenemos dos RDD (A y B): rddA = sc . parallelize ([ 1 , 2 , 3 , 4 ]) rddB = sc . parallelize ([ 3 , 4 , 5 , 6 ]) Enunciado Soluci\u00f3n \u00bfC\u00f3mo conseguimos los elementos que est\u00e1n en A y no B y los de B que no est\u00e1n en A? (es decir [1, 2, 5, 6] )): resultRDD = rddA . subtract ( rddB ) . union ( rddB . subtract ( rddA )) resultRDD . collect () # [1, 2, 5, 6]","title":"Subtract"},{"location":"apuntes/spark01rdd.html#distinct","text":"Si utilizamos distinct eliminaremos los elementos repetidos: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 4 , 5 ]) resultRDD = rdd . distinct () resultRDD . collect () # [4, 1, 5, 2, 3]","title":"Distinct"},{"location":"apuntes/spark01rdd.html#rdd-de-pares","text":"Una t\u00e9cnica muy com\u00fan a la hora de trabajar con RDD es hacerlo con elementos que tienen el formato (clave,valor), las claves y los valores pueden ser de cualquier tipo. listaTuplas = [( 1 , 'a' ), ( 2 , 'b' ), ( 3 , 'c' ), ( 4 , 'd' )] rddTuplas = sc . parallelize ( listaTuplas ) Sobre estos RDD podemos realizar algoritmos MapReduce para muchas funciones de procesamiento de datos, como es la agrupaci\u00f3n, ordenaci\u00f3n, join , count , etc... Para generar un RDD de pares, adem\u00e1s de generarlo nosotros a partir de una lista, podemos emplear las siguientes operaciones: zip : uniendo dos listas del mismo tama\u00f1o: lista1 = [ 'a' , 'b' , 'c' , 'e' , 'f' , 'g' , 'h' ] lista2 = [ 4 , 5 , 6 , 7 , 8 , 9 , 10 ] rddZip = sc . parallelize ( lista1 ) . zip ( sc . parallelize ( lista2 )) . collect () # [('a', 4), ('b', 5), ('c', 6), ('e', 7), ('f', 8), ('g', 9), ('h', 10)] rddZipSecuencia = sc . parallelize ( zip ( lista1 , range ( len ( lista1 )))) # usando el tama\u00f1o de la lista # [('a', 0), ('b', 1), ('c', 2), ('e', 3), ('f', 4), ('g', 5), ('h', 6)] Otros m\u00e9todos relacionados son zipWithIndex , zipWithUniqueId o unir dos RDD mediante zip . map : asignando a cada elemento un valor o c\u00e1lculo sobre \u00e9l mismo: lista = [ 'Hola' , 'Adi\u00f3s' , 'Hasta luego' ] rddMap = sc . parallelize ( lista ) . map ( lambda x : ( x , len ( c )) # [('Hola', 4), ('Adi\u00f3s', 5), ('Hasta luego', 11)] keyBy : permite crear las claves a partir de cada elemento: rddKeyBy = sc . parallelize ( lista ) . keyBy ( lambda x : x [ 0 ]) # creamos una clave con la primera letra # [('H', 'Hola'), ('A', 'Adi\u00f3s'), ('H', 'Hasta luego')] Autoevaluaci\u00f3n A partir de la lista \"Perro Gato Loro Pez Le\u00f3n Tortuga Gallina\" Enunciado Soluci\u00f3n Crea un RDD a partir de esta lista Convierte el RDD normal en un RDD de pares cuya clave sea la primera letra del animal Crea otro RDD de pares pero poniendo como clave un n\u00famero incremental \u00bfY si queremos que el \u00edndice incremental empiece en 100? animales = \"Perro Gato Loro Pez Le\u00f3n Tortuga Gallina\" animalesLista = animales . split ( \" \" ) rdd1 = sc . parallelize ( animalesLista ) rdd2 = rdd1 . keyBy ( lambda x : x [ 0 ]) rdd3 = sc . parallelize ( zip ( range ( len ( animalesLista )), animalesLista )) rdd4 = sc . parallelize ( zip ( range ( 100 , 100 + len ( animalesLista )), animalesLista )) Sobre los RDD de pares, podemos realizar las siguientes transformaciones: keys : devuelve las claves values : devuelve los valores mapValues : Aplica la funci\u00f3n sobre los valores flatMapValues Aplica la funci\u00f3n sobre los valores y los aplana. A continuaci\u00f3n se muestra un fragmento de c\u00f3digo para poner en pr\u00e1ctica las transformaciones comentadas: listaTuplas = [( 'a' , 1 ),( 'z' , 3 ),( 'b' , 4 ),( 'c' , 3 ),( 'a' , 4 )] rddTuplas = sc . parallelize ( listaTuplas ) claves = rddTuplas . keys () # ['a', 'z', 'b', 'c', 'a'] valores = rddTuplas . values () # [1, 3, 4, 3, 4] rddMapValues = rddTuplas . mapValues ( lambda x : ( x , x * 2 )) # [('a', (1, 2)), ('z', (3, 6)), ('b', (4, 8)), ('c', (3, 6)), ('a', (4, 8))] rddFMV = rddTuplas . flatMapValues ( lambda x : ( x , x * 2 )) # [('a', 1), # ('a', 2), # ('z', 3), # ('z', 6), # ('b', 4), # ...","title":"RDD de Pares"},{"location":"apuntes/spark01rdd.html#transformaciones-wide","text":"Las siguientes transformaciones, adem\u00e1s de trabajar con RDD de pares, mezclan los datos de las particiones mediante el shuffle de los elementos.","title":"Transformaciones Wide"},{"location":"apuntes/spark01rdd.html#reducebykey","text":"Mediante la transformaci\u00f3n reducedByKey los datos se calculan utilizando una funci\u00f3n de reducci\u00f3n a partir de la clave combinando en la misma m\u00e1quina las parejas con la misma clave antes de que los datos se barajen. Por ejemplo, vamos a calcular el total de unidades vendidas por pa\u00eds: rdd = sc . textFile ( \"pdi_sales.csv\" ) # Recogemos el pa\u00eds y las unidades de las ventas paisesUnidades = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" )[ 3 ])) # Le quitamos el encabezado header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) # Reducimos por el pais y sumamos las unidades paisesTotalUnidades = paisesUnidadesSinHeader . reduceByKey ( lambda a , b : int ( a ) + int ( b )) paisesTotalUnidades . collect () Funciones lambda en reduce Al aplicar una transformaci\u00f3n de tipo reduce , la funci\u00f3n lambda recibir\u00e1 dos par\u00e1metros, siendo el primero el valor acumulado y el segundo el valor del elemento a operar. Y el resultado: [ ('Ca na da ' , 77609 ) , ('Fra n ce ' , 327730 ) , ('Germa n y' , 244265 ) , ('Mexico ' , 223463 ) ] Autoevaluaci\u00f3n Dada la siguiente lista de compra: lista = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] Enunciado Soluci\u00f3n Calcula: El total que se ha gastado por cada producto Cu\u00e1nto es lo m\u00e1ximo que se ha pagado por cada producto lista = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] rdd = sc . parallelize ( lista ) rdd1 = rdd . reduceByKey ( lambda x , y : x + y ) rdd2 = rdd . reduceByKey ( lambda x , y : max ( x , y ))","title":"ReduceByKey"},{"location":"apuntes/spark01rdd.html#groupbykey","text":"Permite agrupar los datos a partir de una clave, repartiendo los resultados ( shuffle ) entre todos los nodos: rdd = sc . textFile ( \"pdi_sales.csv\" ) # Creamos un RDD de pares con el nombre del pa\u00eds como clave, y una lista con los valores ventas = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" ))) # Quitamos el primer elemento que es el encabezado del CSV header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) # Agrupamos las ventas por nombre del pa\u00eds paisesAgrupados = ventas . groupByKey () paisesAgrupados . collect () Obtendremos para cada pa\u00eds, un iterable con todos sus datos: [ ('Ca na da ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 4 b 2e0 >) , ('Fra n ce ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 816 c 3 a 9700 >) , ('Germa n y' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 96e b 0 >) , ('Mexico ' , <pyspark.resul t i tera ble.Resul t I tera ble a t 0 x 7 f 814 cd 965e0 >) ] Podemos transformar los iterables a una lista: paisesAgrupadosLista = paisesAgrupados . map ( lambda x : ( x [ 0 ], list ( x [ 1 ]))) paisesAgrupadosLista . collect () Obteniendo: [ ('Ca na da ' , [[ ' 725 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 1 ' , ' 115.4 ' , 'Ca na da ' ], [ ' 2235 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 2 ' , ' 131.1 ' , 'Ca na da ' ], [ ' 713 ' , ' 1 / 15 / 1999 ' , 'H 1 B ' , ' 1 ' , ' 160.1 ' , 'Ca na da ' ], ... Autoevaluaci\u00f3n Ahora tenemos las cuentas de las compras de 3 d\u00edas: d\u00eda 1: pan 3\u20ac, agua 2\u20ac, az\u00facar 1\u20ac, leche 2\u20ac, pan 4\u20ac d\u00eda 2: pan 1\u20ac, cereales 3\u20ac, agua 0.5\u20ac, leche 2\u20ac, filetes 5\u20ac d\u00eda 3: filetes 2\u20ac, cereales 1\u20ac Dada la siguiente lista de compra: dia1 = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 4 )] dia2 = [( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] dia3 = [( 'filetes' , 2 ), ( 'cereales' , 1 )] Enunciado Soluci\u00f3n \u00bfC\u00f3mo obtenemos lo que hemos gastado en cada producto? \u00bfY el gasto medio que hemos realizado en cada uno de ellos? dia1 = [( 'pan' , 3 ), ( 'agua' , 2 ), ( 'az\u00facar' , 1 ), ( 'leche' , 2 ), ( 'pan' , 4 )] dia2 = [( 'pan' , 1 ), ( 'cereales' , 3 ), ( 'agua' , 0.5 ), ( 'leche' , 2 ), ( 'filetes' , 5 )] dia3 = [( 'filetes' , 2 ), ( 'cereales' , 1 )] rdd = sc . parallelize ( dia1 ) . union ( sc . parallelize ( dia2 )) . union ( sc . parallelize ( dia3 )) rdd1 = rdd . groupByKey () # [('leche', <pyspark.resultiterable.ResultIterable at 0x7f3494aea130>), # ('agua', <pyspark.resultiterable.ResultIterable at 0x7f3494acd4c0>), # ('pan', <pyspark.resultiterable.ResultIterable at 0x7f3494acd2b0>), # ('cereales', <pyspark.resultiterable.ResultIterable at 0x7f3494acd850>), # ('filetes', <pyspark.resultiterable.ResultIterable at 0x7f34944e3be0>), # ('az\u00facar', <pyspark.resultiterable.ResultIterable at 0x7f34944e3fa0>)] rdd1a = [( x , list ( y )) for x , y in rdd1 . collect ()] # rdd1a ya no es un RDD, es una lista que se ha cargado en el driver rdd1b = rdd1 . map ( lambda x : ( x [ 0 ], list ( x [ 1 ]))) #rdd1b sigue siendo un RDD # [('leche', [2, 2]), # ('agua', [2, 0.5]), # ('pan', [3, 4, 1]), # ('cereales', [3, 1]), # ('filetes', [5, 2]), # ('az\u00facar', [1])] rdd2 = rdd1 . map ( lambda x : ( x [ 0 ], sum ( x [ 1 ]) / len ( x [ 1 ]))) # [('leche', 2.0), # ('agua', 1.25), # ('pan', 2.6666666666666665), # ('cereales', 2.0), # ('filetes', 3.5), # ('az\u00facar', 1.0)] Mejor reduceByKey que groupByKey Si el tipo de operaci\u00f3n a realizar es posible mediante una operaci\u00f3n de reduce , su rendimiento ser\u00e1 una soluci\u00f3n m\u00e1s eficiente. M\u00e1s informaci\u00f3n en el art\u00edculo Avoid Group By","title":"GroupByKey"},{"location":"apuntes/spark01rdd.html#sortbykey","text":"sortByKey permite ordenar los datos a partir de una clave. Los pares de la misma m\u00e1quina se ordenan primero por la misma clave, y luego los datos de las diferentes particiones se barajan. Para ello crearemos una tupla, siendo el primer elemento un valor num\u00e9rico por el cual ordenar, y el segundo el dato asociado. Vamos a partir del ejemplo anterior para ordenar los paises por la cantidad de ventas: # Ejemplo anterior rdd = sc . textFile ( \"pdi_sales.csv\" ) paisesUnidades = rdd . map ( lambda x : ( x . split ( \";\" )[ - 1 ], x . split ( \";\" )[ 3 ])) header = paisesUnidades . first () paisesUnidadesSinHeader = paisesUnidades . filter ( lambda linea : linea != header ) paisesTotalUnidades = paisesUnidadesSinHeader . reduceByKey ( lambda a , b : int ( a ) + int ( b )) # Le damos la vuelta a la lista unidadesPaises = paisesTotalUnidades . map ( lambda x : ( x [ 1 ], x [ 0 ])) unidadesPaises . collect () Ahora tendr\u00edamos: [ ( 77609 , 'Ca na da ') , ( 327730 , 'Fra n ce ') , ( 244265 , 'Germa n y') , ( 223463 , 'Mexico ') ] Y a continuaci\u00f3n los ordenamos: unidadesPaisesOrdenadas = unidadesPaises . sortByKey () unidadesPaisesOrdenadas . collect () Y comprobamos el resultado: [ ( 77609 , 'Ca na da ') , ( 223463 , 'Mexico ') , ( 244265 , 'Germa n y') , ( 327730 , 'Fra n ce ') ] Si quisi\u00e9ramos obtener los datos en orden descendente, le pasamos False a la transformaci\u00f3n: unidadesPaisesOrdenadasDesc = unidadesPaises . sortByKey ( False )","title":"SortByKey"},{"location":"apuntes/spark01rdd.html#sortby","text":"Mediante sortBy podemos ordenar los datos indicando nosotros la funci\u00f3n de ordenaci\u00f3n: paisesTotalUnidades . sortBy ( lambda x : x [ 1 ]) . collect () Obteniendo: [ ('Ca na da ' , 77609 ) , ('Mexico ' , 223463 ) , ('Germa n y' , 244265 ) , ('Fra n ce ' , 327730 ) ] Si queremos ordenar descendentemente, le pasamos un segundo par\u00e1metro a con valor False (indica si la ordenaci\u00f3n es ascendente): paisesTotalUnidades . sortBy ( lambda x : x [ 1 ], False ) . collect () Join Aunque los RDD permitan realizar operaciones join , realmente este tipo de operaciones se realizan mediante DataSet, por lo que omitimos su explicaci\u00f3n en esta sesi\u00f3n.","title":"SortBy"},{"location":"apuntes/spark01rdd.html#particiones","text":"Spark organiza los datos en particiones, consider\u00e1ndolas divisiones l\u00f3gicas de los datos entre los nodos del cl\u00faster. Por ejemplo, si el almacenamiento es HDFS cada partici\u00f3n se almacena en un bloque. Cada una de las particiones va a llevar asociada una tarea de ejecuci\u00f3n, de manera que a m\u00e1s particiones, mayor paralelizaci\u00f3n del proceso. Veamos con c\u00f3digo como podemos trabajar con las particiones: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ]) rdd . getNumPartitions () # 4 rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 2 ) rdd . getNumPartitions () # 2 rddE = sc . textFile ( \"empleados.txt\" ) rddE . getNumPartitions () # 2 rddE = sc . textFile ( \"empleados.txt\" , 3 ) rddE . getNumPartitions () # 3 La mayor\u00eda de operaciones / transformaciones / acciones que trabajan con los datos admiten un par\u00e1metro extra indicando la cantidad de particiones con las que queremos trabajar. +FIXME: revisar https://techmagie.wordpress.com/2015/12/19/understanding-spark-partitioning/ y https://www.projectpro.io/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297","title":"Particiones"},{"location":"apuntes/spark01rdd.html#mappartitions","text":"A diferencia de la transformaci\u00f3n map que se invoca por cada elemento del RDD/DataSet, mapPartitions se llama por cada partici\u00f3n. La funci\u00f3n que recibe como par\u00e1metro recoger\u00e1 como entrada un iterador con los elementos de cada partici\u00f3n: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 2 ) def f ( iterator ): yield sum ( iterator ) resultadoRdd = rdd . mapPartitions ( f ) resultadoRdd . collect () # [6, 15] resultadoRdd2 = rdd . mapPartitions ( lambda iterator : [ list ( iterator )]) resultadoRdd2 . collect () # [[1, 1, 2, 2], [3, 3, 4, 5]] En el ejemplo, ha dividido los datos en dos particiones, la primera con [1, 1, 2, 2] y la otra con [3, 3, 4, 5] , y de ah\u00ed el resultado de sumar sus elementos es [6, 15] . FIXME: revisar https://www.mytechmint.com/pyspark-mappartitions/ y https://sparkbyexamples.com/pyspark/pyspark-mappartitions/","title":"MapPartitions"},{"location":"apuntes/spark01rdd.html#mappartitionswithindex","text":"De forma similar al caso anterior, pero ahora mapPartitionsWithIndex recibe una funci\u00f3n cuyos par\u00e1metros son el \u00edndice de la partici\u00f3n y el iterador con los datos de la misma: def mpwi ( indice , iterador ): return [( indice , list ( iterador ))] resultadoRdd = rdd . mapPartitionsWithIndex ( mpwi ) resultadoRdd . collect () # [(0, [1, 1, 2, 2]), (1, [3, 3, 4, 5])]","title":"mapPartitionsWithIndex"},{"location":"apuntes/spark01rdd.html#modificando-las-particiones","text":"Podemos modificar la cantidad de particiones mediante dos transformaciones wide : coalesce y repartition . Mediante coalesce podemos obtener un nuevo RDD con la cantidad de particiones reducidas: rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 3 ) rdd . getNumPartitions () # 3 rdd1p = rdd . coalesce ( 1 ) rdd1p . getNumPartitions () # 2 En cambio, mediante repartition podemos obtener un nuevo RDD con la cantidad exacta de particiones deseadas (al reducir las particiones, repartition realiza un shuffle para redistribuir los datos, por lo tanto, si queremos reducir la cantidad de particiones, es m\u00e1s eficiente utilizar coalesce ): rdd = sc . parallelize ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 5 ], 3 ) rdd . getNumPartitions () # 3 rdd2p = rdd . repartition ( 2 ) rdd2p . getNumPartitions () # 2","title":"Modificando las particiones"},{"location":"apuntes/spark01rdd.html#acumuladores-y-broadcast","text":"import sys from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"app1\" ) . getOrCreate () # Broadcast broadcastVar = spark . sparkContext . broadcast ([ 1 , 2 , 3 ]) print ( \"\" ) print ( broadcastVar . value ) print ( \"\" ) # Accumulador accum = spark . sparkContext . accumulator ( 0 ) sumatorioError = 0 def myFunc ( x ): global sumatorioError accum . add ( x ) sumatorioError += x rdd = spark . sparkContext . parallelize ([ 1 , 2 , 3 , 4 , 5 ]) rdd . foreach ( myFunc ) print ( \"\" ) print ( accum ) print ( sumatorioError ) print ( \"\" )","title":"Acumuladores y Broadcast"},{"location":"apuntes/spark01rdd.html#spark-ui","text":"Si accedemos a la direcci\u00f3n http://127.0.0.1:4040/ , veremos un interfaz gr\u00e1fico donde podemos monitorizar y analizar el c\u00f3digo Spark ejecutado. La barra superior muestra un men\u00fa con las opciones para visualizar las jobs , stages , almacenamiento, el entorno y sus variables de configuraci\u00f3n, y finalmente los ejecutores: Spark Shell UI Por ejemplo, si ejecutamos el ejemplo de groupByKey , obtenemos el siguiente DAG: Ejemplo de DAG Si pulsamos por ejemplo sobre la fase de groupBy obtendremos sus estad\u00edsticas de ejecuci\u00f3n: Estad\u00edsticas de una fase","title":"Spark UI"},{"location":"apuntes/spark01rdd.html#actividades","text":"En las siguientes actividades vamos a familiarizarnos con el uso de Spark con RDD y las diferentes acciones y transformaciones para trabajar con los datos. A partir de la lista siguiente ['Alicante','Elche','Valencia','Madrid','Barcelona','Bilbao','Sevilla'] : Almacena s\u00f3lo las ciudades que tengan la letra e en su nombre y mu\u00e9stralas. Muestra las ciudades que tienen la letra e y el n\u00famero de veces que aparece en cada nombre. Por ejemplo ('Elche', 2) . Averigua las ciudades que solo tengan una \u00fanica e . Nos han enviado una nueva lista pero no han separado bien las ciudades. Reorganiza la lista y colocalas correctamente, y cuenta las apariciones de la letra e de cada ciudad. ciudades_mal = [['Alicante.Elche','Valencia','Madrid.Barcelona','Bilbao.Sevilla'],['Murcia','San Sebasti\u00e1n','Melilla.Aspe']] (opcional) A partir del fichero de El Quijote : Crear un RDD a partir del fichero y crea una lista con todas las palabras del documento. \u00bfCuantas veces aparece la palabra Dulcinea (independientemente de si est\u00e1 en may\u00fasculas o min\u00fasculas)? \u00bfY Rocinante ? (86 y 120 ocurrencias respectivamente) Devuelve una lista ordenada seg\u00fan el n\u00famero de veces que sale cada palabra de m\u00e1s a menos (las primeras ocurrencias deben ser [('que', 10731), ('de', 9035), ('y', 8668), ('la', 5014), ... ). Almacena el resultado en HDFS en /user/iabd/spark/wcQuijote . Dada una cadena que contiene una lista de nombres Juan, Jimena, Luis, Cristian, Laura, Lorena, Cristina, Jacobo, Jorge , una vez transformada la cadena en una lista y luego en un RDD: Agr\u00fapalos seg\u00fan su inicial, de manera que tengamos tuplas formadas por la letra inicial y todos los nombres que comienzan por dicha letra: [( 'J' , [ 'Juan' , 'Jimena' , 'Jacobo' , 'Jorge' ]), ( 'L' , [ 'Luis' , 'Laura' , 'Lorena' ]), ( 'C' , [ 'Cristian' , 'Cristina' ])] De la lista original, obt\u00e9n una muestra de 5 elementos sin repetir valores. Devuelve una muestra de datos de aproximadamente la mitad de registros que la lista original con datos que pudieran llegar a repetirse. A partir de las siguientes listas: Ingl\u00e9s: hello, table, angel, cat, dog, animal, chocolate, dark, doctor, hospital, computer Espa\u00f1ol: hola, mesa, angel, gato, perro, animal, chocolate, oscuro, doctor, hospital, ordenador Una vez creado un RDD con tuplas de palabras y su traducci\u00f3n (puedes usar zip para unir dos listas): [( 'hello' , 'hola' ), ( 'table' , 'mesa' ), ( 'angel' , 'angel' ), ( 'cat' , 'gato' ) ... Averigua: Palabras que se escriben igual en ingl\u00e9s y en espa\u00f1ol Palabras que en espa\u00f1ol son distintas que en ingl\u00e9s Obt\u00e9n una \u00fanica lista con las palabras en ambos idiomas que son distintas entre ellas ( ['hello', 'hola', 'table', ... ) Haz dos grupos con todas las palabras, uno con las que empiezan por vocal y otro con las que empiecen por consonante. (opcional) Dada una lista de elementos desordenados y algunos repetidos, devolver una muestra de 5 elementos, que est\u00e9n en la lista, sin repetir y ordenados descendentemente. lista = 4 , 6 , 34 , 7 , 9 , 2 , 3 , 4 , 4 , 21 , 4 , 6 , 8 , 9 , 7 , 8 , 5 , 4 , 3 , 22 , 34 , 56 , 98 Selecciona el elemento mayor de la lista resultante. Muestra los dos elementos menores. En una red social sobre cine, tenemos un fichero ratings.txt compuesta por el c\u00f3digo de la pel\u00edcula, el c\u00f3digo del usuario, la calificaci\u00f3n asignada y el timestamp de la votaci\u00f3n con el siguiente formato: 1::1193::5::978300760 1::661::3::978302109 1::914::3::978301968 Se pide crear dos script en Python , as\u00ed como los comandos necesarios para ejecutarlos (mediante spark-submit ) para: Obtener para cada pel\u00edcula, la nota media de todas sus votaciones. Pel\u00edculas cuya nota media sea superior a 3.","title":"Actividades"},{"location":"apuntes/spark01rdd.html#proyectos","text":"A continuaci\u00f3n planteamos dos proyectos para realizar en clase: Tenemos las calificaciones de las asignaturas de matem\u00e1ticas, ingl\u00e9s y f\u00edsica de los alumnos del instituto en 3 documentos de texto. A partir de estos ficheros: Crea 3 RDD de pares, uno para cada asignatura, con los alumnos y sus notas Crea un solo RDD con todas las notas \u00bfCu\u00e1l es la nota m\u00e1s baja que ha tenido cada alumno? \u00bfCu\u00e1l es la nota media de cada alumno? \u00bfCuantos estudiantes suspende cada asignatura? [('Mates', 7), ('F\u00edsica', 8), ('Ingl\u00e9s', 7)] \u00bfEn qu\u00e9 asignatura suspende m\u00e1s gente? Total de notables o sobresalientes por alumno, es decir, cantidad de notas superiores o igual a 7. \u00bfQu\u00e9 alumno no se ha presentado a ingl\u00e9s? \u00bfA cu\u00e1ntas asignaturas se ha presentado cada alumno? Obten un RDD con cada alumno con sus notas Dada la carpeta weblog , con un conjunto de ficheros de log de una web con el el formato Common Log Format : <ipUsuario> - <idUsuario> - [<fecha>] \"<peticionHTTP>\" <codigoHTTP> <bytesTransmitidos> <dominio> <browser> Un fragmento de un fichero ser\u00eda similar a: 116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] \"GET /hive-00031.html HTTP/1.0\" 200 1388 \"http://www.cursoDeFormacionBigData.com\" \"Opera 3.4\" 116.180.70.237 - 128 [15/Sep/2013:23:59:53 +0100] \"GET /theme.css HTTP/1.0\" 200 5531 \"http://www.cursoDeFormacionBigData.com\" \"Opera 3.4\" Carga los logs de la carpeta weblogs Crea un RDD de pares a partir de estos ficheros con el usuario como clave y un valor 1 [('128', 1), ('128', 1), ('94', 1),... ) Crea un nuevo RDD con la suma de los valores para cada identificador de usuario [('54126', 32), ('54', 498), ('21443', 35), ... ) Muestra los 10 usuarios que m\u00e1s peticiones han realizado. [('193', 667), ('13', 636), ('24', 620),... Crea un nuevo RDD de pares donde la clave sea el c\u00f3digo del usuario y el valor una lista de IP desde donde se ha conectado el usuario. En cvs de cuentas, tenemos informaci\u00f3n de los clientes, podr\u00edas a\u00f1adir el nombre y apellido del usuario junto a las apariciones de ese usuario? por ejemplo: Usuario1,(Nombre apellido,30) Dados los ficheros de log de la carpeta weblogs, queremos contar cuantas veces se han enviado datos en formato png,jpg,html,css y txt. Usa acumuladores para recorrer los ficheros y calcular cuantos hay de cada formato en total entre todos los logs","title":"Proyectos"},{"location":"apuntes/spark01rdd.html#referencias","text":"Documentaci\u00f3n oficial de Apache Spark RDD Programming Guide Learning Apache Spark, 2nd Edition https://github.com/vivek-bombatkar/MyLearningNotes/tree/master/spark","title":"Referencias"},{"location":"apuntes/spark02dataframe.html","text":"Spark DataFrames \u00b6 En la sesi\u00f3n anterior hemos introducido Spark y el uso de RDD para interactuar con los datos. Tal como comentamos, los RDD permiten trabajar a bajo nivel, siendo m\u00e1s c\u00f3modo y eficiente hacer uso de DataFrames . DataFrame \u00b6 https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html Un DataFrame es una estructura equivalente a una tabla de base de datos relacional, con un motor bien optimizado para el trabajo en un cl\u00faster. El trabajo con DataFrames es m\u00e1s sencillo y eficiente que el procesamiento con RDD, por eso su uso es predominante en los nuevos desarrollos con Spark . A continuaci\u00f3n veremos como podemos obtener y persistir DataFrames desde diferentes fuentes y formatos de datos El caso m\u00e1s b\u00e1sico es crear un DataFrame a partir de una SparkSession pas\u00e1ndole un RDD: from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () # SparkSession de forma programativa # Creamos un RDD datos = [( \"Aitor\" , 182 ), ( \"Pedro\" , 178 ), ( \"Marina\" , 161 )] rdd = spark . sparkContext . parallelize ( datos ) # Creamos un DF y mostramos su esquema dfRDD = rdd . toDF () dfRDD . printSchema () Y obtenemos: root |-- _1: string (nullable = true) |-- _2: long (nullable = true) Podemos ver como los nombres de las columnas son _1 y _2 . Para indicarle el nombre podemos pasarle una lista con los nombre a la hora de crear de DataFrame: columnas = [ \"nombre\" , \"altura\" ] dfRDD = rdd . toDF ( columnas ) dfRDD . printSchema () Y ahora obtenemos: root |-- nombre: string (nullable = true) |-- altura: long (nullable = true) Tambi\u00e9n podemos crear un DataFrame directamente desde una SparkSession sin crear un RDD previamente: # Tambi\u00e9n podemos crear un DF desde SparkSession dfDesdeDatos = spark . createDataFrame ( datos ) . toDF ( * columnas ) dfDesdeDatos . printSchema () Cargando diferentes formatos \u00b6 Podemos cargar los datos mediante el API de DataFrameReader directamente en un Dataframe mediante diferentes m\u00e9todos dependiendo del formato (admite tanto el nombre de un recurso como una ruta de una carpeta): CSV TXT JSON dfCSV = spark . read . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"delimiter\" , \";\" ) . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"header\" , \"true\" ) . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"header\" , True ) . option ( \"inferSchema\" , True ) . csv ( \"datos.csv\" ) dfCSV = spark . read . format ( \"csv\" ) . load ( \"datos.csv\" ) dfCSV = spark . read . load ( format = \"csv\" , header = \"true\" , inferSchema = \"true\" ) . csv ( \"datos.csv\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfTXT = spark . read . text ( \"datos.txt\" ) # cada fichero se lee entero como un registro dfTXT = spark . read . option ( \"wholetext\" , true ) . text ( \"datos/\" ) dfTXT = spark . read . format ( \"txt\" ) . load ( \"datos.txt\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfJSON = spark . read . json ( \"datos.json\" ) dfJSON = spark . read . format ( \"json\" ) . load ( \"datos.json\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial DataFrames desde JSON Spark espera que cada documento JSON ocupe una \u00fanica l\u00ednea. Una vez cargados los datos, podemos mostrarlos mediante el m\u00e9todo show(cantidad) : dfCiudades = spark . read . json ( \"zips.json\" ) dfCiudades . show ( 5 ) Si lo que queremos es persistir los datos, en vez de read , utilizaremos write : CSV TXT JSON dfCSV . write . csv ( \"datos.csv\" ) dfCSV . write . format ( \"csv\" ) . load ( \"datos.csv\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfTXT . write . text ( \"datos.txt\" ) dfTXT . write . option ( \"lineSep\" , \";\" ) . text ( \"datos.txt\" ) dfTXT . write . format ( \"txt\" ) . load ( \"datos.txt\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfJSON . write . json ( \"datos.json\" ) dfJSON . write . format ( \"json\" ) . load ( \"datos.json\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial Una vez vista la sintaxis, vamos a ver un ejemplo completo de lectura de un archivo CSV (el archivo pdi_sales.csv que hemos utilizado durante todo el curso) que est\u00e1 almacenado en HDFS y que tras leerlo, lo guardamos como JSON de nuevo en HDFS: from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"s8a-dataframe-csv\" ) . getOrCreate () # Lectura de CSV con el ; como separador de columnas y con encabezado df = spark . read . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"hdfs://iabd-virtualbox:9000/user/iabd/pdi_sales.csv\" ) # df.printSchema() df . write . json ( \"hdfs://iabd-virtualbox:9000/user/iabd/pdi_sales_json\" ) Es conveniente destacar que para acceder a HDFS, \u00fanicamente hemos de indicar la URL del recurso con el prefijo hdfs:// m\u00e1s el host del namenode . Comprimiendo los datos \u00b6 Para configurar el algoritmo de compresi\u00f3n, si los datos est\u00e1 en Parquet o Avro, a nivel de la sesi\u00f3n de Spark, podemos realizar su configuraci\u00f3n: spark . setConf ( \"spark.sql.parquet.compression.codec\" , \"snappy\" ) spark . setConf ( \"spark.sql.parquet.compression.codec\" , \"none\" ) spark . setConf ( \"spark.sql.avro.compression.codec\" , \"snappy\" ) Si s\u00f3lo queremos hacerlo para una operaci\u00f3n en particular, para cada lectura/escritura le a\u00f1adimos .option(\"compression\", \"algoritmo\") . Por ejemplo: dfVentas = spark . read . option ( \"compression\" , \"snappy\" ) . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"pdi_sales.csv\" ) dfClientes = spark . read . option ( \"compression\" , \"snappy\" ) . parquet ( \"clientes.parquet\" ) dfVentas . write . option ( \"compression\" , \"snappy\" ) . format ( \"avro\" ) . save ( \"ventas.avro\" ) Datos y Esquemas \u00b6 Hemos visto que al crear un DataFrame desde un archivo externo, podemos inferir el esquema. Si queremos crear un DataFrame desde un esquema propio utilizaremos los tipos StructType , StructField , as\u00ed como StringType e IntegerType . Para ello, primero hemos de importarlos (como puedes observar, estas clases pertenecen a las librer\u00edas SQL de PySpark): from pyspark.sql import SparkSession from pyspark.sql.types import StructType , StructField , StringType , IntegerType Tipos Adem\u00e1s de cadenas y enteros, flotantes ( FloatType ) o dobles ( DoubleType ), tenemos tipos booleanos ( BooleanType ), de fecha (DateType y TimestampType), as\u00ed como tipos complejos como ArrayType , MapType y StructType . Para m\u00e1s informaci\u00f3n, consultar la documentaci\u00f3n oficial . Supongamos que queremos almacenar ciertos datos de clientes, como son su nombre y apellidos, ciudad y sueldo: clientes = [ ( \"Aitor\" , \"Medrano\" , \"Elche\" , 3000 ), ( \"Pedro\" , \"Casas\" , \"Elche\" , 4000 ), ( \"Laura\" , \"Garc\u00eda\" , \"Elche\" , 5000 ), ( \"Miguel\" , \"Ruiz\" , \"Torrellano\" , 6000 ), ( \"Isabel\" , \"Guill\u00e9n\" , \"Alicante\" , 7000 ) ] Para esta estructura, definiremos un esquema con los campos, indicando para cada campo su nombre, tipo y si admite valores nulos: esquema = StructType ([ StrucField ( \"nombre\" , StringType (), False ), StrucField ( \"apellidos\" , StringType (), False ), StrucField ( \"ciudad\" , StringType (), True ), StrucField ( \"sueldo\" , IntegerType (), False ) ]) A continuaci\u00f3n ya podemos crear un DataFrame con datos propios que cumplen un esquema mediante: df = spark . createDataFrame ( data = clientes , schema = esquema ) df . printSchema () # root # |-- nombre: string (nullable = false) # |-- apellidos: string (nullable = false) # |-- ciudad: string (nullable = true) # |-- sueldo: integer (nullable = false) df . show ( truncate = False ) # +------+---------+----------+------+ # |nombre|apellidos|ciudad |sueldo| # +------+---------+----------+------+ # |Aitor |Medrano |Elche |3000 | # |Pedro |Casas |Elche |4000 | # |Laura |Garc\u00eda |Elche |5000 | # |Miguel|Ruiz |Torrellano|6000 | # |Isabel|Guill\u00e9n |Alicante |7000 | # +------+---------+----------+------+ Respecto al esquema, tenemos diferentes propiedades como columns , dtypes y schema con las que obtener su informaci\u00f3n: df . columns # ['nombre', 'apellidos', 'ciudad', 'sueldo'] df . dtypes # [('nombre', 'string'), # ('apellidos', 'string'), # ('ciudad', 'string'), # ('sueldo', 'int')] df . schema # StructType(List(StructField(nombre,StringType,false),StructField(apellidos,StringType,false),StructField(ciudad,StringType,true),StructField(sueldo,IntegerType,false))) Para mostrar los datos, ya hemos visto que podemos utilizar el m\u00e9todo show indicando o no la cantidad de registros a recuperar, as\u00ed como si queremos que los datos se trunquen o no. Tambi\u00e9n podemos mostrar los datos en vertical u obtener un sumario de los datos: df . show ( 2 ) # +------+---------+------+------+ # |nombre|apellidos|ciudad|sueldo| # +------+---------+------+------+ # | Aitor| Medrano| Elche| 3000| # | Pedro| Casas| Elche| 4000| # +------+---------+------+------+ # only showing top 2 rows df . show ( truncate = False ) # +------+---------+----------+------+ # |nombre|apellidos|ciudad |sueldo| # +------+---------+----------+------+ # |Aitor |Medrano |Elche |3000 | # |Pedro |Casas |Elche |4000 | # |Laura |Garc\u00eda |Elche |5000 | # |Miguel|Ruiz |Torrellano|6000 | # |Isabel|Guill\u00e9n |Alicante |7000 | # +------+---------+----------+------+ df . show ( 3 , vertical = True ) # -RECORD 0------------ # nombre | Aitor # apellidos | Medrano # ciudad | Elche # sueldo | 3000 # -RECORD 1------------ # nombre | Pedro # apellidos | Casas # ciudad | Elche # sueldo | 4000 # -RECORD 2------------ # nombre | Laura # apellidos | Garc\u00eda # ciudad | Elche # sueldo | 5000 # only showing top 3 rows df . describe () . show () # +-------+------+---------+----------+------------------+ # |summary|nombre|apellidos| ciudad| sueldo| # +-------+------+---------+----------+------------------+ # | count| 5| 5| 5| 5| # | mean| null| null| null| 5000.0| # | stddev| null| null| null|1581.1388300841897| # | min| Aitor| Casas| Alicante| 3000| # | max| Pedro| Ruiz|Torrellano| 7000| # +-------+------+---------+----------+------------------+ Por \u00faltimo, como un DataFrame por debajo es un RDD, podemos usar collect y take conforme necesitemos y recuperar objetos de tipo Row : df . collect () # [Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000), # Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000), # Row(nombre='Laura', apellidos='Garc\u00eda', ciudad='Elche', sueldo=5000), # Row(nombre='Miguel', apellidos='Ruiz', ciudad='Torrellano', sueldo=6000), # Row(nombre='Isabel', apellidos='Guill\u00e9n', ciudad='Alicante', sueldo=7000)] df . take ( 2 ) # [Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000), # Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000)] DataFrame API \u00b6 Una vez tenemos un DataFrame podemos trabajar con los datos con los siguientes m\u00e9todos: Preparaci\u00f3n Para los siguientes apartados, vamos a trabajar sobre el siguiente DataFrame : from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"s8a-dataframes-api\" ) . getOrCreate () # Lectura de CSV con el ; como separador de columnas y con encabezado df = spark . read . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"pdi_sales_small.csv\" ) df . printSchema () El esquema generado es: root |-- ProductID: string (nullable = true) |-- Date: string (nullable = true) |-- Zip: string (nullable = true) |-- Units: string (nullable = true) |-- Revenue: string (nullable = true) |-- Country: string (nullable = true) select \u00b6 La operaci\u00f3n select permite indicar las columnas a recuperar pas\u00e1ndolas como par\u00e1metros: Consulta de columnas Resultado df . select ( \"ProductID\" , \"Revenue\" ) . show ( 3 ) +---------+-------+ |ProductID|Revenue| +---------+-------+ | 725| 115.5| | 787| 314.9| | 788| 314.9| +---------+-------+ only showing top 3 rows Tambi\u00e9n podemos realizar c\u00e1lculos (referenciando a los campos con nombreDataframe.nombreColumna ) sobre las columnas y crear un alias (operaci\u00f3n asociada a un campo): C\u00e1lculos y creaci\u00f3n de alias Resultado df . select ( df . ProductID ,( df . Revenue + 10 ) . alias ( \"VentasMas10\" )) . show ( 3 ) +---------+-----------+ |ProductID|VentasMas10| +---------+-----------+ | 725| 125.5| | 787| 324.9| | 788| 324.9| +---------+-----------+ only showing top 3 rows FIXME: explicar col y expr sort / orderBy \u00b6 Una vez recuperados los datos deseados, podemos ordenarlos mediante sort u orderBy (son operaciones totalmente equivalentes): df . select ( \"ProductID\" , \"Revenue\" ) . sort ( \"Revenue\" ) . show ( 5 ) df . sort ( \"Revenue\" ) . show ( 5 ) df . sort ( \"Revenue\" , ascending = True ) . show ( 5 ) df . sort ( df . Revenue . asc ()) . show ( 5 ) df . sort ( df . Revenue . desc ()) . show ( 5 ) df . sort ([ \"Revenue\" , \"Units\" ], ascending = [ 0 , 1 ]) . show ( 5 ) Por ejemplo, con la \u00faltima operaci\u00f3n obtendr\u00edamos: +---------+----------+---------------+-----+-------+-------+ |ProductID| Date| Zip|Units|Revenue|Country| +---------+----------+---------------+-----+-------+-------+ | 819| 7/26/2002|75063 CEDEX 02 | 1| 997.5|France | | 819| 7/31/2002|06099 CEDEX 1 | 1| 997.5|France | | 819| 8/29/2002|53804 | 1| 997.5|Germany| | 819| 7/29/2002|75355 SP 07 | 1| 997.5|France | | 819|10/15/2002|7100 | 1| 997.5|Mexico | +---------+----------+---------------+-----+-------+-------+ only showing top 5 rows FIXME: Working with Structured Operations / beginning apache spark 3 groupBy \u00b6 filter / where \u00b6 DataFrames y Pandas \u00b6 PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side. df . toPandas () # a b c d e # 0 1 2.0 string1 2000-01-01 2000-01-01 12:00:00 # 1 2 3.0 string2 2000-02-01 2000-01-02 12:00:00 # 2 3 4.0 string3 2000-03-01 2000-01-03 12:00:00 https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html Referencias \u00b6 Documentaci\u00f3n oficial sobre Spark SQL, DataFrames and Datasets Guide Spark by Examples The Most Complete Guide to pySpark DataFrames https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html","title":"Spark DataFrames"},{"location":"apuntes/spark02dataframe.html#spark-dataframes","text":"En la sesi\u00f3n anterior hemos introducido Spark y el uso de RDD para interactuar con los datos. Tal como comentamos, los RDD permiten trabajar a bajo nivel, siendo m\u00e1s c\u00f3modo y eficiente hacer uso de DataFrames .","title":"Spark DataFrames"},{"location":"apuntes/spark02dataframe.html#dataframe","text":"https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html Un DataFrame es una estructura equivalente a una tabla de base de datos relacional, con un motor bien optimizado para el trabajo en un cl\u00faster. El trabajo con DataFrames es m\u00e1s sencillo y eficiente que el procesamiento con RDD, por eso su uso es predominante en los nuevos desarrollos con Spark . A continuaci\u00f3n veremos como podemos obtener y persistir DataFrames desde diferentes fuentes y formatos de datos El caso m\u00e1s b\u00e1sico es crear un DataFrame a partir de una SparkSession pas\u00e1ndole un RDD: from pyspark.sql import SparkSession spark = SparkSession . builder . getOrCreate () # SparkSession de forma programativa # Creamos un RDD datos = [( \"Aitor\" , 182 ), ( \"Pedro\" , 178 ), ( \"Marina\" , 161 )] rdd = spark . sparkContext . parallelize ( datos ) # Creamos un DF y mostramos su esquema dfRDD = rdd . toDF () dfRDD . printSchema () Y obtenemos: root |-- _1: string (nullable = true) |-- _2: long (nullable = true) Podemos ver como los nombres de las columnas son _1 y _2 . Para indicarle el nombre podemos pasarle una lista con los nombre a la hora de crear de DataFrame: columnas = [ \"nombre\" , \"altura\" ] dfRDD = rdd . toDF ( columnas ) dfRDD . printSchema () Y ahora obtenemos: root |-- nombre: string (nullable = true) |-- altura: long (nullable = true) Tambi\u00e9n podemos crear un DataFrame directamente desde una SparkSession sin crear un RDD previamente: # Tambi\u00e9n podemos crear un DF desde SparkSession dfDesdeDatos = spark . createDataFrame ( datos ) . toDF ( * columnas ) dfDesdeDatos . printSchema ()","title":"DataFrame"},{"location":"apuntes/spark02dataframe.html#cargando-diferentes-formatos","text":"Podemos cargar los datos mediante el API de DataFrameReader directamente en un Dataframe mediante diferentes m\u00e9todos dependiendo del formato (admite tanto el nombre de un recurso como una ruta de una carpeta): CSV TXT JSON dfCSV = spark . read . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"delimiter\" , \";\" ) . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"header\" , \"true\" ) . csv ( \"datos.csv\" ) dfCSV = spark . read . option ( \"header\" , True ) . option ( \"inferSchema\" , True ) . csv ( \"datos.csv\" ) dfCSV = spark . read . format ( \"csv\" ) . load ( \"datos.csv\" ) dfCSV = spark . read . load ( format = \"csv\" , header = \"true\" , inferSchema = \"true\" ) . csv ( \"datos.csv\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfTXT = spark . read . text ( \"datos.txt\" ) # cada fichero se lee entero como un registro dfTXT = spark . read . option ( \"wholetext\" , true ) . text ( \"datos/\" ) dfTXT = spark . read . format ( \"txt\" ) . load ( \"datos.txt\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfJSON = spark . read . json ( \"datos.json\" ) dfJSON = spark . read . format ( \"json\" ) . load ( \"datos.json\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial DataFrames desde JSON Spark espera que cada documento JSON ocupe una \u00fanica l\u00ednea. Una vez cargados los datos, podemos mostrarlos mediante el m\u00e9todo show(cantidad) : dfCiudades = spark . read . json ( \"zips.json\" ) dfCiudades . show ( 5 ) Si lo que queremos es persistir los datos, en vez de read , utilizaremos write : CSV TXT JSON dfCSV . write . csv ( \"datos.csv\" ) dfCSV . write . format ( \"csv\" ) . load ( \"datos.csv\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfTXT . write . text ( \"datos.txt\" ) dfTXT . write . option ( \"lineSep\" , \";\" ) . text ( \"datos.txt\" ) dfTXT . write . format ( \"txt\" ) . load ( \"datos.txt\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial dfJSON . write . json ( \"datos.json\" ) dfJSON . write . format ( \"json\" ) . load ( \"datos.json\" ) M\u00e1s informaci\u00f3n en la documentaci\u00f3n oficial Una vez vista la sintaxis, vamos a ver un ejemplo completo de lectura de un archivo CSV (el archivo pdi_sales.csv que hemos utilizado durante todo el curso) que est\u00e1 almacenado en HDFS y que tras leerlo, lo guardamos como JSON de nuevo en HDFS: from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"s8a-dataframe-csv\" ) . getOrCreate () # Lectura de CSV con el ; como separador de columnas y con encabezado df = spark . read . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"hdfs://iabd-virtualbox:9000/user/iabd/pdi_sales.csv\" ) # df.printSchema() df . write . json ( \"hdfs://iabd-virtualbox:9000/user/iabd/pdi_sales_json\" ) Es conveniente destacar que para acceder a HDFS, \u00fanicamente hemos de indicar la URL del recurso con el prefijo hdfs:// m\u00e1s el host del namenode .","title":"Cargando diferentes formatos"},{"location":"apuntes/spark02dataframe.html#comprimiendo-los-datos","text":"Para configurar el algoritmo de compresi\u00f3n, si los datos est\u00e1 en Parquet o Avro, a nivel de la sesi\u00f3n de Spark, podemos realizar su configuraci\u00f3n: spark . setConf ( \"spark.sql.parquet.compression.codec\" , \"snappy\" ) spark . setConf ( \"spark.sql.parquet.compression.codec\" , \"none\" ) spark . setConf ( \"spark.sql.avro.compression.codec\" , \"snappy\" ) Si s\u00f3lo queremos hacerlo para una operaci\u00f3n en particular, para cada lectura/escritura le a\u00f1adimos .option(\"compression\", \"algoritmo\") . Por ejemplo: dfVentas = spark . read . option ( \"compression\" , \"snappy\" ) . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"pdi_sales.csv\" ) dfClientes = spark . read . option ( \"compression\" , \"snappy\" ) . parquet ( \"clientes.parquet\" ) dfVentas . write . option ( \"compression\" , \"snappy\" ) . format ( \"avro\" ) . save ( \"ventas.avro\" )","title":"Comprimiendo los datos"},{"location":"apuntes/spark02dataframe.html#datos-y-esquemas","text":"Hemos visto que al crear un DataFrame desde un archivo externo, podemos inferir el esquema. Si queremos crear un DataFrame desde un esquema propio utilizaremos los tipos StructType , StructField , as\u00ed como StringType e IntegerType . Para ello, primero hemos de importarlos (como puedes observar, estas clases pertenecen a las librer\u00edas SQL de PySpark): from pyspark.sql import SparkSession from pyspark.sql.types import StructType , StructField , StringType , IntegerType Tipos Adem\u00e1s de cadenas y enteros, flotantes ( FloatType ) o dobles ( DoubleType ), tenemos tipos booleanos ( BooleanType ), de fecha (DateType y TimestampType), as\u00ed como tipos complejos como ArrayType , MapType y StructType . Para m\u00e1s informaci\u00f3n, consultar la documentaci\u00f3n oficial . Supongamos que queremos almacenar ciertos datos de clientes, como son su nombre y apellidos, ciudad y sueldo: clientes = [ ( \"Aitor\" , \"Medrano\" , \"Elche\" , 3000 ), ( \"Pedro\" , \"Casas\" , \"Elche\" , 4000 ), ( \"Laura\" , \"Garc\u00eda\" , \"Elche\" , 5000 ), ( \"Miguel\" , \"Ruiz\" , \"Torrellano\" , 6000 ), ( \"Isabel\" , \"Guill\u00e9n\" , \"Alicante\" , 7000 ) ] Para esta estructura, definiremos un esquema con los campos, indicando para cada campo su nombre, tipo y si admite valores nulos: esquema = StructType ([ StrucField ( \"nombre\" , StringType (), False ), StrucField ( \"apellidos\" , StringType (), False ), StrucField ( \"ciudad\" , StringType (), True ), StrucField ( \"sueldo\" , IntegerType (), False ) ]) A continuaci\u00f3n ya podemos crear un DataFrame con datos propios que cumplen un esquema mediante: df = spark . createDataFrame ( data = clientes , schema = esquema ) df . printSchema () # root # |-- nombre: string (nullable = false) # |-- apellidos: string (nullable = false) # |-- ciudad: string (nullable = true) # |-- sueldo: integer (nullable = false) df . show ( truncate = False ) # +------+---------+----------+------+ # |nombre|apellidos|ciudad |sueldo| # +------+---------+----------+------+ # |Aitor |Medrano |Elche |3000 | # |Pedro |Casas |Elche |4000 | # |Laura |Garc\u00eda |Elche |5000 | # |Miguel|Ruiz |Torrellano|6000 | # |Isabel|Guill\u00e9n |Alicante |7000 | # +------+---------+----------+------+ Respecto al esquema, tenemos diferentes propiedades como columns , dtypes y schema con las que obtener su informaci\u00f3n: df . columns # ['nombre', 'apellidos', 'ciudad', 'sueldo'] df . dtypes # [('nombre', 'string'), # ('apellidos', 'string'), # ('ciudad', 'string'), # ('sueldo', 'int')] df . schema # StructType(List(StructField(nombre,StringType,false),StructField(apellidos,StringType,false),StructField(ciudad,StringType,true),StructField(sueldo,IntegerType,false))) Para mostrar los datos, ya hemos visto que podemos utilizar el m\u00e9todo show indicando o no la cantidad de registros a recuperar, as\u00ed como si queremos que los datos se trunquen o no. Tambi\u00e9n podemos mostrar los datos en vertical u obtener un sumario de los datos: df . show ( 2 ) # +------+---------+------+------+ # |nombre|apellidos|ciudad|sueldo| # +------+---------+------+------+ # | Aitor| Medrano| Elche| 3000| # | Pedro| Casas| Elche| 4000| # +------+---------+------+------+ # only showing top 2 rows df . show ( truncate = False ) # +------+---------+----------+------+ # |nombre|apellidos|ciudad |sueldo| # +------+---------+----------+------+ # |Aitor |Medrano |Elche |3000 | # |Pedro |Casas |Elche |4000 | # |Laura |Garc\u00eda |Elche |5000 | # |Miguel|Ruiz |Torrellano|6000 | # |Isabel|Guill\u00e9n |Alicante |7000 | # +------+---------+----------+------+ df . show ( 3 , vertical = True ) # -RECORD 0------------ # nombre | Aitor # apellidos | Medrano # ciudad | Elche # sueldo | 3000 # -RECORD 1------------ # nombre | Pedro # apellidos | Casas # ciudad | Elche # sueldo | 4000 # -RECORD 2------------ # nombre | Laura # apellidos | Garc\u00eda # ciudad | Elche # sueldo | 5000 # only showing top 3 rows df . describe () . show () # +-------+------+---------+----------+------------------+ # |summary|nombre|apellidos| ciudad| sueldo| # +-------+------+---------+----------+------------------+ # | count| 5| 5| 5| 5| # | mean| null| null| null| 5000.0| # | stddev| null| null| null|1581.1388300841897| # | min| Aitor| Casas| Alicante| 3000| # | max| Pedro| Ruiz|Torrellano| 7000| # +-------+------+---------+----------+------------------+ Por \u00faltimo, como un DataFrame por debajo es un RDD, podemos usar collect y take conforme necesitemos y recuperar objetos de tipo Row : df . collect () # [Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000), # Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000), # Row(nombre='Laura', apellidos='Garc\u00eda', ciudad='Elche', sueldo=5000), # Row(nombre='Miguel', apellidos='Ruiz', ciudad='Torrellano', sueldo=6000), # Row(nombre='Isabel', apellidos='Guill\u00e9n', ciudad='Alicante', sueldo=7000)] df . take ( 2 ) # [Row(nombre='Aitor', apellidos='Medrano', ciudad='Elche', sueldo=3000), # Row(nombre='Pedro', apellidos='Casas', ciudad='Elche', sueldo=4000)]","title":"Datos y Esquemas"},{"location":"apuntes/spark02dataframe.html#dataframe-api","text":"Una vez tenemos un DataFrame podemos trabajar con los datos con los siguientes m\u00e9todos: Preparaci\u00f3n Para los siguientes apartados, vamos a trabajar sobre el siguiente DataFrame : from pyspark.sql import SparkSession spark = SparkSession . builder . appName ( \"s8a-dataframes-api\" ) . getOrCreate () # Lectura de CSV con el ; como separador de columnas y con encabezado df = spark . read . option ( \"delimiter\" , \";\" ) . option ( \"header\" , \"true\" ) . csv ( \"pdi_sales_small.csv\" ) df . printSchema () El esquema generado es: root |-- ProductID: string (nullable = true) |-- Date: string (nullable = true) |-- Zip: string (nullable = true) |-- Units: string (nullable = true) |-- Revenue: string (nullable = true) |-- Country: string (nullable = true)","title":"DataFrame API"},{"location":"apuntes/spark02dataframe.html#select","text":"La operaci\u00f3n select permite indicar las columnas a recuperar pas\u00e1ndolas como par\u00e1metros: Consulta de columnas Resultado df . select ( \"ProductID\" , \"Revenue\" ) . show ( 3 ) +---------+-------+ |ProductID|Revenue| +---------+-------+ | 725| 115.5| | 787| 314.9| | 788| 314.9| +---------+-------+ only showing top 3 rows Tambi\u00e9n podemos realizar c\u00e1lculos (referenciando a los campos con nombreDataframe.nombreColumna ) sobre las columnas y crear un alias (operaci\u00f3n asociada a un campo): C\u00e1lculos y creaci\u00f3n de alias Resultado df . select ( df . ProductID ,( df . Revenue + 10 ) . alias ( \"VentasMas10\" )) . show ( 3 ) +---------+-----------+ |ProductID|VentasMas10| +---------+-----------+ | 725| 125.5| | 787| 324.9| | 788| 324.9| +---------+-----------+ only showing top 3 rows FIXME: explicar col y expr","title":"select"},{"location":"apuntes/spark02dataframe.html#sort-orderby","text":"Una vez recuperados los datos deseados, podemos ordenarlos mediante sort u orderBy (son operaciones totalmente equivalentes): df . select ( \"ProductID\" , \"Revenue\" ) . sort ( \"Revenue\" ) . show ( 5 ) df . sort ( \"Revenue\" ) . show ( 5 ) df . sort ( \"Revenue\" , ascending = True ) . show ( 5 ) df . sort ( df . Revenue . asc ()) . show ( 5 ) df . sort ( df . Revenue . desc ()) . show ( 5 ) df . sort ([ \"Revenue\" , \"Units\" ], ascending = [ 0 , 1 ]) . show ( 5 ) Por ejemplo, con la \u00faltima operaci\u00f3n obtendr\u00edamos: +---------+----------+---------------+-----+-------+-------+ |ProductID| Date| Zip|Units|Revenue|Country| +---------+----------+---------------+-----+-------+-------+ | 819| 7/26/2002|75063 CEDEX 02 | 1| 997.5|France | | 819| 7/31/2002|06099 CEDEX 1 | 1| 997.5|France | | 819| 8/29/2002|53804 | 1| 997.5|Germany| | 819| 7/29/2002|75355 SP 07 | 1| 997.5|France | | 819|10/15/2002|7100 | 1| 997.5|Mexico | +---------+----------+---------------+-----+-------+-------+ only showing top 5 rows FIXME: Working with Structured Operations / beginning apache spark 3","title":"sort / orderBy"},{"location":"apuntes/spark02dataframe.html#groupby","text":"","title":"groupBy"},{"location":"apuntes/spark02dataframe.html#filter-where","text":"","title":"filter / where"},{"location":"apuntes/spark02dataframe.html#dataframes-y-pandas","text":"PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side. df . toPandas () # a b c d e # 0 1 2.0 string1 2000-01-01 2000-01-01 12:00:00 # 1 2 3.0 string2 2000-02-01 2000-01-02 12:00:00 # 2 3 4.0 string3 2000-03-01 2000-01-03 12:00:00 https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html","title":"DataFrames y Pandas"},{"location":"apuntes/spark02dataframe.html#referencias","text":"Documentaci\u00f3n oficial sobre Spark SQL, DataFrames and Datasets Guide Spark by Examples The Most Complete Guide to pySpark DataFrames https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html","title":"Referencias"}]}