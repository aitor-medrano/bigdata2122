{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Inteligencia Artificial y Big Data \u00b6 Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales. Bloque Cloud Computing y Arquitecturas Big Data \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 8 Nov 1p + 2o 2.- Amazon Web Services Lunes 15 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 22 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 29 Nov 1p + 2o 5.- Datos en la nube Lunes 13 Dic 1p + 2o 6.- Arquitecturas Big Data Lunes 20 Dic 1p + 2o Bloque Ingesta de Datos \u00b6 Resultados de aprendizaje del m\u00f3dulo de Sistemas de Big Data \u00b6 Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen. Resultados de aprendizaje del m\u00f3dulo de Big Data Aplicado \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- ETL Lunes 10 Ene 1p + 2o 2.- ETL mediante SQL / Python Lunes 17 Ene 1p + 2o 3.- Herramientas ETL. Pentaho Lunes 24 Ene 1p + 2o 4.- ETL Distribuido. Nifi Lunes 31 Ene 1p + 2o 5.- Nifi II Mi\u00e9rcoles 2 Feb 2p + 4o Para poder continuar con el bloque de ingesta de datos, antes debemos preparar una infraestructura Hadoop Bloque Big Data Aplicado \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen. Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Hadoop Mi\u00e9rcoles 9 Feb 2p + 4o 2.- HDFS Mi\u00e9rcoles 16 Feb 2p + 4o 3.- Hive Mi\u00e9rcoles 16 Feb 2p + 4o 4.- Flume y Sqoop Mi\u00e9rcoles 16 Feb 2p + 4o 5.- Kafka Mi\u00e9rcoles 16 Feb 2p + 4o Bloque Anal\u00edtica de Datos \u00b6 Resultados de aprendizaje \u00b6 Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios Planificaci\u00f3n \u00b6 Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Spark Mi\u00e9rcoles 16 Mar 2p + 4o 2.- Spark RDD Mi\u00e9rcoles 16 Mar 2p + 4o 3.- Spark Avanzado Mi\u00e9rcoles 30 Mar 2p + 4o 4.- Spark SQL Mi\u00e9rcoles 30 Mar 2p + 4o 5.- Spark Streaming I Mi\u00e9rcoles 6 Abr 2p + 4o 6.- Spark Streaming II Mi\u00e9rcoles 6 Abr 2p + 4o","title":"Inicio"},{"location":"index.html#inteligencia-artificial-y-big-data","text":"Apuntes realizados para el curso de especialista de Inteligencia Artificial y Big Data impartido en el IES Severo Ochoa de Elche. El curriculum viene fijado por el Real Decreto 279/2021 . En este sitio web podr\u00e1s consultar los apuntes y ejercicios trabajados durante el curso. Despliega el men\u00fa de la izquierda para consultar los materiales.","title":"Inteligencia Artificial y Big Data"},{"location":"index.html#bloque-cloud-computing-y-arquitecturas-big-data","text":"","title":"Bloque Cloud Computing y Arquitecturas Big Data"},{"location":"index.html#resultados-de-aprendizaje","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Cloud Computing Lunes 8 Nov 1p + 2o 2.- Amazon Web Services Lunes 15 Nov 1p + 2o 3.- Computaci\u00f3n en la nube Lunes 22 Nov 1p + 2o 4.- Almacenamiento en la nube Lunes 29 Nov 1p + 2o 5.- Datos en la nube Lunes 13 Dic 1p + 2o 6.- Arquitecturas Big Data Lunes 20 Dic 1p + 2o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-ingesta-de-datos","text":"","title":"Bloque Ingesta de Datos"},{"location":"index.html#resultados-de-aprendizaje-del-modulo-de-sistemas-de-big-data","text":"Aplica t\u00e9cnicas de an\u00e1lisis de datos que integran, procesan y analizan la informaci\u00f3n, adaptando e implementando sistemas que las utilicen.","title":"Resultados de aprendizaje del m\u00f3dulo de Sistemas de Big Data"},{"location":"index.html#resultados-de-aprendizaje-del-modulo-de-big-data-aplicado","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios","title":"Resultados de aprendizaje del m\u00f3dulo de Big Data Aplicado"},{"location":"index.html#planificacion_1","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- ETL Lunes 10 Ene 1p + 2o 2.- ETL mediante SQL / Python Lunes 17 Ene 1p + 2o 3.- Herramientas ETL. Pentaho Lunes 24 Ene 1p + 2o 4.- ETL Distribuido. Nifi Lunes 31 Ene 1p + 2o 5.- Nifi II Mi\u00e9rcoles 2 Feb 2p + 4o Para poder continuar con el bloque de ingesta de datos, antes debemos preparar una infraestructura Hadoop","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-big-data-aplicado","text":"","title":"Bloque Big Data Aplicado"},{"location":"index.html#resultados-de-aprendizaje_1","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Genera mecanismos de integridad de los datos, comprobando su mantenimiento en los sistemas de ficheros distribuidos y valorando la sobrecarga que conlleva en el tratamiento de los datos. Realiza el seguimiento de la monitorizaci\u00f3n de un sistema, asegurando la fiabilidad y estabilidad de los servicios que se proveen.","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion_2","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Hadoop Mi\u00e9rcoles 9 Feb 2p + 4o 2.- HDFS Mi\u00e9rcoles 16 Feb 2p + 4o 3.- Hive Mi\u00e9rcoles 16 Feb 2p + 4o 4.- Flume y Sqoop Mi\u00e9rcoles 16 Feb 2p + 4o 5.- Kafka Mi\u00e9rcoles 16 Feb 2p + 4o","title":"Planificaci\u00f3n"},{"location":"index.html#bloque-analitica-de-datos","text":"","title":"Bloque Anal\u00edtica de Datos"},{"location":"index.html#resultados-de-aprendizaje_2","text":"Gestiona soluciones a problemas propuestos, utilizando sistemas de almacenamiento y herramientas asociadas al centro de datos. Gestiona sistemas de almacenamiento y el amplio ecosistema alrededor de ellos facilitando el procesamiento de grandes cantidades de datos sin fallos y de forma r\u00e1pida. Valida las t\u00e9cnicas de Big Data para transformar una gran cantidad de datos en informaci\u00f3n significativa, facilitando la toma de decisiones de negocios","title":"Resultados de aprendizaje"},{"location":"index.html#planificacion_3","text":"Sesi\u00f3n Fecha Duraci\u00f3n (h) 1.- Spark Mi\u00e9rcoles 16 Mar 2p + 4o 2.- Spark RDD Mi\u00e9rcoles 16 Mar 2p + 4o 3.- Spark Avanzado Mi\u00e9rcoles 30 Mar 2p + 4o 4.- Spark SQL Mi\u00e9rcoles 30 Mar 2p + 4o 5.- Spark Streaming I Mi\u00e9rcoles 6 Abr 2p + 4o 6.- Spark Streaming II Mi\u00e9rcoles 6 Abr 2p + 4o","title":"Planificaci\u00f3n"},{"location":"apuntes/arquitecturas01.html","text":"Arquitecturas Big Data \u00b6 Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. En esta sesi\u00f3n no vamos a entrar al detalle de ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas. Caracter\u00edsticas \u00b6 Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener. Tipos de arquitecturas \u00b6 Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming. Procesamiento Batch \u00b6 Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realzar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extraci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante. Procesamiento en Streaming \u00b6 Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo. Arquitectura Lambda \u00b6 Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter, y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Para ello, se compone de tres capas: Capa batch : se encarga de (a) gestionar los datos hist\u00f3ricos y (b) recalcular los resultados, por ejemplo, de los modelos de machine learning . De manera espec\u00edfica, la capa batch recibe los datos, los combina con el historico existente y recalcula los resultados iterando sobre todo el conjunto de datos combinado. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: https://www.paradigmadigital.com/techbiz/de-lambda-a-kappa-evolucion-de-las-arquitecturas-big-data/ El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa Arquitectura Kappa \u00b6 https://www.treelogic.com/es/Arquitectura_Kappa.html El t\u00e9rmino Arquitectura Kappa, representada por la letra , fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos \u201cd\u00e9biles\u201d de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Su propuesta consiste en eliminar la capa batch dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Como un proceso batch se puede entender como un stream acotado, podr\u00edamos decir que el procesamiento batch es un subconjunto del procesamiento en streaming. Esta evoluci\u00f3n consiste en una simplificaci\u00f3n de la Arquitectura Lambda, en la que se elimina la capa batch y todo el procesamiento se realiza en una sola capa denominada de tiempo real o Real-time Layer, dando soporte a procesamientos tanto batch como en tiempo real. Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream: las operaciones batch son un subconjunto de las operaciones de streaming, por lo que todo puede ser tratado como un stream. Los datos de partida no se modifican: los datos son almacenados sin ser transformados y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos. Casos de uso \u00b6 \u00bfQu\u00e9 arquitectura se adapta mejor a nuestro problema? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Por lo general, no existe una \u00fanica respuesta. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos para poder decidir es, \u00bfel an\u00e1lisis y el procesamiento que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la Arquitectura Kappa. Como ejemplo real de esta arquitectura podr\u00edamos poner un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Arquitectura Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia una Arquitectura Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende libros en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Para finalizar, hay que destacar lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir con nuestras soluciones Big Data, y eso supone que hay que adaptarse a ellos lo antes posible. Cada problema a resolver tiene unos condicionantes particulares y en muchos casos habr\u00e1 que evolucionar la arquitectura que est\u00e1bamos utilizando hasta el momento, o como se suele decir: \u201crenovarse o morir\u201d. Buenas pr\u00e1cicas \u00b6 En la ingesta de informaci\u00f3n: eval\u00faa tus tipos de fuentes, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s queutilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. Algunas decisiones que tenemos que tomar a la hora de elegir la arquitectura son: Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad? Define tu arquitectura: \u00bfbatch o streaming? \u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes? Arquitectura en la nube \u00b6 Marco de buena arquitectura \u00b6 https://docs.aws.amazon.com/es_es/wellarchitected/latest/framework/the-five-pillars-of-the-framework.html Es una gu\u00eda dise\u00f1ada para ayudarlo a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para sus aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas b\u00e1sicas que pueden ayudarlo a evaluar e implementar sus arquitecturas en la nube. AWS desarroll\u00f3 el Marco de Buena Arquitectura despu\u00e9s de revisar miles de arquitecturas de clientes en AWS. El Marco de Buena Arquitectura de AWS se organiza en cinco pilares: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costo Cada pilar incluye un conjunto de principios de dise\u00f1o y \u00e1reas de pr\u00e1cticas recomendadas. Dentro de cada \u00e1rea de pr\u00e1cticas recomendadas, hay un conjunto de preguntas b\u00e1sicas. Para cada pregunta se proporciona un poco de contexto y una lista de pr\u00e1cticas recomendadas. 1 Learning AWS About This Book This paper book and companion video library are focused on the Amazon Web Services (AWS) cloud\u2014and specifically what is called infrastructure as a service (IaaS)\u2014to help you learn about the cloud services Amazon offers. Services that AWS offers can be broken down into the foundational services of compute, storage, networking, and security\u2014and a big helping of automation. A handy way to think of AWS is as a massive toolbox with a wide variety of specialized tools that can carry out an assortment of infrastructure tasks. If you\u2019re a system administrator, developer, or project manager or you\u2019ve heard about the AWS cloud and want to know more about it, this book is designed for you as a technical baseline of AWS services, what they can do, the major concepts, one of the major components, and how to set up the service to function. I estimate that I reviewed more than 35,000 pages of AWS documentation and summarized all that technical detail into somewhere between 300\u2013400 pages of AWS information. That doesn\u2019t mean you won\u2019t read AWS documentation because you most definitely will; but hopefully this book and the companion video library will catapult your indoctrination into the AWS jungle. You may also want to get certified; however, this is not a book that is directly focused on AWS certification. This book is instead focused on the so-called foundational services. All AWS certification tests are focused on problem-solving based on a particular scenario. Your job is to figure out the best one or two answers; therefore, knowing the foundational services is key. If you want to get certified on AWS cloud services, particularly on AWS architecture, you must know the foundational AWS services inside and out. And you\u2019ll have to spend a few hours doing hands-on work with AWS services. If you want to develop applications that will be hosted at AWS, you will need to know the foundational services in even more detail. And forget about learning everything about AWS in a single book; it\u2019s just not possible, and the reality is that AWS is constantly changing. That\u2019s a notion you will learn to embrace. Each chapter in this book attempts to deal with a specific concept or AWS service and provide a strong detailed technical summary of the AWS service in question. However, there are not pages and pages of step-by-step solutions because the steps change every couple of months. During the writing of this book, AWS changed the design of its icons used in its technical documentation three times. They also added 600 features and made numerous other changes, from cosmetic to substantial. To get around the issue of immediate obsolescence, there is a companion video library associated with this book that shows you how to set up and install and configure many AWS cloud services. You can access these videos by registering your book at informit.com/register. Throughout the remainder of the chapters, you\u2019ll be invited to watch the companion video that relates to the topic that we are covering. The companion step-by-step videos can be changed and updated or added to as AWS changes. The beauty of a video is that you can pause or rewind it as you learn. Let\u2019s begin the journey and see where we end up. This initial chapter includes the following topics: Defining the public cloud Where AWS fits with IaaS and platform as a service (PaaS) Characteristics of cloud computing according to NIST Considerations for migrating applications to AWS Operational benefits for operating in the cloud The cloud service-level agreement (SLA) Data, application, and network security at AWS Compliance at AWS AWS Well-Architected Framework Trying to Define the Cloud The roots of public cloud computing are not new; the public cloud providers Amazon Web Services and Microsoft Azure have been established for well over a decade with strong IaaS and PaaS offerings around the world. The Google Cloud Platform (GCP) and the IBM or Oracle Cloud are other viable alternatives. Gartner\u2019s Magic Quadrant (www.gartner.com/en/research/methodologies/magic-quadrants-research) in Figure 1-1 shows four types of technology provider a company can align their goals and strategies with. In 2018, IaaS market penetration dominated two of those categories. Under the Leaders quadrant, Amazon Web Services led in that area, followed by Microsoft and then Google. Google also aligned closely to the Visionaries Quadrant. Alibaba Cloud, Oracle, and IBM fell in the Niche Players quadrant. Figure 1-1 Top public cloud providers. Gartner, Magic Quadrant for Cloud Infrastructure as a Service, Worldwide, Dennis Smith et al., 23 May 2018. (Gartner Methodologies, Magic Quadrant, www.gartner.com/en/research/methodologies/magic-quadrants-research)1 1Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner\u2019s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. When I started my career as a computer technician back in the 90s, most corporations that I supported used several computer-based services that were not located on premise. Accounting services were accessed through a fast (at the time) 1200 baud modem that was connected using one of those green-screened digital terminals. The serial cable threaded through the drop ceiling to connect the terminal was strong enough to pull a car. A customer of mine at the time was utilizing a mainframe computer for accounting hosted locally in town. However, he couldn\u2019t access his accounting services any time he liked; he had his allotted slice of processing time every Tuesday, and that was that. Payroll services were provided by another remote service called Automatic Data Processing, or ADP for short. Both service companies and their services are still around today. IBM is continuing to release versions of its z series mainframe, and ADP payroll services was one of the first software as a service (SaaS) companies but remains popular today. In 2015, IBM bought a cloud provider based in Texas called SoftLayer and merged it into its public cloud offering, today called the IBM Cloud. The z mainframe has ended up being hosted in the IBM cloud providing hosted mainframe services; in April 2018, IBM announced it was launching what it called a \u201cskinny mainframe\u201d for cloud computing built around the IBM z 14 mainframe. If you work for a bank or financial institution, IBM mainframes probably provide 50% of all your computing services. This could be great news for companies that don\u2019t want to have a local mainframe environment to maintain. Fifty years since the launch of the IBM mainframe, many companies\u2019 mainframes are continuing to be relevant and are now part of the public cloud landscape. The reality is that more than 90 of the world\u2019s largest 100 banks, the top 10 insurance companies, a majority of the 25 largest retailers, and most of the world\u2019s larger airlines still rely on mainframe computers from IBM. If you didn\u2019t use mainframes, you probably lived through the deployment cycle of Novell NetWare and Windows and Active Directory, and virtualization using VMware or Hyper-V. You likely have a private cloud in your own data centers. You may be wondering why your company is moving to the public cloud. The reality these days is that it is expensive to build and maintain data centers. Certainly, building a data center is going to cost millions or billions of dollars. Maintaining an existing data center over the long term is expensive as well. Because of virtualization and the rise of the Internet as a useful communication medium, cloud services have replaced many local data centers and will continue to do so. Figuring out the capital costs of hosting your applications in the public cloud instead of running them in your own data center is sometimes categorized as renting instead of buying, as defined in Figure 1-2. Figure 1-2 No long-term capital expenses Operational expenses (OpEX) are all you pay for using cloud services. The capital expenditure (CapEX) of building a data center does not have to be borne by a single business. Now let\u2019s be clear: operational expenses are still expensive. You might say to your boss, \u201cI don\u2019t need $800 million for data center construction, but I will need $2 million a year forever.\u201d The reality is that the cost of running and hosting your applications in the cloud is cheaper once you add in every expense; however, operating in the cloud is only cheaper if your services being hosted in the cloud are properly designed. Services and applications don\u2019t run 24/7; they are turned off or reduced in size when they\u2019re not needed. A concept that you may not yet be familiar with is automation. Public cloud providers use automated procedures to build, manage, monitor, and scale every cloud service. By the end of this book, you will understand how automation is the secret sauce for successful cloud deployments. Automated procedures will save you money and allow you to sleep at night. Let\u2019s start by defining the public cloud. The cloud is just a collection of data centers. There is no ownership from the customer\u2019s point of view; the cloud provider owns the services, and you rent each service as required. You may be thinking that the cloud is all virtual resources, yet the AWS cloud can provide you bare-metal servers. If you want, Amazon will happily host your applications and databases on bare-metal servers hosted in its data centers. Of course, more commonly, AWS will offer you many virtual servers in well over 150 different sizes and designs. Amazon is also quite happy to allow you to continue to operate your on-premise data centers and coexist with cloud resources and services operating at AWS. Microsoft Azure will offer to sell you a copy of its complete Azure cloud operating system to install on your servers in your data centers. As you can see, it\u2019s hard to define the public cloud these days other than as a massive collection of compute and storage resources hosted on a network stored in the collection of data centers accessible across the Internet, or by using private connections. Anything that you host in the public cloud is using compute and storage resources to execute your software application. And anything that used to be a hardware device, such as a router, switch, or storage array, can be replaced by a third-party software appliance or an AWS-managed software service composed of virtual computers, storage, and networking components. This doesn\u2019t mean that many companies aren\u2019t still using hardware devices. Hardware devices such as routers and switches have incredible speed and can operate much faster in most cases than a software router and switch. But what happens if you can run hundreds or thousands of virtual machines in parallel performing the function of a hardware switch or hardware router device? Perhaps we don\u2019t need any hardware devices at all. Most of the AWS-managed cloud services are hosted on virtual machines (defined as EC2 instances, or Elastic Cloud Compute instances), with massive CPU and RAM resources running in massive server farms with custom-designed applications, providing the storage arrays, networking services, load-balancing, and auto-scaling services that we depend on at AWS. Moving to AWS Once the decision has been made to move to the AWS cloud, countless moving parts begin to churn. People need to be trained, infrastructure changes must take place, developers potentially need to code in a different way, and IT professionals must get up to speed on the cloud provider that has been chosen; there\u2019s no time to waste. Larger companies will usually attempt to convey the message of what moving to the cloud means for them. It\u2019s quite common for executives within the company to have strong opinions about what moving to the cloud will do. Sadly, these opinions are not usually based on technical knowledge or real hands-on experience with the cloud provider that has been chosen. Generally, companies utilizing cloud services fall into several mind-sets: The corporate mentality\u2014You currently have data centers, infrastructure, and virtualized applications. Ever-increasing infrastructure and maintenance costs are driving you to look at what options are available in the public cloud. Born-in-the-cloud mentality\u2014You\u2019re a developer with a great idea, but you don\u2019t want to maintain a local data center. In fact, you don\u2019t have a local data center, and you want to get going as soon as possible. The startup mentality\u2014You\u2019ve just lost your job due to a merger or buyout and are determined to strike out on your own. Your brand-new company has no data center but plenty of ideas combined with a distinct lack of cash. The government client\u2014You\u2019ve been told that, to save costs, your government department is moving to the AWS cloud within a defined timeframe. Each of these starting mind-sets will have differing points of view as to how it should start to migrate or design its cloud infrastructure and hosted applications. Coming from a corporate environment or government department, you will probably expect the cloud provider to have a detailed service-level agreement (SLA) that you can change to match your needs. You will also probably have expectations about how much detail you expect to be provided about the cloud provider\u2019s infrastructure and services. In short, you expect to be in control. If you have started with a public cloud services provider as an individual developer, or you\u2019re working with a startup, you will probably have no comparison with current on-premise costs; therefore, the overall costs that you pay for using a cloud provider will be accepted for the short term but, over time, as your experience grows, your overall cloud costs will be analyzed and managed to be as optimized and as cheap as possible. Note AWS has options for developers who want to craft and deploy applications hosted at AWS. The site https://aws.amazon.com/startups/ is where you can get further information about how you might be able to qualify for what is called AWS Promotional Credit. There\u2019s a possibility of getting up to $15,000 in credits over 2 years, including AWS support and training. The reality is that moving to the cloud means you will be giving up an element of control. After all, it\u2019s not your data center. At AWS, you\u2019re not getting deeper into the infrastructure stack than the subnets that host your applications. Remember, the cloud is a data center; it\u2019s just not your data center. Let\u2019s start by looking at the available public cloud computing models of IaaS and PaaS and where AWS fits within these definitions. Infrastructure as a Service Most of the services AWS offers fall into the infrastructure as a service (IaaS) definition, as shown in Figure 1-3. This is certainly the most mature cloud model offering; virtualized servers and virtualized storage arrays are hosted on a software defined network with each customer\u2019s infrastructure completely isolated as a private resource. Creating resources at AWS typically starts with the creation of what is called a virtual private cloud (VPC). Virtual servers, virtual hard drive volumes, and indeed complete managed services and products can be hosted on your isolated private network. You have the flexibility to create whatever architectural stack you desire at AWS using a vast number of services and utilities contained in the IaaS toolbox. Companies moving to the AWS public cloud will typically first start with IaaS because the compute and storage services closely mirror their current on-premise virtual environment. Figure 1-3 Infrastructure as a service at AWS IaaS cloud services at AWS are bundled with managed services. A managed service is built on the trio of compute, storage, and networking services and customized software providing something you want Amazon to manage and maintain rather than your having to do all the work. For example, AWS offers a managed service called relational database service (RDS). It will build, host, maintain, back up, fail over, synchronize, and monitor a pair of master/standby database servers for you, leaving you the single task of managing your data records. Many other managed services are available at AWS; in fact, many managed services have no additional charges to begin using. For example, an automation service called CloudFormation allows you to automate the procedure of building infrastructure stacks complete with the required compute, storage, networks, and load balancers required for your application stack. In fact, practically anything to do with building, updating, or deleting your infrastructure stacks at AWS can be automated with CloudFormation. Another handy service called CloudTrail is provided free of charge. It tracks and records all application programming interface (API) calls that are carried out in each of your AWS accounts for 90 days. And yes, you can configure CloudTrail to store your API calls forever in S3 storage. Your internal applications that are running in your on-premise data centers are probably a vast soup of proprietary operating systems (HP, AIX, Linux) and of course Windows. Talk to most departments in a small to midsize corporate environment, and the end users typically express unhappiness with some of the current applications that they use daily. They have learned to live with the ongoing issues of each application. Talk to the IT administrators and developers in the corporate data centers; there very well could be a great deal of unhappiness with the inflexibility of the existing infrastructure that they have to use and manage. On top of these issues, perhaps each department has its own IT infrastructure. My company once provided compute services for a midsized hospital with 25 separate networks. Typically, in a larger corporation, compute services can be heavily siloed between departments, or each line of business gets to make its own decisions. Most companies with more than 100 employees have some semblance of virtual infrastructure for their servers typically using VMware. Virtualization was supposed to be the answer to controlling a company\u2019s infrastructure costs. However, the cost for virtualization services has become extremely expensive to host, run, and maintain. Companies now know that capital and licensing costs are some of the biggest expenses they incur when running an ever-expanding on-premise private cloud. Replacing VMware with AWS-hosted virtualized servers and services removes a company\u2019s need for hypervisor administration expertise. And the landscape of applications used by corporations is now widely available in the public cloud as hosted applications defined as software as a service (SaaS) applications. As a result, there is ever-growing interest at the department level or overall company level in using the public cloud to host applications. And the reality is, you may not have a choice. If you\u2019re a Microsoft shop, the odds are quite strong that some of your everyday software applications such as Exchange and Microsoft Office are hosted by Microsoft Azure and Office 365, allowing you to completely replace some of your in-house software deployments. For more details on the compute platform at AWS, check out Chapter 4, \u201cCompute Services: AWS EC2 Instances.\u201d If your company has no experience working with external cloud providers and you are a medium- to large-sized corporation, it\u2019s a certainty your company will fit the private cloud model. Most of your company\u2019s infrastructure will be hosted within several private data centers. For example, your primary data center may be in Philadelphia, and your second data center could be in Nashville. (If you\u2019re a large enough company, your data centers may be spread across multiple continents.) The applications used will number in the hundreds or thousands. You may be lucky enough to have centralized IT standards, but these standards have become an issue due to the applications that multiple departments have installed or created over the years. Maybe if you\u2019re unlucky, one of the central applications used by your company was developed by a summer student and plunked into production without a second thought. At AWS, infrastructure resources are spread across the world in 20 different regions. If you are in a large population center, the odds are that Amazon is close by. If Amazon is not close by, you still may be able to connect into it through one of the edge locations. More details on regions, availability zones, and edge locations can be found in Chapter 2, \u201cDesigning with AWS Global Services.\u201d Platform as a Service Platform as a service (PaaS) cloud providers enable your developers to create custom applications on a variety of popular development platforms such as Java, PHP, and Python. The developers don\u2019t have to manually build the infrastructure components required for each application per se; the required infrastructure resources are defined at the beginning of the development cycle and are created and managed by the PaaS cloud provider. After applications have been developed and tested and are ready for prime time, the application is made available to end users using public URLs. The PaaS cloud provider will host and scale the hosted application based on demand. As more users use the application, the infrastructure resources will scale out or in as required. PaaS environments are installed on the IaaS resources of the PaaS cloud provider, as shown in Figure 1-4. In fact, IaaS is always behind all \u201cas a service\u201d monikers. Examples of PaaS providers include Cloud Foundry and Heroku. Figure 1-4 IaaS hosts the PaaS layer Expanding upon Cloud Foundry, this PaaS solution is the foundation of development at IBM Cloud, where the underlying infrastructure is hosted on the IBM public cloud and running a customized version of the Cloud Foundry platform components. Developers can sign up and focus on writing applications. All requests will be handled by the PaaS layer interfacing with the IaaS layer, where the compute, storage, load-balancing, and scaling services operate. Another popular solution for developing applications in the cloud is Heroku, mentioned in passing earlier. Heroku allows you to create and run hosted applications using a variety of development platforms. Just like the IBM cloud, once the application has been written, Heroku hosts, balances, and auto scales the application as required and sends you a bill for hosting at the end of the month. If you\u2019re dealing with a PaaS provider, remember that programming languages change from time to time; therefore, APIs change as well, and usually without warning. If your developers don\u2019t keep up to date, there can be issues when using a PaaS cloud development platform. Digging into the details on the Heroku website, under \u201cSecurity,\u201d the site states that, \u201cHeroku\u2019s physical infrastructure is hosted and managed within Amazon\u2019s secure data centers and utilize the Amazon Web services technology.\u201d Heroku is owned by another cloud heavyweight, Salesforce. Salesforce indicated in 2018 that future expansion was going to be by utilizing Amazon data center resources. Oh, what a tangled web we weave. An additional reality is that one cloud provider\u2019s PaaS system is not necessarily compatible with another cloud provider\u2019s service. Both AWS and Microsoft Azure offer similar cloud services, but internally each cloud provider operates in a completely different fashion with a completely different set of APIs. There is no single standard for defining just what PaaS must be. Compatibility issues begin to reveal themselves at the lower levels of each vendor\u2019s proposed solution. RESTful interfaces, manifest file formats, framework configurations, external APIs, and component integration are not necessarily compatible across cloud vendors. AWS deals with platform services using Lambda, the API Gateway, and several code deployment tools. The applications that your company may have been developing and using internally will be a variety of two- and three-tier architectures with many local dependencies such as network storage, local storage, local users, and databases. The overall architecture design may have been adequate at the beginning but now is straining to function due to the age of the hardware, the sizing of the hardware, and the lack of any flexibility to change. The distinct difference with on-premise design when compared to hosting applications at AWS is that provisioning hardware and waiting for it to be set up and configured is a thing of the past. In fact, there are many possibilities to consider when designing applications at AWS. Your choice of language and development framework will determine the PaaS vendor you select. Do you do a lot of development in Python? Are you a Java developer? Amazon has a PaaS solution called Elastic Beanstalk that automates the deployment of applications developed in Java, Python, Ruby, and other development platforms on the required infrastructure components for each application including E2 instances or Docker containers, with load-balancing, auto scaling, and monitoring services. Amazon has several development solutions, shown in Figure 1-5, including CodeBuild, CodeCommit, Elastic Beanstalk, CodeDeploy. These can be key components in your application deployment at AWS. Chapter 8, \u201cAutomating AWS Infrastructure,\u201d covers these interesting managed services and additional details on automating your infrastructure. Figure 1-5 Platform options at AWS Essential Characteristics of AWS Cloud Computing If you haven\u2019t heard of National Institute of Standards and Technology (NIST), a branch of the U.S. government, you\u2019re not alone. Around 2010, NIST began documenting the public cloud. After talking to all the major vendors, it released an initial report in June 2011 defining many cloud components that were common across all the public cloud vendors. The report\u2019s genius was in defining what the emerging public cloud actually was (the command components). Over the years, NIST\u2019s cloud definitions have moved from definitions to becoming standards for how many companies view working in the public cloud. According to NIST, five key definitions of the public cloud have really morphed into a definitive standard methodology of operating in the public cloud: On-demand self-service\u2014We not only expect cloud service to be delivered quickly; we demand it. All cloud providers offer a self-serve portal as AWS does, as shown in Figure 1-6. You request a cloud service, and in seconds it\u2019s available in your AWS account ready to configure. Gone are the days of requesting a virtual server via email and waiting several days until it\u2019s built. At AWS, a virtual server can be started and operational in seconds. Procuring a software-defined network at AWS (called a virtual private cloud) is available and operational in seconds. AWS has an expansive self-serve management console that allows you to order and configure many cloud-hosted services in seconds in any AWS region. Any cloud service that you order from AWS is automatically delivered to you through heavily automated procedures. There are no public cloud providers that survive without a self-service portal driven by heavy-duty automation in the background. This NIST definition is now a standard. Figure 1-6 The AWS management portal Broad network access\u2014Cloud services can be accessed from almost anywhere across the globe using the Internet. If you host applications at AWS, perhaps they are public-facing SaaS apps. AWS also provides HTTPS endpoints to access every cloud service hosted at AWS. However, you may not want broad network access, which is defined as public network access to your cloud services. In fact, many companies that are moving to the AWS cloud have no interest in a publicly accessible software solution. They want their hosted cloud services to remain private, accessible only by their employees using private connections. Each cloud customer ultimately defines the real meaning of broad network access. At AWS, applications can be publicly available, or, you can stay completely private. VPN connections from your place of work to AWS are commonplace; in fact, you can order Direct Connect and establish a private fiber connection to AWS running at speeds up to 10 Gbps. Depending on the type of applications you\u2019re using in the cloud, high-speed network access is essential. We can even use, access, and administer AWS service from our phone using AWS apps. Certainly, accessing AWS from any device is possible. For more details on networking, check out Chapter 3, \u201cAWS Networking Services.\u201d Resource Pooling\u2014Infrastructure resources for public cloud providers are pooled together in many data centers across the different regions of the world and are dynamically assigned on demand. A company running an on-premise private cloud would pool its virtual machines, memory, processing, and networking capabilities into one or two data centers, and from its own pool offer limited compute resources. All public cloud providers have a massive pool of resources to serve our various needs. AWS has clusters of data centers (known as AZs or availability zones), and each AZ could have over 80,000 bare-metal servers available and online allowing customers to host their application services with a high level of resiliency and failover. Having many available online resources also enables AWS to keep the price down. Without a massive pool of resources, AWS would not be able to offer its cloud services on demand that are able to scale up and down based on customer demand. Having a massive resource pool is a necessary standard for all public cloud providers; customers do not expect to run out of resources. Take, for example, AWS S3 storage, which is unlimited with no defined maximum limit. For more details on regions and AZs, check out Chapter 2. Rapid Elasticity\u2014Elasticity in the public cloud, or scaling, is the key feature required by all hosted cloud applications. Elasticity at AWS is utilized for both compute and storage. Because most services and applications are built on compute and storage, applications in the AWS cloud have the capability to automatically scale, as shown in Figure 1-7. And elasticity, or scaling, is only useful if it\u2019s automated based on demand. Turning off a virtual server, adding RAM, and turning it back on is not the elasticity that we are interested in; we want horizontal scale\u2014that is, more application servers\u2014not just a bigger server. Real-time monitoring of a hosted cloud application at AWS allows us to react almost instantaneously before the application\u2019s performance is close to degrading. With EC2 Auto Scaling in the background, additional computer resources are automatically ordered and delivered to the application server\u2019s cluster, maintaining the application\u2019s performance. Rapid elasticity based on demand is only possible with real-time monitoring driving automated scale. This is why the public cloud is so popular; with a massive pool of available cloud resources and the ability to automatically scale applications out and in based on demand, at AWS anybody can easily scale application stacks up and down. For more details on deploying scale and elasticity with EC2 Auto Scale, check out Chapter 5, \u201cPlanning for Scale and Resiliency.\u201d Figure 1-7 Applications can scale based on demand in the public cloud Measured Service\u2014In the cloud, you are only billed for what you use; that\u2019s defined as a measured service. Cloud providers make their money by charging for everything that you use in their data centers, including data transfer costs. Packet flow inbound to the public cloud is usually free; outbound packet flow, or traffic between subnets hosted in different data centers, is usually charged an outbound data transfer fee. Charges are per second, or per minute in the case of computer services like AWS EC2 compute instances, or they are per gigabyte per month in the case of storage services like S3 or virtual hard drives, which at AWS are called elastic block storage (EBS). AWS charges can be broken down into compute, storage, and data transfer charges. If an AWS service is on, the meter is running. Cost management is one of your most important jobs when operating in the cloud. AWS has many useful tools to help you control your costs, including the AWS Simple Pricing Calculator, AWS Budgets, and the Cost Explorer, as shown in Figure 1-8. You can find details on these features in Chapter 2. Being billed for consuming cloud services is a reality that we are all used to. What you also may have to get used to is exactly how you are being billed. Again, you must understand and carefully monitor compute, storage, and data transfer costs. For example, you can order a load balancer at AWS for $30 per month. However, there is an additional charge to be aware of: all the data packets transferred through the load balancer are charged, and that by itself can be a hefty price. Figure 1-8 AWS Budgets and Cost Explorer track and alert when costs are over budget Operational Benefits of AWS Operating in the public cloud has certain benefits. Unlimited access to servers and storage and many management services may make it easier than you expected to operate in the cloud. Table 1-1 summarizes the managed services at AWS that may be able to replace or complement your existing on-premise services and procedures. Servers\u2014Underutilized servers in your data center are expensive to run and maintain. Moving applications to the public cloud will reduce the size of your on-premise data center. Because you no longer host as many physical servers, your total hosting costs (heating, cooling, and so on) will be lower as well. You also won\u2019t have to pay for as many software licenses at the processer level because you\u2019re not responsible for running hypervisor services; that\u2019s Amazon\u2019s job. You may think that moving to the AWS cloud means virtualized resources and only virtualization. However, at AWS, you can get a variety of compute options with virtualization of any size and scale, from a single-core CPU with 512MB of RAM to hundreds of CPU cores and terabytes of RAM. You can also order a bare-metal server and do whatever you want with it. You can find further details on compute options in Chapter 4. Storage\u2014Using cloud storage has huge benefits due to the unlimited amount of storage promised by cloud providers. Amazon has many options for storage that are similar, but not exactly the same as your on-premise solutions. For storage area network solutions, Amazon has shareable file solutions: the elastic file system (EFS) for Linux workloads, and FSx, a shared file service specifically for Windows File Server workloads. Virtual hard disks are available using EBS. Unlimited storage, and longer-term archive storage, is provided by S3 and S3 Glacier. Details on all the storage options at AWS can be found in Chapter 6, \u201cCloud Storage.\u201d Managed services\u2014AWS has a variety of managed services, as shown in Table 1-1, that may be able to replace or complement your existing services and utilities currently used on-premise once you move to the AWS cloud. Table 1-1 Managed Services at AWS IT Operations On-Premise AWS Cloud Monitoring Nagios, SolarWinds. CloudWatch monitoring providing metrics for every AWS service. All monitoring and logging data can be stored in S3. All third-party monitoring solutions can access S3 to perform their own custom analysis of log data. Data backup Backup tools such as Commvault and NetBackup. Any third-party vendor that wants to stay in business will be supporting AWS; both Veritas and Commvault have AWS solutions. AWS Storage Gateway can also be installed to cache required content locally, while backing up local disk volumes to an S3 bucket. Backups can be snapshots of local virtual hard disks, or data files from specific volumes can be targeted. Scale Add additional virtual machines or increase/decrease the size of each virtual machine\u2019s RAM and CPU cores. Scale horizontally by placing multiple virtual machines (instances) behind a load balancer and add automated scaling based on demand to increase and decrease the required amount of compute power using EC2 Auto Scaling. Testing Provisioning hardware for testing is expensive. Provisioning resources for short-term testing at AWS is incredibly inexpensive. Signing up for the AWS free tier allows you to test a variety of AWS services for one year completely free. Identity management Active Directory Domain Services for accessing corporate resources. Extend on-premise Active Directory to the AWS cloud with hosted Directory Services. Utilize AWS single sign-on services (SSO) for managing access to popular business applications that third-party cloud providers are hosting. Cloud Provider Limitations Each cloud provider has a published SLA that specifies what services are provided and at what specific operational level. All public cloud providers make promises about how they will handle security, compliance, and overall operations and how their methodology will be contained in the cloud provider\u2019s SLA. The challenge is to live up to that agreement. In the SLA, there will be details about acceptable outage time and the responsibility of the cloud provider when outages occur. There also will be statements about not being responsible for events outside the cloud provider\u2019s control. Another common term typically used in the SLA is \u201cbest effort\u201d or \u201ccommercially reasonable effort.\u201d Regardless of the cloud model, the cloud provider is responsible for overall service operation and deployment, service orchestration, the overall management of the cloud, the security of the cloud components, and maintenance of customer privacy. The responsibility of how each customer, the cloud consumer, is to carry out business with the cloud provider will also be described in some detail in the SLA. Each cloud consumer must fully understand what each cloud service offered provides; this is exactly what the cloud service will and will not do. The reality is that every public cloud provider will not have an SLA that you will like, and the stark reality is that their best effort is the best they can do. This might seem a little harsh, but it\u2019s reality; according to AWS, \u201ceverything fails all the time.\u201d What happens when a key component of your application hosted in the AWS cloud fails? Is it a disaster, or is it manageable? Is it acceptable to expect AWS failures from time to time? It\u2019s a reality; AWS is 100% right; everything fails. Operating in the public cloud means that you must design your hosted application to be able to continue operating even if compute and storage failures occur. That\u2019s our responsibility. All public cloud providers really have the same SLA; here it is, summarized in nine short words: \u201cwe are sorry; we will give you a credit.\u201d This SLA summary applies to every public cloud provider. Here\u2019s another reality check; if you\u2019re down, you will have to prove that you were actually down by providing network traces and appropriate documentation that leaves no doubt that you were down because of an AWS cloud issue. Oh, and here\u2019s another small detail to be aware of: if you didn\u2019t build redundancy into your application design, don\u2019t bother calling for a credit. Application designs that have a single instance hosting the application with no failover or high-availability design parameters have no SLA. AWS expects you to be serious about your application design; we need to understand and use the tools in the AWS toolbox to ensure that your SLA for availability and performance is achieved. Not every service at AWS even has a defined SLA; there are more than 100 services and only 8 defined SLAs. Remember: all managed services\u2014in fact, all services\u2014are built from the resources found in Table 1-2. Table 1-2 SLAs at AWS AWS Service SLA Summary CloudFront 99.9% during any monthly billing cycle DynamoDB Monthly uptime percentage of 99.999% for global tables, or 99.99% for regular tables EC2 instances (includes elastic container service [ECS] and EBS volumes) Monthly uptime percentage of at least 99.99% RDS databases Monthly uptime percentage of at least 99.95% for multi-AZ instances Route 53 DNS service Commercially reasonable efforts to make Route 53 100% available during a monthly billing cycle S3; S3 Glacier object storage The number of errors calculated during each 5-minute period subtracted from 100% Lambda functions Monthly uptime percentage of 99.95% during any monthly billing cycle AWS Shield (Advanced) Any failure of service commitments provided by CloudFront or Route 53 when being protected by AWS Shield Advanced distributed denial of service (DDoS) protection Data Security at AWS We can lose many things while operating in the cloud: instances fail, EBS volumes crash, services stop working. But you can\u2019t go to your boss and say we\u2019ve lost some data. Data security\u2014The reality is that your data is more secure and durable stored in the public cloud. At AWS, except for S3 Glacier archive storage, which is automatically encrypted, all other storage mediums at AWS are unencrypted by default. However, EBS volumes\u2014both boot and data volumes\u2014can be encrypted at rest and at transit using either customer master keys provided by AWS or keys provided by the customer. Shared storage services such as EFS can also be encrypted at rest, as can DynamoDB tables. S3 buckets can be encrypted with keys provided by AWS or supplied by customers, as shown in Figure 1-9. Data durability provides security of a different nature; all data stored in the cloud is stored in multiple locations; EBS volumes are replicated within the data center where they reside. S3 objects are replicated across three separate locations within the selected AWS region, producing a high level of durability. Amazon\u2019s level of S3 durability is humorously defined like this: for every 1,000 objects stored in an S3 bucket, you will lose one of those objects every 10 million years. We cannot possibly duplicate this level of durability and security on-premise. Figure 1-9 S3 buckets can be encrypted using AES-256 or AWS-KMS managed keys Data privacy\u2014AWS does not have data storage isolated for individual customers; all storage arrays at AWS are multitenant in design. This is pretty much the default for all public cloud providers. Amazon\u2019s job is to make sure your stored data records are isolated per AWS account. Data control\u2014Customers are in full control of storing and retrieving their data stored in AWS. All data storage at AWS starts as private, and except for S3 buckets that are changed allowing public access, storage remains private and is not directly accessible from the outside world. Customers can choose to make S3 buckets public; it\u2019s the customer\u2019s responsibility to define the security and accessibility of all data records stored in AWS. Security controls\u2014As previously mentioned, all data records can be encrypted at AWS. Resource policies defining the precise level of security and access can be directly attached to resources such as S3 buckets or EFS shared storage and can be defined by the identity and access management (IAM) user and group security policy using the IAM service. IAM identity and trust policies can be defined at a granular level controlling access by users and roles to all resources at AWS, including any storage medium. Chapter 7, \u201cSecurity Services,\u201d provides details on IAM. You can enable multifactor authentication as an additional security control on S3 buckets to control when deletion of data records is performed. Network Security at AWS At AWS, networking is managed at the subnet level, and all subnets are created as a private subnet with no access to the outside world. Subnets reside on your private networks, which are called a virtual private cloud (VPC) at AWS. Only by adding a gateway service to a VPC will subnets be able to be accessed from either the Internet or a private VPN connection from an on-premise network. Chapter 3 has the details on networking at AWS. It\u2019s important to note that public and private connectivity choices are decisions that are always carried out by each customer; not AWS. Each subnet\u2019s ingress and egress traffic can be controlled by a subnet firewall called Network ACLs that define separate stateless rules for both inbound and outbound packet flow. Each EC2 instance hosted on a subnet is further protected by an additional firewall called a security group, which defines what traffic is allowed into the instance and where outbound traffic is directed. VPC flow logs can be enabled to capture network traffic for the entire VPC, a single subnet, or a network interface. Application Security at AWS Both Web and application servers hosted at AWS should always be located on private subnets. Private subnets are not directly accessible from the Internet. You may be wondering how to access what was supposed to be a public-facing application with no direct public access. The solution to this question is the absolute best practice to follow at AWS: for Web servers that customers across the Internet access, placing the load balancer on a public subnet, in front of the Web servers, provides the correct design solution. Customers requesting access to the application will be directed by DNS to the DNS name of the load balancer. The load balancer directs incoming traffic from the public subnet to the targeted Web servers hosted in the private subnets. One load balancer type offered by AWS is the Application Load Balancer, which can perform authentication and SSL offload services. The end-to-end traffic pattern for a three-tier Web application can be designed using many encryption/decryption points, as shown in Figure 1-10 on its path from source to destination: Web application firewall\u2014A custom traffic filter in front of the Application Load Balancer protecting against malicious traffic. Elastic Load Balancer (ELB)\u2014Accepts only encrypted HTTPS traffic on port 443; provides secure sockets layer/transport layer security (SSL/TLS) decryption and, optionally, user authentication. EC2 instance hosting Web application\u2014EBS boot and data drives can be encrypted. EC2 instance hosting application server\u2014EBS boot and data drives can be encrypted. Database server\u2014EBS boot and data drives and data community can be encrypted, or Dynamo DB tables can be encrypted. Figure 1-10 Encrypted traffic flow at AWS Compliance in the AWS Cloud As a worldwide public cloud provider, AWS operates in many different countries and is subject to a variety of rules and regulations enforced by governments and compliance standards. Depending on the type of business that you operate, there are possibly many different levels of compliance you will have to adhere to when operating in the AWS cloud. Financial, health, and government institutions have strict rules and regulations that must be followed by their clients. In addition, your own company may have specific internal rules and regulations they want to follow. Many countries in the world are enacting laws, regulations, and mandates in serious attempts to protect the privacy of personal data and the security of corporate information and computer systems. The new data protection laws place the burden of protection and security on the custodian of that data; that is where the data is stored when the data is transferred from source to destination. The cloud providers have contractual obligations to ensure that when organizations have data records hosted in their cloud, they can adhere to the promises and commitments made in the SLA. Some of the most common compliance regulations that AWS has been successfully audited against include the compliance standards listed in Table 1-3. Table 1-3 AWS Supports Many Compliance Standards Abbreviation Scope of Operation Purpose of Protection Legal Status HIPPA Healthcare Personal information Law GLBA Financial industry Personal information Law SOX Publicly traded companies Shareholder Law PCI DSS Payment card industry Fraud Industry regulation GDPR EU Personal information Law Health Insurance Portability and Accountability Act\u2014Secures the privacy of individual health information records in the United States. Gramm-Leachy-Billy Act\u2014Mandates protection of customer information by financial industries. Sarbanes-Oxley\u2014Ensures the integrity of financial operations of publicly traded companies. PCI DSS\u2014Ensures the processing integrity of credit card data or authentication data. GDPR\u2014Protects privacy and personal data for all citizens of the EU. Amazon has a decent compliance page at https://aws.amazon.com/compliance/, which has details about all the AWS certifications and attestations that it has achieved or supports. If you are bound by a specific compliance standard, one of your first steps should be to review the AWS services that are available for each compliance standard, as shown in Figure 1-11. Figure 1-11 Check the AWS compliance page to see what services are supported Playing in the AWS Sandbox AWS makes it easy to \u201ctry before you buy,\u201d frequently doling out promotional credits to developers. Even if you are not a developer, every new AWS customer gets limited access to nearly every AWS service for free (Amazon calls this the \u201cfree tier\u201d) during the first year. This is a great way to experiment with AWS. The only thing you must provide is a credit card that won\u2019t be charged unless you choose to use resources that the free tier doesn\u2019t cover. After the first year has passed, you\u2019ll start accruing charges for every service you use; any AWS resources that you built during the first year remain in your account but start accruing charges. In addition, AWS has several free hands-on labs. You can sign up for QwikLabs at https://run.qwiklabs.com/home?locale=en and carry out a variety of AWS tasks in the AWS cloud. Figure 1-12 illustrates some of the learnig and labs that are available from QwikLabs. Figure 1-12 QwikLabs has more than 20 completely free labs for AWS services Running experiments, and performing labs raises additional questions that will help further your AWS cloud knowledge and experience. MAKE SURE TO WATCH THE COMPANION VIDEO \u201cSIGNING UP FOR AWD FREE TIER.\u201d To access the companion videos, register your book at informit.com/register. What\u2019s the Problem That Needs to Be Solved? Typical large organizations run hundreds or thousands of applications on thousands of virtual servers. Which applications can be moved to AWS? What should be prioritized? Start with low value/low risk\u2014It\u2019s quite popular to suggest a starting point of high value and low risk when choosing your first application to move to the AWS cloud. Here\u2019s a reality check: it\u2019s probably going to take you 6 months or longer to move your application to the cloud. Choosing an application with low value provides a valuable timeline to do some additional planning and analysis before finalizing your application in its working form at AWS. I\u2019ve seen many companies make the pronouncement that applications will be moving to the cloud quickly. It rarely happens successfully because there are so many things to learn and consider. Start with low value. Take your time, and select a working application that has been running successfully for a good time period. Then you can document your lessons learned and what to do differently the next time. The second and third application moved to the cloud generally will be much faster than the first application due to the lessons learned and experience gained. Create a brand-new application first\u2014The advantage of creating a completely new application at AWS means you are not constrained by anything, such as the type of database that must be used, the type of programming language that must be used, or the type of compute that must be used. Starting anew at AWS allows you to try out some of the new methods to host applications such as serviceless computing, create a mobile application using stateless components, or use DynamoDB instead of SQL. This is where the real learning about what the AWS cloud can do for you will really appear. Try to solve a single problem\u2014Do you need additional storage? Perhaps that\u2019s a great starting point for your adventure in the cloud. Archiving files in S3 Glacier could be as simple as ordering a Snowball device, connecting it up to your network, filling up with files you\u2019d like to archive, and shipping it back to AWS. This is an excellent first project to start working with AWS support, archiving records, and saving your company money. Define a value proposition\u2014Ideally, the move to AWS is long term and successful. Thousands of companies have been successful moving to AWS; you, too, can be successful. Start off with a defined value proposition that can be validated quickly, in a matter of months rather than years. For developing applications, you could sign up for AWS Cloud9, a cloud-hosted IDE that supports more than 40 programming languages, as shown in Figure 1-13. Armed with a browser, you can try your hand at developing applications at AWS. Figure 1-13 Cloud9 IDE at AWS Access to data records\u2014The number-one problem with larger companies when starting to work with cloud providers is working through the internal politics to allow access to data from the cloud. Data record access, and the steps for successful access, should be considered before you move to the cloud: How can we access our on-premise data from the cloud? What records have to stay on-premise? Are we bound by any compliance rules and regulations? Is our data in the right format for what we need? Migrating Applications For applications that have been chosen as starting candidates to move to the AWS cloud, several decisions need to be made about the application\u2019s journey, or path. Can the application be moved to AWS and hosted on an EC2 instance with no changes? Applications that fit into this category could be migrated to AWS as an EC2 instance image. Server migration tools, and database migration tools discussed in Chapter 2, can carry out these migration paths quite effectively. However, applications that are lifted and shifted to the cloud will have other dependencies and issues that will have to be considered: The application stores its data in a database. Will the database remain on-premise or be moved to the cloud? If the database for the application remains on-premise, are there latency issues that need to be considered when communicating with the database? Will a high-speed connection need to be established between the AWS cloud and the database remaining on-premise? Are there compliance issues regarding the application data? Does the data have to be encrypted at rest? Does communication with the database need to be encrypted? Do users authenticate to the application across the corporate network? If so, are federation services required to be deployed at AWS for single sign-on (SSO)? Are local dependencies installed on the application server that will interfere with the application server\u2019s operation in the AWS cloud? Are there licensing considerations for both the operating system and the application when operating in the cloud? Is there an existing SaaS application hosted by a public cloud provider that should replace the application because it\u2019s a better choice? This can be a very political issue to resolve. With so many hosted cloud applications available in the public cloud, the odds are close to 100% that there will be an existing application that could replace the current on-premise application. Should the application remain on-premise and eventually be deprecated? The application is hosted on legacy hardware that is near end-of-life. The application is not virtualized. The application does not have support. The application is used by a small number of users. The Well-Architected Framework Several years ago, AWS introduced documentation called the Well-Architected Framework to help customers plan properly when moving to the AWS cloud. The goal was to give guidance for cloud architects to build secure, resilient, and decent performing infrastructure to host their applications following recognized best practices that have been developed over time by the experience of many AWS customers. Each best practice still must be evaluated as to whether it meets your criteria. A best practice should not be blindly adopted without understanding why it has achieved a best practice designation. The documentation for the well-architected framework also has many key questions to ponder that can be found in the well-architected framework blueprint. It is useful to discuss these questions out loud with other technical folks in your company; they will help you make key decisions about your infrastructure and applications hosted at AWS. The framework documentation can be found here: https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf. Each application to be deployed at AWS needs to be viewed through the lens of being well architected following these five principles: Operational excellence\u2014How best to execute, deploy, and monitor applications running at AWS using automated deployment monitoring procedures, continuous improvement, and automated solutions for recovering from failures. Key AWS services to utilize include CloudWatch events and alarms, CloudTrail, EC2 Auto Scaling, AWS Config, and the Trusted Advisor. Check out Chapters 5, 7, and 8. Operational excellence questions to consider include these: How are disruptions to applications handled? Manually, or automatically? How can you analyze the ongoing health of your applications and infrastructure components hosted at AWS? Security\u2014How to best design systems that will operate reliably and securely while protecting customer information and data records. Key AWS services to utilize include IAM, AWS Organizations, CloudWatch logs, CloudTrail events, S3 and S3 Glacier, and VPC flow logs. Check out Chapters 3, 6, and 7. Security questions to consider include these: How are security credentials and authentication managed at AWS? How are automated procedures secured? Reliability\u2014How can systems and applications hosted at AWS recover from disruption with minimal downtime? How can applications meet your escalating demands? Key AWS services to utilize include ELB, EC2 Auto Scaling, and CloudWatch alarms. Check out Chapter 5. Reliability questions to consider include these: How do you monitor resources hosted at AWS? How do applications hosted at AWS adapt to changes in demand by end users? Performance efficiency\u2014How to use compute resources to meet and maintain your application requirements on an ongoing basis. Should your compute solution change from EC2 instances to containers or serviceless? Key services include EC2 Auto Scaling, EBS volumes, and RDS. Check out Chapters 4 and 6. Performance efficiency questions to consider include these: Why did you select your database? Why did you select your current compute infrastructure? Cost Optimization\u2014How to design systems that meet your needs at the cheapest price point. Key AWS services include Cost Explorer, Budgets, EC2 Auto Scaling, Trusted Advisor, and the Simple Monthly Calculator. Check out Chapters 2, 5, and 7. Cost optimization questions to consider are as follows: How do you oversee usage and cost? How do you meet cost targets? Are you aware of current data transfer charges based on your AWS designs? Excelencia operativa \u00b6 Seguridad \u00b6 Fiabilidad \u00b6 Eficiencia del rendimiento \u00b6 Optimizaci\u00f3n de costos \u00b6 Fiabilidad y disponibilidad \u00b6 AWS Trusted Advisor \u00b6 Gesti\u00f3n del escalado y la monitorizaci\u00f3n \u00b6 Elastic Load Balancing \u00b6 Amazon CloudWatch \u00b6 Amazon EC2 Auto Scaling \u00b6 Actividades \u00b6 Si nos basamos en una arquitectura Lambda, clasifica los siguientes elementos: Si nos basamos en una arquitectura Kappa, clasifica los siguientes elementos: Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS . Referencias \u00b6 Arquitectura Big Data: \u00bfen qu\u00e9 consiste y para qu\u00e9 se utiliza? Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa/ https://medium.com/dataprophet/4-big-data-architectures-data-streaming-lambda-architecture-kappa-architecture-and-unifield-d9bcbf711eb9","title":"6.- Arquitecturas"},{"location":"apuntes/arquitecturas01.html#arquitecturas-big-data","text":"Ya sabemos en qu\u00e9 consiste Big Data, y que dentro de sus 5V, dos de las m\u00e1s importantes son el volumen y la velocidad . Para cumplir con estas necesidades, necesitamos una infraestructura que dote a nuestras aplicaciones de toda la potencia y robustez necesarias. En esta sesi\u00f3n no vamos a entrar al detalle de ninguna tecnolog\u00eda, ya que el stack de herramientas es muy amplio y en constante crecimiento. A lo largo del curso iremos conociendo las distintas herramientas y aprenderemos c\u00f3mo y cu\u00e1ndo utilizarlas.","title":"Arquitecturas Big Data"},{"location":"apuntes/arquitecturas01.html#caracteristicas","text":"Todas las arquitecturas que dise\u00f1emos / utilicemos deben cumplir las siguientes caracter\u00edsticas: Escalabilidad : permite aumentar f\u00e1cilmente las capacidades de procesamiento y almacenamiento de datos. Tolerancia a fallos : garantiza la disponibilidad del sistema, aunque se produzcan fallos en algunas de las m\u00e1quinas, evitando la p\u00e9rdida de datos. Datos distribuidos : los datos deben estar almacenados entre diferentes m\u00e1quinas evitando as\u00ed el problema de almacenar grandes vol\u00famenes de datos en un \u00fanico nodo central. Procesamiento distribuido : el tratamiento de los datos se realiza entre diferentes m\u00e1quinas para mejorar los tiempos de ejecuci\u00f3n y dotar al sistema de escalabilidad. Localidad del dato : los datos a trabajar y los procesos que los tratan deben estar cerca, para evitar las transmisiones por red que a\u00f1aden latencias y aumentan los tiempos de ejecuci\u00f3n. Antes de conocer las arquitecturas m\u00e1s empleados, es conveniente tener presente siempre cu\u00e1l es el objetivo que debe cumplir nuestra soluci\u00f3n. Es muy f\u00e1cil caer en la sobreingenier\u00eda y montar una arquitectura con una amalgama de productos que luego son dif\u00edciles de configurar y mantener.","title":"Caracter\u00edsticas"},{"location":"apuntes/arquitecturas01.html#tipos-de-arquitecturas","text":"Debido a que las empresas disponen de un volumen de datos cada vez mayor y la necesidad de analizarlos y obtener valor de ellos lo antes posible, surge la necesidad de definir nuevas arquitecturas para cubrir casos de uso distintos a los que hab\u00eda hasta el momento. Las arquitecturas m\u00e1s comunes en estos proyectos son principalmente dos: Lambda y Kappa . La principal diferencia entre ambas son los flujos de tratamiento de datos que intervienen. Un par de conceptos que tenemos que definir antes de ver las caracter\u00edsticas de ambas, son el procesamiento batch y el procesamiento en streaming.","title":"Tipos de arquitecturas"},{"location":"apuntes/arquitecturas01.html#procesamiento-batch","text":"Batch hace referencia a un proceso en el que intervienen un conjunto de datos y que tiene un inicio y un fin en el tiempo. Tambi\u00e9n se le conoce como procesamiento por lotes y se ejecuta sin control directo del usuario. Por ejemplo, si tenemos una aplicaci\u00f3n que muestra el total de casos COVID que hay en cada ciudad, en vez de realizar el c\u00e1lculo sobre el conjunto completo de los datos, podemos realizar una serie de operaciones que hagan esos c\u00e1lculos y los almacenen en tablas temporales (por ejemplo, mediante INSERT ... SELECT ), de manera que si queremos volver a realzar la consulta sobre todos los datos, acceder\u00edamos a los datos ya calculados de la tabla temporal. El problema es que este c\u00e1lculo necesita actualizarse, por ejemplo, de manera diaria, y de ah\u00ed que haya que rehacer todas las tablas temporales. Es el procesamiento que se ha realizado desde los inicios del trabajo con datos, tanto a nivel de bases de datos como con Data Warehouses . De la mano del procesamiento batch se ha implantado el ecosistema Hadoop con todas las herramientas que abarcan un proceso ETL (extraci\u00f3n, transformaci\u00f3n y carga de los datos). Estos conceptos los trabajaremos m\u00e1s adelante.","title":"Procesamiento Batch"},{"location":"apuntes/arquitecturas01.html#procesamiento-en-streaming","text":"Un procesamiento es de tipo streaming cuando est\u00e1 continuamente recibiendo y tratando nueva informaci\u00f3n seg\u00fan va llegando sin tener un fin en lo referente al apartado temporal. Este procesamiento se relaciona con el an\u00e1lisis en tiempo real. Warning No confundir tiempo real con inmediatez. En inform\u00e1tica, un sistema de tiempo real es aquel que responde en un periodo de tiempo finito, normalmente muy peque\u00f1o, pero no tiene por qu\u00e9 ser instantaneo.","title":"Procesamiento en Streaming"},{"location":"apuntes/arquitecturas01.html#arquitectura-lambda","text":"Representada mediante la letra griega, apareci\u00f3 en el a\u00f1o 2012 y se atribuye a Nathan Marz . Nathan Marz La defini\u00f3 en base a su experiencia en sistemas de tratamiento de datos distribuidos durante su etapa como empleado en las empresas Backtype y Twitter, y est\u00e1 inspirada en su art\u00edculo How to beat the CAP theorem . Su objetivo era tener un sistema robusto tolerante a fallos, tanto humanos como de hardware, que fuera linealmente escalable y que permitiese realizar escrituras y lecturas con baja latencia. Para ello, se compone de tres capas: Capa batch : se encarga de (a) gestionar los datos hist\u00f3ricos y (b) recalcular los resultados, por ejemplo, de los modelos de machine learning . De manera espec\u00edfica, la capa batch recibe los datos, los combina con el historico existente y recalcula los resultados iterando sobre todo el conjunto de datos combinado. As\u00ed pues, este capa opera sobre el conjunto completo y permite que el sistema produzca los resultados m\u00e1s precisos. Sin embargo, esto conlleva un coste de alta latencia debido a los requisitos de tiempo de computaci\u00f3n. Capa de streaming / speed : sirve para ofrecer resultados con muy baja latencia, cercano al tiempo real. Este capa recibe los datos y realizar modificaciones incrementales sobre los resultados de la capa batch . Gracias a los algoritmos incrementales implementados en esta capa, se consigue reducir el coste computacional de manera considerable. Capa de serving : permite la consulta de los resultados enviados desde las dos capas anteriores. Podemos ver un esquema de la arquitectura en el siguiente gr\u00e1fico: https://www.paradigmadigital.com/techbiz/de-lambda-a-kappa-evolucion-de-las-arquitecturas-big-data/ El flujo de trabajo es el siguiente: La nueva informaci\u00f3n recogida por el sistema se env\u00eda tanto a la capa batch como a la capa de streaming ( Speed Layer en la imagen anterior). En la capa batch ( Batch Layer ) se gestiona la informaci\u00f3n en crudo, es decir, sin modificar. Los datos nuevos se a\u00f1aden a los ya existentes. Seguidamente se hace un tratamiento mediante un proceso batch cuyo resultado ser\u00e1n las Batch Views , que se usar\u00e1n en la capa que sirve los datos para ofrecer la informaci\u00f3n ya transformada al exterior. La capa que sirve los datos ( Serving Layer ) indexa las Batch Views generadas en el paso anterior de forma que puedan ser consultadas con tiempos de respuesta muy bajos. La capa de streaming compensa la alta latencia de las escrituras que ocurre en la serving layer y solo tiene en cuenta los datos nuevos (incrementos entre los procesos batch y el momento actual). Finalmente, combinando los resultados de las Batch Views y de las vistas en tiempo real ( Real-time Views ), se construye la respuesta a las consultas realizadas. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa","title":"Arquitectura Lambda"},{"location":"apuntes/arquitecturas01.html#arquitectura-kappa","text":"https://www.treelogic.com/es/Arquitectura_Kappa.html El t\u00e9rmino Arquitectura Kappa, representada por la letra , fue introducido en 2014 por Jay Kreps en su art\u00edculo Questioning the Lambda Architecture . En \u00e9l se\u00f1ala los posibles puntos \u201cd\u00e9biles\u201d de la Arquitectura Lambda y c\u00f3mo solucionarlos mediante una evoluci\u00f3n. Su propuesta consiste en eliminar la capa batch dejando solamente la capa de streaming. Esta capa, a diferencia de la de tipo batch, no tiene un comienzo ni un fin desde un punto de vista temporal y est\u00e1 continuamente procesando nuevos datos a medida que van llegando. Como un proceso batch se puede entender como un stream acotado, podr\u00edamos decir que el procesamiento batch es un subconjunto del procesamiento en streaming. Esta evoluci\u00f3n consiste en una simplificaci\u00f3n de la Arquitectura Lambda, en la que se elimina la capa batch y todo el procesamiento se realiza en una sola capa denominada de tiempo real o Real-time Layer, dando soporte a procesamientos tanto batch como en tiempo real. Podemos decir que sus cuatro pilares principales son los siguientes: Todo es un stream: las operaciones batch son un subconjunto de las operaciones de streaming, por lo que todo puede ser tratado como un stream. Los datos de partida no se modifican: los datos son almacenados sin ser transformados y las vistas se derivan de ellos. Un estado concreto puede ser recalculado puesto que la informaci\u00f3n de origen no se modifica. Solo existe un flujo de procesamiento: puesto que mantenemos un solo flujo, el c\u00f3digo, el mantenimiento y la actualizaci\u00f3n del sistema se ven reducidos considerablemente. Posibilidad de volver a lanzar un procesamiento: se puede modificar un procesamiento concreto y su configuraci\u00f3n para variar los resultados obtenidos partiendo de los mismos datos de entrada. Como requisito previo a cumplir, se tiene que garantizar que los eventos se leen y almacenan en el orden en el que se han generado. De esta forma, podremos variar un procesamiento concreto partiendo de una misma versi\u00f3n de los datos.","title":"Arquitectura Kappa"},{"location":"apuntes/arquitecturas01.html#casos-de-uso","text":"\u00bfQu\u00e9 arquitectura se adapta mejor a nuestro problema? \u00bfC\u00faal encaja mejor en nuestro modelo de negocio?. https://www.ericsson.com/en/blog/2015/11/data-processing-architectures--lambda-and-kappa-examples Por lo general, no existe una \u00fanica respuesta. La arquitectura Lambda es m\u00e1s vers\u00e1til y es capaz de cubrir un mayor n\u00famero de casos, muchos de ellos que requieren incluso procesamiento en tiempo real. Una pregunta que debemos plantearnos para poder decidir es, \u00bfel an\u00e1lisis y el procesamiento que vamos a realizar en las capas batch y streaming es el mismo? En ese caso la opci\u00f3n m\u00e1s acertada ser\u00eda la Arquitectura Kappa. Como ejemplo real de esta arquitectura podr\u00edamos poner un sistema de geolocalizaci\u00f3n de usuarios por la cercan\u00eda a una antena de telefon\u00eda m\u00f3vil. Cada vez que se aproximase a una antena que le diese cobertura se generar\u00eda un evento. Este evento se procesar\u00eda en la capa de streaming y servir\u00eda para pintar sobre un mapa su desplazamiento respecto a su posici\u00f3n anterior. Sin embargo, en otras ocasiones necesitaremos acceder a todo el conjunto de datos sin penalizar el rendimiento por lo que la Arquitectura Lambda puede ser m\u00e1s apropiada e incluso m\u00e1s f\u00e1cil de implementar. Tambi\u00e9n nos inclinaremos hacia una Arquitectura Lambda si nuestros algoritmos de batch y streaming generan resultados muy distintos, como puede suceder con operaciones de procesamiento pesado o en modelos de Machine Learning. Un caso de uso real para una arquitectura Lambda podr\u00eda ser un sistema que recomiende libros en funci\u00f3n de los gustos de los usuarios. Por un lado, tendr\u00eda una capa batch encargada de entrenar el modelo e ir mejorando las predicciones; y por otro, una capa streaming capaz de encargarse de las valoraciones en tiempo real. Para finalizar, hay que destacar lo r\u00e1pido que evolucionan los casos de uso que queremos cubrir con nuestras soluciones Big Data, y eso supone que hay que adaptarse a ellos lo antes posible. Cada problema a resolver tiene unos condicionantes particulares y en muchos casos habr\u00e1 que evolucionar la arquitectura que est\u00e1bamos utilizando hasta el momento, o como se suele decir: \u201crenovarse o morir\u201d.","title":"Casos de uso"},{"location":"apuntes/arquitecturas01.html#buenas-pracicas","text":"En la ingesta de informaci\u00f3n: eval\u00faa tus tipos de fuentes, no todas las herramientas sirven para cualquier fuente, y en alg\u00fan caso te encontrar\u00e1s que lo mejor es combinar varias herramientas para cubrir todos tus casos. En el procesamiento: eval\u00faa si tu sistema tiene que ser streaming o batch. Algunos sistemas que no se definen como puramente streaming utilizan lo que denominan micro-batch que suele dar respuesta a problemas que en el uso cotidiano del lenguaje se denomina como streaming. En la monitorizaci\u00f3n: ten en cuenta que estamos hablando de multitud de herramientas y que su monitorizaci\u00f3n, control y gesti\u00f3n puede llegar a ser muy tedioso, por lo que independientemente de que te decidas por instalar un stack completo o por instalar herramientas independientes y generar tu propia arquitectura combusto, te recomiendo adem\u00e1s queutilices herramientas para controlar, monitorizar y gestionar tu arquitectura, esto te facilitar\u00e1 y centralizar\u00e1 todo este tipo de tareas. Algunas decisiones que tenemos que tomar a la hora de elegir la arquitectura son: Enfoca tus casos de uso, cuando tengas tus objetivos claros sabr\u00e1s que debes potenciar en tu arquitectura. \u00bfVolumen, variedad, velocidad? Define tu arquitectura: \u00bfbatch o streaming? \u00bfRealmente necesitas que tu arquitectura soporte streaming? Eval\u00faa tus fuentes de datos: \u00bfC\u00f3mo de heterog\u00e9neas son tus fuentes de datos? \u00bfsoportan las herramientas elegidas todos los tipos de fuentes de datos que tienes?","title":"Buenas pr\u00e1cicas"},{"location":"apuntes/arquitecturas01.html#arquitectura-en-la-nube","text":"","title":"Arquitectura en la nube"},{"location":"apuntes/arquitecturas01.html#marco-de-buena-arquitectura","text":"https://docs.aws.amazon.com/es_es/wellarchitected/latest/framework/the-five-pillars-of-the-framework.html Es una gu\u00eda dise\u00f1ada para ayudarlo a crear la infraestructura con m\u00e1s seguridad, alto rendimiento, resiliencia y eficacia posibles para sus aplicaciones y cargas de trabajo en la nube. Proporciona un conjunto de preguntas y pr\u00e1cticas recomendadas b\u00e1sicas que pueden ayudarlo a evaluar e implementar sus arquitecturas en la nube. AWS desarroll\u00f3 el Marco de Buena Arquitectura despu\u00e9s de revisar miles de arquitecturas de clientes en AWS. El Marco de Buena Arquitectura de AWS se organiza en cinco pilares: excelencia operativa, seguridad, fiabilidad, eficacia del rendimiento y optimizaci\u00f3n de costo Cada pilar incluye un conjunto de principios de dise\u00f1o y \u00e1reas de pr\u00e1cticas recomendadas. Dentro de cada \u00e1rea de pr\u00e1cticas recomendadas, hay un conjunto de preguntas b\u00e1sicas. Para cada pregunta se proporciona un poco de contexto y una lista de pr\u00e1cticas recomendadas. 1 Learning AWS About This Book This paper book and companion video library are focused on the Amazon Web Services (AWS) cloud\u2014and specifically what is called infrastructure as a service (IaaS)\u2014to help you learn about the cloud services Amazon offers. Services that AWS offers can be broken down into the foundational services of compute, storage, networking, and security\u2014and a big helping of automation. A handy way to think of AWS is as a massive toolbox with a wide variety of specialized tools that can carry out an assortment of infrastructure tasks. If you\u2019re a system administrator, developer, or project manager or you\u2019ve heard about the AWS cloud and want to know more about it, this book is designed for you as a technical baseline of AWS services, what they can do, the major concepts, one of the major components, and how to set up the service to function. I estimate that I reviewed more than 35,000 pages of AWS documentation and summarized all that technical detail into somewhere between 300\u2013400 pages of AWS information. That doesn\u2019t mean you won\u2019t read AWS documentation because you most definitely will; but hopefully this book and the companion video library will catapult your indoctrination into the AWS jungle. You may also want to get certified; however, this is not a book that is directly focused on AWS certification. This book is instead focused on the so-called foundational services. All AWS certification tests are focused on problem-solving based on a particular scenario. Your job is to figure out the best one or two answers; therefore, knowing the foundational services is key. If you want to get certified on AWS cloud services, particularly on AWS architecture, you must know the foundational AWS services inside and out. And you\u2019ll have to spend a few hours doing hands-on work with AWS services. If you want to develop applications that will be hosted at AWS, you will need to know the foundational services in even more detail. And forget about learning everything about AWS in a single book; it\u2019s just not possible, and the reality is that AWS is constantly changing. That\u2019s a notion you will learn to embrace. Each chapter in this book attempts to deal with a specific concept or AWS service and provide a strong detailed technical summary of the AWS service in question. However, there are not pages and pages of step-by-step solutions because the steps change every couple of months. During the writing of this book, AWS changed the design of its icons used in its technical documentation three times. They also added 600 features and made numerous other changes, from cosmetic to substantial. To get around the issue of immediate obsolescence, there is a companion video library associated with this book that shows you how to set up and install and configure many AWS cloud services. You can access these videos by registering your book at informit.com/register. Throughout the remainder of the chapters, you\u2019ll be invited to watch the companion video that relates to the topic that we are covering. The companion step-by-step videos can be changed and updated or added to as AWS changes. The beauty of a video is that you can pause or rewind it as you learn. Let\u2019s begin the journey and see where we end up. This initial chapter includes the following topics: Defining the public cloud Where AWS fits with IaaS and platform as a service (PaaS) Characteristics of cloud computing according to NIST Considerations for migrating applications to AWS Operational benefits for operating in the cloud The cloud service-level agreement (SLA) Data, application, and network security at AWS Compliance at AWS AWS Well-Architected Framework Trying to Define the Cloud The roots of public cloud computing are not new; the public cloud providers Amazon Web Services and Microsoft Azure have been established for well over a decade with strong IaaS and PaaS offerings around the world. The Google Cloud Platform (GCP) and the IBM or Oracle Cloud are other viable alternatives. Gartner\u2019s Magic Quadrant (www.gartner.com/en/research/methodologies/magic-quadrants-research) in Figure 1-1 shows four types of technology provider a company can align their goals and strategies with. In 2018, IaaS market penetration dominated two of those categories. Under the Leaders quadrant, Amazon Web Services led in that area, followed by Microsoft and then Google. Google also aligned closely to the Visionaries Quadrant. Alibaba Cloud, Oracle, and IBM fell in the Niche Players quadrant. Figure 1-1 Top public cloud providers. Gartner, Magic Quadrant for Cloud Infrastructure as a Service, Worldwide, Dennis Smith et al., 23 May 2018. (Gartner Methodologies, Magic Quadrant, www.gartner.com/en/research/methodologies/magic-quadrants-research)1 1Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner\u2019s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose. When I started my career as a computer technician back in the 90s, most corporations that I supported used several computer-based services that were not located on premise. Accounting services were accessed through a fast (at the time) 1200 baud modem that was connected using one of those green-screened digital terminals. The serial cable threaded through the drop ceiling to connect the terminal was strong enough to pull a car. A customer of mine at the time was utilizing a mainframe computer for accounting hosted locally in town. However, he couldn\u2019t access his accounting services any time he liked; he had his allotted slice of processing time every Tuesday, and that was that. Payroll services were provided by another remote service called Automatic Data Processing, or ADP for short. Both service companies and their services are still around today. IBM is continuing to release versions of its z series mainframe, and ADP payroll services was one of the first software as a service (SaaS) companies but remains popular today. In 2015, IBM bought a cloud provider based in Texas called SoftLayer and merged it into its public cloud offering, today called the IBM Cloud. The z mainframe has ended up being hosted in the IBM cloud providing hosted mainframe services; in April 2018, IBM announced it was launching what it called a \u201cskinny mainframe\u201d for cloud computing built around the IBM z 14 mainframe. If you work for a bank or financial institution, IBM mainframes probably provide 50% of all your computing services. This could be great news for companies that don\u2019t want to have a local mainframe environment to maintain. Fifty years since the launch of the IBM mainframe, many companies\u2019 mainframes are continuing to be relevant and are now part of the public cloud landscape. The reality is that more than 90 of the world\u2019s largest 100 banks, the top 10 insurance companies, a majority of the 25 largest retailers, and most of the world\u2019s larger airlines still rely on mainframe computers from IBM. If you didn\u2019t use mainframes, you probably lived through the deployment cycle of Novell NetWare and Windows and Active Directory, and virtualization using VMware or Hyper-V. You likely have a private cloud in your own data centers. You may be wondering why your company is moving to the public cloud. The reality these days is that it is expensive to build and maintain data centers. Certainly, building a data center is going to cost millions or billions of dollars. Maintaining an existing data center over the long term is expensive as well. Because of virtualization and the rise of the Internet as a useful communication medium, cloud services have replaced many local data centers and will continue to do so. Figuring out the capital costs of hosting your applications in the public cloud instead of running them in your own data center is sometimes categorized as renting instead of buying, as defined in Figure 1-2. Figure 1-2 No long-term capital expenses Operational expenses (OpEX) are all you pay for using cloud services. The capital expenditure (CapEX) of building a data center does not have to be borne by a single business. Now let\u2019s be clear: operational expenses are still expensive. You might say to your boss, \u201cI don\u2019t need $800 million for data center construction, but I will need $2 million a year forever.\u201d The reality is that the cost of running and hosting your applications in the cloud is cheaper once you add in every expense; however, operating in the cloud is only cheaper if your services being hosted in the cloud are properly designed. Services and applications don\u2019t run 24/7; they are turned off or reduced in size when they\u2019re not needed. A concept that you may not yet be familiar with is automation. Public cloud providers use automated procedures to build, manage, monitor, and scale every cloud service. By the end of this book, you will understand how automation is the secret sauce for successful cloud deployments. Automated procedures will save you money and allow you to sleep at night. Let\u2019s start by defining the public cloud. The cloud is just a collection of data centers. There is no ownership from the customer\u2019s point of view; the cloud provider owns the services, and you rent each service as required. You may be thinking that the cloud is all virtual resources, yet the AWS cloud can provide you bare-metal servers. If you want, Amazon will happily host your applications and databases on bare-metal servers hosted in its data centers. Of course, more commonly, AWS will offer you many virtual servers in well over 150 different sizes and designs. Amazon is also quite happy to allow you to continue to operate your on-premise data centers and coexist with cloud resources and services operating at AWS. Microsoft Azure will offer to sell you a copy of its complete Azure cloud operating system to install on your servers in your data centers. As you can see, it\u2019s hard to define the public cloud these days other than as a massive collection of compute and storage resources hosted on a network stored in the collection of data centers accessible across the Internet, or by using private connections. Anything that you host in the public cloud is using compute and storage resources to execute your software application. And anything that used to be a hardware device, such as a router, switch, or storage array, can be replaced by a third-party software appliance or an AWS-managed software service composed of virtual computers, storage, and networking components. This doesn\u2019t mean that many companies aren\u2019t still using hardware devices. Hardware devices such as routers and switches have incredible speed and can operate much faster in most cases than a software router and switch. But what happens if you can run hundreds or thousands of virtual machines in parallel performing the function of a hardware switch or hardware router device? Perhaps we don\u2019t need any hardware devices at all. Most of the AWS-managed cloud services are hosted on virtual machines (defined as EC2 instances, or Elastic Cloud Compute instances), with massive CPU and RAM resources running in massive server farms with custom-designed applications, providing the storage arrays, networking services, load-balancing, and auto-scaling services that we depend on at AWS. Moving to AWS Once the decision has been made to move to the AWS cloud, countless moving parts begin to churn. People need to be trained, infrastructure changes must take place, developers potentially need to code in a different way, and IT professionals must get up to speed on the cloud provider that has been chosen; there\u2019s no time to waste. Larger companies will usually attempt to convey the message of what moving to the cloud means for them. It\u2019s quite common for executives within the company to have strong opinions about what moving to the cloud will do. Sadly, these opinions are not usually based on technical knowledge or real hands-on experience with the cloud provider that has been chosen. Generally, companies utilizing cloud services fall into several mind-sets: The corporate mentality\u2014You currently have data centers, infrastructure, and virtualized applications. Ever-increasing infrastructure and maintenance costs are driving you to look at what options are available in the public cloud. Born-in-the-cloud mentality\u2014You\u2019re a developer with a great idea, but you don\u2019t want to maintain a local data center. In fact, you don\u2019t have a local data center, and you want to get going as soon as possible. The startup mentality\u2014You\u2019ve just lost your job due to a merger or buyout and are determined to strike out on your own. Your brand-new company has no data center but plenty of ideas combined with a distinct lack of cash. The government client\u2014You\u2019ve been told that, to save costs, your government department is moving to the AWS cloud within a defined timeframe. Each of these starting mind-sets will have differing points of view as to how it should start to migrate or design its cloud infrastructure and hosted applications. Coming from a corporate environment or government department, you will probably expect the cloud provider to have a detailed service-level agreement (SLA) that you can change to match your needs. You will also probably have expectations about how much detail you expect to be provided about the cloud provider\u2019s infrastructure and services. In short, you expect to be in control. If you have started with a public cloud services provider as an individual developer, or you\u2019re working with a startup, you will probably have no comparison with current on-premise costs; therefore, the overall costs that you pay for using a cloud provider will be accepted for the short term but, over time, as your experience grows, your overall cloud costs will be analyzed and managed to be as optimized and as cheap as possible. Note AWS has options for developers who want to craft and deploy applications hosted at AWS. The site https://aws.amazon.com/startups/ is where you can get further information about how you might be able to qualify for what is called AWS Promotional Credit. There\u2019s a possibility of getting up to $15,000 in credits over 2 years, including AWS support and training. The reality is that moving to the cloud means you will be giving up an element of control. After all, it\u2019s not your data center. At AWS, you\u2019re not getting deeper into the infrastructure stack than the subnets that host your applications. Remember, the cloud is a data center; it\u2019s just not your data center. Let\u2019s start by looking at the available public cloud computing models of IaaS and PaaS and where AWS fits within these definitions. Infrastructure as a Service Most of the services AWS offers fall into the infrastructure as a service (IaaS) definition, as shown in Figure 1-3. This is certainly the most mature cloud model offering; virtualized servers and virtualized storage arrays are hosted on a software defined network with each customer\u2019s infrastructure completely isolated as a private resource. Creating resources at AWS typically starts with the creation of what is called a virtual private cloud (VPC). Virtual servers, virtual hard drive volumes, and indeed complete managed services and products can be hosted on your isolated private network. You have the flexibility to create whatever architectural stack you desire at AWS using a vast number of services and utilities contained in the IaaS toolbox. Companies moving to the AWS public cloud will typically first start with IaaS because the compute and storage services closely mirror their current on-premise virtual environment. Figure 1-3 Infrastructure as a service at AWS IaaS cloud services at AWS are bundled with managed services. A managed service is built on the trio of compute, storage, and networking services and customized software providing something you want Amazon to manage and maintain rather than your having to do all the work. For example, AWS offers a managed service called relational database service (RDS). It will build, host, maintain, back up, fail over, synchronize, and monitor a pair of master/standby database servers for you, leaving you the single task of managing your data records. Many other managed services are available at AWS; in fact, many managed services have no additional charges to begin using. For example, an automation service called CloudFormation allows you to automate the procedure of building infrastructure stacks complete with the required compute, storage, networks, and load balancers required for your application stack. In fact, practically anything to do with building, updating, or deleting your infrastructure stacks at AWS can be automated with CloudFormation. Another handy service called CloudTrail is provided free of charge. It tracks and records all application programming interface (API) calls that are carried out in each of your AWS accounts for 90 days. And yes, you can configure CloudTrail to store your API calls forever in S3 storage. Your internal applications that are running in your on-premise data centers are probably a vast soup of proprietary operating systems (HP, AIX, Linux) and of course Windows. Talk to most departments in a small to midsize corporate environment, and the end users typically express unhappiness with some of the current applications that they use daily. They have learned to live with the ongoing issues of each application. Talk to the IT administrators and developers in the corporate data centers; there very well could be a great deal of unhappiness with the inflexibility of the existing infrastructure that they have to use and manage. On top of these issues, perhaps each department has its own IT infrastructure. My company once provided compute services for a midsized hospital with 25 separate networks. Typically, in a larger corporation, compute services can be heavily siloed between departments, or each line of business gets to make its own decisions. Most companies with more than 100 employees have some semblance of virtual infrastructure for their servers typically using VMware. Virtualization was supposed to be the answer to controlling a company\u2019s infrastructure costs. However, the cost for virtualization services has become extremely expensive to host, run, and maintain. Companies now know that capital and licensing costs are some of the biggest expenses they incur when running an ever-expanding on-premise private cloud. Replacing VMware with AWS-hosted virtualized servers and services removes a company\u2019s need for hypervisor administration expertise. And the landscape of applications used by corporations is now widely available in the public cloud as hosted applications defined as software as a service (SaaS) applications. As a result, there is ever-growing interest at the department level or overall company level in using the public cloud to host applications. And the reality is, you may not have a choice. If you\u2019re a Microsoft shop, the odds are quite strong that some of your everyday software applications such as Exchange and Microsoft Office are hosted by Microsoft Azure and Office 365, allowing you to completely replace some of your in-house software deployments. For more details on the compute platform at AWS, check out Chapter 4, \u201cCompute Services: AWS EC2 Instances.\u201d If your company has no experience working with external cloud providers and you are a medium- to large-sized corporation, it\u2019s a certainty your company will fit the private cloud model. Most of your company\u2019s infrastructure will be hosted within several private data centers. For example, your primary data center may be in Philadelphia, and your second data center could be in Nashville. (If you\u2019re a large enough company, your data centers may be spread across multiple continents.) The applications used will number in the hundreds or thousands. You may be lucky enough to have centralized IT standards, but these standards have become an issue due to the applications that multiple departments have installed or created over the years. Maybe if you\u2019re unlucky, one of the central applications used by your company was developed by a summer student and plunked into production without a second thought. At AWS, infrastructure resources are spread across the world in 20 different regions. If you are in a large population center, the odds are that Amazon is close by. If Amazon is not close by, you still may be able to connect into it through one of the edge locations. More details on regions, availability zones, and edge locations can be found in Chapter 2, \u201cDesigning with AWS Global Services.\u201d Platform as a Service Platform as a service (PaaS) cloud providers enable your developers to create custom applications on a variety of popular development platforms such as Java, PHP, and Python. The developers don\u2019t have to manually build the infrastructure components required for each application per se; the required infrastructure resources are defined at the beginning of the development cycle and are created and managed by the PaaS cloud provider. After applications have been developed and tested and are ready for prime time, the application is made available to end users using public URLs. The PaaS cloud provider will host and scale the hosted application based on demand. As more users use the application, the infrastructure resources will scale out or in as required. PaaS environments are installed on the IaaS resources of the PaaS cloud provider, as shown in Figure 1-4. In fact, IaaS is always behind all \u201cas a service\u201d monikers. Examples of PaaS providers include Cloud Foundry and Heroku. Figure 1-4 IaaS hosts the PaaS layer Expanding upon Cloud Foundry, this PaaS solution is the foundation of development at IBM Cloud, where the underlying infrastructure is hosted on the IBM public cloud and running a customized version of the Cloud Foundry platform components. Developers can sign up and focus on writing applications. All requests will be handled by the PaaS layer interfacing with the IaaS layer, where the compute, storage, load-balancing, and scaling services operate. Another popular solution for developing applications in the cloud is Heroku, mentioned in passing earlier. Heroku allows you to create and run hosted applications using a variety of development platforms. Just like the IBM cloud, once the application has been written, Heroku hosts, balances, and auto scales the application as required and sends you a bill for hosting at the end of the month. If you\u2019re dealing with a PaaS provider, remember that programming languages change from time to time; therefore, APIs change as well, and usually without warning. If your developers don\u2019t keep up to date, there can be issues when using a PaaS cloud development platform. Digging into the details on the Heroku website, under \u201cSecurity,\u201d the site states that, \u201cHeroku\u2019s physical infrastructure is hosted and managed within Amazon\u2019s secure data centers and utilize the Amazon Web services technology.\u201d Heroku is owned by another cloud heavyweight, Salesforce. Salesforce indicated in 2018 that future expansion was going to be by utilizing Amazon data center resources. Oh, what a tangled web we weave. An additional reality is that one cloud provider\u2019s PaaS system is not necessarily compatible with another cloud provider\u2019s service. Both AWS and Microsoft Azure offer similar cloud services, but internally each cloud provider operates in a completely different fashion with a completely different set of APIs. There is no single standard for defining just what PaaS must be. Compatibility issues begin to reveal themselves at the lower levels of each vendor\u2019s proposed solution. RESTful interfaces, manifest file formats, framework configurations, external APIs, and component integration are not necessarily compatible across cloud vendors. AWS deals with platform services using Lambda, the API Gateway, and several code deployment tools. The applications that your company may have been developing and using internally will be a variety of two- and three-tier architectures with many local dependencies such as network storage, local storage, local users, and databases. The overall architecture design may have been adequate at the beginning but now is straining to function due to the age of the hardware, the sizing of the hardware, and the lack of any flexibility to change. The distinct difference with on-premise design when compared to hosting applications at AWS is that provisioning hardware and waiting for it to be set up and configured is a thing of the past. In fact, there are many possibilities to consider when designing applications at AWS. Your choice of language and development framework will determine the PaaS vendor you select. Do you do a lot of development in Python? Are you a Java developer? Amazon has a PaaS solution called Elastic Beanstalk that automates the deployment of applications developed in Java, Python, Ruby, and other development platforms on the required infrastructure components for each application including E2 instances or Docker containers, with load-balancing, auto scaling, and monitoring services. Amazon has several development solutions, shown in Figure 1-5, including CodeBuild, CodeCommit, Elastic Beanstalk, CodeDeploy. These can be key components in your application deployment at AWS. Chapter 8, \u201cAutomating AWS Infrastructure,\u201d covers these interesting managed services and additional details on automating your infrastructure. Figure 1-5 Platform options at AWS Essential Characteristics of AWS Cloud Computing If you haven\u2019t heard of National Institute of Standards and Technology (NIST), a branch of the U.S. government, you\u2019re not alone. Around 2010, NIST began documenting the public cloud. After talking to all the major vendors, it released an initial report in June 2011 defining many cloud components that were common across all the public cloud vendors. The report\u2019s genius was in defining what the emerging public cloud actually was (the command components). Over the years, NIST\u2019s cloud definitions have moved from definitions to becoming standards for how many companies view working in the public cloud. According to NIST, five key definitions of the public cloud have really morphed into a definitive standard methodology of operating in the public cloud: On-demand self-service\u2014We not only expect cloud service to be delivered quickly; we demand it. All cloud providers offer a self-serve portal as AWS does, as shown in Figure 1-6. You request a cloud service, and in seconds it\u2019s available in your AWS account ready to configure. Gone are the days of requesting a virtual server via email and waiting several days until it\u2019s built. At AWS, a virtual server can be started and operational in seconds. Procuring a software-defined network at AWS (called a virtual private cloud) is available and operational in seconds. AWS has an expansive self-serve management console that allows you to order and configure many cloud-hosted services in seconds in any AWS region. Any cloud service that you order from AWS is automatically delivered to you through heavily automated procedures. There are no public cloud providers that survive without a self-service portal driven by heavy-duty automation in the background. This NIST definition is now a standard. Figure 1-6 The AWS management portal Broad network access\u2014Cloud services can be accessed from almost anywhere across the globe using the Internet. If you host applications at AWS, perhaps they are public-facing SaaS apps. AWS also provides HTTPS endpoints to access every cloud service hosted at AWS. However, you may not want broad network access, which is defined as public network access to your cloud services. In fact, many companies that are moving to the AWS cloud have no interest in a publicly accessible software solution. They want their hosted cloud services to remain private, accessible only by their employees using private connections. Each cloud customer ultimately defines the real meaning of broad network access. At AWS, applications can be publicly available, or, you can stay completely private. VPN connections from your place of work to AWS are commonplace; in fact, you can order Direct Connect and establish a private fiber connection to AWS running at speeds up to 10 Gbps. Depending on the type of applications you\u2019re using in the cloud, high-speed network access is essential. We can even use, access, and administer AWS service from our phone using AWS apps. Certainly, accessing AWS from any device is possible. For more details on networking, check out Chapter 3, \u201cAWS Networking Services.\u201d Resource Pooling\u2014Infrastructure resources for public cloud providers are pooled together in many data centers across the different regions of the world and are dynamically assigned on demand. A company running an on-premise private cloud would pool its virtual machines, memory, processing, and networking capabilities into one or two data centers, and from its own pool offer limited compute resources. All public cloud providers have a massive pool of resources to serve our various needs. AWS has clusters of data centers (known as AZs or availability zones), and each AZ could have over 80,000 bare-metal servers available and online allowing customers to host their application services with a high level of resiliency and failover. Having many available online resources also enables AWS to keep the price down. Without a massive pool of resources, AWS would not be able to offer its cloud services on demand that are able to scale up and down based on customer demand. Having a massive resource pool is a necessary standard for all public cloud providers; customers do not expect to run out of resources. Take, for example, AWS S3 storage, which is unlimited with no defined maximum limit. For more details on regions and AZs, check out Chapter 2. Rapid Elasticity\u2014Elasticity in the public cloud, or scaling, is the key feature required by all hosted cloud applications. Elasticity at AWS is utilized for both compute and storage. Because most services and applications are built on compute and storage, applications in the AWS cloud have the capability to automatically scale, as shown in Figure 1-7. And elasticity, or scaling, is only useful if it\u2019s automated based on demand. Turning off a virtual server, adding RAM, and turning it back on is not the elasticity that we are interested in; we want horizontal scale\u2014that is, more application servers\u2014not just a bigger server. Real-time monitoring of a hosted cloud application at AWS allows us to react almost instantaneously before the application\u2019s performance is close to degrading. With EC2 Auto Scaling in the background, additional computer resources are automatically ordered and delivered to the application server\u2019s cluster, maintaining the application\u2019s performance. Rapid elasticity based on demand is only possible with real-time monitoring driving automated scale. This is why the public cloud is so popular; with a massive pool of available cloud resources and the ability to automatically scale applications out and in based on demand, at AWS anybody can easily scale application stacks up and down. For more details on deploying scale and elasticity with EC2 Auto Scale, check out Chapter 5, \u201cPlanning for Scale and Resiliency.\u201d Figure 1-7 Applications can scale based on demand in the public cloud Measured Service\u2014In the cloud, you are only billed for what you use; that\u2019s defined as a measured service. Cloud providers make their money by charging for everything that you use in their data centers, including data transfer costs. Packet flow inbound to the public cloud is usually free; outbound packet flow, or traffic between subnets hosted in different data centers, is usually charged an outbound data transfer fee. Charges are per second, or per minute in the case of computer services like AWS EC2 compute instances, or they are per gigabyte per month in the case of storage services like S3 or virtual hard drives, which at AWS are called elastic block storage (EBS). AWS charges can be broken down into compute, storage, and data transfer charges. If an AWS service is on, the meter is running. Cost management is one of your most important jobs when operating in the cloud. AWS has many useful tools to help you control your costs, including the AWS Simple Pricing Calculator, AWS Budgets, and the Cost Explorer, as shown in Figure 1-8. You can find details on these features in Chapter 2. Being billed for consuming cloud services is a reality that we are all used to. What you also may have to get used to is exactly how you are being billed. Again, you must understand and carefully monitor compute, storage, and data transfer costs. For example, you can order a load balancer at AWS for $30 per month. However, there is an additional charge to be aware of: all the data packets transferred through the load balancer are charged, and that by itself can be a hefty price. Figure 1-8 AWS Budgets and Cost Explorer track and alert when costs are over budget Operational Benefits of AWS Operating in the public cloud has certain benefits. Unlimited access to servers and storage and many management services may make it easier than you expected to operate in the cloud. Table 1-1 summarizes the managed services at AWS that may be able to replace or complement your existing on-premise services and procedures. Servers\u2014Underutilized servers in your data center are expensive to run and maintain. Moving applications to the public cloud will reduce the size of your on-premise data center. Because you no longer host as many physical servers, your total hosting costs (heating, cooling, and so on) will be lower as well. You also won\u2019t have to pay for as many software licenses at the processer level because you\u2019re not responsible for running hypervisor services; that\u2019s Amazon\u2019s job. You may think that moving to the AWS cloud means virtualized resources and only virtualization. However, at AWS, you can get a variety of compute options with virtualization of any size and scale, from a single-core CPU with 512MB of RAM to hundreds of CPU cores and terabytes of RAM. You can also order a bare-metal server and do whatever you want with it. You can find further details on compute options in Chapter 4. Storage\u2014Using cloud storage has huge benefits due to the unlimited amount of storage promised by cloud providers. Amazon has many options for storage that are similar, but not exactly the same as your on-premise solutions. For storage area network solutions, Amazon has shareable file solutions: the elastic file system (EFS) for Linux workloads, and FSx, a shared file service specifically for Windows File Server workloads. Virtual hard disks are available using EBS. Unlimited storage, and longer-term archive storage, is provided by S3 and S3 Glacier. Details on all the storage options at AWS can be found in Chapter 6, \u201cCloud Storage.\u201d Managed services\u2014AWS has a variety of managed services, as shown in Table 1-1, that may be able to replace or complement your existing services and utilities currently used on-premise once you move to the AWS cloud. Table 1-1 Managed Services at AWS IT Operations On-Premise AWS Cloud Monitoring Nagios, SolarWinds. CloudWatch monitoring providing metrics for every AWS service. All monitoring and logging data can be stored in S3. All third-party monitoring solutions can access S3 to perform their own custom analysis of log data. Data backup Backup tools such as Commvault and NetBackup. Any third-party vendor that wants to stay in business will be supporting AWS; both Veritas and Commvault have AWS solutions. AWS Storage Gateway can also be installed to cache required content locally, while backing up local disk volumes to an S3 bucket. Backups can be snapshots of local virtual hard disks, or data files from specific volumes can be targeted. Scale Add additional virtual machines or increase/decrease the size of each virtual machine\u2019s RAM and CPU cores. Scale horizontally by placing multiple virtual machines (instances) behind a load balancer and add automated scaling based on demand to increase and decrease the required amount of compute power using EC2 Auto Scaling. Testing Provisioning hardware for testing is expensive. Provisioning resources for short-term testing at AWS is incredibly inexpensive. Signing up for the AWS free tier allows you to test a variety of AWS services for one year completely free. Identity management Active Directory Domain Services for accessing corporate resources. Extend on-premise Active Directory to the AWS cloud with hosted Directory Services. Utilize AWS single sign-on services (SSO) for managing access to popular business applications that third-party cloud providers are hosting. Cloud Provider Limitations Each cloud provider has a published SLA that specifies what services are provided and at what specific operational level. All public cloud providers make promises about how they will handle security, compliance, and overall operations and how their methodology will be contained in the cloud provider\u2019s SLA. The challenge is to live up to that agreement. In the SLA, there will be details about acceptable outage time and the responsibility of the cloud provider when outages occur. There also will be statements about not being responsible for events outside the cloud provider\u2019s control. Another common term typically used in the SLA is \u201cbest effort\u201d or \u201ccommercially reasonable effort.\u201d Regardless of the cloud model, the cloud provider is responsible for overall service operation and deployment, service orchestration, the overall management of the cloud, the security of the cloud components, and maintenance of customer privacy. The responsibility of how each customer, the cloud consumer, is to carry out business with the cloud provider will also be described in some detail in the SLA. Each cloud consumer must fully understand what each cloud service offered provides; this is exactly what the cloud service will and will not do. The reality is that every public cloud provider will not have an SLA that you will like, and the stark reality is that their best effort is the best they can do. This might seem a little harsh, but it\u2019s reality; according to AWS, \u201ceverything fails all the time.\u201d What happens when a key component of your application hosted in the AWS cloud fails? Is it a disaster, or is it manageable? Is it acceptable to expect AWS failures from time to time? It\u2019s a reality; AWS is 100% right; everything fails. Operating in the public cloud means that you must design your hosted application to be able to continue operating even if compute and storage failures occur. That\u2019s our responsibility. All public cloud providers really have the same SLA; here it is, summarized in nine short words: \u201cwe are sorry; we will give you a credit.\u201d This SLA summary applies to every public cloud provider. Here\u2019s another reality check; if you\u2019re down, you will have to prove that you were actually down by providing network traces and appropriate documentation that leaves no doubt that you were down because of an AWS cloud issue. Oh, and here\u2019s another small detail to be aware of: if you didn\u2019t build redundancy into your application design, don\u2019t bother calling for a credit. Application designs that have a single instance hosting the application with no failover or high-availability design parameters have no SLA. AWS expects you to be serious about your application design; we need to understand and use the tools in the AWS toolbox to ensure that your SLA for availability and performance is achieved. Not every service at AWS even has a defined SLA; there are more than 100 services and only 8 defined SLAs. Remember: all managed services\u2014in fact, all services\u2014are built from the resources found in Table 1-2. Table 1-2 SLAs at AWS AWS Service SLA Summary CloudFront 99.9% during any monthly billing cycle DynamoDB Monthly uptime percentage of 99.999% for global tables, or 99.99% for regular tables EC2 instances (includes elastic container service [ECS] and EBS volumes) Monthly uptime percentage of at least 99.99% RDS databases Monthly uptime percentage of at least 99.95% for multi-AZ instances Route 53 DNS service Commercially reasonable efforts to make Route 53 100% available during a monthly billing cycle S3; S3 Glacier object storage The number of errors calculated during each 5-minute period subtracted from 100% Lambda functions Monthly uptime percentage of 99.95% during any monthly billing cycle AWS Shield (Advanced) Any failure of service commitments provided by CloudFront or Route 53 when being protected by AWS Shield Advanced distributed denial of service (DDoS) protection Data Security at AWS We can lose many things while operating in the cloud: instances fail, EBS volumes crash, services stop working. But you can\u2019t go to your boss and say we\u2019ve lost some data. Data security\u2014The reality is that your data is more secure and durable stored in the public cloud. At AWS, except for S3 Glacier archive storage, which is automatically encrypted, all other storage mediums at AWS are unencrypted by default. However, EBS volumes\u2014both boot and data volumes\u2014can be encrypted at rest and at transit using either customer master keys provided by AWS or keys provided by the customer. Shared storage services such as EFS can also be encrypted at rest, as can DynamoDB tables. S3 buckets can be encrypted with keys provided by AWS or supplied by customers, as shown in Figure 1-9. Data durability provides security of a different nature; all data stored in the cloud is stored in multiple locations; EBS volumes are replicated within the data center where they reside. S3 objects are replicated across three separate locations within the selected AWS region, producing a high level of durability. Amazon\u2019s level of S3 durability is humorously defined like this: for every 1,000 objects stored in an S3 bucket, you will lose one of those objects every 10 million years. We cannot possibly duplicate this level of durability and security on-premise. Figure 1-9 S3 buckets can be encrypted using AES-256 or AWS-KMS managed keys Data privacy\u2014AWS does not have data storage isolated for individual customers; all storage arrays at AWS are multitenant in design. This is pretty much the default for all public cloud providers. Amazon\u2019s job is to make sure your stored data records are isolated per AWS account. Data control\u2014Customers are in full control of storing and retrieving their data stored in AWS. All data storage at AWS starts as private, and except for S3 buckets that are changed allowing public access, storage remains private and is not directly accessible from the outside world. Customers can choose to make S3 buckets public; it\u2019s the customer\u2019s responsibility to define the security and accessibility of all data records stored in AWS. Security controls\u2014As previously mentioned, all data records can be encrypted at AWS. Resource policies defining the precise level of security and access can be directly attached to resources such as S3 buckets or EFS shared storage and can be defined by the identity and access management (IAM) user and group security policy using the IAM service. IAM identity and trust policies can be defined at a granular level controlling access by users and roles to all resources at AWS, including any storage medium. Chapter 7, \u201cSecurity Services,\u201d provides details on IAM. You can enable multifactor authentication as an additional security control on S3 buckets to control when deletion of data records is performed. Network Security at AWS At AWS, networking is managed at the subnet level, and all subnets are created as a private subnet with no access to the outside world. Subnets reside on your private networks, which are called a virtual private cloud (VPC) at AWS. Only by adding a gateway service to a VPC will subnets be able to be accessed from either the Internet or a private VPN connection from an on-premise network. Chapter 3 has the details on networking at AWS. It\u2019s important to note that public and private connectivity choices are decisions that are always carried out by each customer; not AWS. Each subnet\u2019s ingress and egress traffic can be controlled by a subnet firewall called Network ACLs that define separate stateless rules for both inbound and outbound packet flow. Each EC2 instance hosted on a subnet is further protected by an additional firewall called a security group, which defines what traffic is allowed into the instance and where outbound traffic is directed. VPC flow logs can be enabled to capture network traffic for the entire VPC, a single subnet, or a network interface. Application Security at AWS Both Web and application servers hosted at AWS should always be located on private subnets. Private subnets are not directly accessible from the Internet. You may be wondering how to access what was supposed to be a public-facing application with no direct public access. The solution to this question is the absolute best practice to follow at AWS: for Web servers that customers across the Internet access, placing the load balancer on a public subnet, in front of the Web servers, provides the correct design solution. Customers requesting access to the application will be directed by DNS to the DNS name of the load balancer. The load balancer directs incoming traffic from the public subnet to the targeted Web servers hosted in the private subnets. One load balancer type offered by AWS is the Application Load Balancer, which can perform authentication and SSL offload services. The end-to-end traffic pattern for a three-tier Web application can be designed using many encryption/decryption points, as shown in Figure 1-10 on its path from source to destination: Web application firewall\u2014A custom traffic filter in front of the Application Load Balancer protecting against malicious traffic. Elastic Load Balancer (ELB)\u2014Accepts only encrypted HTTPS traffic on port 443; provides secure sockets layer/transport layer security (SSL/TLS) decryption and, optionally, user authentication. EC2 instance hosting Web application\u2014EBS boot and data drives can be encrypted. EC2 instance hosting application server\u2014EBS boot and data drives can be encrypted. Database server\u2014EBS boot and data drives and data community can be encrypted, or Dynamo DB tables can be encrypted. Figure 1-10 Encrypted traffic flow at AWS Compliance in the AWS Cloud As a worldwide public cloud provider, AWS operates in many different countries and is subject to a variety of rules and regulations enforced by governments and compliance standards. Depending on the type of business that you operate, there are possibly many different levels of compliance you will have to adhere to when operating in the AWS cloud. Financial, health, and government institutions have strict rules and regulations that must be followed by their clients. In addition, your own company may have specific internal rules and regulations they want to follow. Many countries in the world are enacting laws, regulations, and mandates in serious attempts to protect the privacy of personal data and the security of corporate information and computer systems. The new data protection laws place the burden of protection and security on the custodian of that data; that is where the data is stored when the data is transferred from source to destination. The cloud providers have contractual obligations to ensure that when organizations have data records hosted in their cloud, they can adhere to the promises and commitments made in the SLA. Some of the most common compliance regulations that AWS has been successfully audited against include the compliance standards listed in Table 1-3. Table 1-3 AWS Supports Many Compliance Standards Abbreviation Scope of Operation Purpose of Protection Legal Status HIPPA Healthcare Personal information Law GLBA Financial industry Personal information Law SOX Publicly traded companies Shareholder Law PCI DSS Payment card industry Fraud Industry regulation GDPR EU Personal information Law Health Insurance Portability and Accountability Act\u2014Secures the privacy of individual health information records in the United States. Gramm-Leachy-Billy Act\u2014Mandates protection of customer information by financial industries. Sarbanes-Oxley\u2014Ensures the integrity of financial operations of publicly traded companies. PCI DSS\u2014Ensures the processing integrity of credit card data or authentication data. GDPR\u2014Protects privacy and personal data for all citizens of the EU. Amazon has a decent compliance page at https://aws.amazon.com/compliance/, which has details about all the AWS certifications and attestations that it has achieved or supports. If you are bound by a specific compliance standard, one of your first steps should be to review the AWS services that are available for each compliance standard, as shown in Figure 1-11. Figure 1-11 Check the AWS compliance page to see what services are supported Playing in the AWS Sandbox AWS makes it easy to \u201ctry before you buy,\u201d frequently doling out promotional credits to developers. Even if you are not a developer, every new AWS customer gets limited access to nearly every AWS service for free (Amazon calls this the \u201cfree tier\u201d) during the first year. This is a great way to experiment with AWS. The only thing you must provide is a credit card that won\u2019t be charged unless you choose to use resources that the free tier doesn\u2019t cover. After the first year has passed, you\u2019ll start accruing charges for every service you use; any AWS resources that you built during the first year remain in your account but start accruing charges. In addition, AWS has several free hands-on labs. You can sign up for QwikLabs at https://run.qwiklabs.com/home?locale=en and carry out a variety of AWS tasks in the AWS cloud. Figure 1-12 illustrates some of the learnig and labs that are available from QwikLabs. Figure 1-12 QwikLabs has more than 20 completely free labs for AWS services Running experiments, and performing labs raises additional questions that will help further your AWS cloud knowledge and experience. MAKE SURE TO WATCH THE COMPANION VIDEO \u201cSIGNING UP FOR AWD FREE TIER.\u201d To access the companion videos, register your book at informit.com/register. What\u2019s the Problem That Needs to Be Solved? Typical large organizations run hundreds or thousands of applications on thousands of virtual servers. Which applications can be moved to AWS? What should be prioritized? Start with low value/low risk\u2014It\u2019s quite popular to suggest a starting point of high value and low risk when choosing your first application to move to the AWS cloud. Here\u2019s a reality check: it\u2019s probably going to take you 6 months or longer to move your application to the cloud. Choosing an application with low value provides a valuable timeline to do some additional planning and analysis before finalizing your application in its working form at AWS. I\u2019ve seen many companies make the pronouncement that applications will be moving to the cloud quickly. It rarely happens successfully because there are so many things to learn and consider. Start with low value. Take your time, and select a working application that has been running successfully for a good time period. Then you can document your lessons learned and what to do differently the next time. The second and third application moved to the cloud generally will be much faster than the first application due to the lessons learned and experience gained. Create a brand-new application first\u2014The advantage of creating a completely new application at AWS means you are not constrained by anything, such as the type of database that must be used, the type of programming language that must be used, or the type of compute that must be used. Starting anew at AWS allows you to try out some of the new methods to host applications such as serviceless computing, create a mobile application using stateless components, or use DynamoDB instead of SQL. This is where the real learning about what the AWS cloud can do for you will really appear. Try to solve a single problem\u2014Do you need additional storage? Perhaps that\u2019s a great starting point for your adventure in the cloud. Archiving files in S3 Glacier could be as simple as ordering a Snowball device, connecting it up to your network, filling up with files you\u2019d like to archive, and shipping it back to AWS. This is an excellent first project to start working with AWS support, archiving records, and saving your company money. Define a value proposition\u2014Ideally, the move to AWS is long term and successful. Thousands of companies have been successful moving to AWS; you, too, can be successful. Start off with a defined value proposition that can be validated quickly, in a matter of months rather than years. For developing applications, you could sign up for AWS Cloud9, a cloud-hosted IDE that supports more than 40 programming languages, as shown in Figure 1-13. Armed with a browser, you can try your hand at developing applications at AWS. Figure 1-13 Cloud9 IDE at AWS Access to data records\u2014The number-one problem with larger companies when starting to work with cloud providers is working through the internal politics to allow access to data from the cloud. Data record access, and the steps for successful access, should be considered before you move to the cloud: How can we access our on-premise data from the cloud? What records have to stay on-premise? Are we bound by any compliance rules and regulations? Is our data in the right format for what we need? Migrating Applications For applications that have been chosen as starting candidates to move to the AWS cloud, several decisions need to be made about the application\u2019s journey, or path. Can the application be moved to AWS and hosted on an EC2 instance with no changes? Applications that fit into this category could be migrated to AWS as an EC2 instance image. Server migration tools, and database migration tools discussed in Chapter 2, can carry out these migration paths quite effectively. However, applications that are lifted and shifted to the cloud will have other dependencies and issues that will have to be considered: The application stores its data in a database. Will the database remain on-premise or be moved to the cloud? If the database for the application remains on-premise, are there latency issues that need to be considered when communicating with the database? Will a high-speed connection need to be established between the AWS cloud and the database remaining on-premise? Are there compliance issues regarding the application data? Does the data have to be encrypted at rest? Does communication with the database need to be encrypted? Do users authenticate to the application across the corporate network? If so, are federation services required to be deployed at AWS for single sign-on (SSO)? Are local dependencies installed on the application server that will interfere with the application server\u2019s operation in the AWS cloud? Are there licensing considerations for both the operating system and the application when operating in the cloud? Is there an existing SaaS application hosted by a public cloud provider that should replace the application because it\u2019s a better choice? This can be a very political issue to resolve. With so many hosted cloud applications available in the public cloud, the odds are close to 100% that there will be an existing application that could replace the current on-premise application. Should the application remain on-premise and eventually be deprecated? The application is hosted on legacy hardware that is near end-of-life. The application is not virtualized. The application does not have support. The application is used by a small number of users. The Well-Architected Framework Several years ago, AWS introduced documentation called the Well-Architected Framework to help customers plan properly when moving to the AWS cloud. The goal was to give guidance for cloud architects to build secure, resilient, and decent performing infrastructure to host their applications following recognized best practices that have been developed over time by the experience of many AWS customers. Each best practice still must be evaluated as to whether it meets your criteria. A best practice should not be blindly adopted without understanding why it has achieved a best practice designation. The documentation for the well-architected framework also has many key questions to ponder that can be found in the well-architected framework blueprint. It is useful to discuss these questions out loud with other technical folks in your company; they will help you make key decisions about your infrastructure and applications hosted at AWS. The framework documentation can be found here: https://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf. Each application to be deployed at AWS needs to be viewed through the lens of being well architected following these five principles: Operational excellence\u2014How best to execute, deploy, and monitor applications running at AWS using automated deployment monitoring procedures, continuous improvement, and automated solutions for recovering from failures. Key AWS services to utilize include CloudWatch events and alarms, CloudTrail, EC2 Auto Scaling, AWS Config, and the Trusted Advisor. Check out Chapters 5, 7, and 8. Operational excellence questions to consider include these: How are disruptions to applications handled? Manually, or automatically? How can you analyze the ongoing health of your applications and infrastructure components hosted at AWS? Security\u2014How to best design systems that will operate reliably and securely while protecting customer information and data records. Key AWS services to utilize include IAM, AWS Organizations, CloudWatch logs, CloudTrail events, S3 and S3 Glacier, and VPC flow logs. Check out Chapters 3, 6, and 7. Security questions to consider include these: How are security credentials and authentication managed at AWS? How are automated procedures secured? Reliability\u2014How can systems and applications hosted at AWS recover from disruption with minimal downtime? How can applications meet your escalating demands? Key AWS services to utilize include ELB, EC2 Auto Scaling, and CloudWatch alarms. Check out Chapter 5. Reliability questions to consider include these: How do you monitor resources hosted at AWS? How do applications hosted at AWS adapt to changes in demand by end users? Performance efficiency\u2014How to use compute resources to meet and maintain your application requirements on an ongoing basis. Should your compute solution change from EC2 instances to containers or serviceless? Key services include EC2 Auto Scaling, EBS volumes, and RDS. Check out Chapters 4 and 6. Performance efficiency questions to consider include these: Why did you select your database? Why did you select your current compute infrastructure? Cost Optimization\u2014How to design systems that meet your needs at the cheapest price point. Key AWS services include Cost Explorer, Budgets, EC2 Auto Scaling, Trusted Advisor, and the Simple Monthly Calculator. Check out Chapters 2, 5, and 7. Cost optimization questions to consider are as follows: How do you oversee usage and cost? How do you meet cost targets? Are you aware of current data transfer charges based on your AWS designs?","title":"Marco de buena arquitectura"},{"location":"apuntes/arquitecturas01.html#aws-trusted-advisor","text":"","title":"AWS Trusted Advisor"},{"location":"apuntes/arquitecturas01.html#gestion-del-escalado-y-la-monitorizacion","text":"","title":"Gesti\u00f3n del escalado y la monitorizaci\u00f3n"},{"location":"apuntes/arquitecturas01.html#elastic-load-balancing","text":"","title":"Elastic Load Balancing"},{"location":"apuntes/arquitecturas01.html#amazon-cloudwatch","text":"","title":"Amazon CloudWatch"},{"location":"apuntes/arquitecturas01.html#amazon-ec2-auto-scaling","text":"","title":"Amazon EC2 Auto Scaling"},{"location":"apuntes/arquitecturas01.html#actividades","text":"Si nos basamos en una arquitectura Lambda, clasifica los siguientes elementos: Si nos basamos en una arquitectura Kappa, clasifica los siguientes elementos: Realizar los m\u00f3dulos 9 (Arquitectura en la nube) y 10 (Monitoreo y escalado autom\u00e1tico) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/arquitecturas01.html#referencias","text":"Arquitectura Big Data: \u00bfen qu\u00e9 consiste y para qu\u00e9 se utiliza? Big Data Lambda Architecture - Nathan Marz What Is Lambda Architecture? Arquitectura Lambda vs Arquitectura Kappa https://luminousmen.com/post/modern-big-data-architectures-lambda-kappa/ https://medium.com/dataprophet/4-big-data-architectures-data-streaming-lambda-architecture-kappa-architecture-and-unifield-d9bcbf711eb9","title":"Referencias"},{"location":"apuntes/nube01.html","text":"Cloud Computing \u00b6 La Nube \u00b6 Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet que hace que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software. Ventajas \u00b6 As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto. Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx. CapEx vs OpEx \u00b6 Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan costo previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costos asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las ventajas, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten. Coste total de propiedad \u00b6 El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memor\u00eda): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para contruir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa. Servicios en la nube \u00b6 Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo. IaaS \u00b6 La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas. PaaS \u00b6 La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos. SaaS \u00b6 Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa. Tipos de arquitectura seg\u00fan la infraestructura \u00b6 Arquitecturas on premise \u00b6 Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware. Arquitecturas cloud \u00b6 Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente. Arquitecturas h\u00edbridas \u00b6 Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible. Plataformas Cloud \u00b6 En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: TODO: revisar Infraestructura cloud \u00b6 Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas. Regiones y Zonas de disponibilidad \u00b6 A lo largo de todo el globo terr\u00e1queo, se han construido grandes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ). Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita) Ubicaciones de borde \u00b6 Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. Se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Para ello, despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional), a trav\u00e9s de 90 ciudades en 47 paises. Utiliza Amazon Route 53, el cual es un DNS interno que redirige el tr\u00e1fico a los nodos Cloudfront. M\u00e1s informaci\u00f3n en https://aws.amazon.com/es/cloudfront/ . Despliegue \u00b6 Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions Actividades \u00b6 A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realizar los m\u00f3dulos 1 (Informaci\u00f3n general sobre los conceptos de la nube) y 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . \u00bfcalculadora de costes? Referencias \u00b6 Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure","title":"1.- Cloud Computing"},{"location":"apuntes/nube01.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"apuntes/nube01.html#la-nube","text":"Ya hemos visto que la industria 4.0 incluye el Big Data y la computaci\u00f3n en la nube como uno de los elementos principales de su transformaci\u00f3n. El Cloud Computing permite obtener servicios de computaci\u00f3n a trav\u00e9s de internet que hace que s\u00f3lo se pague por los recursos que usa y en el momento en que los necesita. Dicho de otro modo, es la entrega bajo demanda de potencia de c\u00f3mputo, bases de datos, almacenamiento, aplicaciones y otros recursos inform\u00e1ticos, a trav\u00e9s de Internet con un sistema de precios de pago por uso. Los modelos productivos basados en la adquisici\u00f3n de hardware de manera propietaria ha quedado atr\u00e1s, al implicar un proceso largo y costoso de compra de licencias, recursos f\u00edsicos como oficinas y equipamiento y recursos humanos (tanto t\u00e9cnicos como de seguridad) para su implantaci\u00f3n, gesti\u00f3n y mantenimiento. As\u00ed pues, plantea un cambio de perspectiva. La infraestructura se deja de considerar hardware para verla (y usarla) como software.","title":"La Nube"},{"location":"apuntes/nube01.html#ventajas","text":"As\u00ed pues, los beneficios que ofrece la nube son: Alta disponibilidad , dada su naturaleza de recursos distribuidos. Escalabilidad : Si un usuario necesita m\u00e1s o menos capacidad de proceso o de almacenamiento, el proveedor se lo facilitar\u00e1 pr\u00e1cticamente en tiempo real. Tolerancia a fallos , ya que ofrecen una arquitectura de respaldo de copias de seguridad y a prueba de ataques. Elasticidad : de la misma manera que podemos escalar, podemos reducir los requisitos y buscar soluciones m\u00e1s econ\u00f3micas. Alcance global : cualquier usuario autorizado puede acceder o actualizar informaci\u00f3n desde cualquier lugar del mundo, en cualquier momento y mediante cualquier dispositivo. Agilidad : Permite amoldar los recursos al crecimiento de la empresa/proyecto. Capacidades de latencia del cliente , pudiendo elegir c\u00f3mo de cerca se despliegan las aplicaciones. C\u00e1lculo de costes de manera predictiva , siguiendo un modelo basado en el consumo. S\u00f3lo se paga por los recursos que se utilizan, para ello se proporciona el precio de cada recurso por hora. Una de las ventajas m\u00e1s interesante para las empresas puede que sea la reducci\u00f3n de los costes, ya que no necesitamos instalar ning\u00fan tipo de hardware ni software, ni pagar por las actualizaciones futuras en t\u00e9rminos de ese hardware y software que ya no vamos a necesitar o que se ha quedado corto para nuestras necesidades. En relaci\u00f3n con los costes, es conveniente aclarar dos conceptos relacionados con la contabilidad y las finanzas: CapEx y OpEx.","title":"Ventajas"},{"location":"apuntes/nube01.html#capex-vs-opex","text":"Hay dos tipos diferentes de gastos que se deben tener en cuenta: La inversi\u00f3n de capital ( CapEx , Capital Expenditure ) hace referencia a la inversi\u00f3n previa de dinero en infraestructura f\u00edsica, que se podr\u00e1 deducir a lo largo del tiempo. El coste previo de CapEx tiene un valor que disminuye con el tiempo. Los gastos operativos ( OpEx , Operational Expenses ) son dinero que se invierte en servicios o productos y se factura al instante. Este gasto se puede deducir el mismo a\u00f1o que se produce. No hay ning\u00fan costo previo, ya que se paga por un servicio o producto a medida que se usa. As\u00ed pues, si nuestra empresa es due\u00f1a de su infraestructura, comprar\u00e1 equipos que se incluir\u00e1n como recursos en su balance de cuentas. Dado que se ha realizado una inversi\u00f3n de capital, los contables clasifican esta transacci\u00f3n como CapEx. Con el tiempo, a fin de contabilizar la duraci\u00f3n \u00fatil limitada de los activos, estos se deprecian o se amortizan. Los servicios en la nube, por otro lado, se clasifican como OpEx debido a su modelo de consumo. Si nuestra empresa utiliza la nube, no tiene ning\u00fan recurso que pueda amortizar, y su proveedor de servicios en la nube (AWS / Azure) administra los costos asociados con la compra y la vida \u00fatil del equipo f\u00edsico. En consecuencia, los gastos de explotaci\u00f3n tienen un impacto directo en el beneficio neto, la base imponible y los gastos asociados en el balance contable. En resumen, CapEx requiere unos costes financieros previos considerables, as\u00ed como unos gastos continuos de mantenimiento y soporte t\u00e9cnico. En cambio, OpEx es un modelo basado en el consumo, y los gastos se deducen en el mismo a\u00f1o. As\u00ed pues, la inform\u00e1tica en la nube es un modelo basado en el consumo, lo que significa que los usuarios finales solo pagan por los recursos que usan. Lo que usan es lo que pagan. Volviendo a las ventajas, los modelos basados en el consumo y OpEx aportan una serie de ventajas: Sin costes por adelantado. No es necesario comprar ni administrar infraestructuras costosas que es posible que los usuarios no aprovechen del todo. Se puede pagar para obtener recursos adicionales cuando se necesiten. Se puede dejar de pagar por los recursos que ya no se necesiten.","title":"CapEx vs OpEx"},{"location":"apuntes/nube01.html#coste-total-de-propiedad","text":"El coste total de propiedad (CTO) es la estimaci\u00f3n financiera que ayuda a identificar los costes directos e indirectos de un sistema. Permite comparar el coste de ejecutar una infraestructura completa o una carga de trabajo espec\u00edfica en las instalaciones del cliente frente a hacerlo en la nube. Los elementos a considerar sobre el coste total de propiedad son: Cuando migramos a una soluci\u00f3n en la nube, por ejemplo AWS, los \u00fanicos costes que deberemos pagar son: Costes de computaci\u00f3n (procesador, memor\u00eda): se factura por horas o por segundos (s\u00f3lo m\u00e1quinas Linux) Costes de almacenamiento: se factura por GB Costes de transferencia de datos: se factura por GB de salida (excepto casos excepcionales, los datos de entrada no se facturan) As\u00ed pues, es necesario presupuestar y desarrollar casos de negocio para migrar a la nube y ver si son viables para nuestra organizaci\u00f3n. Para ello, podemos utilizar la calculadora de costes que ofrecen las plataformas cloud: AWS: https://calculator.aws y en concreto en https://calculator.s3.amazonaws.com/index.html Azure: https://azure.microsoft.com/es-es/pricing/tco/calculator/ Google Cloud: https://cloud.google.com/products/calculator?hl=es Estas calculadoras permiten: Calcular los costes mensuales. Identificar oportunidades para reducir los costes mensuales. Utilizar plantillas para comparar servicios y modelos de implementaci\u00f3n. La realidad es que el coste de desplegar y utilizar las aplicaciones en la nube es menor cada vez que se a\u00f1ade un gasto. Sin embargo, operar en la nube realmente abarata los costes cuando automatizamos los procesos y los servicios se dise\u00f1an para trabajar en la nube, es decir, la mayor\u00eda de servicios no se ejecutan 24x7, sino que se detienen o reducen en tama\u00f1o cuando no son necesarios. As\u00ed pues, los proveedores cloud utilizan procesos automatizados para contruir, gestionar, monitorizan y escalar todos sus servicios. Esta automatizaci\u00f3n de los procesos nos permitir\u00e1n ahorrar dinero e irnos el fin de semana tranquilos a casa.","title":"Coste total de propiedad"},{"location":"apuntes/nube01.html#servicios-en-la-nube","text":"Los servicios en la nube son servicios que se utilizan a trav\u00e9s de Internet, eliminando las limitaciones de nuestros equipos. Su principal ventaja es que su CapEx es 0, ya que no necesita ning\u00fan tipo de inversi\u00f3n inicial ni contrato a largo plazo.","title":"Servicios en la nube"},{"location":"apuntes/nube01.html#iaas","text":"La infraestructura como servicio ( Infraestructure as a Service ) proporciona a las empresas recursos inform\u00e1ticos, incluyendo servidores, m\u00e1quinas virtuales, redes, almacenamiento y espacio en centro de datos con pago en funci\u00f3n del uso. Los elementos que forman parte de IaaS son: Servidores y almacenamiento. Firewall y seguridad en red. Planta f\u00edsica o edificio del centro de datos. Se contrata el hardware y el cliente es el responsable de la instalaci\u00f3n y mantenimiento del software que corre en dichas m\u00e1quinas, as\u00ed como configurar la red, el almacenamiento y el control de acceso. Configurar una m\u00e1quina virtual nueva es considerablemente m\u00e1s r\u00e1pido que adquirir, instalar y configurar un servidor f\u00edsico. Adem\u00e1s, permite escalar la intraestructura bajo demanda para dar soporte a las cargas de trabajo din\u00e1micas.","title":"IaaS"},{"location":"apuntes/nube01.html#paas","text":"La plataforma como servicio ( Platform as a Service ) proporciona un entorno basado en cloud con todos los requisitos necesarios para dar soporte a todo el ciclo de vida de creaci\u00f3n y puesta en marcha de aplicaciones basadas en web (cloud), sin el coste y la complejidad de comprar y gestionar el hardware, software, aprovisionamiento y alojamiento necesario. Los elementos que forman parte de PaaS son todos los de IaaS m\u00e1s: Sistema operativo Herramientas de desarrollo, administraci\u00f3n de bases de datos, an\u00e1lisis empresarial, etc... Este enfoque acelera el desarrollo y la comercializaci\u00f3n de aplicaciones, ya que desplegar una nueva aplicaci\u00f3n es cuesti\u00f3n de minutos. El cliente no necesita administrar la infraestructura subyacente. El provedor cloud gestiona el sistema operativo, la implementaci\u00f3n de parches a la base de datos, la configuraci\u00f3n del firewall y la recuperaci\u00f3n de desastres. De esta manera, el cliente puede centrarse en la administraci\u00f3n de c\u00f3digo o datos.","title":"PaaS"},{"location":"apuntes/nube01.html#saas","text":"Finalmente, las aplicaciones basadas en cloud, o software como servicio ( Sofware as a Service ), se ejecutan en sistemas en la nube que no tienen porque residir en la misma m\u00e1quina ni en la misma red. Estos servicios pertenecen y los administran otras empresas a las cuales el cliente se conecta a trav\u00e9s de Internet y, por lo general, de un navegador web. As\u00ed pues, podemos considerar SaaS como aplicaciones hospedadas y utilizables dentro de un PaaS. Respecto al usuario, cuenta con una licencia seg\u00fan un modelo de suscripci\u00f3n o de pago por uso y no necesitan administrar la infraestructura que respalda el servicio. Por ello, SaaS permite iniciar sesi\u00f3n y empezar r\u00e1pidamente a utilizar las aplicaciones desde el minuto 0. Si el sistema fallase, no se pierden datos, ya que al estar en el cloud hay copias de seguridad continuas y al ser tolerante a fallos y el\u00e1stico, el servicio permite escalar din\u00e1micamente en funci\u00f3n de las necesidades de uso. Cada uno de estos tipos de servicios implican en mayor o menor medida al usuario, compartiendo la responsabilidad de cada \u00e1rea entre el proveedor cloud y el usuario. \u00bfQu\u00e9 es la inform\u00e1tica sin servidor / Serverless computing ? Igual que PaaS, la inform\u00e1tica sin servidor permite que los desarrolladores creen aplicaciones m\u00e1s r\u00e1pidamente, ya que elimina la necesidad de administrar la infraestructura. En las aplicaciones sin servidor, el proveedor de servicios en la nube aprovisiona, escala y administra autom\u00e1ticamente la infraestructura necesaria para ejecutar el c\u00f3digo. Las arquitecturas sin servidor son muy escalables y controladas por eventos , y solo usan recursos cuando se produce una funci\u00f3n o un desencadenador concretos. Es importante tener en cuenta que los servidores siguen ejecutando el c\u00f3digo. El t\u00e9rmino \"sin servidor\" procede del hecho de que las tareas asociadas a la administraci\u00f3n y el aprovisionamiento de la infraestructura son invisibles para el desarrollador. Este enfoque permite a los desarrolladores centrar su atenci\u00f3n en la l\u00f3gica de negocio y ofrecer m\u00e1s valor al n\u00facleo de la empresa.","title":"SaaS"},{"location":"apuntes/nube01.html#tipos-de-arquitectura-segun-la-infraestructura","text":"","title":"Tipos de arquitectura seg\u00fan la infraestructura"},{"location":"apuntes/nube01.html#arquitecturas-on-premise","text":"Tambi\u00e9n conocido como in-house es la arquitectura cl\u00e1sica, en la que la empresa adquiere el hardware por adelantado. De esta manera, las empresas tienen el control total sobre los recursos y la seguridad, pero tambi\u00e9n la responsabilidad respecto a su mantenimiento y actualizaci\u00f3n del hardware.","title":"Arquitecturas on premise"},{"location":"apuntes/nube01.html#arquitecturas-cloud","text":"Son aquellas donde los recursos se virtualizan y no son propiedad de la empresa, sino que se pueden aprovisionar y quitar bajo las necesidades de cada momento. S\u00f3lo se paga por lo que se consume. A su vez, podemos distinguirlas entre: Nube p\u00fablica : los recursos virtualizados se comparten de forma p\u00fablica y entre varios clientes a la vez, permitiendo el acceso via internet. Los clouds p\u00fablicos pertenecen y son administrados por proveedores que ofrecen a trav\u00e9s de una red p\u00fablica acceso r\u00e1pido a recursos inform\u00e1ticos asequibles. Nube privada : los recursos virtualizados son privados, mediante un cluster dedicado para el cliente, normalmente mediante una conexi\u00f3n privada, ya sea de fibra propia o una VPN. Este tipo de nube lo utiliza \u00fanicamente una \u00fanica organizaci\u00f3n, ya sea gestionada internamente o por terceros y alojada internamente o externamente.","title":"Arquitecturas cloud"},{"location":"apuntes/nube01.html#arquitecturas-hibridas","text":"Brindan gran flexibilidad, ya que las empresas deciden donde se ejecutan sus aplicaciones, ya sea en su propia infraestructura in-house o con servicios cloud. De esta manera, controlan la seguridad y el cumplimiento de los requisitos legales de sus aplicaciones. Un cloud h\u00edbrido utiliza una base de cloud privado combinada con la integraci\u00f3n y el uso de servicios cloud p\u00fablicos. En realidad, un cloud privado no puede existir aislado del resto de los recursos TIC de una empresa ni del cloud p\u00fablico. La mayor\u00eda de las empresas con clouds privados evolucionan para gestionar cargas de trabajo en todos los centros de datos (privados y p\u00fablicos) creando as\u00ed clouds h\u00edbridos. Normalmente, las aplicaciones cr\u00edticas y los datos confidenciales se mantienen en el cloud privado, dejando el cloud p\u00fablico para las aplicaciones m\u00e1s recientes y la infraestructura IaaS para obtener recursos virtuales de forma flexible.","title":"Arquitecturas h\u00edbridas"},{"location":"apuntes/nube01.html#plataformas-cloud","text":"En la actualidad existen multitud de proveedores que ofrecen servicios en la nube clasificados de acuerdo al modelo de servicio. A continuaci\u00f3n nombramos los m\u00e1s conocidos y m\u00e1s utilizados. Los proveedores cloud de nube p\u00fablica m\u00e1s importantes son: Amazon, con Amazon Web Services ( https://aws.amazon.com/es/ ): Amazon fue el primer proveedor cloud, pionero y con mayor crecimiento. AWS proporciona una plataforma confiable en la nube que utilizan miles de empresa en todo el mundo. Microsoft, con Azure ( https://aws.amazon.com/es/ ): Ha realizado una fuerte inversi\u00f3n en los \u00faltimos a\u00f1os y es la plataforma cloud con mayor crecimiento. Ofrece servicios en las tres capas, no s\u00f3lo en IaaS, sino tambi\u00e9n PaaS y SaaS. Google, con Google Cloud ( https://cloud.google.com ): Google tambi\u00e9n es un proveedor de nube p\u00fablica mediante su plataforma Google Cloud Platform (GCP) . Le cost\u00f3 entrar en este \u00e1rea, pero en los \u00faltimos a\u00f1os ha crecido mucho y actualmente es ampliamente utilizada por grandes compa\u00f1\u00edas. En el caso de nube privada, destacar a OpenStack ( https://www.openstack.org ). Se trata de un proyecto de software de infraestructura de computaci\u00f3n en la nube, es de c\u00f3digo abierto y es uno de los proyectos open source m\u00e1s activos del mundo. Si entramos a ejemplos concretos para cada tipo de servicio en la nube tenemos: Tipo de Servicio Proveedor Descripci\u00f3n IaaS AWS EC2 M\u00e1quinas virtuales en Amazon, con procesdor, memoria y almacenamiento a medida Azure y sus m\u00e1quina virtuales Igual pero en Azure Google Cloud Platform Igual pero en Google PaaS AWS RDS, AWS Lambda Base de datos, funciones serverless Google App Engine Alojamiento y despliegue web Heroku Plataforma que permite el despliegue de aplicaciones en la nube SaaS Microsoft Office 365 Paquete ofim\u00e1tico de Microsoft en la nube Aplicaciones web de Google Correo electr\u00f3nico, calendario, fotos Trello, Notion, GitHub Tableros Kanban, gesti\u00f3n de tareas, repositorio de c\u00f3digo fuente Herramientas DevOps relacionadas Aunque se salen del \u00e1mbito del curso de IABD, es conveniente conocer algunas herramientas asociadas a perfiles DevOps como: Terraform ( https://www.terraform.io/ ): Facilita la definici\u00f3n, aprovisionamiento y orquestaci\u00f3n de servicios mediante un lenguaje declarativo. Ansible ( https://www.ansible.com/ ): Permite centralizar la configuraci\u00f3n de numerosos servidores, dispositivos de red y proveedores cloud de una forma sencilla y automatizada. Docker ( https://www.docker.com/ ): Permite la creaci\u00f3n de contenedores a modo de m\u00e1quinas virtuales ligeras, donde se instalan los servicios/recursos necesairos. Kubernetes (K8s) ( https://kubernetes.io/es/ ): Orquesta los contenedores para facilitar el despliegue, la supervisi\u00f3n de servicios, el reemplazo, el escalado autom\u00e1tico y la administraci\u00f3n de los servicios. Facilita la portabilidad de contenedores a la nube. En Octubre de 2020, el informe de Synergy Cloud Market Growth Rate Nudges Up as Amazon and Microsoft Solidify Leadership permite observar el predominio de Amazon seguido del crecimiento de la plataforma Azure: TODO: revisar","title":"Plataformas Cloud"},{"location":"apuntes/nube01.html#infraestructura-cloud","text":"Las diferentes plataformas cloud ofrecen una infraestructura dividida en regiones y zonas.","title":"Infraestructura cloud"},{"location":"apuntes/nube01.html#regiones-y-zonas-de-disponibilidad","text":"A lo largo de todo el globo terr\u00e1queo, se han construido grandes centros de datos que se conocen como regiones . Estas regiones son zonas geogr\u00e1ficas, y dentro de cada una de ellas hay diferentes grupo de centros de datos l\u00f3gicos que se conocen como zonas de disponibilidad (AZ - Availability Zone ). Normalmente cada regi\u00f3n contiene 3 o m\u00e1s zonas de disponibilidad. Dicho de otro modo, cada regi\u00f3n consta de varias zonas de disponibilidad aisladas y separadas f\u00edsicamente dentro de un \u00e1rea geogr\u00e1fica. Cada zona de disponibilidad tiene alimentaci\u00f3n, refrigeraci\u00f3n y seguridad f\u00edsica independientes y est\u00e1 conectada a trav\u00e9s de redes redundantes de latencia ultrabaja. Si seguimos desgranando, cada zona de disponibilidad contiene al menos 3 centros de datos, y cada centro de datos suele albergar entre 50.000 y 80.000 servidor f\u00edsicos. Si hacemos c\u00e1lculos podemos ver que una regi\u00f3n puede incluir varios cientos de miles de servidores. Las zonas de disponibilidad permiten que los clientes trabajen con bases de datos y aplicaciones de producci\u00f3n con un nivel de disponibilidad, tolerancia a errores y escalabilidad mayor que el que ofrecer\u00eda un centro de datos \u00fanico. Tolerancia a fallos La soluci\u00f3n ideal es replicar los datos y la aplicaci\u00f3n en varias zonas de disponibilidad de una regi\u00f3n, y posteriormente, replicarlos a su vez entre diferentes regiones. Las AZ est\u00e1n f\u00edsicamente separadas entre s\u00ed por una distancia significativa de muchos kil\u00f3metros, aunque todas est\u00e1n dentro de un rango de 100 km de separaci\u00f3n. La replicaci\u00f3n de datos entre regiones y zonas de disponibilidad es responsabilidad del cliente. La elecci\u00f3n de una regi\u00f3n se basa normalmente en los requisitos de conformidad o en la intenci\u00f3n de reducir la latencia. Cuanto m\u00e1s cerca est\u00e9 la regi\u00f3n de los clientes finales, m\u00e1s r\u00e1pido ser\u00e1 su acceso. AWS Academy Dentro de AWS Academy siempre vamos a trabajar dentro de la regi\u00f3n us-east-1 , correspondiente al Norte de Virginia (es la regi\u00f3n asignada tambi\u00e9n a la capa gratuita)","title":"Regiones y Zonas de disponibilidad"},{"location":"apuntes/nube01.html#ubicaciones-de-borde","text":"Las ubicaciones de borde y las cach\u00e9s de borde regionales mejoran el rendimiento almacenando en cach\u00e9 el contenido lo m\u00e1s cerca de los usuarios para reducir la latencia al m\u00ednimo. Se trata de un CDN ( Content Delivery Network ) que se utiliza para distribuir el contenido (datos, v\u00eddeos, aplicaciones y API) a los usuarios finales. Para ello, despliega m\u00e1s de 225 puntos de presencia (m\u00e1s de 215 ubicaciones de borde y 13 cach\u00e9s de nivel medio regional), a trav\u00e9s de 90 ciudades en 47 paises. Utiliza Amazon Route 53, el cual es un DNS interno que redirige el tr\u00e1fico a los nodos Cloudfront. M\u00e1s informaci\u00f3n en https://aws.amazon.com/es/cloudfront/ .","title":"Ubicaciones de borde"},{"location":"apuntes/nube01.html#despliegue","text":"Por ejemplo, en el siguiente gr\u00e1fico podemos ver las 25 regiones que tiene AWS que incluyen 81 zonas de disponibilidad (se puede observar como la regi\u00f3n en Espa\u00f1a est\u00e1 en proceso de implantaci\u00f3n): Pod\u00e9is consultar el mapa interactivo de: AWS en https://aws.amazon.com/es/about-aws/global-infrastructure/ (y las regiones en https://aws.amazon.com/es/about-aws/global-infrastructure/regions_az/ ) Azure en https://infrastructuremap.microsoft.com/explore . Google Cloud en https://cloud.google.com/about/locations#regions","title":"Despliegue"},{"location":"apuntes/nube01.html#actividades","text":"A lo largo de este bloque, vamos a trabajar con AWS como plataforma Cloud. Para ello, es necesario activar una cuenta educativa. En breve, recibir\u00e9is un email para daros de alta y poder realizar las actividades. As\u00ed pues, esta actividad consiste en la creaci\u00f3n de la cuenta de AWS y la realizaci\u00f3n del m\u00f3dulo 0 (Introducci\u00f3n al curso). Realizar los m\u00f3dulos 1 (Informaci\u00f3n general sobre los conceptos de la nube) y 2 (Facturaci\u00f3n y econom\u00eda de la nube) del curso ACF de AWS . \u00bfcalculadora de costes?","title":"Actividades"},{"location":"apuntes/nube01.html#referencias","text":"Azure Fundamentals AZ-900 FAQ Google Cloud vs AWS en 2021 Conceptos fundamentales de Azure","title":"Referencias"},{"location":"apuntes/nube02.html","text":"Amazon Web Services \u00b6 Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones. Servicios \u00b6 Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados: Almacenamiento \u00b6 Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de archivos de Network File System(NFS) el\u00e1stico escalable y completamente administrado para su uso con los servicios en la nube de AWS y los recursos en las instalaciones. Est\u00e1 dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo costo para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Almacenamiento en AWS . Inform\u00e1tica / Computaci\u00f3n \u00b6 Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento, compatible con los contenedores Docker. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de mantenimiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores. S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada) Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS. Amazon Fargate : motor inform\u00e1tico para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Computaci\u00f3n en AWS . Bases de Datos \u00b6 Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 6.- Datos en AWS . Redes \u00b6 Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones e interfaces de programaci\u00f3n de aplicaciones (API) de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( in-house ) a una \u00fanica gateway. Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS. Seguridad en AWS \u00b6 Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones que se permiten en sus cuentas.Amazon Cognito le permite incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea selectos. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS. Servicios de administraci\u00f3n de costes \u00b6 Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y costo de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS le permite definitir presupuestos personalizados que generar\u00e1n una alerta cuando los costos o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costos y el uso de AWS a lo largo del tiempo.Formaci\u00f3n y certificaci\u00f3n de AWS Administraci\u00f3n y gobernanza de datos \u00b6 La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que lo ayuda a realizar un seguimiento del inventario de recursos y sus cambios. Amazon CloudWatch : permite monitorear recursos y aplicaciones. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : lo ayuda a optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar sus cargas de trabajo. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla: Redes en AWS \u00b6 TODO: REVISAR Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , en la nube nos ahorraremos gran parte de la complejidad. Amazon VPC \u00b6 Amazon Virtual Private Cloud Explicar VPC, tablas de enrutamiento, gateway y NAT gateway Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard, la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. Amazon Virtual Private Cloud (Amazon VPC) le permite lanzar recursos de Amazon Web Services (AWS) en la red virtual que usted defina. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00eda en su propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS. Puede crear una VPC que abarque varias zonas de disponibilidad. Una gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. NAT gateway. Despu\u00e9s de crear una VPC, puede agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. El asistente tambi\u00e9n crear\u00e1 una Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Ahora, configurar\u00e1 las subredes privadas para dirigir el tr\u00e1fico orientado hacia Internet a la gateway NAT a fin de que los recursos de la subred privada puedan conectarse a Internet, a la vez que mantienen los recursos privados. Esto se realiza mediante la configuraci\u00f3n de una tabla de enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se coocan de m\u00e1s a menos restrictivas. crear\u00e1 un grupo de seguridad de VPC, que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Puede agregar reglas a cada grupo de seguridad que permitan el tr\u00e1fico hacia las instancias asociadas o desde ellas. Un p\u00e1rrafo de Route 53 (DNS) Un p\u00e1rrafo de Cloudfront (CDN) https://www.josemariagonzalez.es/amazon-web-services-aws/todo-lo-que-deberias-saber-sobre-las-redes-virtuales-en-amazon-aws.html Seguridad en la Nube \u00b6 La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero. Modelo de responsabilidad compartida de AWS \u00b6 La seguridad es una caracter\u00edstica que tien una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general. Responsabilidad de AWS \u00b6 AWS es responsable de proteger la infraestructura en la que se ejecutan todos los servicios ofrecidos por la nube de AWS (en algunas preguntas de la certificaci\u00f3n se refieren a ellos por servicios de la nube): Seguridad f\u00edsica de los centros de datos con acceso controlado basado en las necesidades en instalaciones sin identificaci\u00f3n, con guardias de seguridad permanentes, autenticaci\u00f3n de dos factores, revisi\u00f3n y registro de accesos, videovigilancia, y destrucci\u00f3n y desmagnetizaci\u00f3n de discos. Infraestructura de hardware , como servidores, dispositivos de almacenamiento y otros dispositivos de los que dependen los servicios de AWS. Infraestructura de software ,que aloja sistemas operativos, aplicaciones de servicios y software de virtualizaci\u00f3n. Infraestructura de red , como routers, conmutadores, balanceadores de carga, firewalls y cables. AWS tambi\u00e9n monitorea la red en l\u00edmites externos, protege los puntos de acceso y proporciona infraestructura redundante con detecci\u00f3n de intrusiones de forma constante Responsabilidad del cliente \u00b6 El cliente es responsable del cifrado de los datos en reposo y los datos en tr\u00e1nsito, de todo lo que se pone en la nube. Los pasos de seguridad que debe tomar depender\u00e1n de los servicios que utilice y de la complejidad del sistema. Si entramos en m\u00e1s detalle, es responsable de: El sistema operativo de la instancia de Amazon EC2 : incluidas las actualizaciones, los parches de seguridad y su mantenimiento. La protecci\u00f3n de las aplicaciones que se lanzan en los recursos AWS: contrase\u00f1as, acceso basado en roles, etc. Configuraci\u00f3n del grupo de seguridad . SO o firewalls basados en host: incluidos los sistemas de detecci\u00f3n o prevenci\u00f3n de intrusiones. Configuraciones de red . Administraci\u00f3n de cuentas : Configuraci\u00f3n de inicio de sesi\u00f3n y credenciales para cada usuario. Respecto al contenido cr\u00edtico, el cliente es responsable de administrar: El contenido que eligen almacenar en AWS. Los servicios de AWS que se utilizan con el contenido. En qu\u00e9 pa\u00eds se almacena ese contenido. El formato y la estructura de ese contenido y si est\u00e1 enmascarado, cifrado o es an\u00f3nimo. Qui\u00e9n tiene acceso a ese contenido y c\u00f3mo se conceden, administran y revocan esos derechos de acceso. AWS IAM \u00b6 AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz. Autenticaci\u00f3n \u00b6 Cuando se define un usuario de IAM se indica qu\u00e9 tipo de acceso puede utilizar el usuario para obtener acceso a los recursos de AWS: acceso mediante programaci\u00f3n: mediante email y clave de acceso secreta cuando realice una llamada a la API de AWS mediante la CLI de AWS, el SDK de AWS o cualquier otra herramienta de desarrollo. acceso a la consola de administraci\u00f3n de AWS: mediante usuario / contrase\u00f1a m\u00e1s el ID/alias de cuenta. Es recomendable activar MFA ( Multi-Factor Authentication ) para a\u00f1adir una capa m\u00e1s de seguridad. acceso mediante ambos tipos Autorizaci\u00f3n \u00b6 Una vez que el usuario se ha autenticado, se ha de determinar qu\u00e9 permisos debe concederse a un usuario, servicio o aplicaci\u00f3n. De forma predeterminada, los usuarios de IAM no tienen permiso para obtener acceso a los recursos o los datos en una cuenta de AWS. En su lugar, debe conceder permisos de forma expl\u00edcita a un usuario, grupo o rol mediante la creaci\u00f3n de una pol\u00edtica de IAM, ya que por defecto, se denegar\u00e1n todas las acciones que no se hayan permitido expl\u00edcitamente. Consejo Seguir el principio de m\u00ednimo privilegio: conceder \u00fanicamente los privilegios de usuario m\u00ednimos que necesita el usuario. El alcance de las configuraciones del servicio de IAM es global, se aplican en todas las regiones de AWS. Pol\u00edticas IAM \u00b6 Una pol\u00edtica de IAM es una instrucci\u00f3n formal mediante un documento JSON con los permisos que se conceder\u00e1 a una entidad. Las entidad es incluyen usuarios, grupos, roles o recursos. Las pol\u00edticas especifican cu\u00e1les son las acciones permitidas, cu\u00e1les son los recursos a los que estas tienen permiso y cu\u00e1l ser\u00e1 el efecto cuando el usuario solicite acceso a los recursos. Info Una sola pol\u00edtica se puede asociar a varias entidades. Una sola entidad puede tener varias pol\u00edticas asociadas a ella. Hay dos tipos de pol\u00edticas de IAM: pol\u00edticas basadas en identidad : controlan qu\u00e9 acciones puede realizar dicha identidad, en qu\u00e9 recursos y en qu\u00e9 condiciones. A su vez se dividen en administradas (asociada a varios usuarios/grupos/roles) o insertadas (un \u00fanico usuario/grupo/rol). pol\u00edticas basadas en recursos : son documentos de pol\u00edtica JSON que se asocia a un recurso (por ejemplo, un bucket de S3). Estas pol\u00edticas controlan qu\u00e9 acciones puede realizar una entidad principal especificada en dicho recurso y en qu\u00e9 condiciones. Destacar que no todos los servicios de AWS soportan este tipo de pol\u00edticas. Pol\u00edticas y permisos \u00b6 El usuario solo podr\u00e1 realizar la acci\u00f3n si la acci\u00f3n solicitada no est\u00e1 denegada de forma expl\u00edcita y est\u00e1 permitida de forma expl\u00edcita Cuando IAM determina si se concede un permiso, primero comprueba la existencia de cualquier pol\u00edtica de denegaci\u00f3n expl\u00edcita aplicable. Si no existe ninguna denegaci\u00f3n expl\u00edcita, comprueba si existe alguna pol\u00edtica de permisos expl\u00edcitos aplicable. Si no existe una pol\u00edtica de denegaci\u00f3n expl\u00edcita ni de permiso expl\u00edcito, IAM vuelve a la forma predeterminada, que consiste en denegar el acceso. Este proceso se denomina denegaci\u00f3n impl\u00edcita . Otros servicios relacionados con la seguridad AWS Organizations : Permite configurar los permisos de una organizaci\u00f3n que contiene varias cuentas de usuario en unidades organizativas (UO), y unificar tanto la seguridad como la facturaci\u00f3n AWS Key Management Service (AWS KMS): servicio que permite crear y administrar claves de cifrado Amazon Cognito : permite controlar el acceso a recursos de AWS desde aplicaciones con una credencial \u00fanica mediante SAML. AWS Shield : servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones ejecutadas en AWS. AWS CLI \u00b6 AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema opertivo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola vocareum , tras clickar en el bot\u00f3n azul de Acount Details podr\u00e9is ver los datos de acceso temporales en la ventana Credentials . Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno: export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias. Actividades \u00b6 Realizar los m\u00f3dulos 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) y 4 (Seguridad en la nube) del curso ACF de AWS . Realizar el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS . Referencias \u00b6 Overview of Amazon Web Services","title":"2.- AWS"},{"location":"apuntes/nube02.html#amazon-web-services","text":"Amazon Web Services ofrece un conjunto de servicios que funcionan a modo de piezas de un puzzle, de manera que uniendo unos con otros podemos dise\u00f1ar la arquitectura necesaria para nuestras aplicaciones.","title":"Amazon Web Services"},{"location":"apuntes/nube02.html#servicios","text":"Los servicios de AWS se clasifican en categor\u00edas: A continuaci\u00f3n vamos a comentar las categor\u00edas m\u00e1s importantes junto a algunos de sus servicios m\u00e1s destacados:","title":"Servicios"},{"location":"apuntes/nube02.html#almacenamiento","text":"Los servicios que ofrece AWS para gestionar el almacenamiento de datos son: Amazon Simple Storage Service ( Amazon S3 ): servicio de almacenamiento de objetos que ofrece escalabilidad, disponibilidad de datos, seguridad y rendimiento. Se utiliza para almacenar y proteger cualquier cantidad de datos para sitios web, aplicaciones m\u00f3viles, copias de seguridad y restauraci\u00f3n, archivado, aplicaciones empresariales, dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de bigdata. Amazon Elastic Block Store ( Amazon EBS ): almacenamiento en bloque de alto rendimiento dise\u00f1ado para utilizarse con Amazon EC2 para cargas de trabajo que hacen un uso intensivo de transacciones y de rendimiento. Se utiliza para una amplia gama de cargas de trabajo, como bases de datos relacionales y no relacionales, aplicaciones empresariales, aplicaciones en contenedores, motores de an\u00e1lisis de bigdata, sistemas de archivos y flujos de trabajo multimedia Amazon Elastic File System ( Amazon EFS ): proporciona un sistema de archivos de Network File System(NFS) el\u00e1stico escalable y completamente administrado para su uso con los servicios en la nube de AWS y los recursos en las instalaciones. Est\u00e1 dise\u00f1ado para escalar a petabytes bajo demanda, y aumenta y reduce su tama\u00f1o autom\u00e1ticamente a medida que se agregan y se eliminan archivos. Reduce la necesidad de aprovisionar y administrar capacidad para admitir el crecimiento Amazon Simple Storage Service Glacier ( Amazon S3 Glacier ): es un tipo de almacenamiento en la nube de Amazon S3 seguro, duradero y de muy bajo costo para archivar datos y realizar copias de seguridad a largo plazo. Est\u00e1 dise\u00f1ado para ofrecer una durabilidad del 99,999999999% y proporcionar capacidades integrales de seguridad y conformidad que permiten cumplir requisitos normativos estrictos Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 5.- Almacenamiento en AWS .","title":"Almacenamiento"},{"location":"apuntes/nube02.html#informatica-computacion","text":"Los servicios que ofrece AWS relativos a la inform\u00e1tica o computaci\u00f3n son: Amazon Elastic Compute Cloud ( Amazon EC2 ): proporciona capacidad inform\u00e1tica de tama\u00f1o ajustable en forma de m\u00e1quinas virtuales en la nube Amazon EC2 Auto Scaling : permite agregar o eliminar autom\u00e1ticamente instancias EC2 de acuerdo con las condiciones que defina. Amazon Elastic Container Service ( Amazon ECS ): servicio de organizaci\u00f3n de contenedores altamente escalable y de gran rendimiento, compatible con los contenedores Docker. Amazon EC2 Container Registry ( Amazon ECR ): registro de contenedores Docker completamente administrado que facilita las tareas de mantenimiento, administraci\u00f3n e implementaci\u00f3n de im\u00e1genes de contenedores Docker . Amazon Elastic Beanstalk : servicio para implementar y escalar aplicaciones y servicios web en servicios web conocidos, como Apache o IIS. AWS Lambda : permite ejecutar c\u00f3digo sin necesidad de aprovisionar ni administrador servidores. S\u00f3lo se paga por el tiempo de computaci\u00f3n (cuando el c\u00f3digo no se ejecuta, no se paga nada) Amazon Elastic Kubernetes Service ( Amazon EKS ): facilita la implementaci\u00f3n, administraci\u00f3n y el escalado de aplicaciones en contenedores que utilizan Kubernetes en AWS. Amazon Fargate : motor inform\u00e1tico para ECS que permite ejecutar contenedores sin tener que administrar servidores ni cl\u00fasteres. Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 4.- Computaci\u00f3n en AWS .","title":"Inform\u00e1tica / Computaci\u00f3n"},{"location":"apuntes/nube02.html#bases-de-datos","text":"Los servicios que ofrece AWS para gestionar los datos son: Amazon Relational Database Service ( Amazon RDS ): facilita las tareas de configuraci\u00f3n, operaci\u00f3n y escalado de una base de datos relacional en la nube. El servicio ofrece capacidad de tama\u00f1o ajustable al mismo tiempo que automatiza tareas administrativas que demandan mucho tiempo, como el aprovisionamiento de hardware, la configuraci\u00f3n de bases de datos, la implementaci\u00f3n de parches y la creaci\u00f3n de copias de seguridad Amazon Aurora : es una base de datos relacional compatible con MySQL/MariaDB y PostgreSQL. Amazon vende que es hasta cinco veces m\u00e1s r\u00e1pida que las bases de datos MySQL est\u00e1ndar y tres veces m\u00e1s r\u00e1pida que las bases de datos PostgreSQL est\u00e1ndar. Amazon Redshift : es un servicio de datawarehouse que permite ejecutar consultas anal\u00edticas de petabytes de datos almacenados localmente en Amazon Redshift, adem\u00e1s de ejecutar consultas anal\u00edticas de exabytes de datos almacenados en Amazon S3 de forma directa. Ofrece un rendimiento r\u00e1pido a cualquier escala. Amazon DynamoDB : es una base de datos de documentos y clave-valor que ofrece un rendimiento de milisegundos de un solo d\u00edgito a cualquier escala, con seguridad integrada, copias de seguridad y restauraci\u00f3n, y almacenamiento en cach\u00e9 en memoria. Estos servicios los veremos en mayor profundidad en la sesi\u00f3n 6.- Datos en AWS .","title":"Bases de Datos"},{"location":"apuntes/nube02.html#redes","text":"Los servicios que ofrece AWS para gestionar las redes son: Amazon Virtual Private Cloud ( Amazon VPC ): permite aprovisionar secciones aisladas de forma l\u00f3gica de la nube de AWS. Elastic Load Balancing : distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones en varios destinos, tales como instancias de Amazon EC2, contenedores, direcciones IP y funciones Lambda. Amazon CloudFront : servicio r\u00e1pido de red de entrega de contenido (CDN) que suministra datos, videos, aplicaciones e interfaces de programaci\u00f3n de aplicaciones (API) de manera segura a clientes de todo el mundo, con baja latencia y altas velocidades de transferencia. AWS Transit Gateway : servicio que permite a los clientes conectar sus nubes privadas virtuales de Amazon (VPC) y sus redes en las instalaciones ( in-house ) a una \u00fanica gateway. Amazon Route 53 : servicio web de DNS escalable y en la nube dise\u00f1ado para direccionar a los usuarios finales a las aplicaciones de Internet de una forma confiable. AWS Direct Connect : ofrece una manera de establecer una conexi\u00f3n de red privada dedicada desde un centro de datos u oficina a AWS, lo que puede reducir los costes de red y aumentar el rendimiento del ancho de banda. AWS VPN : proporciona un t\u00fanel privado seguro desde una red o dispositivo a la red global de AWS.","title":"Redes"},{"location":"apuntes/nube02.html#seguridad-en-aws","text":"Los servicios que ofrece AWS para gestionar la seguridad, identidad y conformidad son: AWS Identity and Access Management ( IAM ): le permite administrar el acceso a los recursos y servicios de AWS de manera segura. Con IAM, puede crear y administrar usuarios y grupos de AWS. Puede utilizar los permisos de IAM para permitir y denegar el acceso de usuarios y grupos a los recursos de AWS. AWS Organizations : permite restringir los servicios y acciones que se permiten en sus cuentas.Amazon Cognito le permite incorporar control de acceso, inscripci\u00f3n e inicio de sesi\u00f3n de usuarios a sus aplicaciones web y m\u00f3viles. AWS Artifact proporciona acceso bajo demanda a los informes de seguridad y conformidad de AWS y a los acuerdos en l\u00ednea selectos. AWS Key Management Service ( AWS KMS ): permite crear y administrar claves. Puede utilizar AWS KMS para controlar el uso del cifrado en una amplia gama de servicios de AWS y en sus aplicaciones. AWS Shield : es un servicio administrado de protecci\u00f3n contra ataques de denegaci\u00f3n de servicio distribuidos (DDoS) que protege las aplicaciones que se ejecutan en AWS.","title":"Seguridad en AWS"},{"location":"apuntes/nube02.html#servicios-de-administracion-de-costes","text":"Los servicios que ofrece AWS para administrar los costes son: Informe de uso y coste de AWS contiene el conjunto m\u00e1s completo de datos de uso y costo de AWS disponibles e incluye metadatos adicionales sobre los servicios, los precios y las reservas de AWS. Presupuestos de AWS le permite definitir presupuestos personalizados que generar\u00e1n una alerta cuando los costos o el uso superen, o se prev\u00e9 que superen, el importe presupuestado. AWS Cost Explorer cuenta con una interfaz sencilla que permite visualizar, comprender y administrar los costos y el uso de AWS a lo largo del tiempo.Formaci\u00f3n y certificaci\u00f3n de AWS","title":"Servicios de administraci\u00f3n de costes"},{"location":"apuntes/nube02.html#administracion-y-gobernanza-de-datos","text":"La consola de administraci\u00f3n de AWS proporciona una interfaz de usuario basada en la web que permite obtener acceso a su cuenta de AWS. Los servicios que ofrece AWS para administrar y gobernar los datos son: AWS Config : proporciona un servicio que lo ayuda a realizar un seguimiento del inventario de recursos y sus cambios. Amazon CloudWatch : permite monitorear recursos y aplicaciones. AWS Auto Scaling : ofrece caracter\u00edsticas que permiten escalar varios recursos para satisfacer la demanda. Interfaz de l\u00ednea de comandos de AWS ( AWS CLI ) proporciona una herramienta unificada para administrar los servicios de AWS. AWS TrustedAdvisor : lo ayuda a optimizar el rendimiento y la seguridad. AWS Well-Architected Tool : ayuda a revisar y mejorar sus cargas de trabajo. AWS CloudTrail : realiza un seguimiento de la actividad de los usuarios y del uso de la API. Por ejemplo, haciendo usos de esos servicios se puede mostrar una soluci\u00f3n sencilla:","title":"Administraci\u00f3n y gobernanza de datos"},{"location":"apuntes/nube02.html#redes-en-aws","text":"TODO: REVISAR Suponemos que los conceptos de red, subred y direcci\u00f3n IP y el modelo de la OSI est\u00e1n claros. Dentro de AWS se utiliza el m\u00e9todo CIDR para describir redes, por ejemplo, 192.0.2.0/24 (los primeros 24 bits son est\u00e1ticos, y los \u00faltimos 8 flexibles). Muchos de los conceptos de redes f\u00edsicas son validos para las redes cloud , en la nube nos ahorraremos gran parte de la complejidad.","title":"Redes en AWS"},{"location":"apuntes/nube02.html#amazon-vpc","text":"Amazon Virtual Private Cloud Explicar VPC, tablas de enrutamiento, gateway y NAT gateway Siempre hay una VPC predeterminada. Muchas de las configuraciones se pueden realizar mediante el asistente de VPC Wizard, la cual facilita la creaci\u00f3n de arquitecturas de red v\u00e1lidas para soluciones cloud e h\u00edbridas. Amazon Virtual Private Cloud (Amazon VPC) le permite lanzar recursos de Amazon Web Services (AWS) en la red virtual que usted defina. Esta red virtual se asemeja en gran medida a una red tradicional que ejecutar\u00eda en su propio centro de datos, con los beneficios de utilizar la infraestructura escalable de AWS. Puede crear una VPC que abarque varias zonas de disponibilidad. Una gateway de Internet (IGW) es un componente de la VPC que permite la comunicaci\u00f3n entre instancias de la VPC e Internet. NAT gateway. Despu\u00e9s de crear una VPC, puede agregar subredes. Cada subred est\u00e1 ubicada por completo dentro de una zona de disponibilidad y no puede abarcar otras zonas. Si el tr\u00e1fico de una subred se direcciona a una gateway de Internet, la subred recibe el nombre de subred p\u00fablica. Si una subred no dispone de una ruta a la gateway de Internet, recibe el nombre de subred privada. El asistente tambi\u00e9n crear\u00e1 una Gateway NAT, que se utiliza para proporcionar conectividad a Internet a instancias EC2 en las subredes privadas. Ahora, configurar\u00e1 las subredes privadas para dirigir el tr\u00e1fico orientado hacia Internet a la gateway NAT a fin de que los recursos de la subred privada puedan conectarse a Internet, a la vez que mantienen los recursos privados. Esto se realiza mediante la configuraci\u00f3n de una tabla de enrutamiento. Una tabla de enrutamiento contiene un conjunto de reglas llamadas rutas que se utilizan para determinar el destino del tr\u00e1fico de red. Cada subred de una VPC debe estar asociada a una tabla de enrutamiento, que es la que controla el direccionamiento de la subred. Las reglas de las tablas de enrutamiento se coocan de m\u00e1s a menos restrictivas. crear\u00e1 un grupo de seguridad de VPC, que act\u00faa como un firewall virtual. Cuando se lanza una instancia, se asocia uno o varios grupos de seguridad a ella. Puede agregar reglas a cada grupo de seguridad que permitan el tr\u00e1fico hacia las instancias asociadas o desde ellas. Un p\u00e1rrafo de Route 53 (DNS) Un p\u00e1rrafo de Cloudfront (CDN) https://www.josemariagonzalez.es/amazon-web-services-aws/todo-lo-que-deberias-saber-sobre-las-redes-virtuales-en-amazon-aws.html","title":"Amazon VPC"},{"location":"apuntes/nube02.html#seguridad-en-la-nube","text":"La capacidad de proteger la integridad y la confidencialidad de los datos es esencial. Un agujero de seguridad puede tirar a la basura todo nuestro trabajo y hacer perder a la empresa el prestigio y much\u00edsimo dinero.","title":"Seguridad en la Nube"},{"location":"apuntes/nube02.html#modelo-de-responsabilidad-compartida-de-aws","text":"La seguridad es una caracter\u00edstica que tien una responsabilidad compartida entre AWS y el cliente. Este modelo de responsabilidad compartida est\u00e1 dise\u00f1ado para minimizar la carga operativa del cliente, pero a\u00fan as\u00ed sigue siendo responsable de algunos aspectos de la seguridad general.","title":"Modelo de responsabilidad compartida de AWS"},{"location":"apuntes/nube02.html#aws-iam","text":"AWS Identity and Access Management (IAM) permite administrar el acceso a los recursos de AWS (de inform\u00e1tica, almacenamiento, base de datos, ...). IAM se puede utilizar para gestionar la autenticaci\u00f3n y para especificar y aplicar pol\u00edticas de autorizaci\u00f3n para especificar qu\u00e9 usuarios pueden obtener acceso a cada servicio. Es decir, permite definir qui\u00e9n, a qu\u00e9 y c\u00f3mo se accede a los recursos AWS. Los principales componentes son: Usuario : persona o aplicaci\u00f3n que se puede autenticar en AWS. Cada usuario debe tener un nombre \u00fanico (sin espacios en el nombre) dentro de la cuenta de AWS y un conjunto de credenciales de seguridad que no se comparte con otros usuarios. Estas credenciales son diferentes de las credenciales de seguridad de usuario ra\u00edz de la cuenta de AWS. Cada usuario est\u00e1 definido en una \u00fanica cuenta de AWS. Grupo : conjunto de usuarios de IAM, a los que se les concede una autorizaci\u00f3n id\u00e9ntica. As\u00ed pues, permite asociar las mismas pol\u00edticas a varios usuarios de una manera sencilla. Hay que tener en cuenta que: Un grupo puede contener muchos usuarios y un usuario puede pertenecer a varios grupos. Un grupo solo puede contener usuarios y, a su vez, un grupo no puede contener otros grupos. No hay ning\u00fan grupo predeterminado que incluya autom\u00e1ticamente a todos los usuarios de la cuenta de AWS. Pol\u00edtica de IAM : documento que define permisos para determinar lo que los usuarios pueden hacer en la cuenta de AWS. Una pol\u00edtica normalmente concede acceso a recursos determinados y especifica lo que el usuario puede hacer con esos recursos, aunque tambi\u00e9n pueden denegar expl\u00edcitamente el acceso. Rol : herramienta para conceder acceso temporal a recursos de AWS espec\u00edficos de una cuenta de AWS. Un rol de IAM puede tener asociadas pol\u00edticas de permisos y se puede utilizar para delegar acceso temporal a usuarios o aplicaciones. Dicho de otro modo, un rol de IAM es similar a un usuario, ya que es una identidad de AWS con pol\u00edticas de permisos que establecen qu\u00e9 puede hacer o no la identidad en AWS. Sin embargo, en lugar de estar asociada \u00fanicamente a una persona, el objetivo es que pueda asignarse un rol a cualquier persona que lo necesite. Tambi\u00e9n es conveniente destacar que cuando se asume un rol, se proporcionan credenciales de seguridad temporales para la sesi\u00f3n de rol, de manera que es conveniente utilizar roles para delegar el acceso a usuarios, aplicaciones o servicios que normalmente no tendr\u00edan acceso a los recursos de AWS. Veremos el uso de roles en la configuraci\u00f3n de la creaci\u00f3n de instancias EC2 . Consejo Es recomendable crear una cuenta de usuario IAM por separado con privilegios administrativos en lugar de utilizar el usuario de la cuenta ra\u00edz.","title":"AWS IAM"},{"location":"apuntes/nube02.html#aws-cli","text":"AWS permite el acceso mediante la consola para administrar todos los servicios. Primero hemos de instalar la herramienta AWS CLI ( https://aws.amazon.com/es/cli/ ) que facilita la administraci\u00f3n de los productos de AWS desde un terminal. Antes de continuar, comprueba que no tengas una versi\u00f3n antigua instalada: aws --version Nos centraremos en su versi\u00f3n 2, la cual es la m\u00e1s reciente. Versi\u00f3n 2 Si tienes instalada la versi\u00f3n 1, es recomendable desinstalarla e instalar la versi\u00f3n 2. Para su instalaci\u00f3n, dependiendo del sistema opertivo que utilicemos, tenemos diferentes instaladores en https://docs.aws.amazon.com/es_es/cli/latest/userguide/install-cliv2.html El siguiente paso ser\u00e1 validarse en AWS. Para ello, desde nuestra consola vocareum , tras clickar en el bot\u00f3n azul de Acount Details podr\u00e9is ver los datos de acceso temporales en la ventana Credentials . Esos datos los podemos pegar en el archivo ~/.aws/credentials o exportarlos como variables de entorno: export AWS_ACCESS_KEY_ID=ASDFEJEMPLO export AWS_SECRET_ACCESS_KEY=asdfClaveEjemplo export AWS_SESSION_TOKEN=asdfr...<resto del token de seguridad> Para comprobar que todo ha ido bien, mediante aws sts get-caller-identity podremos ver nuestro id de usuario. Una vez configurado nuestro usuario, mediante aws ec2 describe-instances podremos obtener informaci\u00f3n sobre nuestras instancias.","title":"AWS CLI"},{"location":"apuntes/nube02.html#actividades","text":"Realizar los m\u00f3dulos 3 (Informaci\u00f3n general sobre la infraestructura global de AWS) y 4 (Seguridad en la nube) del curso ACF de AWS . Realizar el m\u00f3dulo 5 (Redes y entrega de contenido) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube02.html#referencias","text":"Overview of Amazon Web Services","title":"Referencias"},{"location":"apuntes/nube03.html","text":"Servicios de computaci\u00f3n en la nube \u00b6 Introducci\u00f3n \u00b6 Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio. Amazon EC2 \u00b6 Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual. Paso 1: AMI \u00b6 Una imagen de Amazon Machine (AMI) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde. Tipos de AMI \u00b6 Puede elegir entre los siguientes tipos de AMI: Quick Start : AWS ofrece una serie de AMI predise\u00f1adas, tanto Linux como Windows, para lanzar las instancias. Mis AMI : estas son las AMI que hemos creado nosotros. AWS Marketplace : cat\u00e1logo que incluye miles de soluciones de software creadas por empresas terceras. Estas AMI pueden ofrecer casos de uso espec\u00edficos para que pueda ponerse en marcha r\u00e1pidamente. AMI de la comunidad : estas son AMI creadas por personas de todo el mundo.AWS no controla estas AMI, as\u00ed que deben utilizarse bajo la propia responsabilidad, evitando su uso en entornos corporativos o de producci\u00f3n. Las AMI se crean a partir de una instancia EC2. Si queremos crear una AMI propia, podemos importar una m\u00e1quina virtual para que se convierta en una instancia EC2 y, luego guardar la instancia EC2 como una AMI. O partir de una AMI existente, modificarla conforme a nuestras necesidades y luego crear la nueva AMI. Las AMI dependen de la regi\u00f3n Las AMI que creamos se hacen en la regi\u00f3n en la que estamos conectados. Si la necesitamos en otra regi\u00f3n, debemos realizar un proceso de copia. Paso 2: Tipo de instancias \u00b6 El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino. Categor\u00edas \u00b6 Las categor\u00edas de tipos de instancia incluyen instancias de uso general, optimizadas para inform\u00e1tica, optimizadas para memoria, optimizadas para almacenamiento y de inform\u00e1tica acelerada. Categor\u00eda Tipo de instancia Caso de uso Uso general a1, m4, m5, t2, t3 Amplio Computaci\u00f3n c4, c5 Alto rendimiento Memoria r4, r5 , x1, z1 Big Data Inform\u00e1tica acelerada f1, g3, g4, p2, p3 Machine Learning Almacenamiento d2, h1, i3 Sistemas de archivos distribuidos Tipos de instancias \u00b6 Los tipos de instancias ( https://aws.amazon.com/es/ec2/instance-types/ ) ofrecen familias, generaciones y tama\u00f1os . As\u00ed pues, el tipo de instancia t3.large referencia a la familia T , de la tercera generaci\u00f3n y con un tama\u00f1o large . En general, los tipos de instancia que son de una generaci\u00f3n superior son m\u00e1s potentes y ofrecen una mejor relaci\u00f3n calidad/precio. Comparando tipos de instancias Cuando se comparan los tama\u00f1os hay que examinar la parte del coeficiente en la categor\u00eda de tama\u00f1o. Por ejemplo, una instancia t3.2xlarge tiene el doble de CPU virtual y memoria que una t3.xlarge . A su vez, la instancia t3.xlarge tiene el doble de CPU virtual y memoria que una t3.large . Tambi\u00e9n se debe tener en cuenta que el ancho de banda de red tambi\u00e9n est\u00e1 vinculado al tama\u00f1o de la instancia de Amazon EC2. Si ejecutar\u00e1 trabajos que requieren un uso muy intensivo de la red, es posible que deba aumentar las especificaciones de la instancia para que satisfaga sus necesidades. Paso 3: Configuraci\u00f3n de la instancia / red \u00b6 El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita. Asociar un rol de IAM \u00b6 Si necesitamos que nuestras instancias EC2 ejecuten una aplicaci\u00f3n que debe realizar llamadas seguras de la API a otros servicios de AWS, en vez de dejar anotadas las credenciales en el c\u00f3digo de la aplicaci\u00f3n (esto es una muy mala pr\u00e1ctica que puede acarrear problemas de seguridad), debemos asociar un rol de IAM a una instancia EC2. El rol de IAM asociado a una instancia EC2 se almacena en un perfil de instancia . Si creamos el rol desde esta misma pantalla, AWS crear\u00e1 un perfil de instancia autom\u00e1ticamente y le otorgar\u00e1 el mismo nombre que al rol. En el desplegable la lista que se muestra es, en realidad, una lista de nombres de perfiles de instancia. Cuando definimos un rol que una instancia EC2 puede utilizar, estamos configurando qu\u00e9 cuentas o servicios de AWS pueden asumir dicho rol, as\u00ed como qu\u00e9 acciones y recursos de la API puede utilizar la aplicaci\u00f3n despu\u00e9s de asumir el rol. Si cambia un rol, el cambio se extiende a todas las instancias que tengan el rol asociado. La asociaci\u00f3n del rol no est\u00e1 limitada al momento del lanzamiento de la instancia, tambi\u00e9n se puede asociar un rol a una instancia que ya exista. Script de datos de usuario \u00b6 Al momento de crear las instancias EC2, de forma opcional, podemos especificar un script de datos de usuario durante el lanzamiento de la instancia. Los datos de usuario pueden automatizar la finalizaci\u00f3n de las instalaciones y las configuraciones durante el lanzamiento de la instancia. Por ejemplo, un script de datos de usuario podr\u00eda colocar parches en el sistema operativo de la instancia y actualizarlo, recuperar e instalar claves de licencia de software, o instalar sistemas de software adicionales. Script en Windows Si nuestra instancia es de Windows, el script de datos de usuario debe escribirse en un formato que sea compatible con una ventana del s\u00edmbolo del sistema (comandos por lotes) o con Windows PowerShell. De forma predeterminada, los datos de usuario s\u00f3lo se ejecutan la primera vez que se inicia la instancia. Paso 4: Almacenamiento \u00b6 Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias). Paso 5: Etiquetas \u00b6 Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera, permiten clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso. Paso 6: Grupo de seguridad \u00b6 Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad. Paso 7: An\u00e1lisis e identificaci\u00f3n \u00b6 El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente, continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que .la clave privada que la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc... Uso de la consola \u00b6 Tambi\u00e9n puede lanzar instancias EC2 mediante programaci\u00f3n, ya sea a trav\u00e9s de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS) o uno de los kits de desarrollo de software (SDK) de AWS.En el comando de la CLI de AWS de ejemplo, ver\u00e1 un solo comando que especifica la informaci\u00f3n m\u00ednima necesaria para lanzar una instancia. El comando incluye la siguiente informaci\u00f3n: aws: especifica la invocaci\u00f3n de la utilidad de l\u00ednea de comandos aws. ec2: especifica la invocaci\u00f3n del comando del servicioec2. run-instances: es el subcomando que se invoca.El resto del comando especifica varios par\u00e1metros, entre los que se incluyen los siguientes: image-id: este par\u00e1metro va seguido de un ID de AMI. Todas las AMI tienen un ID de \u00fanico.count: puede especificar m\u00e1s de una instancia. instance-type: puede especificar el tipo de instancia que se crear\u00e1, como una instancia c3.large.key-name: en el ejemplo, supongamos que MyKeyPairya existe. security-groups: en este ejemplo, supongamos que MySecurityGroupya existe. region: las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. El comando deber\u00eda crear la instancia EC2 correctamente si suceden los siguientes supuestos:El comando tiene el formato correcto.Los recursos que el comando necesita ya existen.Cuenta con los permisos necesarios para ejecutar el comando.Tiene capacidad suficiente en la cuenta de AWS.Si el comando se ejecuta correctamente, la API responde al comando con el ID de la instancia y otros datos importantes para que la aplicaci\u00f3n los utilice en las solicitudes a la API posteriores. 45 Ciclo de vida de las instancias \u00b6 Este es el ciclo de vida de una instancia. Las flechas muestran las acciones que puede realizar, y las casillas indican el estadoque tendr\u00e1 la instancia despu\u00e9s de dicha acci\u00f3n. Las instancias pueden tener uno de los siguientes estados: Pending(pendiente): cuando se lanza una instancia por primera vez desde una AMI o cuando se activa una instancia detenida, esta pasa al estado pending cuando arranca y se implementa en un equipo de alojamiento. El tipo de instancia que especific\u00f3 durante el lanzamiento determinar\u00e1 el hardware del equipo de alojamiento para la instancia. Running(en ejecuci\u00f3n): cuando ya arranc\u00f3 la instancia por completo y est\u00e1 lista, sale del estado pending y pasa al estado running. Puede conectarse a trav\u00e9s de Internet a la instancia en ejecuci\u00f3n. Rebooting(reiniciada): AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de invocar una acci\u00f3n de reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo nombre DNS p\u00fablico y la misma direcci\u00f3n IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shuttingdown(en proceso de cierre):este es un estado intermedio entre running y terminated. Terminated(terminada): las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se elimine la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping(en proceso de detenci\u00f3n): las instancias que cuentan con el respaldo de Amazon EBS se pueden detener. Entran en el estado stoppingantes de alcanzar el estado stopped por completo. Stopped(detenida):una instancia en el estado stopped no generar\u00e1 los mismos costos que una instancia en el estado running. Si se inicia una instancia en el estado stopped, esta vuelve al estado pending y se traslada a una nueva m\u00e1quina de alojamiento. 47 Algunas instancias que cuentan con el respaldo de Amazon EBS admiten la hibernaci\u00f3n. Cuando pone una instancia a hibernar, el sistema operativo invitado guarda el contenido de la memoria de la instancia (RAM) en el volumen ra\u00edz de Amazon EBS. Cuando reinicie la instancia, el volumen ra\u00edz se restaurar\u00e1 al estado anterior, se volver\u00e1 a cargar el contenido de la RAM y se reanudar\u00e1n los procesos que estaban en ejecuci\u00f3n anteriormente en la instancia.Solo algunas AMI de Linux que cuentan con el respaldo de Amazon EBS y determinados tipos de instancias admiten la hibernaci\u00f3n. La hibernaci\u00f3n tambi\u00e9n requiere el cifrado del volumen ra\u00edz de EBS. Adem\u00e1s, se debe habilitar la hibernaci\u00f3n con el lanzamiento inicial de la instancia. No puede habilitarla en una instancia existente que no ten\u00eda habilitada esta funci\u00f3n originalmente. Para obtener m\u00e1s informaci\u00f3n sobre los requisitos previos y el costo, consulte Hiberne su instancia de Linux en la p\u00e1gina de documentaci\u00f3n de AWS. 48 De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco (5) direcciones IP el\u00e1sticas por regi\u00f3n porque las direcciones p\u00fablicas de Internet (IPv4) son un recurso p\u00fablico escaso. Sin embargo, este l\u00edmite no es fijo, y se puede solicitar un aumento del l\u00edmite (que podr\u00eda aprobarse). 50 Metadatos de las instancias \u00b6 Los metadatos de la instancia son datos sobre la instancia. Puede verlos mientras est\u00e9 conectado a la instancia. Para acceder a ellos en un navegador, dir\u00edjase a la siguiente direcci\u00f3n URL: http://169.254.169.254/latest/meta-data/. Tambi\u00e9n puede leer los datos mediante programaci\u00f3n, por ejemplo, desde una ventana de terminal que tenga la utilidad cURL. En la ventana de terminal, ejecute cURLhttp://169.254.169.254/latest/meta-data/para recuperarlos. La direcci\u00f3n IP 169.254.169.254es una direcci\u00f3n de enlace local y solo es v\u00e1lida desde la instancia. Los metadatos de la instancia proporcionan en general la misma informaci\u00f3n acerca de la instancia en ejecuci\u00f3n que puede encontrar en la consola de administraci\u00f3n de AWS. Por ejemplo, puede conocer la direcci\u00f3n IP p\u00fablica, la direcci\u00f3n IP privada, el nombre de host p\u00fablico, el ID de la instancia, los grupos de seguridad, la regi\u00f3n, la zona de disponibilidad y mucho m\u00e1s. Tambi\u00e9n se puede acceder a todos los datos de usuario especificados durante el lanzamiento de la instancia en la siguiente direcci\u00f3n URL: http://169.254.169.254/latest/user-data.Los metadatos de instancias EC2 se pueden utilizar para configurar o administrar una instancia en ejecuci\u00f3n. Por ejemplo, puede crear un script de configuraci\u00f3n que tenga acceso a la informaci\u00f3n de metadatos y la utilice para configurar las aplicaciones o el sistema operativo. 51 IPs est\u00e1ticas Una direcci\u00f3n IP p\u00fablicaes una direcci\u00f3n IPv4 a la que se puede acceder desde Internet. A cada instancia que recibe una direcci\u00f3n IP p\u00fablica se le asigna tambi\u00e9n un nombre de host DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25, el nombre de host DNS externo podr\u00eda serec2-203-0-113-25.compute-1.amazonaws.com.Si especifica que se debe asignar una direcci\u00f3n IP p\u00fablica a la instancia, esta se asigna desde el grupo de direcciones IPv4 p\u00fablicas de AWS. La direcci\u00f3n IP p\u00fablica no est\u00e1 asociada a su cuenta de AWS. Cuando se desvincula una direcci\u00f3n IP p\u00fablica de la instancia, se vuelve a liberar en el grupo de direcciones IPv4 p\u00fablicas, y no podr\u00e1 indicar que desea volver a utilizarla. AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia.Si necesita una direcci\u00f3n IP p\u00fablica permanente, le recomendamos asociar una direcci\u00f3n IP el\u00e1stica a la instancia. Para hacerlo, primero debe asignar una nueva direcci\u00f3n IP el\u00e1stica en la regi\u00f3n donde se encuentra la instancia. Una vez asignada, puede asociar la direcci\u00f3n IP el\u00e1stica a una instancia EC2. 49 Monitorizaci\u00f3n \u00b6 Puede monitorizar las instancias con Amazon CloudWatch, el cual recopila y procesa los datos sin formato de Amazon EC2, y los convierte en m\u00e9tricas legibles casi en tiempo real. Estas estad\u00edsticas se registran durante un periodo de 15meses, de forma que pueda acceder a la informaci\u00f3n hist\u00f3rica y obtener una mejor perspectiva acerca del rendimiento de su servicio o aplicaci\u00f3n web.De forma predeterminada, Amazon EC2 proporciona un monitoreo b\u00e1sico,que env\u00eda datos de m\u00e9tricas a CloudWatch en intervalos de 5minutos. Para enviar los datos de las m\u00e9tricas de la instancia a CloudWatch cada 1 minuto, puede habilitar el monitoreo detallado en la instancia. Para obtener m\u00e1s informaci\u00f3n, consulte Habilitar o deshabilitar el monitoreo detallado de las instancias. La consola de Amazon EC2 muestra una serie de gr\u00e1ficos basados en los datos sin procesar de Amazon CloudWatch. En funci\u00f3n de sus necesidades, es posible que prefiera obtener los datos para las instancias de Amazon CloudWatch, en lugar de los gr\u00e1ficos en la consola. De forma predeterminada, Amazon CloudWatch no proporciona m\u00e9tricas de la RAM para las instancias EC2, pero puede configurar esta opci\u00f3n si desea que Cloud Watch recopile esos datos. 52 RESUMEN Las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de su cuenta.Puede elegir entre muchos tipos de instancias. Cada tipo de instancia ofrece diferentes combinaciones de capacidades de CPU, RAM, almacenamiento y redes.Puede configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos).Los datos de usuario le permiten especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Solo se pueden detener las instancias que cuentan con el respaldo de Amazon EBS. Puede utilizar Amazon CloudWatch para capturar y revisar m\u00e9tricas en instancias EC2. Optimizaci\u00f3n de costes \u00b6 AWS ofrece diferentes tipos de instancia Los modelos de precios de Amazon EC2 incluyen instancias bajo demanda, instancias reservadas, instancias de spot, instancias dedicadas y hosts dedicados. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias de spot se pueden interrumpir con una notificaci\u00f3n de 2 minutos. Sin embargo, pueden significar un ahorro considerable en comparaci\u00f3n con las instancias bajo demanda. Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o Aumento de la elasticidad Modelo de precios \u00f3ptimo Optimizaci\u00f3n de las opciones de almacenamiento Servicios de contenedores \u00b6 Los contenedores pueden abarcar todo lo que una aplicaci\u00f3n necesita para ejecutarse. Docker es una plataforma de software que empaqueta software en contenedores. Una sola aplicaci\u00f3n puede abarcar varios contenedores. Amazon Elastic Container Service(Amazon ECS) organiza la ejecuci\u00f3n de los contenedores de Docker. Kuberneteses un software de c\u00f3digo abierto para la organizaci\u00f3n de contenedores. Amazon Elastic Kubernetes Service (Amazon EKS) le permite ejecutar Kubernetes en AWS. Amazon Elastic Container Registry (Amazon ECR) le permite almacenar, administrar e implementar sus contenedores de Docker AWS Lambda \u00b6 La inform\u00e1tica sin servidor le permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona las funcionalidades integradas de tolerancia a errores y escalado autom\u00e1tico, el cual se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Mediante AWS Step Functions se pueden crear flujos de trabajo encadenando llamadas a funciones lambda Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos. AWS Elastic Beanstalk \u00b6 AWS ElasticBeanstalk mejora la productividad de los desarrolladores. Simplifica el proceso de implementaci\u00f3n de la aplicaci\u00f3n. Reduce la complejidad de administraci\u00f3n. ElasticBeanstalk es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker. No se aplican cargos por utilizar ElasticBeanstalk. Pague \u00fanicamente por los recursos de AWS que utilice. Escalado y Balanceo de carga \u00b6 Elastic Load Balancing distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2 Adem\u00e1s, le permite obtener tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Auto Scaling permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Puede utilizar Auto Scaling para asegurarse de que se ejecuta la cantidad deseada de instancias de Amazon EC2. Con Auto Scaling, tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costos. Auto Scaling es adecuado para aplicaciones con patrones de demanda estables o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana. Actividades \u00b6 Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS . Referencias \u00b6","title":"3.- Computaci\u00f3n en AWS"},{"location":"apuntes/nube03.html#servicios-de-computacion-en-la-nube","text":"","title":"Servicios de computaci\u00f3n en la nube"},{"location":"apuntes/nube03.html#introduccion","text":"Los servicios de m\u00e1quinas virtuales fueron los primeros servicios tanto de AWS como de Azure, los cuales proporcionan infraestructura como servicio ( IaaS ). Posteriormente se a\u00f1adieron otros servicios como tecnolog\u00eda sin servidor ( serverless ), tecnolog\u00eda basada en contenedores y plataforma como servicio ( PaaS ). Ya hemos comentado el coste de ejecutar servidores in-house (compra, mantenimiento del centro de datos, personal, etc...) adem\u00e1s de la posibilidad de que la capacidad del servidor podr\u00eda permanecer sin uso e inactiva durante gran parte del tiempo de ejecuci\u00f3n de los servidores, lo que implica un desperdicio.","title":"Introducci\u00f3n"},{"location":"apuntes/nube03.html#amazon-ec2","text":"Amazon Elastic Compute Cloud ( Amazon EC2 - https://docs.aws.amazon.com/ec2/ ) proporciona m\u00e1quinas virtuales en las que podemos alojar el mismo tipo de aplicaciones que podr\u00edamos ejecutar en un servidor en nuestras oficinas. Adem\u00e1s, ofrece capacidad de c\u00f3mputo segura y de tama\u00f1o ajustable en la nube. Las instancias EC2 admiten distintas cargas de trabajo (servidores de aplicaciones, web, de base de datos, de correo, multimedia, de archivos, etc..) La computaci\u00f3n el\u00e1stica ( Elastic Compute ) se refiere a la capacidad para aumentar o reducir f\u00e1cilmente la cantidad de servidores que ejecutan una aplicaci\u00f3n de manera autom\u00e1tica, as\u00ed como para aumentar o reducir la capacidad de procesamiento (CPU), memoria RAM o almacenamiento de los servidores existentes. La primera vez que lancemos una instancia de Amazon EC2, utilizaremos el asistente de lanzamiento de instancias de la consola de administraci\u00f3n de AWS, el cual nos facilita paso a paso la configuraci\u00f3n y creaci\u00f3n de nuestra m\u00e1quina virtual.","title":"Amazon EC2"},{"location":"apuntes/nube03.html#paso-1-ami","text":"Una imagen de Amazon Machine (AMI) proporciona la informaci\u00f3n necesaria para lanzar una instancia EC2. As\u00ed pues, el primer paso consiste en elegir cual ser\u00e1 la AMI de nuestra instancia. Por ejemplo, una AMI que contenga un servidor de aplicaciones y otra que contenga un servidor de base de datos. Si vamos a montar un cluster, tambi\u00e9n podemos lanzar varias instancias a partir de una sola AMI. Las AMI incluyen los siguientes componentes: Una plantilla para el volumen ra\u00edz de la instancia, el cual contiene un sistema operativo y todo lo que se instal\u00f3 en \u00e9l (aplicaciones, librer\u00edas, etc.). Amazon EC2 copia la plantilla en el volumen ra\u00edz de una instancia EC2 nueva y, a continuaci\u00f3n, la inicia. Permisos de lanzamiento que controlan qu\u00e9 cuentas de AWS pueden usar la AMI. La asignaci\u00f3n de dispositivos de bloques que especifica los vol\u00famenes que deben asociarse a la instancia en su lanzamiento, si corresponde.","title":"Paso 1: AMI"},{"location":"apuntes/nube03.html#paso-2-tipo-de-instancias","text":"El segundo paso es seleccionar un tipo de instancia, seg\u00fan nuestro caso de uso. Los tipos de instancia incluyen diversas combinaciones de capacidad de CPU, memoria, almacenamiento y red. Cada tipo de instancia se ofrece en uno o m\u00e1s tama\u00f1os, lo cual permite escalar los recursos en funci\u00f3n de los requisitos de la carga de trabajo de destino.","title":"Paso 2: Tipo de instancias"},{"location":"apuntes/nube03.html#paso-3-configuracion-de-la-instancia-red","text":"El siguiente paso es especificar la ubicaci\u00f3n de red en la que se implementar\u00e1 la instancia EC2, teniendo en cuenta la regi\u00f3n donde nos encontramos antes de lanzar la instancia. En este paso, elegiremos la VPC y la subred dentro de la misma, ya sea de las que tenemos creadas o pudiendo crear los recursos en este paso. Respecto a la asignaci\u00f3n p\u00fablica de ip sobre esta instancia, cuando se lanza una instancia en una VPC predeterminada, AWS le asigna una direcci\u00f3n IP p\u00fablica de forma predeterminada. En caso contrario, si la VPC no es la predeterminada, AWS no asignar\u00e1 una direcci\u00f3n IP p\u00fablica, a no ser que lo indiquemos de forma expl\u00edcita.","title":"Paso 3: Configuraci\u00f3n de la instancia / red"},{"location":"apuntes/nube03.html#paso-4-almacenamiento","text":"Al lanzar la instancia EC2 configuraremos las opciones de almacenamiento. Por ejemplo el tama\u00f1o del volumen ra\u00edz en el que est\u00e1 instalado el sistema operativo invitado o vol\u00famenes de almacenamiento adicionales cuando lance la instancia. Algunas AMI est\u00e1n configuradas para lanzar m\u00e1s de un volumen de almacenamiento de forma predeterminada y, de esa manera, proporcionar almacenamiento independiente del volumen ra\u00edz. Para cada volumen que tenga la instancia, podemos indicar el tama\u00f1o de los discos, los tipos de volumen, si el almacenamiento se conservar\u00e1 en el caso de terminaci\u00f3n de la instancia y si se debe utilizar el cifrado. En la sesi\u00f3n anterior ya comentamos algunos de los servicios de almacenamiento que estudiaremos en profundidad en la siguiente sesi\u00f3n, como pueden ser Amazon EBS (almacenamiento por bloques de alto rendimiento) o Amazon EFS (almacenamiento el\u00e1stico compartido entre diferentes instancias).","title":"Paso 4: Almacenamiento"},{"location":"apuntes/nube03.html#paso-5-etiquetas","text":"Las etiquetas son marcas que se asignan a los recursos de AWS. Cada etiqueta est\u00e1 formada por una clave y un valor opcional, siendo ambos campos case sensitive . El etiquetado es la forma en que asocia metadatos a una instancia EC2. De esta manera, permiten clasificar los recursos de AWS, como las instancias EC2, de diferentes maneras. Por ejemplo, en funci\u00f3n de la finalidad, el propietario o el entorno. Los beneficios potenciales del etiquetado son la capacidad de filtrado, la automatizaci\u00f3n, la asignaci\u00f3n de costes y el control de acceso.","title":"Paso 5: Etiquetas"},{"location":"apuntes/nube03.html#paso-6-grupo-de-seguridad","text":"Un grupo de seguridad es un conjunto de reglas de firewall que controlan el tr\u00e1fico de red de una o m\u00e1s instancias, por lo que se encuentra fuera del sistema operativo de la instancia. Dentro del grupo, agregaremos reglas para habilitar el tr\u00e1fico hacia o desde nuestras instancias asociadas. Para cada una de estas reglas especificaremos el puerto, el protocolo (TCP, UDP, ICMP), as\u00ed como el origen (por ejemplo, una direcci\u00f3n IP u otro grupo de seguridad) que tiene permiso para utilizar la regla. De forma predeterminada, se incluye una regla de salida que permite todo el tr\u00e1fico saliente. Es posible quitar esta regla y agregar reglas de salida que solo permitan tr\u00e1fico saliente espec\u00edfico. AWS eval\u00faa las reglas de todos los grupos de seguridad asociados a una instancia para decidir si permite que el tr\u00e1fico llegue a ella. Si desea lanzar una instancia en una nube virtual privada (VPC), debe crear un grupo de seguridad nuevo o utilizar uno que ya exista en esa VPC. Las reglas de un grupo de seguridad se pueden modificar en cualquier momento, y las reglas nuevas se aplicar\u00e1n autom\u00e1ticamente a todas las instancias que est\u00e9n asociadas al grupo de seguridad.","title":"Paso 6: Grupo de seguridad"},{"location":"apuntes/nube03.html#paso-7-analisis-e-identificacion","text":"El paso final es una p\u00e1gina resumen con todos los datos introducidos. Cuando le damos a lanzar la nueva instancia configurada, nos aparecer\u00e1 un cuadro de di\u00e1logo donde se solicita que elijamos un par de claves existente, continuar sin un par de claves o crear un par de claves nuevo antes de crear y lanzar la instancia EC2. Amazon EC2 utiliza la criptograf\u00eda de clave p\u00fablica para cifrar y descifrar la informaci\u00f3n de inicio de sesi\u00f3n. La clave p\u00fablica la almacena AWS, mientras que .la clave privada que la almacenamos nosotros. Guarda tus claves Si creamos una par de claves nuevas, hemos de descargarlas y guardarlas en un lugar seguro. Esta es la \u00fanica oportunidad de guardar el archivo de clave privada. Si perdemos las claves, tendremos que destruir la instancia y volver a crearla. Para conectarnos a la instancia desde nuestra m\u00e1quina local, necesitamos hacerlo via un cliente SSH / Putty adjuntando el par de claves descargado. Si la AMI es de Windows, utilizaremos la clave privada para obtener la contrase\u00f1a de administrador que necesita para iniciar sesi\u00f3n en la instancia. En cambio, si la AMI es de Linux, lo haremos mediante ssh: ssh -i /path/miParClaves.pem miNombreUsuarioInstancia@miPublicDNSInstancia M\u00e1s informaci\u00f3n en: https://docs.aws.amazon.com/es_es/AWSEC2/latest/UserGuide/AccessingInstances.html Por \u00faltimo, una vez lanzada la instancia, podemos observar la informacion disponible sobre la misma: direcci\u00f3n IP y la direcci\u00f3n DNS, el tipo de instancia, el ID de instancia \u00fanico asignado a la instancia, el ID de la AMI que utiliz\u00f3 para lanzar la instancia, el ID de la VPC, el ID de la subred, etc...","title":"Paso 7: An\u00e1lisis e identificaci\u00f3n"},{"location":"apuntes/nube03.html#uso-de-la-consola","text":"Tambi\u00e9n puede lanzar instancias EC2 mediante programaci\u00f3n, ya sea a trav\u00e9s de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS) o uno de los kits de desarrollo de software (SDK) de AWS.En el comando de la CLI de AWS de ejemplo, ver\u00e1 un solo comando que especifica la informaci\u00f3n m\u00ednima necesaria para lanzar una instancia. El comando incluye la siguiente informaci\u00f3n: aws: especifica la invocaci\u00f3n de la utilidad de l\u00ednea de comandos aws. ec2: especifica la invocaci\u00f3n del comando del servicioec2. run-instances: es el subcomando que se invoca.El resto del comando especifica varios par\u00e1metros, entre los que se incluyen los siguientes: image-id: este par\u00e1metro va seguido de un ID de AMI. Todas las AMI tienen un ID de \u00fanico.count: puede especificar m\u00e1s de una instancia. instance-type: puede especificar el tipo de instancia que se crear\u00e1, como una instancia c3.large.key-name: en el ejemplo, supongamos que MyKeyPairya existe. security-groups: en este ejemplo, supongamos que MySecurityGroupya existe. region: las AMI se encuentran en una regi\u00f3n de AWS, por lo que debe especificar la regi\u00f3n donde la CLI de AWS encontrar\u00e1 la AMI y lanzar\u00e1 la instancia EC2. El comando deber\u00eda crear la instancia EC2 correctamente si suceden los siguientes supuestos:El comando tiene el formato correcto.Los recursos que el comando necesita ya existen.Cuenta con los permisos necesarios para ejecutar el comando.Tiene capacidad suficiente en la cuenta de AWS.Si el comando se ejecuta correctamente, la API responde al comando con el ID de la instancia y otros datos importantes para que la aplicaci\u00f3n los utilice en las solicitudes a la API posteriores. 45","title":"Uso de la consola"},{"location":"apuntes/nube03.html#ciclo-de-vida-de-las-instancias","text":"Este es el ciclo de vida de una instancia. Las flechas muestran las acciones que puede realizar, y las casillas indican el estadoque tendr\u00e1 la instancia despu\u00e9s de dicha acci\u00f3n. Las instancias pueden tener uno de los siguientes estados: Pending(pendiente): cuando se lanza una instancia por primera vez desde una AMI o cuando se activa una instancia detenida, esta pasa al estado pending cuando arranca y se implementa en un equipo de alojamiento. El tipo de instancia que especific\u00f3 durante el lanzamiento determinar\u00e1 el hardware del equipo de alojamiento para la instancia. Running(en ejecuci\u00f3n): cuando ya arranc\u00f3 la instancia por completo y est\u00e1 lista, sale del estado pending y pasa al estado running. Puede conectarse a trav\u00e9s de Internet a la instancia en ejecuci\u00f3n. Rebooting(reiniciada): AWS recomienda reiniciar las instancias con la consola de Amazon EC2, la CLI de AWS o los SDK de AWS, en lugar de invocar una acci\u00f3n de reinicio desde el sistema operativo invitado. Una instancia reiniciada permanece en el mismo host f\u00edsico, mantiene el mismo nombre DNS p\u00fablico y la misma direcci\u00f3n IP p\u00fablica y, si tiene vol\u00famenes del almac\u00e9n de instancias, conserva los datos en ellos. Shuttingdown(en proceso de cierre):este es un estado intermedio entre running y terminated. Terminated(terminada): las instancias terminadas permanecen visibles en la consola de Amazon EC2 durante un tiempo antes de que se elimine la m\u00e1quina virtual. Sin embargo, no es posible conectarse a una instancia terminada ni recuperarla. Stopping(en proceso de detenci\u00f3n): las instancias que cuentan con el respaldo de Amazon EBS se pueden detener. Entran en el estado stoppingantes de alcanzar el estado stopped por completo. Stopped(detenida):una instancia en el estado stopped no generar\u00e1 los mismos costos que una instancia en el estado running. Si se inicia una instancia en el estado stopped, esta vuelve al estado pending y se traslada a una nueva m\u00e1quina de alojamiento. 47 Algunas instancias que cuentan con el respaldo de Amazon EBS admiten la hibernaci\u00f3n. Cuando pone una instancia a hibernar, el sistema operativo invitado guarda el contenido de la memoria de la instancia (RAM) en el volumen ra\u00edz de Amazon EBS. Cuando reinicie la instancia, el volumen ra\u00edz se restaurar\u00e1 al estado anterior, se volver\u00e1 a cargar el contenido de la RAM y se reanudar\u00e1n los procesos que estaban en ejecuci\u00f3n anteriormente en la instancia.Solo algunas AMI de Linux que cuentan con el respaldo de Amazon EBS y determinados tipos de instancias admiten la hibernaci\u00f3n. La hibernaci\u00f3n tambi\u00e9n requiere el cifrado del volumen ra\u00edz de EBS. Adem\u00e1s, se debe habilitar la hibernaci\u00f3n con el lanzamiento inicial de la instancia. No puede habilitarla en una instancia existente que no ten\u00eda habilitada esta funci\u00f3n originalmente. Para obtener m\u00e1s informaci\u00f3n sobre los requisitos previos y el costo, consulte Hiberne su instancia de Linux en la p\u00e1gina de documentaci\u00f3n de AWS. 48 De manera predeterminada, todas las cuentas de AWS est\u00e1n limitadas a cinco (5) direcciones IP el\u00e1sticas por regi\u00f3n porque las direcciones p\u00fablicas de Internet (IPv4) son un recurso p\u00fablico escaso. Sin embargo, este l\u00edmite no es fijo, y se puede solicitar un aumento del l\u00edmite (que podr\u00eda aprobarse). 50","title":"Ciclo de vida de las instancias"},{"location":"apuntes/nube03.html#metadatos-de-las-instancias","text":"Los metadatos de la instancia son datos sobre la instancia. Puede verlos mientras est\u00e9 conectado a la instancia. Para acceder a ellos en un navegador, dir\u00edjase a la siguiente direcci\u00f3n URL: http://169.254.169.254/latest/meta-data/. Tambi\u00e9n puede leer los datos mediante programaci\u00f3n, por ejemplo, desde una ventana de terminal que tenga la utilidad cURL. En la ventana de terminal, ejecute cURLhttp://169.254.169.254/latest/meta-data/para recuperarlos. La direcci\u00f3n IP 169.254.169.254es una direcci\u00f3n de enlace local y solo es v\u00e1lida desde la instancia. Los metadatos de la instancia proporcionan en general la misma informaci\u00f3n acerca de la instancia en ejecuci\u00f3n que puede encontrar en la consola de administraci\u00f3n de AWS. Por ejemplo, puede conocer la direcci\u00f3n IP p\u00fablica, la direcci\u00f3n IP privada, el nombre de host p\u00fablico, el ID de la instancia, los grupos de seguridad, la regi\u00f3n, la zona de disponibilidad y mucho m\u00e1s. Tambi\u00e9n se puede acceder a todos los datos de usuario especificados durante el lanzamiento de la instancia en la siguiente direcci\u00f3n URL: http://169.254.169.254/latest/user-data.Los metadatos de instancias EC2 se pueden utilizar para configurar o administrar una instancia en ejecuci\u00f3n. Por ejemplo, puede crear un script de configuraci\u00f3n que tenga acceso a la informaci\u00f3n de metadatos y la utilice para configurar las aplicaciones o el sistema operativo. 51 IPs est\u00e1ticas Una direcci\u00f3n IP p\u00fablicaes una direcci\u00f3n IPv4 a la que se puede acceder desde Internet. A cada instancia que recibe una direcci\u00f3n IP p\u00fablica se le asigna tambi\u00e9n un nombre de host DNS externo. Por ejemplo, si la direcci\u00f3n IP p\u00fablica asignada a la instancia es 203.0.113.25, el nombre de host DNS externo podr\u00eda serec2-203-0-113-25.compute-1.amazonaws.com.Si especifica que se debe asignar una direcci\u00f3n IP p\u00fablica a la instancia, esta se asigna desde el grupo de direcciones IPv4 p\u00fablicas de AWS. La direcci\u00f3n IP p\u00fablica no est\u00e1 asociada a su cuenta de AWS. Cuando se desvincula una direcci\u00f3n IP p\u00fablica de la instancia, se vuelve a liberar en el grupo de direcciones IPv4 p\u00fablicas, y no podr\u00e1 indicar que desea volver a utilizarla. AWS libera la direcci\u00f3n IP p\u00fablica de la instancia cuando la instancia se detiene o se termina. La instancia detenida recibe una direcci\u00f3n IP p\u00fablica nueva cuando se reinicia.Si necesita una direcci\u00f3n IP p\u00fablica permanente, le recomendamos asociar una direcci\u00f3n IP el\u00e1stica a la instancia. Para hacerlo, primero debe asignar una nueva direcci\u00f3n IP el\u00e1stica en la regi\u00f3n donde se encuentra la instancia. Una vez asignada, puede asociar la direcci\u00f3n IP el\u00e1stica a una instancia EC2. 49","title":"Metadatos de las instancias"},{"location":"apuntes/nube03.html#monitorizacion","text":"Puede monitorizar las instancias con Amazon CloudWatch, el cual recopila y procesa los datos sin formato de Amazon EC2, y los convierte en m\u00e9tricas legibles casi en tiempo real. Estas estad\u00edsticas se registran durante un periodo de 15meses, de forma que pueda acceder a la informaci\u00f3n hist\u00f3rica y obtener una mejor perspectiva acerca del rendimiento de su servicio o aplicaci\u00f3n web.De forma predeterminada, Amazon EC2 proporciona un monitoreo b\u00e1sico,que env\u00eda datos de m\u00e9tricas a CloudWatch en intervalos de 5minutos. Para enviar los datos de las m\u00e9tricas de la instancia a CloudWatch cada 1 minuto, puede habilitar el monitoreo detallado en la instancia. Para obtener m\u00e1s informaci\u00f3n, consulte Habilitar o deshabilitar el monitoreo detallado de las instancias. La consola de Amazon EC2 muestra una serie de gr\u00e1ficos basados en los datos sin procesar de Amazon CloudWatch. En funci\u00f3n de sus necesidades, es posible que prefiera obtener los datos para las instancias de Amazon CloudWatch, en lugar de los gr\u00e1ficos en la consola. De forma predeterminada, Amazon CloudWatch no proporciona m\u00e9tricas de la RAM para las instancias EC2, pero puede configurar esta opci\u00f3n si desea que Cloud Watch recopile esos datos. 52 RESUMEN Las instancias EC2 se lanzan desde una plantilla de AMI en una VPC de su cuenta.Puede elegir entre muchos tipos de instancias. Cada tipo de instancia ofrece diferentes combinaciones de capacidades de CPU, RAM, almacenamiento y redes.Puede configurar grupos de seguridad para controlar el acceso a las instancias (especificar el origen y los puertos permitidos).Los datos de usuario le permiten especificar un script que se ejecutar\u00e1 la primera vez que se lance una instancia. Solo se pueden detener las instancias que cuentan con el respaldo de Amazon EBS. Puede utilizar Amazon CloudWatch para capturar y revisar m\u00e9tricas en instancias EC2.","title":"Monitorizaci\u00f3n"},{"location":"apuntes/nube03.html#optimizacion-de-costes","text":"AWS ofrece diferentes tipos de instancia Los modelos de precios de Amazon EC2 incluyen instancias bajo demanda, instancias reservadas, instancias de spot, instancias dedicadas y hosts dedicados. La facturaci\u00f3n por segundo est\u00e1 disponible para las instancias bajo demanda, las instancias reservadas y las instancias de spot que solo utilizan Amazon Linux y Ubuntu. Las instancias de spot se pueden interrumpir con una notificaci\u00f3n de 2 minutos. Sin embargo, pueden significar un ahorro considerable en comparaci\u00f3n con las instancias bajo demanda. Los cuatro pilares de la optimizaci\u00f3n de costes son: Adaptaci\u00f3n del tama\u00f1o Aumento de la elasticidad Modelo de precios \u00f3ptimo Optimizaci\u00f3n de las opciones de almacenamiento","title":"Optimizaci\u00f3n de costes"},{"location":"apuntes/nube03.html#servicios-de-contenedores","text":"Los contenedores pueden abarcar todo lo que una aplicaci\u00f3n necesita para ejecutarse. Docker es una plataforma de software que empaqueta software en contenedores. Una sola aplicaci\u00f3n puede abarcar varios contenedores. Amazon Elastic Container Service(Amazon ECS) organiza la ejecuci\u00f3n de los contenedores de Docker. Kuberneteses un software de c\u00f3digo abierto para la organizaci\u00f3n de contenedores. Amazon Elastic Kubernetes Service (Amazon EKS) le permite ejecutar Kubernetes en AWS. Amazon Elastic Container Registry (Amazon ECR) le permite almacenar, administrar e implementar sus contenedores de Docker","title":"Servicios de contenedores"},{"location":"apuntes/nube03.html#aws-lambda","text":"La inform\u00e1tica sin servidor le permite crear y ejecutar aplicaciones y servicios sin aprovisionar ni administrar servidores. AWS Lambda ( https://aws.amazon.com/es/lambda/ ) es un servicio de inform\u00e1tica sin servidor que proporciona las funcionalidades integradas de tolerancia a errores y escalado autom\u00e1tico, el cual se factura por el tiempo de ejecuci\u00f3n (cantidad de milisegundos por el n\u00famero de invocaciones a la funci\u00f3n). Para ello, permite la ejecuci\u00f3n de c\u00f3digo en el servidor con soporte para m\u00faltiples lenguajes (Java, C#, Python, Go, ...) sin necesidad de configurar una instancia EC2. Un origen de eventos es un servicio de AWS ( S3 , DynamoDB , ...) o una aplicaci\u00f3n creada por un desarrollador que desencadena la ejecuci\u00f3n de una funci\u00f3n de Lambda. Mediante AWS Step Functions se pueden crear flujos de trabajo encadenando llamadas a funciones lambda Las restricciones m\u00e1s destacables son: Permite hasta 1000 ejecuciones simult\u00e1neas en una \u00fanica regi\u00f3n. La cantidad m\u00e1xima de memoria que se puede asignar para una sola funci\u00f3n Lambda es de 3008 MB. El tiempo de ejecuci\u00f3n m\u00e1ximo para una funci\u00f3n Lambda es de 15 minutos.","title":"AWS Lambda"},{"location":"apuntes/nube03.html#aws-elastic-beanstalk","text":"AWS ElasticBeanstalk mejora la productividad de los desarrolladores. Simplifica el proceso de implementaci\u00f3n de la aplicaci\u00f3n. Reduce la complejidad de administraci\u00f3n. ElasticBeanstalk es compatible con Java, .NET, PHP, Node.js, Python, Ruby, Go y Docker. No se aplican cargos por utilizar ElasticBeanstalk. Pague \u00fanicamente por los recursos de AWS que utilice.","title":"AWS Elastic Beanstalk"},{"location":"apuntes/nube03.html#escalado-y-balanceo-de-carga","text":"Elastic Load Balancing distribuye autom\u00e1ticamente el tr\u00e1fico entrante de las aplicaciones entre varias instancias de Amazon EC2 Adem\u00e1s, le permite obtener tolerancia a errores en las aplicaciones, ya que proporciona de forma constante la capacidad de balanceo de carga necesaria para dirigir el tr\u00e1fico de estas. Auto Scaling permite mantener la disponibilidad de las aplicaciones y aumentar o reducir autom\u00e1ticamente la capacidad de Amazon EC2 seg\u00fan las condiciones que se definan. Puede utilizar Auto Scaling para asegurarse de que se ejecuta la cantidad deseada de instancias de Amazon EC2. Con Auto Scaling, tambi\u00e9n se puede aumentar autom\u00e1ticamente la cantidad de instancias de Amazon EC2 durante los picos de demanda para mantener el rendimiento y reducir la capacidad durante los per\u00edodos de baja demanda con el objeto de minimizar los costos. Auto Scaling es adecuado para aplicaciones con patrones de demanda estables o para aquellas cuyo uso var\u00eda cada hora, d\u00eda o semana.","title":"Escalado y Balanceo de carga"},{"location":"apuntes/nube03.html#actividades","text":"Realizar el m\u00f3dulo 6 (Inform\u00e1tica) del curso ACF de AWS .","title":"Actividades"},{"location":"apuntes/nube03.html#referencias","text":"","title":"Referencias"},{"location":"apuntes/nube04.html","text":"Almacenamiento en la nube \u00b6 El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar. Amazon EBS \u00b6 Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de AmazonEC2 solamente dentro de la misma zona de disponibilidad. Vol\u00famenes \u00b6 Los vol\u00famenes de EBS proporcionan almacenamiento fuera de las instancias que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones. FIXME: Completar Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de ..... Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Los vol\u00famenes de Amazon EBS tienen un alto nivel de disponibilidad y de confianza, y pueden aprovecharse como particiones de arranque de instancias de Amazon EC2 o asociarse a una instancia de Amazon EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen durabilidad mucho mayor que los almacenes de instancias de Amazon EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: los vol\u00famenes de Amazon EBS son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: los vol\u00famenes de Amazon EBS son iguales que las unidades de Amazon EC2 locales o mejores que ellas. Nivel de fiabilidad alto: los vol\u00famenes de Amazon EBS tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: los vol\u00famenes de Amazon EBS se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Solo una instancia de AmazonEC2 a la vez puede montarse en un volumen de Amazon EBS. Instant\u00e1neas \u00b6 Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon Simple Storage Service (Amazon S3) y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS y permiten proteger la durabilidad de sus datos a largo plazo. Tambi\u00e9n puede compartirlas f\u00e1cilmente con colegas y otros desarrolladores de AWS. Cuando lo desee, podr\u00e1 crear una cantidad ilimitada de instant\u00e1neas uniformes de un momento espec\u00edfico de los vol\u00famenes de Amazon EBS. Las instant\u00e1neas de Amazon EBS se almacenan en Amazon S3 con un alto nivel de durabilidad. Se pueden crear vol\u00famenes de Amazon EBS nuevos a partir de instant\u00e1neas para clonar o restaurar copias de seguridad. Las instant\u00e1neas de Amazon EBS tambi\u00e9n pueden compartirse f\u00e1cilmente entre usuarios de AWS o copiarse entre regiones de AWS. Amazon S3 \u00b6 AmazonS3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Los datos como objetos dentro de recursos conocidos como buckets . S3 es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket, le permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. Los objetos pueden ser de hasta 5TB. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Los datos que almacena en AmazonS3 no est\u00e1n asociados a ning\u00fan servidor en particular. Adem\u00e1s, no necesita administrar ninguna infraestructura por su cuenta. Puede colocar tantos objetos como quiera en Amazon S3. Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Ya que Amazon S3 admite objetos de hasta varios terabytes de tama\u00f1o, le permite incluso almacenar instant\u00e1neas de bases de datos como objetos. Amazon S3 tambi\u00e9n ofrece acceso de baja latencia a los datos a trav\u00e9s de Internet mediante el protocolo de transferencia de hipertexto (HTTP) o el HTTP seguro (HTTPS), para que pueda recuperar datos en cualquier momento y desde cualquier lugar. Tambi\u00e9n puede acceder a AmazonS3 de forma privada a trav\u00e9s de un punto de enlace de nube virtual privada (VPC). Obtendr\u00e1 un control detallado sobre qui\u00e9n puede acceder a sus datos a trav\u00e9s de las pol\u00edticas de AWS Identity and Access Management (IAM), las pol\u00edticas de bucket de AmazonS3 e, incluso, las listas de control de acceso por objeto. De forma predeterminada, no se comparte ninguno de sus datos p\u00fablicamente. Tambi\u00e9n puede cifrar los datos en tr\u00e1nsito y elegir habilitar el cifrado del lado del servidor en sus objetos. Puede acceder a AmazonS3 a trav\u00e9s de la consola de administraci\u00f3n de AWS basada en la web, de forma program\u00e1tica a trav\u00e9s de la API y los SDK, o con soluciones de terceros que utilizan la API o los SDK. AmazonS3 incluye notificaciones de eventos que le permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucketespec\u00edfico. Se le pueden enviar estas notificaciones o pueden utilizarse para desencadenar otros procesos, como funciones de AWS Lambda. Clases de almacenamiento \u00b6 S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: AmazonS3 Est\u00e1ndar: la clase de almacenamiento Est\u00e1ndar de S3 est\u00e1 dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, Est\u00e1ndar de S3 es una opci\u00f3n adecuada para una amplia variedad de casos de uso, como las aplicaciones en la nube, los sitios web din\u00e1micos, la distribuci\u00f3n de contenido, las aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de bigdata. AmazonS3 Intelligent-Tiering: la clase de almacenamiento AmazonS3 Intelligent-Tieringse ha dise\u00f1ado para optimizar los costos mediante la migraci\u00f3n autom\u00e1tica de los datos a la capa de acceso m\u00e1s rentable, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Por una peque\u00f1a tarifa mensual de monitoreo y automatizaci\u00f3n por objeto, AmazonS3 monitorea los patrones de acceso de los objetos en AmazonS3 IntelligentTieringy traslada aquellos a los que no se ha accedido durante 30d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. No se aplican tarifas por la recuperaci\u00f3n si se usa la clase de almacenamiento AmazonS3 IntelligentTieringni tampoco se cobran tarifas adicionales cuando los objetos se trasladan de una capa de acceso a otra. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. AmazonS3 Est\u00e1ndar -Acceso poco frecuente: esta clase de almacenamiento de AmazonS3 se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. La clase Est\u00e1ndar -Acceso poco frecuente de S3 est\u00e1 dise\u00f1ada para ofrecer la alta durabilidad, el alto rendimiento y la baja latencia de Est\u00e1ndar de S3, con precios bajos de almacenamiento por GB y de recuperaci\u00f3n por GB. Esta combinaci\u00f3n de alto rendimiento y bajo costo convierte a Est\u00e1ndar -Acceso poco frecuente de S3 en una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de en un almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. AmazonS3 \u00danica zona \u2013Acceso poco frecuente: esta clase de almacenamiento de AmazonS3 est\u00e1 dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. A diferencia de otras clases de almacenamiento de AmazonS3 que guardan datos en un m\u00ednimo de tres zonas de disponibilidad, \u00danica zona -Acceso poco frecuente de S3 almacena datos en una \u00fanica zona de disponibilidad y cuesta menos que Est\u00e1ndar -Acceso poco frecuente de S3. La clase \u00danica zona -Acceso poco frecuente de S3 es ideal para aquellos clientes que desean una opci\u00f3n de menor costo para los datos a los que se accede con poca frecuencia y que no necesitan el nivel de disponibilidad ni la resiliencia de Est\u00e1ndar de S3 o de Est\u00e1ndar -Acceso poco frecuente de S3. Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. Tambi\u00e9n puede utilizarla como una opci\u00f3n de almacenamiento rentable para los datos que se replican desde otra regi\u00f3n de AWS con la replicaci\u00f3n entre regiones de AmazonS3. AmazonS3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): esta es una clase de almacenamiento seguro, duradero y de bajo costo para archivar datos. Puede almacenar con confianza cualquier cantidad de datos a costos que son competitivos o m\u00e1s econ\u00f3micos que los de las soluciones en las instalaciones. Para que los costos se mantengan bajos, pero sigan siendo aptos para diversas necesidades, AmazonS3 Glacier proporciona tres opciones de recuperaci\u00f3n, que van desde unos pocos minutos a unas horas. Puede cargar objetos directamente en AmazonS3 Glacier o utilizar pol\u00edticas de ciclo de vida de AmazonS3 para transferir datos entre cualquiera de las clases de almacenamiento de AmazonS3 para datos activos (Est\u00e1ndar, IntelligentTiering, Est\u00e1ndar -Acceso poco frecuente y \u00danica zona -Acceso poco frecuente) y AmazonS3 Glacier. AmazonS3 Glacier Deep Archive: esta es la clase de almacenamiento de menor costo en AmazonS3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Se dise\u00f1\u00f3 para clientes, en particular, para aquellos clientes que pertenecen a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de7 a10a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. AmazonS3 Glacier Deep Archive tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Se trata de una alternativa rentable y f\u00e1cil de administrar a los sistemas de cintas magn\u00e9ticas, independientemente de si estos sistemas de cintas son bibliotecas en las instalaciones o servicios fuera de ellas. AmazonS3 Glacier Deep Archive complementa a AmazonS3 Glacier y tambi\u00e9n est\u00e1 dise\u00f1ado para proporcionar una durabilidad del 99,999999999% (11nueves). Todos los objetos almacenados en AmazonS3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12horas.Para obtener m\u00e1s informaci\u00f3n, consulteClases de almacenamiento de AmazonS3. Con el an\u00e1lisis de clase de almacenamiento, puede evaluar los patrones de acceso al almacenamiento y transferir los datos correctos a la clase de almacenamiento adecuada. La caracter\u00edstica de an\u00e1lisis de AmazonS3 identifica autom\u00e1ticamente la pol\u00edtica de ciclo de vida \u00f3ptima para transferir el almacenamiento al que se accede con menos frecuencia a Est\u00e1ndar -Acceso poco frecuente de S3. Puede configurar una pol\u00edtica de an\u00e1lisis de la clase de almacenamiento para monitorear un bucket completo, un prefijo o una etiqueta de objeto. Una vez que se observa un patr\u00f3n de acceso poco frecuente, se puede crear con facilidad una nueva pol\u00edtica de ciclo de vida en funci\u00f3n de los resultados. El an\u00e1lisis de clases de almacenamiento tambi\u00e9n proporciona visualizaciones diarias del uso del almacenamiento en la consola de administraci\u00f3n de AWS. Puede exportarlas a un bucketde AmazonS3 para analizarlas utilizando las herramientas de inteligencia empresarial que prefiera, como Amazon QuickSight. Buckets \u00b6 Para utilizar AmazonS3 de forma eficaz, debe comprender algunos conceptos sencillos. En primer lugar, Amazon S3 almacena los datos en buckets. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo AmazonS3 a nivel mundial. Los buckets son contenedores l\u00f3gicos de objetos. Puede tener uno o m\u00e1s buckets en su cuenta. Puede controlar el acceso a cada bucket, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n puede ver registros de acceso al buckety a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde AmazonS3 almacenar\u00e1 el buckety su contenido. Para cargar sus datos (como fotos, videos o documentos), cree un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargue casi cualquier cantidad de objetos en el bucket. En el ejemplo, AmazonS3 se utiliz\u00f3 para crear un bucketen la regi\u00f3n de Tokio, que se identifica formalmente dentro de AWS con el c\u00f3digo de regi\u00f3n ap-northeast-1.La direcci\u00f3n URL de un bucketest\u00e1 estructurada como en los ejemplos. Puede utilizar dos estilos de URL diferentes para hacer referencia a los buckets. AmazonS3 se refiere a los archivos como objetos.Tan pronto como tenga un bucket, podr\u00e1 almacenar casi cualquier cantidad de objetos dentro de \u00e9l. Un objeto est\u00e1 compuesto por los datos y por cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. Para almacenar un objeto en AmazonS3, debe cargar en un bucketel archivo que quiera almacenar. Cuando carga un archivo, puede establecer permisos sobre los datos y cualquier metadato. En este ejemplo, el objeto Preview2.mp4 se almacena dentro del bucket. La direcci\u00f3n URL del archivo incluye el nombre del objeto al final. Cuando se crea un bucketen AmazonS3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacena datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. AmazonS3 est\u00e1 dise\u00f1ado para almacenar sus datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. AmazonS3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de su bucketa medida que aumenta la cantidad de datos. Puede comenzar de inmediato, y el almacenamiento de sus datos aumentar\u00e1 en funci\u00f3n de las necesidades de la aplicaci\u00f3n. AmazonS3 tambi\u00e9n es escalable, lo que le permitir\u00e1 gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se le facturar\u00e1 por lo que utilice. Puede obtener acceso a AmazonS3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n puede acceder a los datos de su bucketdirectamente a trav\u00e9s de los puntos de enlace basados en REST. Los puntos de enlace admiten el acceso HTTP o HTTPS. Para admitir este tipo de acceso basado en URL, los nombres de los bucketsde AmazonS3 deben ser \u00fanicos a nivel mundial y tambi\u00e9n deben cumplir los requisitos del servidor de nombres de dominio (DNS). Adem\u00e1s, las claves de objetos deben utilizar caracteres que sean seguros para las direcciones URL. FIXME: Ejemplo con pantallazos de como crear un bucket Casos de uso \u00b6 Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a AmazonS3 en un servicio adecuado para distintos casos. Ahora, consideraremos algunos casos de uso para AmazonS3: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, los bucketsde AmazonS3 proporcionan una ubicaci\u00f3n compartida para almacenar objetos a los que cualquier instancia de su aplicaci\u00f3n puede acceder, incluso las aplicaciones de AmazonEC2 o hasta los servidores tradicionales. Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, es posible delegar una parte de la entrega de ese contenido de su aplicaci\u00f3n y permitir que los clientes consigan ellos mismos de forma directa los datos de AmazonS3. Para el alojamiento web est\u00e1tico, los bucketsde AmazonS3 pueden entregar el contenido est\u00e1tico de su sitio web, que incluye HTML, CSS, JavaScript y otros archivos. La gran durabilidad de AmazonS3 lo convierte en una buena opci\u00f3n para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, AmazonS3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucketde AmazonS3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de AmazonS3. Copia de seguridad y almacenamiento:preste servicios de copia de seguridad y almacenamiento de datos a terceros. Alojamiento de aplicaciones:preste servicios que implementen, instalen y administren aplicaciones web. Alojamiento multimedia:cree una infraestructura redundante, escalable y de alta disponibilidad que aloje cargas y descargas de videos, fotos o m\u00fasica. Entrega de software:aloje sus aplicaciones de software para que los clientes puedan descargarlas. Costes \u00b6 Con AmazonS3, los costos espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizaron. Solo paga por lo que utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantesa AmazonS3 ni por las transferencias salientes desde AmazonS3 a las ubicaciones de borde de Amazon CloudFrontdentro de esa misma regi\u00f3n. Para comenzar a calcular los costos de AmazonS3, debe tener en cuenta lo siguiente: 1. Clase de almacenamiento: * El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999999999% (11nueves) de durabilidad y 99,99% (4nueves) de disponibilidad. * Est\u00e1ndar -Acceso poco frecuente de S3es una opci\u00f3n de almacenamiento dentro de AmazonS3 que puede utilizar para reducir sus costos al guardar los datos a los que se accede con menos frecuencia con niveles de redundancia ligeramente inferiores a los del almacenamiento est\u00e1ndar de AmazonS3. El almacenamiento Est\u00e1ndar -Acceso poco frecuente est\u00e1 dise\u00f1ado para ofrecer la misma durabilidad de 99,999999999% (11nueves) de AmazonS3, pero con 99,9% (3nueves) de disponibilidad en un a\u00f1o concreto. Cada clase tiene diferentes tasas. 2. Cantidad de almacenamiento:se refiere a la cantidad y al tama\u00f1o de los objetos almacenados en los bucketsde AmazonS3. 3. Solicitudes:se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos a tasas diferentes de las de otras solicitudes, como PUT y COPY. * GET:recupera un objeto en AmazonS3. Debe tener acceso de LECTURA para utilizar esta operaci\u00f3n. * PUT: agrega un objeto a un bucket. Debe contar con permisos de ESCRITURA en un bucketpara agregarle un objeto. * COPY:crea una copia de un objeto que ya est\u00e1 almacenado en AmazonS3. Una operaci\u00f3n COPY equivale a realizar una solicitud GET y luego una PUT. 4. Transferencia de datos:se considera lacantidad de datos transferidos fuera de la regi\u00f3n de AmazonS3. Recuerde que la transferencia entrante de datos es gratuita, pero se cobra un cargo por la transferencia saliente. S3 Select \u00b6 FIXME: completar Amazon S3 Select le permite utilizar instrucciones de lenguaje de consulta estructurada (SQL) simples para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesita. Si utiliza Amazon S3 Select para filtrar estos datos, puede reducir la cantidad de datos que Amazon S3 transfiere, lo que reduce tambi\u00e9n los costos y la latencia para recuperarlos. Amazon S3 Select funciona con objetos almacenados en formato CSV, JSON o Apache Parquet. Tambi\u00e9n funciona con objetos comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON), as\u00ed como con objetos cifrados del lado del servidor. Puede especificar el formato de los resultados como CSV o JSON, y tambi\u00e9n puede determinar c\u00f3mo se delimitan los registros en los resultados. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles con Amazon S3 Select, consulte Referencia de SQL para Amazon S3 Select y S3 Glacier Select.. Puede realizar consultas de SQL con los SDK de AWS, la API de REST SELECT Object Content, la AWS Command Line Interface (AWS CLI) o la consola de Amazon S3. La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, utilice la AWS CLI o la API. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremon S3 mediante Python directamente y utilizando AWS Lambda. Amazon EFS \u00b6 AmazonEFS ( https://aws.amazon.com/es/efs/ ) implementa almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea. Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS). AmazonEFS es un sistema de archivos de uso compartido que varias instancias de AmazonEC2 pueden montar al mismo tiempo. Amazon Elastic File System (AmazonEFS) proporciona un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Ofrece una interfaz sencilla que le permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. AmazonEFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos. Est\u00e1 dise\u00f1ado para que sus aplicaciones dispongan del almacenamiento que necesiten, cuando lo necesiten. AmazonEFS proporciona almacenamiento de archivos en la nube que resulta ideal para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. AmazonEFS escala de manera ascendente o descendente a medida que se agregan o eliminan archivos y solo requiere pago por lo que se utiliza. AmazonEFS es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS. Arquitecturas en la nube \u00b6 M\u00f3dulo 9 Introducci\u00f3n Secci\u00f3n 1: Principios de dise\u00f1o del marco de buena arquitectura de AWS External Tool Secci\u00f3n 1: Principios de dise\u00f1o del marco de buena arquitectura de AWS Secci\u00f3n 2: Excelencia operativa External Tool Secci\u00f3n 2: Excelencia operativa Secci\u00f3n 3: Seguridad External Tool Secci\u00f3n 3: Seguridad Secci\u00f3n 4: Fiabilidad External Tool Secci\u00f3n 4: Fiabilidad Secci\u00f3n 5: Eficiencia del rendimiento External Tool Secci\u00f3n 5: Eficiencia del rendimiento Secci\u00f3n 6: Optimizaci\u00f3n de costos External Tool Secci\u00f3n 6: Optimizaci\u00f3n de costos Secci\u00f3n 7: Fiabilidad y disponibilidad External Tool Secci\u00f3n 7: Fiabilidad y disponibilidad Secci\u00f3n 8: AWS Trusted Advisor External Tool Secci\u00f3n 8: AWS Trusted Advisor M\u00f3dulo 2 Conclusi\u00f3n External Tool M\u00f3dulo 2 Conclusi\u00f3n Student Guide External Tool Student Guide M\u00f3dulo 9 Evaluaci\u00f3n de conocimientos Assignment M\u00f3dulo 9 Evaluaci\u00f3n de conocimientos Actividades \u00b6 Realizar los m\u00f3dulo 7 (Almacenamiento) y del curso ACF de AWS . A partir del ejemplo de S3 Select, realiza un script en Python que cargue los siguientes datos en un bucket, y recupere XXXXX. A partir del archiov XXXXX, crear un bucket que muestre el contenido como un sitio web. Referencias \u00b6 https://docs.aws.amazon.com/es_es/whitepapers/latest/big-data-analytics-options/welcome.html","title":"4.- Almacenamiento en AWS"},{"location":"apuntes/nube04.html#almacenamiento-en-la-nube","text":"El almacenamiento en la nube, por lo general, es m\u00e1s confiable, escalable y seguro que los sistemas de almacenamiento tradicionales en las instalaciones. El an\u00e1lisis de Big Data , el almacenamiento de datos, el Internet de las cosas (IoT), las bases de datos y las aplicaciones de copias de seguridad y archivo dependen de alg\u00fan tipo de arquitectura de almacenamiento de datos. El almacenamiento m\u00e1s b\u00e1sico es el que incluyen las propias instancias, tambi\u00e9n conocido como el almac\u00e9n de instancias , o almacenamiento ef\u00edmero, es un almacenamiento temporal que se agrega a la instancia de AmazonEC2. El almac\u00e9n de instancias es una buena opci\u00f3n para el almacenamiento temporal de informaci\u00f3n que cambia con frecuencia, como buffers, memorias cach\u00e9, datos de pruebas y dem\u00e1s contenido temporal. Tambi\u00e9n se puede utilizar para los datos que se replican en una flota de instancias, como un grupo de servidores web con balanceo de carga. Si las instancias se detienen, ya sea debido a un error del usuario o un problema de funcionamiento, se eliminar\u00e1n los datos en el almac\u00e9n de instancias. Almacenamiento de bloque o de objeto AWS permite almacenar los datos en bloques o como objetos. Si el almacenamiento es en bloques, los datos se almacenan por trozos (bloques), de manera si se modifica una parte de los datos, solo se ha de modificar el bloque que lo contiene. En cambio, si el almacenamiento es a nivel de objeto, una modificaci\u00f3n implica tener que volver a actualizar el objeto entero. Esto provoca que el almacenamiento por bloque sea m\u00e1s r\u00e1pido. En cambio, el almacenamiento de objetos es m\u00e1s sencillo y por tanto m\u00e1s barato. AWS ofrece m\u00faltiples soluciones que vamos a revisar.","title":"Almacenamiento en la nube"},{"location":"apuntes/nube04.html#amazon-ebs","text":"Amazon Elastic Block Store ( https://aws.amazon.com/es/ebs/ ) ofrece vol\u00famenes de almacenamiento a nivel de bloque de alto rendimiento para utilizarlos con instancias de Amazon EC2 para las cargas de trabajo con un uso intensivo de transacciones y de rendimiento. Los beneficios adicionales incluyen la replicaci\u00f3n en la misma zona de disponibilidad, el cifrado f\u00e1cil y transparente, los vol\u00famenes el\u00e1sticos y las copias de seguridad mediante instant\u00e1neas. Importante AmazonEBS se puede montar en una instancia de AmazonEC2 solamente dentro de la misma zona de disponibilidad.","title":"Amazon EBS"},{"location":"apuntes/nube04.html#volumenes","text":"Los vol\u00famenes de EBS proporcionan almacenamiento fuera de las instancias que persiste independientemente de la vida de la instancia. Son similares a discos virtuales en la nube. AmazonEBS ofrece tres tipos de vol\u00famenes: SSD de uso general, SSD de IOPS provisionadas y magn\u00e9ticos (HDD). Los tres tipos de vol\u00famenes difieren en caracter\u00edsticas de rendimiento y coste, para ofrecer diferentes posibilidades seg\u00fan las necesidades de las aplicaciones. FIXME: Completar Para crear o configurar un volumen, dentro de las instancias EC2, en el men\u00fa lateral podemos ver las opciones de ..... Los vol\u00famenes de Amazon EBS est\u00e1n asociados a la red, y su duraci\u00f3n es independiente a la vida de una instancia. Los vol\u00famenes de Amazon EBS tienen un alto nivel de disponibilidad y de confianza, y pueden aprovecharse como particiones de arranque de instancias de Amazon EC2 o asociarse a una instancia de Amazon EC2 en ejecuci\u00f3n como dispositivos de bloques est\u00e1ndar. Cuando se utilizan como particiones de arranque, las instancias de Amazon EC2 pueden detenerse y, posteriormente, reiniciarse, lo que le permite pagar solo por los recursos de almacenamiento utilizados al mismo tiempo que conserva el estado de la instancia. Los vol\u00famenes de Amazon EBS tienen durabilidad mucho mayor que los almacenes de instancias de Amazon EC2 locales porque los vol\u00famenes de Amazon EBS se replican autom\u00e1ticamente en el backend (en una \u00fanica zona de disponibilidad). Los vol\u00famenes de Amazon EBS ofrecen las siguientes caracter\u00edsticas: Almacenamiento persistente: el tiempo de vida de los vol\u00famenes es independiente de cualquier instancia de Amazon EC2. De uso general: los vol\u00famenes de Amazon EBS son dispositivos de bloques sin formato que se pueden utilizar en cualquier sistema operativo. Alto rendimiento: los vol\u00famenes de Amazon EBS son iguales que las unidades de Amazon EC2 locales o mejores que ellas. Nivel de fiabilidad alto: los vol\u00famenes de Amazon EBS tienen redundancia integrada dentro de una zona de disponibilidad. Dise\u00f1ados para ofrecer resiliencia: la AFR (tasa anual de errores) de Amazon EBS oscila entre 0,1 % y 1 %. Tama\u00f1o variable: los tama\u00f1os de los vol\u00famenes var\u00edan entre 1 GB y 16 TB. F\u00e1ciles de usar: los vol\u00famenes de Amazon EBS se pueden crear, asociar, almacenar en copias de seguridad, restaurar y eliminar f\u00e1cilmente. Solo una instancia de AmazonEC2 a la vez puede montarse en un volumen de Amazon EBS.","title":"Vol\u00famenes"},{"location":"apuntes/nube04.html#instantaneas","text":"Sin embargo, para los que quieran a\u00fan m\u00e1s durabilidad, con Amazon EBS es posible crear instant\u00e1neas uniformes puntuales de los vol\u00famenes, que luego se almacenan en Amazon Simple Storage Service (Amazon S3) y se replican autom\u00e1ticamente en varias zonas de disponibilidad. Estas instant\u00e1neas se pueden utilizar como punto de partida para nuevos vol\u00famenes de Amazon EBS y permiten proteger la durabilidad de sus datos a largo plazo. Tambi\u00e9n puede compartirlas f\u00e1cilmente con colegas y otros desarrolladores de AWS. Cuando lo desee, podr\u00e1 crear una cantidad ilimitada de instant\u00e1neas uniformes de un momento espec\u00edfico de los vol\u00famenes de Amazon EBS. Las instant\u00e1neas de Amazon EBS se almacenan en Amazon S3 con un alto nivel de durabilidad. Se pueden crear vol\u00famenes de Amazon EBS nuevos a partir de instant\u00e1neas para clonar o restaurar copias de seguridad. Las instant\u00e1neas de Amazon EBS tambi\u00e9n pueden compartirse f\u00e1cilmente entre usuarios de AWS o copiarse entre regiones de AWS.","title":"Instant\u00e1neas"},{"location":"apuntes/nube04.html#amazon-s3","text":"AmazonS3 ( https://aws.amazon.com/es/s3/ ) es un servicio de almacenamiento persistente de objetos creado para almacenar y recuperar cualquier cantidad de datos desde cualquier lugar mediante una URL: sitios web y aplicaciones m\u00f3viles, aplicaciones corporativas y datos de sensores o dispositivos de Internet de las cosas (IoT) y an\u00e1lisis de Big Data . S3 es un servicio de almacenamiento a nivel de objetos , y tal como hab\u00edamos comentado, significa que si desea cambiar una parte de un archivo, tiene que realizar la modificaci\u00f3n y luego volver a cargar todo el archivo modificado. Los datos como objetos dentro de recursos conocidos como buckets . S3 es una soluci\u00f3n administrada de almacenamiento en la nube que se dise\u00f1\u00f3 para brindar un escalado sin problemas y 99,999999999% (11 nueves) de durabilidad. Adem\u00e1s de poder almacenar pr\u00e1cticamente todos los objetos que desee dentro de un bucket, le permite realizar operaciones de escritura, lectura y eliminaci\u00f3n de los objetos almacenados en el bucket. Los nombres de los buckets son universales y deben ser \u00fanicos entre todos los nombres de buckets existentes en Amazon S3. Los objetos pueden ser de hasta 5TB. De forma predeterminada, en Amazon S3 los datos se almacenan de forma redundante en varias instalaciones y en diferentes dispositivos de cada instalaci\u00f3n. Los datos que almacena en AmazonS3 no est\u00e1n asociados a ning\u00fan servidor en particular. Adem\u00e1s, no necesita administrar ninguna infraestructura por su cuenta. Puede colocar tantos objetos como quiera en Amazon S3. Amazon S3 contiene billones de objetos y, con regularidad, tiene picos de millones de solicitudes por segundo. Los objetos pueden ser pr\u00e1cticamente cualquier archivo de datos, como im\u00e1genes, videos o registros del servidor. Ya que Amazon S3 admite objetos de hasta varios terabytes de tama\u00f1o, le permite incluso almacenar instant\u00e1neas de bases de datos como objetos. Amazon S3 tambi\u00e9n ofrece acceso de baja latencia a los datos a trav\u00e9s de Internet mediante el protocolo de transferencia de hipertexto (HTTP) o el HTTP seguro (HTTPS), para que pueda recuperar datos en cualquier momento y desde cualquier lugar. Tambi\u00e9n puede acceder a AmazonS3 de forma privada a trav\u00e9s de un punto de enlace de nube virtual privada (VPC). Obtendr\u00e1 un control detallado sobre qui\u00e9n puede acceder a sus datos a trav\u00e9s de las pol\u00edticas de AWS Identity and Access Management (IAM), las pol\u00edticas de bucket de AmazonS3 e, incluso, las listas de control de acceso por objeto. De forma predeterminada, no se comparte ninguno de sus datos p\u00fablicamente. Tambi\u00e9n puede cifrar los datos en tr\u00e1nsito y elegir habilitar el cifrado del lado del servidor en sus objetos. Puede acceder a AmazonS3 a trav\u00e9s de la consola de administraci\u00f3n de AWS basada en la web, de forma program\u00e1tica a trav\u00e9s de la API y los SDK, o con soluciones de terceros que utilizan la API o los SDK. AmazonS3 incluye notificaciones de eventos que le permiten configurar notificaciones autom\u00e1ticas cuando se producen determinados eventos, como la carga o la eliminaci\u00f3n de un objeto en un bucketespec\u00edfico. Se le pueden enviar estas notificaciones o pueden utilizarse para desencadenar otros procesos, como funciones de AWS Lambda.","title":"Amazon S3"},{"location":"apuntes/nube04.html#clases-de-almacenamiento","text":"S3 ofrece una variedad de clases de almacenamiento ( https://docs.aws.amazon.com/es_es/AmazonS3/latest/userguide/storage-class-intro.html ) a nivel de objetos que est\u00e1n dise\u00f1adas para diferentes casos de uso. Entre estas clases se incluyen las siguientes: AmazonS3 Est\u00e1ndar: la clase de almacenamiento Est\u00e1ndar de S3 est\u00e1 dise\u00f1ada para ofrecer almacenamiento de objetos de alta durabilidad, disponibilidad y rendimiento para los datos a los que se accede con frecuencia. Como ofrece baja latencia y alto nivel de rendimiento, Est\u00e1ndar de S3 es una opci\u00f3n adecuada para una amplia variedad de casos de uso, como las aplicaciones en la nube, los sitios web din\u00e1micos, la distribuci\u00f3n de contenido, las aplicaciones para dispositivos m\u00f3viles y videojuegos, y el an\u00e1lisis de bigdata. AmazonS3 Intelligent-Tiering: la clase de almacenamiento AmazonS3 Intelligent-Tieringse ha dise\u00f1ado para optimizar los costos mediante la migraci\u00f3n autom\u00e1tica de los datos a la capa de acceso m\u00e1s rentable, sin que se perjudique el rendimiento ni se produzca una sobrecarga operativa. Por una peque\u00f1a tarifa mensual de monitoreo y automatizaci\u00f3n por objeto, AmazonS3 monitorea los patrones de acceso de los objetos en AmazonS3 IntelligentTieringy traslada aquellos a los que no se ha accedido durante 30d\u00edas consecutivos a la capa de acceso poco frecuente. Si se accede a un objeto en la capa de acceso poco frecuente, este se traslada autom\u00e1ticamente a la capa de acceso frecuente. No se aplican tarifas por la recuperaci\u00f3n si se usa la clase de almacenamiento AmazonS3 IntelligentTieringni tampoco se cobran tarifas adicionales cuando los objetos se trasladan de una capa de acceso a otra. Funciona bien con datos de larga duraci\u00f3n con patrones de acceso desconocidos o impredecibles. AmazonS3 Est\u00e1ndar -Acceso poco frecuente: esta clase de almacenamiento de AmazonS3 se utiliza para los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. La clase Est\u00e1ndar -Acceso poco frecuente de S3 est\u00e1 dise\u00f1ada para ofrecer la alta durabilidad, el alto rendimiento y la baja latencia de Est\u00e1ndar de S3, con precios bajos de almacenamiento por GB y de recuperaci\u00f3n por GB. Esta combinaci\u00f3n de alto rendimiento y bajo costo convierte a Est\u00e1ndar -Acceso poco frecuente de S3 en una opci\u00f3n ideal para el almacenamiento y las copias de seguridad a largo plazo, adem\u00e1s de en un almac\u00e9n de datos para los archivos de recuperaci\u00f3n de desastres. AmazonS3 \u00danica zona \u2013Acceso poco frecuente: esta clase de almacenamiento de AmazonS3 est\u00e1 dise\u00f1ada para guardar los datos a los que se accede con menos frecuencia, pero que requieren acceso r\u00e1pido cuando es necesario. A diferencia de otras clases de almacenamiento de AmazonS3 que guardan datos en un m\u00ednimo de tres zonas de disponibilidad, \u00danica zona -Acceso poco frecuente de S3 almacena datos en una \u00fanica zona de disponibilidad y cuesta menos que Est\u00e1ndar -Acceso poco frecuente de S3. La clase \u00danica zona -Acceso poco frecuente de S3 es ideal para aquellos clientes que desean una opci\u00f3n de menor costo para los datos a los que se accede con poca frecuencia y que no necesitan el nivel de disponibilidad ni la resiliencia de Est\u00e1ndar de S3 o de Est\u00e1ndar -Acceso poco frecuente de S3. Es una buena opci\u00f3n para almacenar copias de seguridad secundarias de los datos que se encuentran en las instalaciones o de los datos que se pueden volver a crear f\u00e1cilmente. Tambi\u00e9n puede utilizarla como una opci\u00f3n de almacenamiento rentable para los datos que se replican desde otra regi\u00f3n de AWS con la replicaci\u00f3n entre regiones de AmazonS3. AmazonS3 Glacier ( https://aws.amazon.com/es/s3/glacier/ ): esta es una clase de almacenamiento seguro, duradero y de bajo costo para archivar datos. Puede almacenar con confianza cualquier cantidad de datos a costos que son competitivos o m\u00e1s econ\u00f3micos que los de las soluciones en las instalaciones. Para que los costos se mantengan bajos, pero sigan siendo aptos para diversas necesidades, AmazonS3 Glacier proporciona tres opciones de recuperaci\u00f3n, que van desde unos pocos minutos a unas horas. Puede cargar objetos directamente en AmazonS3 Glacier o utilizar pol\u00edticas de ciclo de vida de AmazonS3 para transferir datos entre cualquiera de las clases de almacenamiento de AmazonS3 para datos activos (Est\u00e1ndar, IntelligentTiering, Est\u00e1ndar -Acceso poco frecuente y \u00danica zona -Acceso poco frecuente) y AmazonS3 Glacier. AmazonS3 Glacier Deep Archive: esta es la clase de almacenamiento de menor costo en AmazonS3. Admite la retenci\u00f3n a largo plazo y la preservaci\u00f3n digital de datos a los que es posible que se acceda solo una o dos veces por a\u00f1o. Se dise\u00f1\u00f3 para clientes, en particular, para aquellos clientes que pertenecen a sectores con niveles de regulaci\u00f3n muy estrictos, como los servicios financieros, la sanidad y los sectores p\u00fablicos, los cuales retienen conjuntos de datos durante un periodo de7 a10a\u00f1os o m\u00e1s para cumplir los requisitos de conformidad normativa. AmazonS3 Glacier Deep Archive tambi\u00e9n se puede utilizar para casos de uso de copias de seguridad y de recuperaci\u00f3n de desastres. Se trata de una alternativa rentable y f\u00e1cil de administrar a los sistemas de cintas magn\u00e9ticas, independientemente de si estos sistemas de cintas son bibliotecas en las instalaciones o servicios fuera de ellas. AmazonS3 Glacier Deep Archive complementa a AmazonS3 Glacier y tambi\u00e9n est\u00e1 dise\u00f1ado para proporcionar una durabilidad del 99,999999999% (11nueves). Todos los objetos almacenados en AmazonS3 Glacier Deep Archive se replican y almacenan en al menos tres zonas de disponibilidad geogr\u00e1ficamente dispersas, y se pueden restaurar en 12horas.Para obtener m\u00e1s informaci\u00f3n, consulteClases de almacenamiento de AmazonS3. Con el an\u00e1lisis de clase de almacenamiento, puede evaluar los patrones de acceso al almacenamiento y transferir los datos correctos a la clase de almacenamiento adecuada. La caracter\u00edstica de an\u00e1lisis de AmazonS3 identifica autom\u00e1ticamente la pol\u00edtica de ciclo de vida \u00f3ptima para transferir el almacenamiento al que se accede con menos frecuencia a Est\u00e1ndar -Acceso poco frecuente de S3. Puede configurar una pol\u00edtica de an\u00e1lisis de la clase de almacenamiento para monitorear un bucket completo, un prefijo o una etiqueta de objeto. Una vez que se observa un patr\u00f3n de acceso poco frecuente, se puede crear con facilidad una nueva pol\u00edtica de ciclo de vida en funci\u00f3n de los resultados. El an\u00e1lisis de clases de almacenamiento tambi\u00e9n proporciona visualizaciones diarias del uso del almacenamiento en la consola de administraci\u00f3n de AWS. Puede exportarlas a un bucketde AmazonS3 para analizarlas utilizando las herramientas de inteligencia empresarial que prefiera, como Amazon QuickSight.","title":"Clases de almacenamiento"},{"location":"apuntes/nube04.html#buckets","text":"Para utilizar AmazonS3 de forma eficaz, debe comprender algunos conceptos sencillos. En primer lugar, Amazon S3 almacena los datos en buckets. Los buckets son esencialmente el prefijo de un conjunto de archivos y, como tales, deben tener un nombre \u00fanico en todo AmazonS3 a nivel mundial. Los buckets son contenedores l\u00f3gicos de objetos. Puede tener uno o m\u00e1s buckets en su cuenta. Puede controlar el acceso a cada bucket, es decir, qui\u00e9n puede crear, eliminar y enumerar objetos en el bucket. Tambi\u00e9n puede ver registros de acceso al buckety a sus objetos, adem\u00e1s de elegir la regi\u00f3n geogr\u00e1fica donde AmazonS3 almacenar\u00e1 el buckety su contenido. Para cargar sus datos (como fotos, videos o documentos), cree un bucket en una regi\u00f3n de AWS y, a continuaci\u00f3n, cargue casi cualquier cantidad de objetos en el bucket. En el ejemplo, AmazonS3 se utiliz\u00f3 para crear un bucketen la regi\u00f3n de Tokio, que se identifica formalmente dentro de AWS con el c\u00f3digo de regi\u00f3n ap-northeast-1.La direcci\u00f3n URL de un bucketest\u00e1 estructurada como en los ejemplos. Puede utilizar dos estilos de URL diferentes para hacer referencia a los buckets. AmazonS3 se refiere a los archivos como objetos.Tan pronto como tenga un bucket, podr\u00e1 almacenar casi cualquier cantidad de objetos dentro de \u00e9l. Un objeto est\u00e1 compuesto por los datos y por cualquier metadato que describa a ese archivo, incluida la direcci\u00f3n URL. Para almacenar un objeto en AmazonS3, debe cargar en un bucketel archivo que quiera almacenar. Cuando carga un archivo, puede establecer permisos sobre los datos y cualquier metadato. En este ejemplo, el objeto Preview2.mp4 se almacena dentro del bucket. La direcci\u00f3n URL del archivo incluye el nombre del objeto al final. Cuando se crea un bucketen AmazonS3, este se asocia a una regi\u00f3n de AWS espec\u00edfica. Cuando almacena datos en el bucket, estos se almacenan de forma redundante en varias instalaciones de AWS dentro de la regi\u00f3n seleccionada. AmazonS3 est\u00e1 dise\u00f1ado para almacenar sus datos de forma duradera, incluso en el caso de producirse una p\u00e9rdida de datos simult\u00e1nea en dos instalaciones de AWS. AmazonS3 administra autom\u00e1ticamente el almacenamiento detr\u00e1s de su bucketa medida que aumenta la cantidad de datos. Puede comenzar de inmediato, y el almacenamiento de sus datos aumentar\u00e1 en funci\u00f3n de las necesidades de la aplicaci\u00f3n. AmazonS3 tambi\u00e9n es escalable, lo que le permitir\u00e1 gestionar un volumen elevado de solicitudes. No es necesario aprovisionar el almacenamiento ni el rendimiento, y solo se le facturar\u00e1 por lo que utilice. Puede obtener acceso a AmazonS3 a trav\u00e9s de la consola, de la interfaz de l\u00ednea de comandos de AWS (CLI de AWS), o del SDK de AWS. Tambi\u00e9n puede acceder a los datos de su bucketdirectamente a trav\u00e9s de los puntos de enlace basados en REST. Los puntos de enlace admiten el acceso HTTP o HTTPS. Para admitir este tipo de acceso basado en URL, los nombres de los bucketsde AmazonS3 deben ser \u00fanicos a nivel mundial y tambi\u00e9n deben cumplir los requisitos del servidor de nombres de dominio (DNS). Adem\u00e1s, las claves de objetos deben utilizar caracteres que sean seguros para las direcciones URL. FIXME: Ejemplo con pantallazos de como crear un bucket","title":"Buckets"},{"location":"apuntes/nube04.html#casos-de-uso","text":"Esta flexibilidad para almacenar una cantidad pr\u00e1cticamente ilimitada de datos y para acceder a ellos desde cualquier lugar convierte a AmazonS3 en un servicio adecuado para distintos casos. Ahora, consideraremos algunos casos de uso para AmazonS3: Como ubicaci\u00f3n para cualquier dato de aplicaci\u00f3n, los bucketsde AmazonS3 proporcionan una ubicaci\u00f3n compartida para almacenar objetos a los que cualquier instancia de su aplicaci\u00f3n puede acceder, incluso las aplicaciones de AmazonEC2 o hasta los servidores tradicionales. Esta caracter\u00edstica puede resultar \u00fatil para los archivos multimedia generados por el usuario, los registros del servidor u otros archivos que su aplicaci\u00f3n deba almacenar en una ubicaci\u00f3n com\u00fan. Adem\u00e1s, como el contenido se puede obtener de manera directa a trav\u00e9s de Internet, es posible delegar una parte de la entrega de ese contenido de su aplicaci\u00f3n y permitir que los clientes consigan ellos mismos de forma directa los datos de AmazonS3. Para el alojamiento web est\u00e1tico, los bucketsde AmazonS3 pueden entregar el contenido est\u00e1tico de su sitio web, que incluye HTML, CSS, JavaScript y otros archivos. La gran durabilidad de AmazonS3 lo convierte en una buena opci\u00f3n para almacenar copias de seguridad de sus datos. Para una disponibilidad y capacidad de recuperaci\u00f3n de desastres incluso mejores, AmazonS3 puede hasta configurarse para admitir la replicaci\u00f3n entre regiones, de modo que los datos ubicados en un bucketde AmazonS3 en una regi\u00f3n puedan replicarse de forma autom\u00e1tica en otra regi\u00f3n de AmazonS3. Copia de seguridad y almacenamiento:preste servicios de copia de seguridad y almacenamiento de datos a terceros. Alojamiento de aplicaciones:preste servicios que implementen, instalen y administren aplicaciones web. Alojamiento multimedia:cree una infraestructura redundante, escalable y de alta disponibilidad que aloje cargas y descargas de videos, fotos o m\u00fasica. Entrega de software:aloje sus aplicaciones de software para que los clientes puedan descargarlas.","title":"Casos de uso"},{"location":"apuntes/nube04.html#costes","text":"Con AmazonS3, los costos espec\u00edficos var\u00edan en funci\u00f3n de la regi\u00f3n y de las solicitudes espec\u00edficas que se realizaron. Solo paga por lo que utiliza, lo que incluye gigabytes por mes; transferencias desde otras regiones; y solicitudes PUT, COPY, POST, LIST y GET. Como regla general, solo paga por las transferencias que cruzan el l\u00edmite de su regi\u00f3n, lo que significa que no paga por las transferencias entrantesa AmazonS3 ni por las transferencias salientes desde AmazonS3 a las ubicaciones de borde de Amazon CloudFrontdentro de esa misma regi\u00f3n. Para comenzar a calcular los costos de AmazonS3, debe tener en cuenta lo siguiente: 1. Clase de almacenamiento: * El almacenamiento est\u00e1ndar est\u00e1 dise\u00f1ado para proporcionar 99,999999999% (11nueves) de durabilidad y 99,99% (4nueves) de disponibilidad. * Est\u00e1ndar -Acceso poco frecuente de S3es una opci\u00f3n de almacenamiento dentro de AmazonS3 que puede utilizar para reducir sus costos al guardar los datos a los que se accede con menos frecuencia con niveles de redundancia ligeramente inferiores a los del almacenamiento est\u00e1ndar de AmazonS3. El almacenamiento Est\u00e1ndar -Acceso poco frecuente est\u00e1 dise\u00f1ado para ofrecer la misma durabilidad de 99,999999999% (11nueves) de AmazonS3, pero con 99,9% (3nueves) de disponibilidad en un a\u00f1o concreto. Cada clase tiene diferentes tasas. 2. Cantidad de almacenamiento:se refiere a la cantidad y al tama\u00f1o de los objetos almacenados en los bucketsde AmazonS3. 3. Solicitudes:se consideran la cantidad y el tipo de las solicitudes. Las solicitudes GET generan cargos a tasas diferentes de las de otras solicitudes, como PUT y COPY. * GET:recupera un objeto en AmazonS3. Debe tener acceso de LECTURA para utilizar esta operaci\u00f3n. * PUT: agrega un objeto a un bucket. Debe contar con permisos de ESCRITURA en un bucketpara agregarle un objeto. * COPY:crea una copia de un objeto que ya est\u00e1 almacenado en AmazonS3. Una operaci\u00f3n COPY equivale a realizar una solicitud GET y luego una PUT. 4. Transferencia de datos:se considera lacantidad de datos transferidos fuera de la regi\u00f3n de AmazonS3. Recuerde que la transferencia entrante de datos es gratuita, pero se cobra un cargo por la transferencia saliente.","title":"Costes"},{"location":"apuntes/nube04.html#s3-select","text":"FIXME: completar Amazon S3 Select le permite utilizar instrucciones de lenguaje de consulta estructurada (SQL) simples para filtrar el contenido de los objetos de Amazon S3 y recuperar exactamente el subconjunto de datos que necesita. Si utiliza Amazon S3 Select para filtrar estos datos, puede reducir la cantidad de datos que Amazon S3 transfiere, lo que reduce tambi\u00e9n los costos y la latencia para recuperarlos. Amazon S3 Select funciona con objetos almacenados en formato CSV, JSON o Apache Parquet. Tambi\u00e9n funciona con objetos comprimidos con GZIP o BZIP2 (solo para objetos CSV y JSON), as\u00ed como con objetos cifrados del lado del servidor. Puede especificar el formato de los resultados como CSV o JSON, y tambi\u00e9n puede determinar c\u00f3mo se delimitan los registros en los resultados. Las expresiones SQL se pasan a Amazon S3 en la solicitud. Amazon S3 Select es compatible con un subconjunto de SQL. Para obtener m\u00e1s informaci\u00f3n sobre los elementos SQL compatibles con Amazon S3 Select, consulte Referencia de SQL para Amazon S3 Select y S3 Glacier Select.. Puede realizar consultas de SQL con los SDK de AWS, la API de REST SELECT Object Content, la AWS Command Line Interface (AWS CLI) o la consola de Amazon S3. La consola de Amazon S3 limita la cantidad de datos devueltos a 40 MB. Para recuperar m\u00e1s datos, utilice la AWS CLI o la API. Trabajando programativamente con S3 / S3 Select En el bloque de ingesta de datos, atacaremon S3 mediante Python directamente y utilizando AWS Lambda.","title":"S3 Select"},{"location":"apuntes/nube04.html#amazon-efs","text":"AmazonEFS ( https://aws.amazon.com/es/efs/ ) implementa almacenamiento para las instancias EC2 a las que pueden acceder varias m\u00e1quinas virtuales de forma simult\u00e1nea. Se ha implementado como un sistema de archivos de uso compartido que utiliza el protocolo de sistemas de archivos de red (NFS). AmazonEFS es un sistema de archivos de uso compartido que varias instancias de AmazonEC2 pueden montar al mismo tiempo. Amazon Elastic File System (AmazonEFS) proporciona un almacenamiento de archivos simple, escalable y el\u00e1stico para utilizarlo con los servicios de AWS y los recursos disponibles en las instalaciones. Ofrece una interfaz sencilla que le permite crear y configurar sistemas de archivos de forma r\u00e1pida y simple. AmazonEFS est\u00e1 dise\u00f1ado para escalar a petabytes de manera din\u00e1mica bajo demanda sin interrumpir las aplicaciones, por lo que se ampliar\u00e1 y reducir\u00e1 de forma autom\u00e1tica a medida que agregue o elimine archivos. Est\u00e1 dise\u00f1ado para que sus aplicaciones dispongan del almacenamiento que necesiten, cuando lo necesiten. AmazonEFS proporciona almacenamiento de archivos en la nube que resulta ideal para bigdata y an\u00e1lisis, flujos de trabajo de procesamiento multimedia, administraci\u00f3n de contenido, servidores web y directorios principales. AmazonEFS escala de manera ascendente o descendente a medida que se agregan o eliminan archivos y solo requiere pago por lo que se utiliza. AmazonEFS es un servicio completamente administrado al que se puede acceder desde la consola, una API o la CLI de AWS.","title":"Amazon EFS"},{"location":"apuntes/nube04.html#arquitecturas-en-la-nube","text":"M\u00f3dulo 9 Introducci\u00f3n Secci\u00f3n 1: Principios de dise\u00f1o del marco de buena arquitectura de AWS External Tool Secci\u00f3n 1: Principios de dise\u00f1o del marco de buena arquitectura de AWS Secci\u00f3n 2: Excelencia operativa External Tool Secci\u00f3n 2: Excelencia operativa Secci\u00f3n 3: Seguridad External Tool Secci\u00f3n 3: Seguridad Secci\u00f3n 4: Fiabilidad External Tool Secci\u00f3n 4: Fiabilidad Secci\u00f3n 5: Eficiencia del rendimiento External Tool Secci\u00f3n 5: Eficiencia del rendimiento Secci\u00f3n 6: Optimizaci\u00f3n de costos External Tool Secci\u00f3n 6: Optimizaci\u00f3n de costos Secci\u00f3n 7: Fiabilidad y disponibilidad External Tool Secci\u00f3n 7: Fiabilidad y disponibilidad Secci\u00f3n 8: AWS Trusted Advisor External Tool Secci\u00f3n 8: AWS Trusted Advisor M\u00f3dulo 2 Conclusi\u00f3n External Tool M\u00f3dulo 2 Conclusi\u00f3n Student Guide External Tool Student Guide M\u00f3dulo 9 Evaluaci\u00f3n de conocimientos Assignment M\u00f3dulo 9 Evaluaci\u00f3n de conocimientos","title":"Arquitecturas en la nube"},{"location":"apuntes/nube04.html#actividades","text":"Realizar los m\u00f3dulo 7 (Almacenamiento) y del curso ACF de AWS . A partir del ejemplo de S3 Select, realiza un script en Python que cargue los siguientes datos en un bucket, y recupere XXXXX. A partir del archiov XXXXX, crear un bucket que muestre el contenido como un sitio web.","title":"Actividades"},{"location":"apuntes/nube04.html#referencias","text":"https://docs.aws.amazon.com/es_es/whitepapers/latest/big-data-analytics-options/welcome.html","title":"Referencias"},{"location":"apuntes/nube05.html","text":"Datos en la nube \u00b6 Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia AWS se encarga de gestionar el escalado, la tolerancia a errores y la disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostreSQL, ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches. Datos relacionales - Amazon RDS \u00b6 AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube relacional, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones. Instancias de bases de datos \u00b6 El bloque de creaci\u00f3n b\u00e1sico de Amazon RDS es la instancia de base de datos. Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que le permite adaptar el costo y el rendimiento a las necesidades de su base de datos. Cuando elige crear una instancia de base de datos, primero tiene que especificar qu\u00e9 motor de base de datos ejecutar. Actualmente, AmazonRDS admite seis bases de datos: MySQL, Amazon Aurora, MicrosoftSQL Server, PostgreSQL, MariaDBy Oracle. Puede ejecutar una instancia utilizando Amazon Virtual PrivateCloud (AmazonVPC). Cuando utiliza una nube virtual privada (VPC), tiene control sobre su entorno de red virtual. Puede seleccionar su propio intervalo de direccionesIP, crear subredes y configurar las listas de control de acceso (ACL) y el direccionamiento. La funcionalidad b\u00e1sica de AmazonRDS es la misma ya sea que se ejecute o no en una VPC. En general, la instancia de base de datos se encuentra aislada en una subred privada, y solo pueden acceder directamente a ella las instancias de aplicaci\u00f3n determinadas. Las subredes de una VPC est\u00e1n asociadas a una \u00fanica zona de disponibilidad, por lo que, cuando selecciona la subred, tambi\u00e9n elige la zona de disponibilidad (o la ubicaci\u00f3n f\u00edsica) de su instancia de base de datos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.18 Alta disponibilidad \u00b6 Una de las caracter\u00edsticas m\u00e1s importantes de AmazonRDS es la capacidad de configurar su instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ. Una vez configurada la implementaci\u00f3n Multi-AZ, AmazonRDS genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma sincr\u00f3nica a la copia en espera. La ejecuci\u00f3n de una instancia de base de datos en una implementaci\u00f3n Multi-AZ puede mejorar la disponibilidad durante el mantenimiento programado del sistema y ayudar a proteger sus bases de datos contra los errores de las instancias de base de datos y la interrupci\u00f3n de la zona de disponibilidad. Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, AmazonRDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal. La replicaci\u00f3n sincr\u00f3nica minimiza la posibilidad de que haya p\u00e9rdida de datos. Dado que sus aplicaciones hacen referencia a la base de datos por su nombre mediante el punto de enlace del sistema de nombres de dominio (DNS) de AmazonRDS, no tiene que cambiar nada en el c\u00f3digo de las aplicaciones a fin de utilizar la copia en espera para la conmutaci\u00f3n por error. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.20 Replica de lectura AmazonRDS tambi\u00e9n admite la creaci\u00f3n de r\u00e9plicas de lectura para MySQL, MariaDB, PostgreSQLy Amazon Aurora. Las actualizaciones que se realizan a la instancia de base de datos de origen se copian de manera as\u00edncrona en la instancia de r\u00e9plica de lectura. Puede reducir la carga sobre la instancia de base de datos de origen por medio del direccionamiento de las consultas de lectura desde sus aplicaciones a la r\u00e9plica de lectura. Si se utilizan r\u00e9plicas de lectura, tambi\u00e9n puede realizar un escalado horizontal m\u00e1s all\u00e1 de los l\u00edmites de capacidad de una \u00fanica instancia de base de datos para cargas de trabajo de base de datos con operaciones intensivas de lectura. Las r\u00e9plicas de lectura tambi\u00e9n pueden convertirse en la instancia de base de datos principal, pero, debido a la replicaci\u00f3n as\u00edncrona, esto debe hacerse de forma manual. Las r\u00e9plicas de lectura pueden crearse en una regi\u00f3n diferente a la utilizada por la base de datos principal. Esta caracter\u00edstica puede ayudar a cumplir los requisitos de recuperaci\u00f3n de desastres o a disminuir la latencia al dirigir las lecturas a una r\u00e9plica de lectura que est\u00e9 m\u00e1s cerca del usuario Casos de uso \u00b6 AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Como no existen restricciones de licencia en AmazonRDS, el servicio se ajusta a los patrones de uso variable de estas aplicaciones. AmazonRDS ofrece a las empresas de comercio electr\u00f3nico grandes y peque\u00f1as una soluci\u00f3n de base de datos flexible, segura y de bajo costo para la venta minorista y el comercio en l\u00ednea. Los juegos m\u00f3viles y en l\u00ednea requieren una plataforma de base de datos con alto rendimiento y disponibilidad. AmazonRDS administra la infraestructura de la base de datos, de manera que los desarrolladores de videojuegos no tengan que preocuparse por aprovisionar, escalar o monitorear los servidores de base de datos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.22 Utilice AmazonRDS cuando su aplicaci\u00f3n necesite lo siguiente: \u2022Transacciones o consultas complejas\u2022Tasa de consulta o escritura media a alta: hasta 30000IOPS (15000lecturas+ 15000escrituras)\u2022No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo\u2022Alta durabilidadNo utilice AmazonRDS cuando su aplicaci\u00f3n necesite lo siguiente:\u2022Tasas de lectura o escritura muy grandes (por ejemplo, 150000 escrituras por segundo)\u2022Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento \u2022Solicitudes y consultas GET o PUT simples que una base de datos NoSQLpueda manejar\u2022O personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (RDBMS)En los casos en que no deba utilizar Amazon RDS, considere la posibilidad de utilizar una soluci\u00f3n de base de datos NoSQL, como DynamoDB, o ejecutar su motor de base de datos relacional en instancias de AmazonEC2 en lugar de AmazonRDS, lo cual le proporcionar\u00e1 m\u00e1s opciones para personalizar la base de datos. Costes \u00b6 Cuando comience a calcular el costo de AmazonRDS, debe tener en cuenta las horas de reloj del servicio, que son recursos que generan cargos cuando se ejecutan (por ejemplo, desde el momento en que lanza una instancia de base de datos hasta que la termina). Tambi\u00e9n hay que tener en cuenta las caracter\u00edsticas de la base de datos. La capacidad f\u00edsica de la base de datos que elija tendr\u00e1 un efecto sobre cu\u00e1nto se le cobre. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tama\u00f1o y la clase de memoria de la base de datos Tenga en cuenta el tipo de compra de base de datos. Cuando utiliza instancias bajo demanda, paga la capacidad de c\u00f3mputo por cada hora en que se ejecuta su instancia de base de datos, sin necesidad de compromisos m\u00ednimos. Con las instancias reservadas, puede realizar un peque\u00f1o pago inicial \u00fanico por cada instancia de base de datos que desee reservar por un periodo de1 o 3a\u00f1os. Tambi\u00e9n debe tener en cuenta la cantidad de instancias de base de datos. Con AmazonRDS, puede aprovisionar varias instancias de base de datos para gestionar cargas m\u00e1ximas. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.25 Considere el almacenamiento aprovisionado. No hay cargos adicionales por el almacenamiento para copias de seguridad de hasta el 100% de su almacenamiento de base de datos aprovisionado para una instancia de base de datos activa. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. Tambi\u00e9n tenga en cuenta la cantidad de almacenamiento para copias de seguridad que se suma a la cantidad de almacenamiento aprovisionado, la cual se factura por GB por mes. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.26 Adem\u00e1s, considere la cantidad de solicitudes de entrada y de salida que se realizaron a la base de datos.Considere el tipo de implementaci\u00f3n. Puede implementar su instancia de base de datos en una \u00fanica zona de disponibilidad (lo que equivale a un centro de datos independiente) o en varias zonas de disponibilidad (lo que equivale a un centro de datos secundario para disponer de mayor disponibilidad y durabilidad). Los cargos por almacenamiento y operaciones de E/S var\u00edan seg\u00fan la cantidad de zonas de disponibilidad en las que se realiza la implementaci\u00f3n. Por \u00faltimo, tenga en cuenta la transferencia de datos. La transferencia entrante de datos es gratuita, y los costos por la transferencia saliente se aplican por niveles. Seg\u00fan las necesidades de la aplicaci\u00f3n, es posible optimizar los costos de las instancias de base de datos de AmazonRDS mediante la adquisici\u00f3n de instancias reservadas. Para comprar instancias reservadas, debe realizar un peque\u00f1o pago \u00fanico por cada instancia que desee reservar. Como resultado, recibir\u00e1 un descuento considerable en el cargo por uso por hora de dicha instancia.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.27 Amazon Aurora \u00b6 Amazon Aurora es una base de datos relacional compatible con MySQLy PostgreSQLque se cre\u00f3 para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. El uso de Amazon Aurora puede reducir los costos de la base de datos y, al mismo tiempo, mejorar la fiabilidad y la disponibilidad de la base de datos. Como servicio completamente administrado, Aurora est\u00e1 dise\u00f1ado para automatizar tareas cuya realizaci\u00f3n requiere mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, la realizaci\u00f3n de copias de seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y la reparaci\u00f3n.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.60 En resumen, Amazon Aurora es una base de datos relacional administrada de manera rentable, con alta disponibilidad y buen rendimiento.Aurora ofrece un subsistema de almacenamiento distribuido de alto rendimiento. El uso de Amazon Aurora puede reducir los costos de la base de datos y, al mismo tiempo, mejorar su fiabilidad. Aurora tambi\u00e9n est\u00e1 dise\u00f1ado para ofrecer alta disponibilidad. Posee almacenamiento tolerante a errores y con recuperaci\u00f3n autom\u00e1tica creado para la nube. Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en AmazonS3. Hay varios niveles de seguridad disponibles, incluidos el aislamiento de la red con Amazon VPC, el cifrado en reposo por medio de claves creadas y controladas con AWS Key Management Service(KMS) y el cifrado de los datos en tr\u00e1nsito con la capa de conexi\u00f3n segura (SSL).El motor de base de datos Amazon Aurora es compatible con las bases de datos de c\u00f3digo abierto MySQLy PostgreSQLexistentes, e incorpora compatibilidad con las nuevas versiones de manera frecuente.Por \u00faltimo, Amazon Aurora est\u00e1 completamente administrado por Amazon RDS. Aurora automatiza las tareas de administraci\u00f3n de bases de datos, como el aprovisionamiento de hardware, la implementaci\u00f3n de parches en el software, la instalaci\u00f3n, la configuraci\u00f3n o la realizaci\u00f3n de copias de seguridad. Datos NoSQL - DynamoDB \u00b6 DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio de base de datos NoSQL r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de milisegundos de un solo d\u00edgito a cualquier escala. Amazon administra toda la infraestructura subyacente de datos para este servicio y almacena los datos de manera redundante en varias instalaciones dentro de una regi\u00f3n nativa de EE.UU., como parte de la arquitectura tolerante a errores. Con DynamoDB, puede crear tablas y elementos. Puede agregar elementos a una tabla. El sistema particionasus datos autom\u00e1ticamente y cuenta con el almacenamiento de tablas necesario para cumplir con los requisitos de carga de trabajo. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Uno de los beneficios de las bases de datos NoSQLes que los elementos de la misma tabla pueden tener atributos diferentes. Esto le da flexibilidad para agregar atributos a medida que la aplicaci\u00f3n evoluciona. Puede almacenar elementos con formatos m\u00e1s nuevos junto a otros con formatos m\u00e1s antiguos en la misma tabla, sin tener que realizar migraciones de esquema.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.39 A medida que su aplicaci\u00f3n se vuelve m\u00e1s popular y los usuarios contin\u00faan interactuando con ella, el almacenamiento puede crecer seg\u00fan las necesidades de la aplicaci\u00f3n. Todos los datos de DynamoDBse almacenan en unidades de estado s\u00f3lido (SSD), y su lenguaje de consulta simple permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDBle permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para su tabla. A medida que aumenta la cantidad de usuarios de la aplicaci\u00f3n, las tablas de DynamoDBse pueden escalar para admitir el incremento de las solicitudes de lectura y escritura mediante el aprovisionamiento manual. De forma alternativa, puede habilitar el escalado autom\u00e1tico para que DynamoDBmonitoree la carga de la tabla e incremente o disminuya el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave incluyen las tablas globales que le permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.40 DynamoDBes un servicio de base de datos NoSQLcompletamente administrado. Amazon administra toda la infraestructura de datos subyacente para este servicio y almacena los datos de manera redundante en varias instalaciones dentro de una regi\u00f3n nativa de EE.UU. como parte de la arquitectura tolerante a errores. Con DynamoDB, puede crear tablas y elementos. Puede agregar elementos a una tabla. El sistema particionasus datos autom\u00e1ticamente y cuenta con el almacenamiento de tablas necesario para cumplir con los requisitos de carga de trabajo. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Uno de los beneficios de una base de datos NoSQLes que los elementos en la misma tabla pueden tener diferentes atributos. Esto le da flexibilidad para agregar atributos a medida que la aplicaci\u00f3n evoluciona. Puede almacenar elementos con formatos m\u00e1s nuevos junto a otros con formatos m\u00e1s antiguos en la misma tabla sin tener que realizar migraciones de esquema.A medida que su aplicaci\u00f3n se vuelve m\u00e1s popular y los usuarios contin\u00faan interactuando con ella, el almacenamiento puede crecer seg\u00fan las necesidades de la aplicaci\u00f3n. Todos los datos en DynamoDBse almacenan en discos de estado s\u00f3lido, y el lenguaje de consulta simple permite un rendimiento uniforme de las Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.41 consultas de baja latencia. Adem\u00e1s de proporcionar escalado del almacenamiento, DynamoDBle permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para su tabla. A medida que aumenta la cantidad de usuarios de la aplicaci\u00f3n, las tablas de DynamoDBse pueden escalar para admitir el incremento de solicitudes de escritura y lectura mediante el aprovisionamiento manual. De forma alternativa, puede habilitar el escalado autom\u00e1tico para que DynamoDBmonitoree la carga de la tabla e incremente o disminuya el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave de diferenciaci\u00f3n incluyen las tablas globales que le permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.42 Los componentes principales de DynamoDBson los elementos, las tablas y los atributos.\u2022Una tabla es un conjunto de datos.\u2022Los elementos son un grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos.\u2022Un atributo es un elemento de datos fundamental que no es preciso seguir dividiendo.DynamoDBadmite dos tipos distintos de claves principales: La clave de partici\u00f3nes una clave principal simple que consta de un atributo denominado clave de ordenamiento.La clave de partici\u00f3n y de ordenamiento, tambi\u00e9n conocidas como clave principal compuesta,est\u00e1 conformada por dos atributos.Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo funciona DynamoDB, consulte Atributos de los elementos de la tablaFormaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.43 A medida que aumenta el volumen de datos, la clave principal particionae indexa los datos de la tabla. Puede recuperar los datos de una tabla de DynamoDBde dos formas distintas:\u2022Seg\u00fan el primer m\u00e9todo, la operaci\u00f3n de consultas aprovecha la partici\u00f3n para localizar de manera eficaz los elementos por medio de la clave principal. \u2022El segundo m\u00e9todo se lleva a cabo mediante un escaneo, que le permite localizar los elementos de la tabla a partir de las coincidencias con las condiciones en los atributos que no son clave. El segundo m\u00e9todo le da la flexibilidad necesaria para localizar elementos por medio de otros atributos. Sin embargo, esta operaci\u00f3n es menos eficiente debido a que DynamoDBescanea todos los elementos de la tabla a fin de encontrar los que coinciden con sus par\u00e1metros. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.44 Para aprovechar al m\u00e1ximo las operaciones de consulta y DynamoDB, es importante que la clave utilizada identifique de forma exclusiva los elementos de la tabla de DynamoDB. Puede configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme, como el identificador \u00fanico global (GUID)u otros identificadores aleatorios. Por ejemplo, si quisiera modelar una tabla con productos, podr\u00eda utilizar algunos atributos, como el ID de producto. De forma alternativa, puede especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. En este ejemplo, si tuviera una tabla con libros, podr\u00eda utilizar la combinaci\u00f3n de autor y t\u00edtulo para identificar de forma exclusiva los elementos de la tabla. Este m\u00e9todo podr\u00eda ser \u00fatil si espera evaluar libros por autor con frecuencia, ya que entonces podr\u00eda utilizar la consulta Resumen DynamoDBse ejecuta exclusivamente en SSD y admite modelos de almacenamiento de clave-valor y documentos. DynamoDBfunciona bien con las aplicaciones m\u00f3viles, web, de videojuegos, de tecnolog\u00eda publicitaria y de Internet de las cosas (IoT).Se puede acceder a este servicio a trav\u00e9s de la consola, de la CLI de AWS y de las llamadas a la API. La capacidad de escalar sus tablas en t\u00e9rminos de almacenamiento y rendimiento de aprovisionamiento hace que DynamoDBsea una buena opci\u00f3n para los datos estructurados de las aplicaciones web, m\u00f3viles y de IoT. Por ejemplo, puede tener una gran cantidad de clientes que generan datos de manera continua y realizan numerosas solicitudes por segundo. En este caso, el escalado del rendimiento de DynamoDBproporciona un rendimiento uniforme a sus clientes. DynamoDBtambi\u00e9n se utiliza en aplicaciones que se ven afectadas por la latencia. El rendimiento predecible de las consultas, incluso en tablas grandes, lo hace \u00fatil para los casos en los que la latencia variable podr\u00eda causar un impacto significativo en la experiencia del usuario o en los objetivos empresariales, como en el \u00e1mbito de la tecnolog\u00eda publicitaria o los videojuegos Con la caracter\u00edstica de tablas globales de DynamoDB, se reduce la tarea de replicar los datos entre las regiones y de solucionar los conflictos de actualizaci\u00f3n. Con esta caracter\u00edstica, se replican autom\u00e1ticamente las tablas de DynamoDBen las regiones de AWS que elija. Las tablas globales pueden ayudar a que las aplicaciones mantengan la disponibilidad y el rendimiento necesarios para la continuidad del negocio DataWarehouse - Redshift \u00b6 Amazon Redshift es un servicio de datawarehouses r\u00e1pido y completamente administrado. A medida que el negocio crece, puede escalar de manera sencilla y sin tiempo de inactividad, ya que solo necesita agregar m\u00e1s nodos. Amazon Redshiftagrega de manera autom\u00e1tica los nodos a su cl\u00faster y redistribuye los datos para alcanzar el m\u00e1ximo rendimiento.Este servicio est\u00e1 dise\u00f1ado para ofrecer alto rendimiento de manera uniforme. Utiliza almacenamiento en columnas y una arquitectura de procesamiento en paralelo masivo. Estas caracter\u00edsticas ubican en paralelo y distribuyen los datos y las consultas entre varios nodos. Amazon Redshifttambi\u00e9n monitorea su cl\u00faster y realiza copias de seguridad de los datos de forma autom\u00e1tica para que pueda restaurarlos con facilidad si es necesario. El cifrado est\u00e1 integrado; solo tiene que habilitarlo.Para obtener m\u00e1s informaci\u00f3n acerca de Amazon Redshift, consultehttps://aws.amazon.com/redshift/. Casos de Uso \u00b6 Como se ha visto en este m\u00f3dulo,la nube contin\u00faa reduciendo el costo de almacenamiento y de c\u00f3mputo. Ha surgido una nueva generaci\u00f3n de aplicaciones,la cual, a su vez, ha creado un nuevo conjunto de requisitos para las bases de datos. Estasaplicacionesnecesitan bases de datos capaces de almacenar desde terabytes hasta petabytesde nuevos tiposde datos, proporcionar acceso a los datos con una latencia de milisegundos, procesar millones de solicitudes porsegundo y escalar para admitir millones de usuariosen cualquier partedel mundo. Para cumplir estos requisitos, se necesitan bases de datos relacionales y no relacionales dise\u00f1adas especialmente para gestionar las necesidades espec\u00edficas de las aplicaciones. AWS ofrece una amplia variedad de bases de datos personalizadas para sus casos de uso de aplicacionesespec\u00edficos. Actividades \u00b6 Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Crear una bd en RDS y cargar datos Crear una tabla en DynamoDB y cargar datos Referencias \u00b6","title":"5.- Datos en AWS"},{"location":"apuntes/nube05.html#datos-en-la-nube","text":"Ya hemos visto que el almacenamiento en la nube ofrece un gran n\u00famero de ventajas. Otro de los productos estrella de la computaci\u00f3n en la nube es el uso de bases de datos, ya sean distribuidas o no. La principal ventaja de utilizar un servicio de base de datos basado en la nube es que no requieren de la administraci\u00f3n por parte del usuario. \u00c9ste s\u00f3lo utiliza el servicio sin necesidad de tener conocimientos avanzados sobre su administraci\u00f3n. Estos servicios se conocen como administrados , ya que la propia AWS se encarga de gestionar el escalado, la tolerancia a errores y la disponibilidad, y por tanto, estos servicios forman parte de una soluci\u00f3n PaaS. Si nosotros cre\u00e1semos una instancia EC2 e instal\u00e1semos cualquier sistema gestor de base de datos, como MariaDB o PostreSQL, ser\u00edamos responsables de varias tareas administrativas, como el mantenimiento del servidor y la huella energ\u00e9tica, el software, la instalaci\u00f3n, la implementaci\u00f3n de parches y las copias de seguridad de la base de datos, as\u00ed como de garantizar su alta disponibilidad, de planificar la escalabilidad y la seguridad de los datos, y de instalar el sistema operativo e instalarle los respectivos parches.","title":"Datos en la nube"},{"location":"apuntes/nube05.html#datos-relacionales-amazon-rds","text":"AWS ofrece Amazon RDS ( https://aws.amazon.com/es/rds/ ) como servicio administrado que configura y opera una base de datos relacional en la nube relacional, de manera que como desarrolladores s\u00f3lo hemos de enfocar nuestros esfuerzos en los datos y optimizar nuestras aplicaciones.","title":"Datos relacionales - Amazon RDS"},{"location":"apuntes/nube05.html#instancias-de-bases-de-datos","text":"El bloque de creaci\u00f3n b\u00e1sico de Amazon RDS es la instancia de base de datos. Una instancia de base de datos es un entorno de base de datos aislado que puede contener varias bases de datos creadas por el usuario. Se puede acceder a \u00e9l utilizando las mismas herramientas y aplicaciones que utiliza con una instancia de base de datos independiente. Los recursos que se encuentran en una instancia de base de datos se definen en funci\u00f3n de la clase de instancia de base de datos, y el tipo de almacenamiento se determina por el tipo de disco. Las instancias y el almacenamiento de base de datos difieren en cuanto a las caracter\u00edsticas de rendimiento y al precio, lo que le permite adaptar el costo y el rendimiento a las necesidades de su base de datos. Cuando elige crear una instancia de base de datos, primero tiene que especificar qu\u00e9 motor de base de datos ejecutar. Actualmente, AmazonRDS admite seis bases de datos: MySQL, Amazon Aurora, MicrosoftSQL Server, PostgreSQL, MariaDBy Oracle. Puede ejecutar una instancia utilizando Amazon Virtual PrivateCloud (AmazonVPC). Cuando utiliza una nube virtual privada (VPC), tiene control sobre su entorno de red virtual. Puede seleccionar su propio intervalo de direccionesIP, crear subredes y configurar las listas de control de acceso (ACL) y el direccionamiento. La funcionalidad b\u00e1sica de AmazonRDS es la misma ya sea que se ejecute o no en una VPC. En general, la instancia de base de datos se encuentra aislada en una subred privada, y solo pueden acceder directamente a ella las instancias de aplicaci\u00f3n determinadas. Las subredes de una VPC est\u00e1n asociadas a una \u00fanica zona de disponibilidad, por lo que, cuando selecciona la subred, tambi\u00e9n elige la zona de disponibilidad (o la ubicaci\u00f3n f\u00edsica) de su instancia de base de datos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.18","title":"Instancias de bases de datos"},{"location":"apuntes/nube05.html#alta-disponibilidad","text":"Una de las caracter\u00edsticas m\u00e1s importantes de AmazonRDS es la capacidad de configurar su instancia de base de datos para una alta disponibilidad con una implementaci\u00f3n Multi-AZ. Una vez configurada la implementaci\u00f3n Multi-AZ, AmazonRDS genera de manera autom\u00e1tica una copia en espera de la instancia de base de datos en otra zona de disponibilidad dentro de la misma VPC. Despu\u00e9s de propagar la copia de la base de datos, las transacciones se replican de forma sincr\u00f3nica a la copia en espera. La ejecuci\u00f3n de una instancia de base de datos en una implementaci\u00f3n Multi-AZ puede mejorar la disponibilidad durante el mantenimiento programado del sistema y ayudar a proteger sus bases de datos contra los errores de las instancias de base de datos y la interrupci\u00f3n de la zona de disponibilidad. Por lo tanto, si la instancia de base de datos principal falla en una implementaci\u00f3n Multi-AZ, AmazonRDS activa autom\u00e1ticamente la instancia de base de datos en espera como la nueva instancia principal. La replicaci\u00f3n sincr\u00f3nica minimiza la posibilidad de que haya p\u00e9rdida de datos. Dado que sus aplicaciones hacen referencia a la base de datos por su nombre mediante el punto de enlace del sistema de nombres de dominio (DNS) de AmazonRDS, no tiene que cambiar nada en el c\u00f3digo de las aplicaciones a fin de utilizar la copia en espera para la conmutaci\u00f3n por error. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.20 Replica de lectura AmazonRDS tambi\u00e9n admite la creaci\u00f3n de r\u00e9plicas de lectura para MySQL, MariaDB, PostgreSQLy Amazon Aurora. Las actualizaciones que se realizan a la instancia de base de datos de origen se copian de manera as\u00edncrona en la instancia de r\u00e9plica de lectura. Puede reducir la carga sobre la instancia de base de datos de origen por medio del direccionamiento de las consultas de lectura desde sus aplicaciones a la r\u00e9plica de lectura. Si se utilizan r\u00e9plicas de lectura, tambi\u00e9n puede realizar un escalado horizontal m\u00e1s all\u00e1 de los l\u00edmites de capacidad de una \u00fanica instancia de base de datos para cargas de trabajo de base de datos con operaciones intensivas de lectura. Las r\u00e9plicas de lectura tambi\u00e9n pueden convertirse en la instancia de base de datos principal, pero, debido a la replicaci\u00f3n as\u00edncrona, esto debe hacerse de forma manual. Las r\u00e9plicas de lectura pueden crearse en una regi\u00f3n diferente a la utilizada por la base de datos principal. Esta caracter\u00edstica puede ayudar a cumplir los requisitos de recuperaci\u00f3n de desastres o a disminuir la latencia al dirigir las lecturas a una r\u00e9plica de lectura que est\u00e9 m\u00e1s cerca del usuario","title":"Alta disponibilidad"},{"location":"apuntes/nube05.html#casos-de-uso","text":"AmazonRDS es ideal para las aplicaciones web y m\u00f3viles que necesitan una base de datos con alto rendimiento, enorme escalabilidad en el almacenamiento y alta disponibilidad. Como no existen restricciones de licencia en AmazonRDS, el servicio se ajusta a los patrones de uso variable de estas aplicaciones. AmazonRDS ofrece a las empresas de comercio electr\u00f3nico grandes y peque\u00f1as una soluci\u00f3n de base de datos flexible, segura y de bajo costo para la venta minorista y el comercio en l\u00ednea. Los juegos m\u00f3viles y en l\u00ednea requieren una plataforma de base de datos con alto rendimiento y disponibilidad. AmazonRDS administra la infraestructura de la base de datos, de manera que los desarrolladores de videojuegos no tengan que preocuparse por aprovisionar, escalar o monitorear los servidores de base de datos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.22 Utilice AmazonRDS cuando su aplicaci\u00f3n necesite lo siguiente: \u2022Transacciones o consultas complejas\u2022Tasa de consulta o escritura media a alta: hasta 30000IOPS (15000lecturas+ 15000escrituras)\u2022No m\u00e1s de una \u00fanica partici\u00f3n o nodo de trabajo\u2022Alta durabilidadNo utilice AmazonRDS cuando su aplicaci\u00f3n necesite lo siguiente:\u2022Tasas de lectura o escritura muy grandes (por ejemplo, 150000 escrituras por segundo)\u2022Fragmentaci\u00f3n causada por el gran tama\u00f1o de los datos o las altas demandas de rendimiento \u2022Solicitudes y consultas GET o PUT simples que una base de datos NoSQLpueda manejar\u2022O personalizaci\u00f3n del sistema de administraci\u00f3n de bases de datos relacionales (RDBMS)En los casos en que no deba utilizar Amazon RDS, considere la posibilidad de utilizar una soluci\u00f3n de base de datos NoSQL, como DynamoDB, o ejecutar su motor de base de datos relacional en instancias de AmazonEC2 en lugar de AmazonRDS, lo cual le proporcionar\u00e1 m\u00e1s opciones para personalizar la base de datos.","title":"Casos de uso"},{"location":"apuntes/nube05.html#costes","text":"Cuando comience a calcular el costo de AmazonRDS, debe tener en cuenta las horas de reloj del servicio, que son recursos que generan cargos cuando se ejecutan (por ejemplo, desde el momento en que lanza una instancia de base de datos hasta que la termina). Tambi\u00e9n hay que tener en cuenta las caracter\u00edsticas de la base de datos. La capacidad f\u00edsica de la base de datos que elija tendr\u00e1 un efecto sobre cu\u00e1nto se le cobre. Las caracter\u00edsticas de la base de datos var\u00edan seg\u00fan el motor, el tama\u00f1o y la clase de memoria de la base de datos Tenga en cuenta el tipo de compra de base de datos. Cuando utiliza instancias bajo demanda, paga la capacidad de c\u00f3mputo por cada hora en que se ejecuta su instancia de base de datos, sin necesidad de compromisos m\u00ednimos. Con las instancias reservadas, puede realizar un peque\u00f1o pago inicial \u00fanico por cada instancia de base de datos que desee reservar por un periodo de1 o 3a\u00f1os. Tambi\u00e9n debe tener en cuenta la cantidad de instancias de base de datos. Con AmazonRDS, puede aprovisionar varias instancias de base de datos para gestionar cargas m\u00e1ximas. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.25 Considere el almacenamiento aprovisionado. No hay cargos adicionales por el almacenamiento para copias de seguridad de hasta el 100% de su almacenamiento de base de datos aprovisionado para una instancia de base de datos activa. Una vez que se termina la instancia de base de datos, el almacenamiento para copias de seguridad se factura por GB por mes. Tambi\u00e9n tenga en cuenta la cantidad de almacenamiento para copias de seguridad que se suma a la cantidad de almacenamiento aprovisionado, la cual se factura por GB por mes. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.26 Adem\u00e1s, considere la cantidad de solicitudes de entrada y de salida que se realizaron a la base de datos.Considere el tipo de implementaci\u00f3n. Puede implementar su instancia de base de datos en una \u00fanica zona de disponibilidad (lo que equivale a un centro de datos independiente) o en varias zonas de disponibilidad (lo que equivale a un centro de datos secundario para disponer de mayor disponibilidad y durabilidad). Los cargos por almacenamiento y operaciones de E/S var\u00edan seg\u00fan la cantidad de zonas de disponibilidad en las que se realiza la implementaci\u00f3n. Por \u00faltimo, tenga en cuenta la transferencia de datos. La transferencia entrante de datos es gratuita, y los costos por la transferencia saliente se aplican por niveles. Seg\u00fan las necesidades de la aplicaci\u00f3n, es posible optimizar los costos de las instancias de base de datos de AmazonRDS mediante la adquisici\u00f3n de instancias reservadas. Para comprar instancias reservadas, debe realizar un peque\u00f1o pago \u00fanico por cada instancia que desee reservar. Como resultado, recibir\u00e1 un descuento considerable en el cargo por uso por hora de dicha instancia.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.27","title":"Costes"},{"location":"apuntes/nube05.html#amazon-aurora","text":"Amazon Aurora es una base de datos relacional compatible con MySQLy PostgreSQLque se cre\u00f3 para la nube. Combina el rendimiento y la disponibilidad de las bases de datos comerciales de alta gama con la simplicidad y la rentabilidad de las bases de datos de c\u00f3digo abierto. El uso de Amazon Aurora puede reducir los costos de la base de datos y, al mismo tiempo, mejorar la fiabilidad y la disponibilidad de la base de datos. Como servicio completamente administrado, Aurora est\u00e1 dise\u00f1ado para automatizar tareas cuya realizaci\u00f3n requiere mucho tiempo, como el aprovisionamiento, la implementaci\u00f3n de parches, la realizaci\u00f3n de copias de seguridad, la recuperaci\u00f3n, la detecci\u00f3n de errores y la reparaci\u00f3n.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.60 En resumen, Amazon Aurora es una base de datos relacional administrada de manera rentable, con alta disponibilidad y buen rendimiento.Aurora ofrece un subsistema de almacenamiento distribuido de alto rendimiento. El uso de Amazon Aurora puede reducir los costos de la base de datos y, al mismo tiempo, mejorar su fiabilidad. Aurora tambi\u00e9n est\u00e1 dise\u00f1ado para ofrecer alta disponibilidad. Posee almacenamiento tolerante a errores y con recuperaci\u00f3n autom\u00e1tica creado para la nube. Aurora replica varias copias de los datos en m\u00faltiples zonas de disponibilidad y realiza copias de seguridad continuas de los datos en AmazonS3. Hay varios niveles de seguridad disponibles, incluidos el aislamiento de la red con Amazon VPC, el cifrado en reposo por medio de claves creadas y controladas con AWS Key Management Service(KMS) y el cifrado de los datos en tr\u00e1nsito con la capa de conexi\u00f3n segura (SSL).El motor de base de datos Amazon Aurora es compatible con las bases de datos de c\u00f3digo abierto MySQLy PostgreSQLexistentes, e incorpora compatibilidad con las nuevas versiones de manera frecuente.Por \u00faltimo, Amazon Aurora est\u00e1 completamente administrado por Amazon RDS. Aurora automatiza las tareas de administraci\u00f3n de bases de datos, como el aprovisionamiento de hardware, la implementaci\u00f3n de parches en el software, la instalaci\u00f3n, la configuraci\u00f3n o la realizaci\u00f3n de copias de seguridad.","title":"Amazon Aurora"},{"location":"apuntes/nube05.html#datos-nosql-dynamodb","text":"DynamoDB ( https://aws.amazon.com/es/dynamodb/ ) es un servicio de base de datos NoSQL r\u00e1pido y flexible para todas las aplicaciones que requieren una latencia uniforme de milisegundos de un solo d\u00edgito a cualquier escala. Amazon administra toda la infraestructura subyacente de datos para este servicio y almacena los datos de manera redundante en varias instalaciones dentro de una regi\u00f3n nativa de EE.UU., como parte de la arquitectura tolerante a errores. Con DynamoDB, puede crear tablas y elementos. Puede agregar elementos a una tabla. El sistema particionasus datos autom\u00e1ticamente y cuenta con el almacenamiento de tablas necesario para cumplir con los requisitos de carga de trabajo. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Uno de los beneficios de las bases de datos NoSQLes que los elementos de la misma tabla pueden tener atributos diferentes. Esto le da flexibilidad para agregar atributos a medida que la aplicaci\u00f3n evoluciona. Puede almacenar elementos con formatos m\u00e1s nuevos junto a otros con formatos m\u00e1s antiguos en la misma tabla, sin tener que realizar migraciones de esquema.Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.39 A medida que su aplicaci\u00f3n se vuelve m\u00e1s popular y los usuarios contin\u00faan interactuando con ella, el almacenamiento puede crecer seg\u00fan las necesidades de la aplicaci\u00f3n. Todos los datos de DynamoDBse almacenan en unidades de estado s\u00f3lido (SSD), y su lenguaje de consulta simple permite un rendimiento de las consultas uniforme y de baja latencia. Adem\u00e1s de escalar el almacenamiento, DynamoDBle permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para su tabla. A medida que aumenta la cantidad de usuarios de la aplicaci\u00f3n, las tablas de DynamoDBse pueden escalar para admitir el incremento de las solicitudes de lectura y escritura mediante el aprovisionamiento manual. De forma alternativa, puede habilitar el escalado autom\u00e1tico para que DynamoDBmonitoree la carga de la tabla e incremente o disminuya el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave incluyen las tablas globales que le permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.40 DynamoDBes un servicio de base de datos NoSQLcompletamente administrado. Amazon administra toda la infraestructura de datos subyacente para este servicio y almacena los datos de manera redundante en varias instalaciones dentro de una regi\u00f3n nativa de EE.UU. como parte de la arquitectura tolerante a errores. Con DynamoDB, puede crear tablas y elementos. Puede agregar elementos a una tabla. El sistema particionasus datos autom\u00e1ticamente y cuenta con el almacenamiento de tablas necesario para cumplir con los requisitos de carga de trabajo. No existe ning\u00fan l\u00edmite pr\u00e1ctico respecto de la cantidad de elementos que se pueden almacenar en una tabla. Por ejemplo, algunos clientes tienen tablas de producci\u00f3n con miles de millones de elementos. Uno de los beneficios de una base de datos NoSQLes que los elementos en la misma tabla pueden tener diferentes atributos. Esto le da flexibilidad para agregar atributos a medida que la aplicaci\u00f3n evoluciona. Puede almacenar elementos con formatos m\u00e1s nuevos junto a otros con formatos m\u00e1s antiguos en la misma tabla sin tener que realizar migraciones de esquema.A medida que su aplicaci\u00f3n se vuelve m\u00e1s popular y los usuarios contin\u00faan interactuando con ella, el almacenamiento puede crecer seg\u00fan las necesidades de la aplicaci\u00f3n. Todos los datos en DynamoDBse almacenan en discos de estado s\u00f3lido, y el lenguaje de consulta simple permite un rendimiento uniforme de las Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.41 consultas de baja latencia. Adem\u00e1s de proporcionar escalado del almacenamiento, DynamoDBle permite aprovisionar el volumen del rendimiento de lectura o escritura que necesita para su tabla. A medida que aumenta la cantidad de usuarios de la aplicaci\u00f3n, las tablas de DynamoDBse pueden escalar para admitir el incremento de solicitudes de escritura y lectura mediante el aprovisionamiento manual. De forma alternativa, puede habilitar el escalado autom\u00e1tico para que DynamoDBmonitoree la carga de la tabla e incremente o disminuya el rendimiento aprovisionado de manera autom\u00e1tica. Algunas otras caracter\u00edsticas clave de diferenciaci\u00f3n incluyen las tablas globales que le permiten generar r\u00e9plicas de manera autom\u00e1tica en las regiones de AWS que elija, el cifrado en reposo y la visibilidad del tiempo de vida (TTL) de los elementos. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.42 Los componentes principales de DynamoDBson los elementos, las tablas y los atributos.\u2022Una tabla es un conjunto de datos.\u2022Los elementos son un grupo de atributos que se puede identificar de forma exclusiva entre todos los dem\u00e1s elementos.\u2022Un atributo es un elemento de datos fundamental que no es preciso seguir dividiendo.DynamoDBadmite dos tipos distintos de claves principales: La clave de partici\u00f3nes una clave principal simple que consta de un atributo denominado clave de ordenamiento.La clave de partici\u00f3n y de ordenamiento, tambi\u00e9n conocidas como clave principal compuesta,est\u00e1 conformada por dos atributos.Para obtener m\u00e1s informaci\u00f3n sobre c\u00f3mo funciona DynamoDB, consulte Atributos de los elementos de la tablaFormaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.43 A medida que aumenta el volumen de datos, la clave principal particionae indexa los datos de la tabla. Puede recuperar los datos de una tabla de DynamoDBde dos formas distintas:\u2022Seg\u00fan el primer m\u00e9todo, la operaci\u00f3n de consultas aprovecha la partici\u00f3n para localizar de manera eficaz los elementos por medio de la clave principal. \u2022El segundo m\u00e9todo se lleva a cabo mediante un escaneo, que le permite localizar los elementos de la tabla a partir de las coincidencias con las condiciones en los atributos que no son clave. El segundo m\u00e9todo le da la flexibilidad necesaria para localizar elementos por medio de otros atributos. Sin embargo, esta operaci\u00f3n es menos eficiente debido a que DynamoDBescanea todos los elementos de la tabla a fin de encontrar los que coinciden con sus par\u00e1metros. Formaci\u00f3n y certificaci\u00f3n de AWSM\u00f3dulo 8: Bases de datos\u00a9 2020 Amazon Web Services, Inc. o sus filiales Todos los derechos reservados.44 Para aprovechar al m\u00e1ximo las operaciones de consulta y DynamoDB, es importante que la clave utilizada identifique de forma exclusiva los elementos de la tabla de DynamoDB. Puede configurar una clave principal simple basada en un \u00fanico atributo de los valores de los datos con una distribuci\u00f3n uniforme, como el identificador \u00fanico global (GUID)u otros identificadores aleatorios. Por ejemplo, si quisiera modelar una tabla con productos, podr\u00eda utilizar algunos atributos, como el ID de producto. De forma alternativa, puede especificar una clave compuesta, que incluye una clave de partici\u00f3n y una clave secundaria. En este ejemplo, si tuviera una tabla con libros, podr\u00eda utilizar la combinaci\u00f3n de autor y t\u00edtulo para identificar de forma exclusiva los elementos de la tabla. Este m\u00e9todo podr\u00eda ser \u00fatil si espera evaluar libros por autor con frecuencia, ya que entonces podr\u00eda utilizar la consulta Resumen DynamoDBse ejecuta exclusivamente en SSD y admite modelos de almacenamiento de clave-valor y documentos. DynamoDBfunciona bien con las aplicaciones m\u00f3viles, web, de videojuegos, de tecnolog\u00eda publicitaria y de Internet de las cosas (IoT).Se puede acceder a este servicio a trav\u00e9s de la consola, de la CLI de AWS y de las llamadas a la API. La capacidad de escalar sus tablas en t\u00e9rminos de almacenamiento y rendimiento de aprovisionamiento hace que DynamoDBsea una buena opci\u00f3n para los datos estructurados de las aplicaciones web, m\u00f3viles y de IoT. Por ejemplo, puede tener una gran cantidad de clientes que generan datos de manera continua y realizan numerosas solicitudes por segundo. En este caso, el escalado del rendimiento de DynamoDBproporciona un rendimiento uniforme a sus clientes. DynamoDBtambi\u00e9n se utiliza en aplicaciones que se ven afectadas por la latencia. El rendimiento predecible de las consultas, incluso en tablas grandes, lo hace \u00fatil para los casos en los que la latencia variable podr\u00eda causar un impacto significativo en la experiencia del usuario o en los objetivos empresariales, como en el \u00e1mbito de la tecnolog\u00eda publicitaria o los videojuegos Con la caracter\u00edstica de tablas globales de DynamoDB, se reduce la tarea de replicar los datos entre las regiones y de solucionar los conflictos de actualizaci\u00f3n. Con esta caracter\u00edstica, se replican autom\u00e1ticamente las tablas de DynamoDBen las regiones de AWS que elija. Las tablas globales pueden ayudar a que las aplicaciones mantengan la disponibilidad y el rendimiento necesarios para la continuidad del negocio","title":"Datos NoSQL - DynamoDB"},{"location":"apuntes/nube05.html#datawarehouse-redshift","text":"Amazon Redshift es un servicio de datawarehouses r\u00e1pido y completamente administrado. A medida que el negocio crece, puede escalar de manera sencilla y sin tiempo de inactividad, ya que solo necesita agregar m\u00e1s nodos. Amazon Redshiftagrega de manera autom\u00e1tica los nodos a su cl\u00faster y redistribuye los datos para alcanzar el m\u00e1ximo rendimiento.Este servicio est\u00e1 dise\u00f1ado para ofrecer alto rendimiento de manera uniforme. Utiliza almacenamiento en columnas y una arquitectura de procesamiento en paralelo masivo. Estas caracter\u00edsticas ubican en paralelo y distribuyen los datos y las consultas entre varios nodos. Amazon Redshifttambi\u00e9n monitorea su cl\u00faster y realiza copias de seguridad de los datos de forma autom\u00e1tica para que pueda restaurarlos con facilidad si es necesario. El cifrado est\u00e1 integrado; solo tiene que habilitarlo.Para obtener m\u00e1s informaci\u00f3n acerca de Amazon Redshift, consultehttps://aws.amazon.com/redshift/.","title":"DataWarehouse - Redshift"},{"location":"apuntes/nube05.html#casos-de-uso_1","text":"Como se ha visto en este m\u00f3dulo,la nube contin\u00faa reduciendo el costo de almacenamiento y de c\u00f3mputo. Ha surgido una nueva generaci\u00f3n de aplicaciones,la cual, a su vez, ha creado un nuevo conjunto de requisitos para las bases de datos. Estasaplicacionesnecesitan bases de datos capaces de almacenar desde terabytes hasta petabytesde nuevos tiposde datos, proporcionar acceso a los datos con una latencia de milisegundos, procesar millones de solicitudes porsegundo y escalar para admitir millones de usuariosen cualquier partedel mundo. Para cumplir estos requisitos, se necesitan bases de datos relacionales y no relacionales dise\u00f1adas especialmente para gestionar las necesidades espec\u00edficas de las aplicaciones. AWS ofrece una amplia variedad de bases de datos personalizadas para sus casos de uso de aplicacionesespec\u00edficos.","title":"Casos de Uso"},{"location":"apuntes/nube05.html#actividades","text":"Realizar el m\u00f3dulo 8 (Bases de Datos) del curso ACF de AWS . Crear una bd en RDS y cargar datos Crear una tabla en DynamoDB y cargar datos","title":"Actividades"},{"location":"apuntes/nube05.html#referencias","text":"","title":"Referencias"}]}